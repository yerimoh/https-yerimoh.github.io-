---
title: "[+] Deep learning 1: 오버피팅과 해결법"
date:   2020-03-8
excerpt: ""
category: [Deep Learning]
layout: post
tag:
- Deep Learning
order: 0

comments: true
---


# 목차

  * [오버피팅](#오버피팅)
  * [가중치 감소(weight decay)](#가중치-감소-weight-decay-)
  * [드롭아웃](#드롭아웃)
- [적절한 하이퍼파라미터 값 찾기](#적절한-하이퍼파라미터 값 찾기)
  * [하이퍼파라미터 최적화](#하이퍼파라미터-최적화)
- [정리](#정리)




---



👀, 🤷‍♀️ , 📜    
이 아이콘들을 누르시면 코드, 개념 부가 설명을 보실 수 있습니다:)

----
----

# 오버피팅
오버피팅이 일어나는 경우    
- 매개변수가 많고 표현력이 높은 모델    
- 훈련 데이터가 적음     
![image](https://user-images.githubusercontent.com/76824611/128638516-3382b030-c7e7-41e1-a0b0-f7747066537b.png) 
더 단순한 모델이 새 데이터를 더 잘 일반화함     
즉 왼쪽과 같은 경우가 오버피팅     


----

## 가중치 감소(weight decay)
오버피팅 억제기제        
학습 과정에서 **큰 가중치**에 대해 **큰 페널티**를 부과하여 **오버피팅 억제**     
(오버피팅은 가중치 매개변수의 값이 커서 발생하는 경우가 많기 때문)         

신경망 학습의 목적은 손실 함수의 값↓     
가중치의 제곱 노름 norm (L2 노름)(||W||2)( $$1/2 λw^2$$)을 손실 함수에 (+)      
➡ 손실함수 ↑       
➡ 가중치(W)가 커지는 것을 억제 가능(손실함수 줄이려고)         
  
**L2노름에 따른 가중치 감소= $$1/2 λw^2$$**     
(λ 람다 는 정규화의 세기를 조절하는 하이퍼파라미터)         
* λ를 크게 설정할수록 큰 가중치에 대한 페널티가 커짐     
* 1/2 미분 결과인 $$λw^2$$를 조절하는 상수      

* L2 노름은 각 원소의 제곱들을 더한 것       
가중치 W = (w 1 , w 2 , … ,w n ) ->√(W_I^2+⋯+W_n^2 )


---


## 드롭아웃
존재 이유: 신경망 모델이 복잡해지면 가중치 감소만으로는 대응하기 어려워짐    
**[의미]**     
* 뉴런을 임의로 삭제하면서 학습하는 방법.             
* 훈련 때 은닉층의 뉴런을 무작위로 골라 삭제.      
➡  삭제된 뉴런은 신호 전달 X       

**훈련 시**: 데이터를 흘릴 때마다 삭제할 뉴런을 무작위로 선택     
**시험 시**: 모든 뉴런에 신호를 전달       
➡  단, 시험 때는 **각 뉴런의 출력에 훈련 때 삭제 안 한 비율을 곱하여 출력**      
![image](https://user-images.githubusercontent.com/76824611/128639006-e40191a8-27ee-4cf7-bc23-88e22f9c1e95.png)
 

<details>
<summary>👀코드 보기</summary>
<div markdown="1">
  
```python
class Dropout:
# self.mask는 x와 형상이 같은 배열을 무작위로 생성하고, 그 값이 
# dropout_ ratio보다 큰 원소만 True로 설정
def _ _init _ _(self, dropout _ratio = 0.5):
self.dropout _ratio = dropout _ratio 
self.mask = Non

def forward(self, x, train _flg = True): 
if train _flg:
self.mask = np.random.rand(*x.shape) > 
self.dropout _ratio 
return x * self.mask 
else:
return x * (1.0 - self.dropout _ratio)

# 역전파 때의 동작은 ReLU와 같음
# 즉, 순전파때 신호를 통과시키는 뉴런은 역전파 때도 신호를 그대로 통과시키고
# 순전파 때 통과시키지 않은 뉴런은 역전파 때도 신호를 차단
def backward(self, dout):
return dout * self.mask
```
  
</div>
</details>
  
**[forward(순전파)]**    
메서드에서는 훈련 때(```train_flg = True일 때```)만 잘 계산해두면 시험 때. 삭제 안 한 비율은 곱하지 않아도 OK      
실제 딥러닝 프레임워크들도 비율을 곱하지 않습니다.    
* 핵심은 훈련 시에는 순전파 때마다 **self.mask에 삭제할 뉴런을 False**로 표시       
 
![image](https://user-images.githubusercontent.com/76824611/128675347-7f54af52-f146-41ed-a391-225842b5281a.png)
* 훈련 데이터와 시험 데이터에 대한 **정확도 차이↓**      
   * 훈련 데이터에 대한 정확도가 100%에 도달X       
* **표현력↑** 오버피팅을 억제가능



----

## 앙상블 학습(ensemble learning)
**개별**적으로 학습시킨 여러 모델의 출력을 **평균** 내어 추론하는 방식        
* 신경망의 정확도 개선     
* 드롭아웃과 밀접
   * **학습 시**: 드롭아웃이 뉴런을 무작위로 삭제하는 행위를 매번 다른 모델을 학습시키는 것으로 해석         
   * **추론 시**: 뉴런의 출력에 삭제한 비율(이를테면 0.5 등)을 곱함으로써 앙상블 학습에서 여러 모델
의 평균을 내는 것과 같은 효과       
* 드롭아웃은 앙상블 학습과 같은 효과를 (대략) **하나의 네트워크**로 구현했다고 생각 가능    



  



----

# 정리
● 매개변수 갱신 방법에는 확률적 경사 하강법(SGD ) 외에도 모멘텀, AdaGrad, Adam 등이 있다.     
● 가중치 초깃값을 정하는 방법은 올바른 학습을 하는 데 매우 중요.     
● 가중치의 초깃값으로는 ‘Xavier 초깃값’과 ‘He 초깃값’이 효과적       
● 배치 정규화를 이용하면 학습을 빠르게 진행할 수 있으며, 초깃값에 영향을 덜 받게 된다.       
● 오버피팅을 억제하는 정규화 기술로는 가중치 감소와 드롭아웃이 있다.        
● 하이퍼파라미터 값 탐색은 최적 값이 존재할 법한 범위를 점차 좁히면서 하는 것이 효과적이다.       
