---
title: "[02] Deep learning 1: 활성화 함수 "
date:   2020-03-11
excerpt: "신경망, 활성화 함수, 시그모이드 함수, 비선형 함수, 활성화 함수, 다차원 배열의 계산, 출력층 설계, 소프트 맥스 함수"
category: [Deep Learning]
layout: post
tag:
- Deep Learning
order: 0

comments: true
---

# 활성화 함수 
- [활성화 함수(activation function)](#활성화-함수(activation-function))
- [퍼셉트론에서 이용하는 활성화 함수](#퍼셉트론에서-이용하는-활성화-함수)
  * [계단 함수 step function](#계단-함수-step-function)
    + [구현](#구현)
    + [시각화](#시각화)
- [신경망에서 이용하는 활성화 함수](#신경망에서-이용하는-활성화-함수)
  * [sigmoid function(시그모이드 함수)](#sigmoid-function(시그모이드-함수))
  * [구현](#구현)
  * [시각화](#시각화)
- [비선형 함수](#비선형-함수)
  * [활성화 함수(activation function)](#활성화-함수(activation-function))
    + [ReLU 함수](#relu-함수)
      - [구현](#구현)
- [다차원 배열의 계산](#다차원-배열의-계산)
  * [배열의 차원 수 확인](#배열의-차원-수-확인)
  * [행렬의 곱](#행렬의-곱)
  * [신경망에서의 행렬의 곱](#신경망에서의-행렬의-곱)
  * [3층 신경망 구현하기](#33층-신경망-구현하기)
- [출력층 설계](#출력층-설계)
  * [항등함수와 소프트맥스 함수 구현하기](#항등함수와-소프트맥스-함수-구현하기)
  * [소프트맥스 함수 구현 시 주의점](#소프트맥스-함수-구현-시-주의점)
  * [구현개선](#구현개선)
  * [특징](#특징)
  * [출력층의 뉴런 수 정하기](#출력층의-뉴런-수-정하기)
- [한줄정리](#한줄정리)







---

# INTRO

**신경망 성질**: 가중치 매개변수의 적절한 값을 데이터로부터 자동으로 학습하는 능력

![11](https://user-images.githubusercontent.com/76824611/131263423-b12eb941-d885-49f8-8e01-bf43d0977f87.gif)
 
- **‘2층 신경망’**: 가중치를 갖는 층은 2개뿐이기 때문

- 뉴런이 연결되는 **방식**은 앞 장의 퍼셉트론에서 달라진 것X

- **은닉층의 뉴런**: (입력층이나 출력층과 달리) 사람 눈에 보이지X

⛔ 그렇다면 모든 경우에서 입력층에서 출력층까지 다 넘어갈까??    

⭕ 아니다! 
* **1)** 모든 화살표에서 계산이 일어나고    
* **2)** 그 계산을 바탕으로 활성화할지 정한다     

**[퍼셉트론의 계산 식]**     

아래의 그림을 보면 알겠지만 은닉층에서 계산을 한다고 무조건 넘어가는것은 아니다         
이를 쉽게하기 위해 출력층 하나를 줄여보고, ⭕의 식을 정의해보자        
* **1)** $$𝒚=𝒉(𝒃+𝒘,𝒙_𝟏+𝒘_𝟐 𝒙_𝟐 )$$       
* **2)** $$𝒉(𝒙)$$       
  * 이것이 활성화 할지정하는 함수이다(이제 이 함수의 종류를 알아볼 것이다.)    
  * 지금 이 활성화 함수는 임의로 정한 것이다     

[활성화 되는 경우]   
![333](https://user-images.githubusercontent.com/76824611/131264169-4dbd831c-fb29-47de-b5d5-470304cc3287.gif)

[활성화 되지 않는 경우]   
![22](https://user-images.githubusercontent.com/76824611/131264166-419f5b8e-e614-47c4-84ca-9afbf947f654.gif)


즉 이 넘어가는 기준을 정해주는 함수가(**$$𝒉(𝒙)$$**) 활성화 함수이다

-----



# 활성화 함수(activation function)

• 등장
  - 의의: 조금 전 h(x)라는 함수가 등장했는데, 이처럼 입력 신호의 **총합을 출력 신호**로 변환하는 함수
  - 역할: 입력 신호의 **총합이 활성화를 일으키는지를 정함**

  [식K] 다시 구성
  <br>1단계) 가중치가 곱해진 입력 신호의 총합을 계산
  <br>2단계) 그 합을 활성화 함수에 입력해 결과를 냄
  
 
![image](https://user-images.githubusercontent.com/76824611/131264426-9011cfa5-d7cd-487a-a8ce-8423532e7201.png)
  -> 기존 뉴런의 원을 키우고, 그 안에 활성화 함수의 처리 과정을 명시적으로 그려 넣음

  -> a 노드(가중치 신호를 조합한 결과)가 활성화 함수 h ( )를 통과하여 y라는 노드로 변환되는 과정이 분명히 드러남

  -> 퍼셉트론에서 신경망으로 가기 위한 길잡이

   * 단순 퍼셉트론
    : 단층 네트워크에서 계단 함수(임계값을 경계로 출력이 바뀌는 함수)를 활성화 함수로 사용한 모델
 
   * 다층 퍼셉트론
    : 신경망(여러 층으로 구성되고 시그모이드 함수 등의 매끈한 활성화 함수를 사용하는 네트워크)을 가리킵니다.

• 활성화 함수
  : h(x)와 같이 임계값을 경계로 출력이 바뀌는 함수


---


# 퍼셉트론에서 이용하는 활성화 함수
## 계단 함수 step function
: 활성화 함수로 쓸 수 있는 여러 후보 중에서 퍼셉트론은 **계단 함수를 채용**
 <br>: 활성화 함수를 계단 함수에서 다른 함수로 변경하는 것이 신경망의 세계로 나아가는 열쇠

<details>
<summary>👀코드 보기</summary>
<div markdown="1">
  
```python
def step_function(x):
  if x > 0:
    return 1 
  else: 
    return 0
```
  
</div>
</details>

**장점**      
* 단순함, 구현하기 쉬움     

**단점**        
* 인수X는 실수만 받아들임      
* 넘파이 배열을 인수로 받아들일 수 없음    
* ```step_function (np.array ([1.0, 2.0 ] ) )``` 불가    

<details>
<summary>👀코드 보기</summary>
<div markdown="1">
  
```python
def step_function(x):        #-> 넘파이 가능
  y = x > 0                  #-> bool형 출력
  return y.astype(np.int)    #-> bool에서 int형으로
```
파이썬에서 bool을 int로 변환하면 True는 1, False는 0으로
 
</div>
</details>


**[시각화]**
<details>
<summary>👀코드 보기</summary>
<div markdown="1">
  
```python
Import numpy as np
import matplotlib.pylab as plt

def step _function(x):
      return np.array(x > 0, dtype = np.int)

x = np.arange(-5.0, 5.0, 0.1)
y = step _function(x) 
plt.plot(x, y) 
plt.ylim(-0.1, 1.1) # y축의 범위 지정
plt.show()
```
-> ```np.arange (-5.0, 5.0, 0.1 )```: -5.0에서 5.0 전까지 0.1 간격의 넘파이 배열을 생성
 
</div>
</details>



![image](https://user-images.githubusercontent.com/76824611/117391540-170b4080-af2b-11eb-8e32-f96186e3dc9b.png)


---


# 신경망에서 이용하는 활성화 함수
## sigmoid function(시그모이드 함수)


-> $$exp (-x )= e^-x$$  
-> e는 자연상수로  2.7182...의 값을 갖는 실수   
 
: 신경망에서는 활성화 함수로 시그모이드 함수를 이용하여 신호를 변환, 그 변환된 신호를 다음 뉴런에 전달합니다. 
: 퍼셉트론과 신경망의 주된 차이는 이 활성화 함수뿐
  
**[구현]**
```python  
def sigmoid(x):
  return 1 / (1 + np.exp(-x))
```
넘파이 배열이어도 올바른 결과가 나옴

**[시각화]**
<details>
<summary>👀코드 보기</summary>
<div markdown="1">
  
```python
x = np.arange(-5.0, 5.0, 0.1)
y = sigmoid(x) 
plt.plot(x, y) plt.ylim(-0.1, 1.1) # y축 범위 지정
plt.show()
```
  
</div>
</details>

![image](https://user-images.githubusercontent.com/76824611/117391613-3efaa400-af2b-11eb-8cb8-d9628a62367a.png)

---

# 비선형 함수
- 계단 함수와 시그모이드 함수의 공통점
  <br>직선 1개로는 그릴 수 없는 함수
- **신경망**에서는 활성화 함수로 **비선형 함수**만 사용해야함
  -> 선형 함수를 이용하면 신경망의 층을 깊게 하는 의미가 없어지기 때문

+ 선형함수의 문제: 층을 아무리 깊게 해도 ‘은닉층이 없는 네트워크’로도 똑같은 기능을 할 수 있음
                    여러 층으로 구성하는 이점을 살릴 수 없음

## 활성화 함수(activation function)
: 시그모이드 함수는 신경망 분야에서 오래전부터 이용해왔으나, 
<br>**최근**에는 **ReLU** Rectified Linear Unit, 렐루 함수를 주로 이용

### ReLU 함수
  : 입력이 0을 넘으면 그 입력을 그대로 출력하고, 0 이하이면 0을 출력하는 함수
  
![image](https://user-images.githubusercontent.com/76824611/117391675-66ea0780-af2b-11eb-90c2-5e3747208204.png)
![image](https://user-images.githubusercontent.com/76824611/117391740-8ed96b00-af2b-11eb-8ba5-3d45a40d7f20.png)


**[구현]**
<details>
<summary>👀코드 보기</summary>
<div markdown="1">
  
```python  
def relu(x): 
  return np.maximum(0, x)
```
-> 여기에서는 넘파이의 ```maximum``` 함수를 사용했습니다. 
<br>```Maximum```: 두 입력 중 큰 값을 선택해 반환하는 함수
 
</div>
</details>



---


# 다차원 배열의 계산

## 배열의 차원 수 확인
<details>
<summary>👀코드 보기</summary>
<div markdown="1">
  
```python  
#np.ndim ( )         -> 차원수 반환
#or
#함수이름.shape      -> 튜플 반환

>>> B = np.array([[1,2], [3,4], [5,6]]) 
>>> print(B) [[1 2] [3 4] [5 6]] 
>>> np.ndim(B) 
2
>>> B.shape 
(3, 2)
```
  
</div>
</details>

## 행렬의 곱
<details>
<summary>👀코드 보기</summary>
<div markdown="1">
  
```python
#np.dot(A,B)

>>> A = np.array([[1,2], [3,4]]) 
>>> A.shape 
(2, 2) 
>>> B = np.array([[5,6], [7,8]]) 
>>> B.shape 
(2, 2) 
>>> np.dot(A, B) 
array([[19, 22], 
[43, 50]]) 
```
  
</div>
</details>
+ 유의

![image](https://user-images.githubusercontent.com/76824611/117392439-0b207e00-af2d-11eb-8f50-63d69ecb5ab1.png)
![image](https://user-images.githubusercontent.com/76824611/117392441-0e1b6e80-af2d-11eb-984c-482c3df0579c.png)

 
## 신경망에서의 행렬의 곱

![image](https://user-images.githubusercontent.com/76824611/117392452-12478c00-af2d-11eb-9081-8df7232c985b.png)
 
## 3층 신경망 구현하기

![image](https://user-images.githubusercontent.com/76824611/117392533-3b681c80-af2d-11eb-8537-cd893686562d.png)
 
* 표기

![image](https://user-images.githubusercontent.com/76824611/117392594-5a66ae80-af2d-11eb-8d8d-0dd3916ae4a4.png)
 
* 편향

![image](https://user-images.githubusercontent.com/76824611/117392626-6ce0e800-af2d-11eb-9c4c-2699589c5986.png)
 
편향: 오른쪽 아래 인덱스가 하나밖에 없음
<br>회색 노드가 편향
<br>-> 앞 층의 편향 뉴런이 하나뿐이기 때문


![image](https://user-images.githubusercontent.com/76824611/117392758-b7626480-af2d-11eb-9d83-c6d18278392b.png)


-> 행렬의 곱을 이용하여 단순화

![image](https://user-images.githubusercontent.com/76824611/117392772-bb8e8200-af2d-11eb-9f16-c696aea83a8d.png)
 

![image](https://user-images.githubusercontent.com/76824611/117392783-bfba9f80-af2d-11eb-8d97-72e150fc48bf.png)

- 구현

![image](https://user-images.githubusercontent.com/76824611/117392809-cb0dcb00-af2d-11eb-9fb7-f9b7fccfe01c.png)

* 시각화 형태

**1) 활성화 함수를 시그모이드 함수로 사용**

![image](https://user-images.githubusercontent.com/76824611/117392851-dc56d780-af2d-11eb-96f5-c455d7623eda.png)
-> 은닉층에서의 가중치 합(가중 신호와 편향의 총합)을 a로 표기
<br>-> 활성화 함수 h ( )로 변환된 신호를 z로 표기
<br>-> 활성화 함수를 시그모이드 함수로 사용

**[구현]**
<details>
<summary>👀코드 보기</summary>
<div markdown="1">
  
```python
Z1 = sigmoid(A1) 
print(A1) # [0.3, 0.7, 1.1] 
print(Z1) # [0.57444252, 0.66818777, 0.75026011]
```
  
</div>
</details>


**2) 1층에서 2층으로의 신호 전달**

![image](https://user-images.githubusercontent.com/76824611/117392911-f5f81f00-af2d-11eb-93c7-df5e04b78976.png)

**[구현]**
<details>
<summary>👀코드 보기</summary>
<div markdown="1">
  
```python
 W2 = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]]) 
 B2 = np.array([0.1, 0.2])

 print(Z1.shape) # (3,) 
 print(W2.shape) # (3, 2) 
 print(B2.shape) # (2,)

 A2 = np.dot(Z1, W2) + B2
 Z2 = sigmoid(A2)
```
  
</div>
</details>

**3) 2층에서 출력층으로 신호 전달**

![image](https://user-images.githubusercontent.com/76824611/117392999-2213a000-af2e-11eb-9a2f-8f6a17006693.png)

[구현]
<details>
<summary>👀코드 보기</summary>
<div markdown="1">
  
```python    
  def identity _function(x):
    return x

  W3 = np.array([[0.1, 0.3], [0.2, 0.4]]) 
  B3 = np.array([0.1, 0.2])

  A3 = np.dot(Z2, W3) + B3
  Y = identity _function(A3) # 혹은 Y = A3
```
  
</div>
</details>
 
**[구현 정리]**
<details>
<summary>👀코드 보기</summary>
<div markdown="1">
  
```python 
   def init _network():
      network = {} 
      network['W1'] = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]]) 
      network['b1'] = np.array([0.1, 0.2, 0.3]) 
      network['W2'] = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]]) 
      network['b2'] = np.array([0.1, 0.2]) 
      network['W3'] = np.array([[0.1, 0.3], [0.2, 0.4]]) 
      network['b3'] = np.array([0.1, 0.2])
 
      return network

  def forward(network, x):
      W1, W2, W3 = network['W1'], network['W2'], network['W3'] 
      b1, b2, b3 = network['b1'], network['b2'], network['b3']

      a1 = np.dot(x, W1) + b1
      z1 = sigmoid(a1) 
      a2 = np.dot(z1, W2) + b2
      z2 = sigmoid(a2) 
      a3 = np.dot(z2, W3) + b3
      y = identity _function(a3)
      return y

  network = init _network() 
  x = np.array([1.0, 0.5]) 
  y = forward(network, x) 
  print(y) # [ 0.31682708 0.69627909]
```
  
</div>
</details>

* 함수정리
  * init_network ( ) 함수 : 가중치와 편향을 초기화하고 이들을 딕셔너리 변수인 network에 
      * Network-> 각 층에 필요한 매개변수(가중치와 편향)를 저장 
  * forward ( ) 함수: 입력 신호를 출력으로 변환하는 처리 과정을 모두 구현하고 있음


---


# 출력층 설계

- 기계학습의 문제는 분류와 회귀로 나뉨
    - 분류 classification: 데이터가 어느 클래스 class 에 속하느냐는 문제
                           <br>사진 속 인물의 성별을 분류하는 문제
    - 회귀 regression: 입력 데이터에서 (연속적인) 수치를 예측하는 문제
                       <br>사진 속 인물의 몸무게(57.4kg?)를 예측


## 항등함수와 소프트맥스 함수 구현하기
- 항등 함수 identity function: 입력을 그대로 출력
   
![image](https://user-images.githubusercontent.com/76824611/117393095-55562f00-af2e-11eb-8e4b-7ab4c60f9045.png)

   
- 소프트맥스 함수 softmax function: 분류에 사용    
   그 노드가 나올 확률    
![image](https://user-images.githubusercontent.com/76824611/117393112-5edf9700-af2e-11eb-9c82-bc50598f4145.png)


   - 변수 설명
     - n: 출력층의 뉴런 수,
     - $$y_k$$: 그중 k번째 출력임을 뜻함
  
   - 식 설명
     - 내 클래스의 값/각 클래스의 합
     - 즉, 모든값 합하면 1이 나옴
  
  ![image](https://user-images.githubusercontent.com/76824611/117393194-90f0f900-af2e-11eb-86b9-08070fc827ae.png)
   
   -  소프트맥스의 출력은 모든 입력 신호로부터 화살표를 받음
      -> 분모에서 보듯, 출력층의 각 뉴런이 모든 입력 신호에서 영향을 받기 때문(다합 해서 1이 되어야 하니까(확률))     


**[구현]**

<details>
<summary>👀코드 보기</summary>
<div markdown="1">
  
```python
>>> a = np.array([0.3, 2.9, 4.0]) 
>>> 
>>> exp _a = np.exp(a) # 지수 함수
>>> print(exp _a) [ 1.34985881 18.17414537 54.59815003] 
>>> 
>>> sum _exp _a = np.sum(exp _a) # 지수 함수의 합
>>> print(sum _exp _a) 74.1221542102
>>> 
>>> y = exp _a / sum _exp _a 
>>> print(y) [ 0.01821127 0.24519181 0.73659691]
```
  
</div>
</details>

[정의]

<details>
<summary>👀코드 보기</summary>
<div markdown="1">
  
```python
def softmax(a): 
 exp _a = np.exp(a) 
 sum _exp _a = np.sum(exp _a) 
 y = exp _a / sum _exp _a

 return y
```
  
</div>
</details>

## 소프트맥스 함수 구현 시 주의점
- **오버플로**: 지수 함수란 것이 쉽게 아주 큰 값을 내뱉어 이런 큰 값끼리 나눗셈을 하면 결과 수치가 ‘불안정’해짐


## 구현개선

![image](https://user-images.githubusercontent.com/76824611/117393285-c269c480-af2e-11eb-94a3-7eea5531e844.png)

- **C**->어떤 값을 대입해도 상관없긴 하지만 오버플로를 막기 위해선 입력신호 중 최댓값을 넣음

[구현]
<details>
<summary>👀코드 보기</summary>
<div markdown="1">
  
```python
def softmax(a): 
 c = np.max(a)
 exp _a = np.exp(a - c) # 오버플로 대책
 sum _exp _a = np.sum(exp _a) 
 y = exp _a / sum _exp _a
 return y
```
  
</div>
</details>

## 특징
- 출력: 0에서 1.0 사이의 실수/출력의 총합은 1
       <br>-> 확률로 계산 가능
- 주의: 소프트맥스 함수를 적용해도 각 원소의 대소 관계는 변하지X
       <br>지수 함수 y = exp (x )가 단조 증가 함수이기 떄문
       <br>-> 신경망으로 분류할 때는 출력층의 소프트맥스 함수를 생략가능

+ 기계학습의 2단계
  <br>학습: 소프트맥스 함수를 사용
  <br>추론: 소프트맥스 함수를 생략

## 출력층의 뉴런 수 정하기
  : 예를 들어 입력 이미지를 숫자 0부터 9 중 하나로 분류하는 문제라면 [그림 3-23 ]처럼 출력층의 뉴런을 10개로 설정]
 
![image](https://user-images.githubusercontent.com/76824611/117393320-d7deee80-af2e-11eb-9ed5-09a5c7b35822.png) 





----

# 한줄정리

[신경망의 2단계]

1)	훈련 데이터(학습 데이터)를 사용해 가중치 매개변수를 학습(훈련 데이터로부터 가중치 매개변수의 최적값을 자동으로 획득하는 것)

<br>2)	추론 단계: 앞서 학습한 매개변수를 사용하여 입력 데이터를 분류

