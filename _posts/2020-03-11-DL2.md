---
title: "[02] Deep learning 1 (밑바닥 부터 시작하는 딥러닝 1) "
date:   2020-03-11
excerpt: "Deep learning starting from the bottom 1"
category: [Deep Learning]
layout: post
tag:
- Deep Learning
order: 0

comments: true
---

# 목차 [02 신경망]

- [신경망](#신경망)
- [활성화 함수(activation function)](#활성화-함수(activation-function))
- [퍼셉트론에서 이용하는 활성화 함수](#퍼셉트론에서-이용하는-활성화-함수)
  * [계단 함수 step function](#계단-함수-step-function)
    + [구현](#구현)
    + [시각화](#시각화)
- [신경망에서 이용하는 활성화 함수](#신경망에서-이용하는-활성화 함수)
  * [sigmoid function(시그모이드 함수)](#sigmoid-function(시그모이드-함수))
  * [구현](#구현)
  * [시각화](#시각화)
- [비선형 함수](#비선형-함수)
  * [활성화 함수(activation function)](#활성화 함수(activation-function))
    + [ReLU 함수](#relu-함수)
      - [구현](#구현)
- [다차원 배열의 계산](#다차원-배열의-계산)
  * [배열의 차원 수 확인](#배열의-차원-수-확인)
  * [행렬의 곱](#행렬의-곱)
  * [신경망에서의 행렬의 곱](#신경망에서의-행렬의-곱)
  * [3층 신경망 구현하기](#33층-신경망-구현하기)
- [출력층 설계](#출력층-설계)
  * [항등함수와 소프트맥스 함수 구현하기](#항등함수와-소프트맥스-함수-구현하기)
  * [소프트맥스 함수 구현 시 주의점](#소프트맥스-함수-구현-시-주의점)
  * [구현개선](#구현개선)
  * [특징](#특징)
  * [출력층의 뉴런 수 정하기](#출력층의-뉴런-수-정하기)
- [한줄정리](#한줄정리)







---

# 신경망

* 성질: 가중치 매개변수의 적절한 값을 데이터로부터 자동으로 학습하는 능력

![image](https://user-images.githubusercontent.com/76824611/111866405-8dbaa180-89b0-11eb-9d56-6276063484c6.png)

 
- ‘2층 신경망’: 가중치를 갖는 층은 2개뿐이기 때문

- 뉴런이 연결되는 방식은 앞 장의 퍼셉트론에서 달라진 것X

- 은닉층의 뉴런: (입력층이나 출력층과 달리) 사람 눈에 보이지X


 + 퍼셉트론

<img src = "https://user-images.githubusercontent.com/76824611/111866435-b9d62280-89b0-11eb-80e0-5e5be17cae79.png" width="70%">


[식K]

<img src = "https://user-images.githubusercontent.com/76824611/111866437-be024000-89b0-11eb-85c2-53073de73953.png" width="70%">

-----



# 활성화 함수(activation function)

• 등장
  - 의의: 조금 전 h(x)라는 함수가 등장했는데, 이처럼 입력 신호의 **총합을 출력 신호**로 변환하는 함수
  - 역할: 입력 신호의 **총합이 활성화를 일으키는지를 정함**

  [식K] 다시 구성
  <br>1단계) 가중치가 곱해진 입력 신호의 총합을 계산
  <br>2단계) 그 합을 활성화 함수에 입력해 결과를 냄
  
 
  <img src = "https://user-images.githubusercontent.com/76824611/111866544-8ea00300-89b1-11eb-83c5-bfc684b0317d.png" width="70%">
  -> 기존 뉴런의 원을 키우고, 그 안에 활성화 함수의 처리 과정을 명시적으로 그려 넣음

  -> a 노드(가중치 신호를 조합한 결과)가 활성화 함수 h ( )를 통과하여 y라는 노드로 변환되는 과정이 분명히 드러남

  -> 퍼셉트론에서 신경망으로 가기 위한 길잡이

   * 단순 퍼셉트론
    : 단층 네트워크에서 계단 함수(임계값을 경계로 출력이 바뀌는 함수)를 활성화 함수로 사용한 모델
 
   * 다층 퍼셉트론
    : 신경망(여러 층으로 구성되고 시그모이드 함수 등의 매끈한 활성화 함수를 사용하는 네트워크)을 가리킵니다.

• 활성화 함수
  : h(x)와 같이 임계값을 경계로 출력이 바뀌는 함수


---


# 퍼셉트론에서 이용하는 활성화 함수
## 계단 함수 step function
: 활성화 함수로 쓸 수 있는 여러 후보 중에서 퍼셉트론은 **계단 함수를 채용**
 <br>: 활성화 함수를 계단 함수에서 다른 함수로 변경하는 것이 신경망의 세계로 나아가는 열쇠

### 구현
```python

def step_function(x):
  if x > 0:
    return 1 
  else: 
    return 0
    
```
* 장점: 단순함, 구현하기 쉬움
* 단점: 인수X는 실수만 받아들임
       넘파이 배열을 인수로 받아들일 수 없음
       step_ function (np.array ([1.0, 2.0 ] ) ) 불가

```python
def step_function(x):        #-> 넘파이 가능
  y = x > 0                  #-> bool형 출력
  return y.astype(np.int)    #-> bool에서 int형으로
```
+ 파이썬에서 bool을 int로 변환하면 True는 1, False는 0으로


### 시각화
```python
Import numpy as np
import matplotlib.pylab as plt

def step _function(x):
      return np.array(x > 0, dtype = np.int)

x = np.arange(-5.0, 5.0, 0.1)
y = step _function(x) 
plt.plot(x, y) 
plt.ylim(-0.1, 1.1) # y축의 범위 지정
plt.show()
```
-> np.arange (-5.0, 5.0, 0.1 ): -5.0에서 5.0 전까지 0.1 간격의 넘파이 배열을 생성


<img src = "https://user-images.githubusercontent.com/76824611/111866987-81d0de80-89b4-11eb-8fae-19f765168f9d.png" width="70%">

---


# 신경망에서 이용하는 활성화 함수
## sigmoid function(시그모이드 함수)

<img src = "https://user-images.githubusercontent.com/76824611/111867017-afb62300-89b4-11eb-87a0-9d21d90b2815.png" width="70%">
-> exp (-x )= e^-x  
-> e는 자연상수로  2.7182...의 값을 갖는 실수   
 
: 신경망에서는 활성화 함수로 시그모이드 함수를 이용하여 신호를 변환, 그 변환된 신호를 다음 뉴런에 전달합니다. 
: 퍼셉트론과 신경망의 주된 차이는 이 활성화 함수뿐
  
## 구현
```python  
def sigmoid(x):
  return 1 / (1 + np.exp(-x))
```
넘파이 배열이어도 올바른 결과가 나옴

## 시각화
```python 
x = np.arange(-5.0, 5.0, 0.1)
y = sigmoid(x) 
plt.plot(x, y) plt.ylim(-0.1, 1.1) # y축 범위 지정
plt.show()
```

<img src = "https://user-images.githubusercontent.com/76824611/111867212-eccee500-89b5-11eb-933e-cff82ba4f43f.png" width="70%">

---

# 비선형 함수
- 계단 함수와 시그모이드 함수의 공통점
  <br>직선 1개로는 그릴 수 없는 함수
- **신경망**에서는 활성화 함수로 **비선형 함수**만 사용해야함
  -> 선형 함수를 이용하면 신경망의 층을 깊게 하는 의미가 없어지기 때문

+ 선형함수의 문제: 층을 아무리 깊게 해도 ‘은닉층이 없는 네트워크’로도 똑같은 기능을 할 수 있음
                    여러 층으로 구성하는 이점을 살릴 수 없음

## 활성화 함수(activation function)
: 시그모이드 함수는 신경망 분야에서 오래전부터 이용해왔으나, 
<br>**최근**에는 **ReLU** Rectified Linear Unit, 렐루 함수를 주로 이용

### ReLU 함수
  : 입력이 0을 넘으면 그 입력을 그대로 출력하고, 0 이하이면 0을 출력하는 함수
  
  <img src = "https://user-images.githubusercontent.com/76824611/111867339-a2019d00-89b6-11eb-901e-38ad64b47f7d.png" width="70%">
  <img src = "https://user-images.githubusercontent.com/76824611/111867341-a62dba80-89b6-11eb-9922-1813019d3bf4.png" width="70%">


#### 구현
```python  
def relu(x): 
  return np.maximum(0, x)
``` 
-> 여기에서는 넘파이의 maximum 함수를 사용했습니다. 
<br>Maximum: 두 입력 중 큰 값을 선택해 반환하는 함수


---


# 다차원 배열의 계산

## 배열의 차원 수 확인
```python  
#np.ndim ( )         -> 차원수 반환
#or
#함수이름.shape      -> 튜플 반환

>>> B = np.array([[1,2], [3,4], [5,6]]) 
>>> print(B) [[1 2] [3 4] [5 6]] 
>>> np.ndim(B) 
2
>>> B.shape 
(3, 2)
``` 

## 행렬의 곱
```python 
#np.dot(A,B)

>>> A = np.array([[1,2], [3,4]]) 
>>> A.shape 
(2, 2) 
>>> B = np.array([[5,6], [7,8]]) 
>>> B.shape 
(2, 2) 
>>> np.dot(A, B) 
array([[19, 22], 
[43, 50]]) 
```
+ 유의

 <img src = "https://user-images.githubusercontent.com/76824611/111867582-4fc17b80-89b8-11eb-9466-00d7def85de4.png" width="70%">
 
## 신경망에서의 행렬의 곱

 <img src = "https://user-images.githubusercontent.com/76824611/111867585-5ea82e00-89b8-11eb-8c0d-931fe897ba37.png" width="70%"> 
 
## 3층 신경망 구현하기

 <img src = "https://user-images.githubusercontent.com/76824611/111867596-7a133900-89b8-11eb-8edb-7eeb236128da.png" width="70%">  
 
* 표기

 <img src = "https://user-images.githubusercontent.com/76824611/111867600-7da6c000-89b8-11eb-9249-199731a7637a.png" width="70%">
 
* 편향

 <img src = "https://user-images.githubusercontent.com/76824611/111867619-957e4400-89b8-11eb-8330-b9aa15ecc79a.png" width="70%">
 
편향: 오른쪽 아래 인덱스가 하나밖에 없음
<br>회색 노드가 편향
<br>-> 앞 층의 편향 뉴런이 하나뿐이기 때문


<img src = "https://user-images.githubusercontent.com/76824611/111867644-c2325b80-89b8-11eb-8c9e-94518d987942.png" width="70%">


-> 행렬의 곱을 이용하여 단순화

<img src = "https://user-images.githubusercontent.com/76824611/111867655-d0807780-89b8-11eb-875e-8c8b753126ba.png" width="70%">
 

<img src = "https://user-images.githubusercontent.com/76824611/111867656-d2e2d180-89b8-11eb-8ce1-7c3bfc60594a.png" width="70%">

- 구현

<img src = "https://user-images.githubusercontent.com/76824611/111867669-ec841900-89b8-11eb-99f4-dfc93a386771.png" width="70%">

* 시각화 형태

1) 활성화 함수를 시그모이드 함수로 사용

<img src = "https://user-images.githubusercontent.com/76824611/111867682-fe65bc00-89b8-11eb-8349-675311747aa0.png" width="70%">
-> 은닉층에서의 가중치 합(가중 신호와 편향의 총합)을 a로 표기
<br>-> 활성화 함수 h ( )로 변환된 신호를 z로 표기
<br>-> 활성화 함수를 시그모이드 함수로 사용

  - 구현
  ```python
  Z1 = sigmoid(A1) 
  print(A1) # [0.3, 0.7, 1.1] 
  print(Z1) # [0.57444252, 0.66818777, 0.75026011]
  ```


2) 1층에서 2층으로의 신호 전달

<img src = "https://user-images.githubusercontent.com/76824611/111867929-86989100-89ba-11eb-9a0a-75507eac90f3.png" width="70%">


  - 구현
  
  ```python
   W2 = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]]) 
   B2 = np.array([0.1, 0.2])

   print(Z1.shape) # (3,) 
   print(W2.shape) # (3, 2) 
   print(B2.shape) # (2,)

   A2 = np.dot(Z1, W2) + B2
   Z2 = sigmoid(A2)
  ```

3) 2층에서 출력층으로 신호 전달

    <img src = "https://user-images.githubusercontent.com/76824611/111867913-75e81b00-89ba-11eb-8093-47a7ce5456eb.png" width="70%">

 ```python    
  def identity _function(x):
    return x

  W3 = np.array([[0.1, 0.3], [0.2, 0.4]]) 
  B3 = np.array([0.1, 0.2])

  A3 = np.dot(Z2, W3) + B3
  Y = identity _function(A3) # 혹은 Y = A3
 ```
 
- 구현 정리
 ```python  
   def init _network():
      network = {} 
      network['W1'] = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]]) 
      network['b1'] = np.array([0.1, 0.2, 0.3]) 
      network['W2'] = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]]) 
      network['b2'] = np.array([0.1, 0.2]) 
      network['W3'] = np.array([[0.1, 0.3], [0.2, 0.4]]) 
      network['b3'] = np.array([0.1, 0.2])
 
      return network

  def forward(network, x):
      W1, W2, W3 = network['W1'], network['W2'], network['W3'] 
      b1, b2, b3 = network['b1'], network['b2'], network['b3']

      a1 = np.dot(x, W1) + b1
      z1 = sigmoid(a1) 
      a2 = np.dot(z1, W2) + b2
      z2 = sigmoid(a2) 
      a3 = np.dot(z2, W3) + b3
      y = identity _function(a3)
      return y

  network = init _network() 
  x = np.array([1.0, 0.5]) 
  y = forward(network, x) 
  print(y) # [ 0.31682708 0.69627909]
 ``` 
* 함수정리
  * init_network ( ) 함수 : 가중치와 편향을 초기화하고 이들을 딕셔너리 변수인 network에 
      * Network-> 각 층에 필요한 매개변수(가중치와 편향)를 저장 
  * forward ( ) 함수: 입력 신호를 출력으로 변환하는 처리 과정을 모두 구현하고 있음


---


# 출력층 설계

- 기계학습의 문제는 분류와 회귀로 나뉨
    - 분류 classification: 데이터가 어느 클래스 class 에 속하느냐는 문제
                           <br>사진 속 인물의 성별을 분류하는 문제
    - 회귀 regression: 입력 데이터에서 (연속적인) 수치를 예측하는 문제
                       <br>사진 속 인물의 몸무게(57.4kg?)를 예측


## 항등함수와 소프트맥스 함수 구현하기
- 항등 함수 identity function: 입력을 그대로 출력
   
    <img src = "https://user-images.githubusercontent.com/76824611/111875579-de94bf00-89dd-11eb-9c2d-824f263c5dd9.png" width="70%">

   
- 소프트맥스 함수 softmax function: 분류에 사용
  $$y_k=exp⁡(a_k )/(∑_(i=1)^n▒〖exp⁡(a_I ̇  ) 〗)$$

   - 변수 설명
     - n: 출력층의 뉴런 수,
     - y_k: 그중 k번째 출력임을 뜻함
  
   - 식 설명
     - 내 클래스의 값/각 클래스의 합
     - 즉, 모든값 합하면 1이 나옴
   
      <img src = "https://user-images.githubusercontent.com/76824611/111875734-9cb84880-89de-11eb-95f9-8bbfb2a089c9.png" width="70%">
   
   -  소프트맥스의 출력은 모든 입력 신호로부터 화살표를 받음
      -> 분모에서 보듯, 출력층의 각 뉴런이 모든 입력 신호에서 영향을 받기 때문


- 구현

```python
>>> a = np.array([0.3, 2.9, 4.0]) 
>>> 
>>> exp _a = np.exp(a) # 지수 함수
>>> print(exp _a) [ 1.34985881 18.17414537 54.59815003] 
>>> 
>>> sum _exp _a = np.sum(exp _a) # 지수 함수의 합
>>> print(sum _exp _a) 74.1221542102
>>> 
>>> y = exp _a / sum _exp _a 
>>> print(y) [ 0.01821127 0.24519181 0.73659691]
```

- 정의

```python
def softmax(a): 
 exp _a = np.exp(a) 
 sum _exp _a = np.sum(exp _a) 
 y = exp _a / sum _exp _a

 return y
```

## 소프트맥스 함수 구현 시 주의점
- 오버플로: 지수 함수란 것이 쉽게 아주 큰 값을 내뱉어 이런 큰 값끼리 나눗셈을 하면 결과 수치가 ‘불안정’해짐


## 구현개선

 <img src = "https://user-images.githubusercontent.com/76824611/111875996-f705d900-89df-11eb-9a57-2f19d6b51ee4.png" width="70%"> 

- **C**->어떤 값을 대입해도 상관없긴 하지만 오버플로를 막기 위해선 입력신호 중 최댓값을 넣음

```python

def softmax(a): 
 c = np.max(a)
 exp _a = np.exp(a - c) # 오버플로 대책
 sum _exp _a = np.sum(exp _a) 
 y = exp _a / sum _exp _a
 return y

```

## 특징
- 출력: 0에서 1.0 사이의 실수/출력의 총합은 1
       <br>-> 확률로 계산 가능
- 주의: 소프트맥스 함수를 적용해도 각 원소의 대소 관계는 변하지X
       <br>지수 함수 y = exp (x )가 단조 증가 함수이기 떄문
       <br>-> 신경망으로 분류할 때는 출력층의 소프트맥스 함수를 생략가능

+ 기계학습의 2단계
  <br>학습: 소프트맥스 함수를 사용
  <br>추론: 소프트맥스 함수를 생략

## 출력층의 뉴런 수 정하기
  : 예를 들어 입력 이미지를 숫자 0부터 9 중 하나로 분류하는 문제라면 [그림 3-23 ]처럼 출력층의 뉴런을 10개로 설정]
 
 <img src = "https://user-images.githubusercontent.com/76824611/111876143-bc507080-89e0-11eb-912f-b2eccdca5ce3.png" width="70%">  

---



+ 신경망의 2단계
1)	훈련 데이터(학습 데이터)를 사용해 가중치 매개변수를 학습(훈련 데이터로부터 가중치 매개변수의 최적값을 자동으로 획득하는 것)

3)	추론 단계: 앞서 학습한 매개변수를 사용하여 입력 데이터를 분류





----

# 한줄정리
- 신경
