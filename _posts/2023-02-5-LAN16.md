---
title: "TRPO: Trust Region Policy Optimization 정리"
date:   2023-02-5
excerpt: "Effective Approaches to Attention-based Neural Machine Translation paper review"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---

# 목차



----


# Abstract
본 논문은 단조로운 개선이 보장된 **policies 최적화**를 위한 **반복적인 절차를 설명**한다.          
**이론적**으로 정당화된 절차에 대한 몇 가지 근사치를 만들어 Trust Region Policy Optimization (TRPO)라는 실용적인 algorithm을 개발한다.          
이 algorithm은 natural policy gradient 방법과 유사하며, 신경망과 같은 대규모 nonlinear policies을 최적화하는 데 효과적이다.       

**[성과]**     
* 우리의 실험은 시뮬레이션된 로봇 수영, 깡충깡충 뛰기, 걷기 운동 학습, 화면 이미지를 입력으로 사용하여 아타리 게임을 하는 등 다양한 작업에서 강력한 성능을 보여준다.          
* 이론에서 벗어난 근사치에도 불구하고, TRPO는 hyperparameters의 조정이 거의 없이 monotonic한 개선을 제공하는 경향이 있다.


----

# 1 Introduction
policy optimization를 위한 대부분의 algorithms은 크게 세 가지 범주로 분류할 수 있다:     
**(1) policy iteration methods:** 현재 policy에 따른 **가치 함수 추정**과 **policy 개선**이 교대로 이뤄짐(Bertsekas, 2005)     
**(2) policy gradient methods:** 기대 수익률 (총 보상) 목표의 estimator of the gradient를 사용 (Peters & Schaal, 2008a) (그리고 이후 본 논문에 나오는 policy iteration과 밀접한 관련이 있음),         
**(3) derivative-free optimization methods**: cross-entropy method (CEM) 및 covariance matrix adaptation (CMA)과 최적화 방법에서 도출되며, 이는 반환을 정책 매개변수 측면에서 최적화될 블랙박스 함수로 취급(Szita & Lorincz, 2006). ¨


CEM 및 CMA와 같은 일반적인 **미분 없는 확률적 최적화 방법**은 이해하고 구현하기 **간단**하면서도 **좋은 결과**를 얻기 때문에 많은 문제에서 선호된다.    
ex) 예를 들어, 테트리스는 approximate dynamic programming (ADP) 방법에 대한 고전적인 벤치마크 문제이지만, 확률적 최적화 방법은 이 작업에서 이기기 어렵다(Gabillon et al., 2013).    

* **지속적인 제어 문제**의 경우, CMA와 같은 방법은 저차원 매개 변수화가 있는 수동 엔지니어링 정책 클래스를 제공할 때 이동과 같은 어려운 작업에 대한 **제어 정책을 학습하는 데 성공**했다(Wampler & Popovic, 2009).       
* 지속적인 **그레이디언트 기반 최적화**는 엄청난 수의 매개 변수를 가진 지도 학습 작업에 대한 학습 함수 근사치에서 매우 성공적이었으며, 그 성공을 **강화 학습으로 확장하면 복잡하고 강력한 정책을 효율적으로 훈련할 수 있다**   
➡ 즉, 강화학습에 **그레이디언트 기반 최적화**를 적용하겠다.     


**[논문의 구성]**      
* **1)** 특정 **surrogate objective function를 최소화하는 것**이 non-trivial step sizes로 **policy 개선을 보장**한다는 것을 증명한다.               
* **2)** 이론적으로 증명된 알고리즘에 대한 **일련의 근사치를 만들어 실제 알고리즘을 산출**하며, 이를 trust region policy optimization (TRPO)라고 한다.       
* **3)** 우리는 이 알고리듬의 두 가지 변형을 설명한다.       
   * **첫째,** model-free에서 적용할 수 있는 **single-path method**    
   * **둘째,** 시스템을 일반적으로 시뮬레이션에서만 가능한 특정 상태로 복원해야 하는 **vine method**.        
 
우리의 실험에서, 우리는 동일한 TRPO 방법이 원시 이미지에서 직접 아타리 게임을 할 뿐만 아니라 수영, 깡충깡충 뛰기, 걷기에 대한 복잡한 정책을 배울 수 있다는 것을 보여준다.      



----
----

# 2 Preliminaries


## MDP

**[MDP: Markov Decision Process]**    
$$MDP: (S,A,P,r, 𝜌_0, 𝛾)$$   
* **$$S$$**: state들의 유한한 set     
* **$$A$$**: action들의 유한한 set      
* **$$P$$**: $$S * A * S$$ ➡ $$ℝ$$은 전통적인 확률      
 어떤 state에서 어떤 act를 하면은 다음 state로 갈 확률이 얼만지     
* **$$r$$**: $$S$$ ➡ $$ℝ$$, reward 함수    
* **$$𝜌_0$$**: $$S$$ ➡ $$ℝ$$, 처음 state($$s_0$$)의 분포       
시작하는 state가 여러개가 있을 수 있는데 이 시작 위치의 분포
* **$$𝛾 ∈ (0,1)$$**: discount factor
0~1사이의 값을 갖는 Discount Factor (조금 더 효율적인 path를 찾게 해줌)    
현재 얻는 보상이 미래에 얻는 보상보다 얼마나 더 중요한지 의미하는 값 (미래와 비교한 현재 보상의 가치)         

**[Policy]**      
$$𝜋 : S * A ➡ [0,1]$$   


**[η(π) expected discounted reward]**:     
* η(π)는 policy를 하나 주면 이 policy가 얼마나 좋은지 알려줌(**policy의 성능**을 알려줌)  
* 즉 우리는 이를 최적화 해야하는 것이 목표다       
![image](https://user-images.githubusercontent.com/76824611/219015698-515fb90e-7a3a-45ef-937f-7eba0787b406.png)     
   * t는 0부터 무한대까지 계속 reward * discount factor를 시간 단위로 더한다.    
     즉, 이 값이 **높아질수록 좋은 것**이다.     
   * policy에 대한거니까 시작($$s_0$$)부터 있음    
   * $$s_0$$는 $$ρ_0(s_0)$$       
   * $$a_t$$는 $$π(a_t|s_t)$$    
   * $$s_{t+1}$$는 $$P(s_{t+1}|s_t, a_t)$$   

**action value function $$Q_π$$**)    
state에서 어떤 state에서 **action**에 일을 할 때, 그때부터 추가로 받을 reward들의 합     
어느 특정 state로부터 정의되는 것이라 $$s_{t+1}$$부터 시작됨    
![image](https://user-images.githubusercontent.com/76824611/219017464-1cb805fa-7093-4e99-9288-02ae9e09b567.png)

**value function $$V_π$$**)    
그 state로부터 게임이 끝날때 까지 받는 reward들의 기대값      
어느 특정 state로부터 정의되는 것이라 $$s_{t+1}$$부터 시작됨    
![image](https://user-images.githubusercontent.com/76824611/219017565-85ecfd94-c0a1-49bd-963c-bbdb73afe8c4.png)

**advantage function $$A_π$$**)    
adventage이다.      
예상했던 것(V(s))보다 얼마나 더 좋은 값인지를 판단하는 값    
![image](https://user-images.githubusercontent.com/76824611/219017616-c3574929-587e-4de1-a0cf-fcd824da0ef1.png)

![image](https://user-images.githubusercontent.com/76824611/219017695-39a6b362-95ca-40cd-a292-4aa12f29bae1.png)

<details>
<summary>📜 Advantage </summary>
<div markdown="1">
  
**[Advantage Actor-Critic, A2C]**    
Actor-Critic 의 Actor 의 기대출력으로 Advantage 를 사용    

Advantage 는 예상했던 것(V(s))보다 얼마나 더 좋은 값인지를 판단하는 값   
Q(s,a)에서 V(s)를 빼준 값을 많이 사용      
* Q(s,a)를 구하는 부분이 나와 있지 않으므로    
![image](https://user-images.githubusercontent.com/76824611/219020047-4b307ce0-fe1c-4e8c-8354-864a0da05c40.png)
     - 현재 가치: 보상      
     - 미래 가치: 다음 상태의 가치 함수 V(s′)     
     - 실제가치: Q(s,a)     
       ![image](https://user-images.githubusercontent.com/76824611/219020070-e94703dc-0b9b-4511-b05d-28f3c8f9e7c7.png)
* 즉 Advantage 를 구하는 식은 아래와 같이 바뀜
  ![image](https://user-images.githubusercontent.com/76824611/219020087-1922b9a3-a60f-40c5-b1af-c8847afae234.png)

  - Critic Network 에서 계산한 V(s) 값이 Actor Network 의 계산에도 영향을 끼치게 됨

**단점:** 특히 DQN 처럼 replay buffer 를 활용하지 않고 탐색 데이터를 즉시 학습에 이용하기 때문에 잘못된 길로 학습했을 경우 결과값이 안좋게 나올 수 있음
  
</div>
</details> 


---

## Kakade & Langford    
$$𝜋 ̃
$$ kakade&langford(2002)와 해당 논문의 appendix A에 따라,    
다른 policy π∼의 expected reward를 구하기 쉬운 policy π를 이용해 구할 수 있는 다음의 식이 유도되었으며,  


![image](https://user-images.githubusercontent.com/76824611/219031058-b0171e63-870d-4ded-add4-6dcd9c777012.png)


나중에 추가   (그림)          


우선 π의 성능을 구하고 그 값에다가 adventage의 기대값이다.  
$$𝜋 ̃ $$랑 π는 다른 policy이다.     
$$𝜋 ̃ $$의 성능을 알고 싶으면 우선 π를 구하고 그 값에다가 $$𝜋 ̃ $$로 부터 샘플링한다.      
$$s_0$$와 $$a_0$$를 $$𝜋 ̃ $$로 sampling하고있다.       
즉 π를 따랐을 때의 adventage를 가중치해서 더해준 후에 $$𝜋 ̃ $$를 따라다니면 $$𝜋 ̃ $$를 구할 수 있다.    

시간에 따른 표현법을 state에 따른 표현법으로 변환한 식이 다음과 같이 표현될 수 있다.      

q value는 2 + 0 인데 전 노드가 3이므로 advantage가 -1이다.   

어떤 다른 policy를 알고 싶을땐 내가 아는 policy로 부터 도출해낼 수 있다.



나중에 추가   (증명)          



**[식의 의미]**     
![image](https://user-images.githubusercontent.com/76824611/219037332-d971cdef-4bfe-408d-85e5-ef90f8383342.png)
1. 만일 모든 state s 에 대해 아래와 같다면, policy의 성능인 η이 증가하는게 보장된다는 뜻임     
즉, η(π)가 **policy의 성능**이므로 성능이 양수이면 좋다는 뜻이다.     
![image](https://user-images.githubusercontent.com/76824611/219037771-f6f64cbf-b28f-4445-9f2b-39ae5caee118.png)
2. 그러나, 실제에선 $$A_π$$를 측정할 때 뉴럴넷의 approximation error로 인해 아래와 같은 state, action 쌍이 섞여있을 것이다.     
![image](https://user-images.githubusercontent.com/76824611/219039044-3271cb38-052d-4c31-9324-792c2e7a1ac1.png)     
➡ 즉, 이는 policy가 보장됨을 보여주는 함수이다.       


특정한 구역인 trust region에서만 아래식이 치환이 가능해진다.        
![image](https://user-images.githubusercontent.com/76824611/219041666-89d3944c-c798-479f-9b8b-803957771b1e.png)  


왜냐하면 아래식과 같기 때문이다.   
![image](https://user-images.githubusercontent.com/76824611/219041971-3b814283-b864-42a7-966b-7a31c6db4bc3.png)
* 이 식의 의미는 Lπ와  η이 




