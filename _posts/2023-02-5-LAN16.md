---
title: "TRPO: Trust Region Policy Optimization 정리"
date:   2023-02-5
excerpt: "Effective Approaches to Attention-based Neural Machine Translation paper review"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---

# 목차



----


# Abstract
본 논문은 단조로운 개선이 보장된 **policies 최적화**를 위한 **반복적인 절차를 설명**한다.          
**이론적**으로 정당화된 절차에 대한 몇 가지 근사치를 만들어 Trust Region Policy Optimization (TRPO)라는 실용적인 algorithm을 개발한다.          
이 algorithm은 natural policy gradient 방법과 유사하며, 신경망과 같은 대규모 nonlinear policies을 최적화하는 데 효과적이다.       

**[성과]**     
* 우리의 실험은 시뮬레이션된 로봇 수영, 깡충깡충 뛰기, 걷기 운동 학습, 화면 이미지를 입력으로 사용하여 아타리 게임을 하는 등 다양한 작업에서 강력한 성능을 보여준다.          
* 이론에서 벗어난 근사치에도 불구하고, TRPO는 hyperparameters의 조정이 거의 없이 monotonic한 개선을 제공하는 경향이 있다.


----

# 1 Introduction
policy optimization를 위한 대부분의 algorithms은 크게 세 가지 범주로 분류할 수 있다:     
**(1) policy iteration methods:** 현재 policy에 따른 **가치 함수 추정**과 **policy 개선**이 교대로 이뤄짐(Bertsekas, 2005)     
**(2) policy gradient methods:** 기대 수익률 (총 보상) 목표의 estimator of the gradient를 사용 (Peters & Schaal, 2008a) (그리고 이후 본 논문에 나오는 policy iteration과 밀접한 관련이 있음),         
**(3) derivative-free optimization methods**: cross-entropy method (CEM) 및 covariance matrix adaptation (CMA)과 최적화 방법에서 도출되며, 이는 반환을 정책 매개변수 측면에서 최적화될 블랙박스 함수로 취급(Szita & Lorincz, 2006). ¨


CEM 및 CMA와 같은 일반적인 **미분 없는 확률적 최적화 방법**은 이해하고 구현하기 **간단**하면서도 **좋은 결과**를 얻기 때문에 많은 문제에서 선호된다.    
ex) 예를 들어, 테트리스는 approximate dynamic programming (ADP) 방법에 대한 고전적인 벤치마크 문제이지만, 확률적 최적화 방법은 이 작업에서 이기기 어렵다(Gabillon et al., 2013).    

* **지속적인 제어 문제**의 경우, CMA와 같은 방법은 저차원 매개 변수화가 있는 수동 엔지니어링 정책 클래스를 제공할 때 이동과 같은 어려운 작업에 대한 **제어 정책을 학습하는 데 성공**했다(Wampler & Popovic, 2009).       
* 지속적인 **그레이디언트 기반 최적화**는 엄청난 수의 매개 변수를 가진 지도 학습 작업에 대한 학습 함수 근사치에서 매우 성공적이었으며, 그 성공을 **강화 학습으로 확장하면 복잡하고 강력한 정책을 효율적으로 훈련할 수 있다**   
➡ 즉, 강화학습에 **그레이디언트 기반 최적화**를 적용하겠다.     


**[논문의 구성]**      
* **1)** 특정 **surrogate objective function를 최소화하는 것**이 non-trivial step sizes로 **policy 개선을 보장**한다는 것을 증명한다.               
* **2)** 이론적으로 증명된 알고리즘에 대한 **일련의 근사치를 만들어 실제 알고리즘을 산출**하며, 이를 trust region policy optimization (TRPO)라고 한다.       
* **3)** 우리는 이 알고리듬의 두 가지 변형을 설명한다.       
   * **첫째,** model-free에서 적용할 수 있는 **single-path method**    
   * **둘째,** 시스템을 일반적으로 시뮬레이션에서만 가능한 특정 상태로 복원해야 하는 **vine method**.        
 
우리의 실험에서, 우리는 동일한 TRPO 방법이 원시 이미지에서 직접 아타리 게임을 할 뿐만 아니라 수영, 깡충깡충 뛰기, 걷기에 대한 복잡한 정책을 배울 수 있다는 것을 보여준다.      



----
----

# 2 Preliminaries
Consider an infinite-horizon discounted Markov decision
process (MDP), defined by the tuple (S, A, P, r, ρ0, γ),
where S is a finite set of states, A is a finite set of actions,
P : S × A × S → R is the transition probability distri
bution, r : S → R is the reward function, ρ0 : S → R is
the distribution of the initial state s0, and γ ∈ (0, 1) is the
discount factor.


Let π denote a stochastic policy π : S × A → [0, 1], and
let η(π) denote its expected discounted reward:

η(π) = Es0,a0,... "X∞
t=0
γ
t
r(st)
#
, where
s0 ∼ ρ0(s0), at ∼ π(at|st), st+1 ∼ P(st+1|st, at).


We will use the following standard definitions of the stateaction value function Qπ, the value function Vπ, and the
advantage function Aπ:
Qπ(st, at) = Est+1,at+1,... "X∞
l=0
γ
l
r(st+l)
#
,
Vπ(st) = Eat,st+1,... "X∞
l=0
γ
l
r(st+l)
#
,
Aπ(s, a) = Qπ(s, a) − Vπ(s), where
at ∼ π(at|st), st+1 ∼ P(st+1|st, at) for t ≥ 0.




