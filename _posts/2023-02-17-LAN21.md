---
title: "MatSciBERT: A materials domain language model for text mining and information extraction 정리"
date:   2023-02-15
excerpt: "Specializing Multi-domain NMT via Penalizing Low Mutual Information paper review"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---



materials science에 대한 knowledge가 많이 생성되며, 텍스트로 저장되어왔다.    

<span style="background-color:#F5F5F5">**[문제]**</span>         
Bidirectional Encoder Representations from Transformers (BERT)과 같은 NLP 모델은 유용한 정보 추출 도구이지만, 이러한 모델은 <span style="background-color:#fff5b1">**materials science 특정 표기법(materials science specific notations)**과 **전문 용어(jargons)**에 대해 train되지 않았기 때문에</span> materials science에 적용할 때 좋은 결과를 얻지 못한다.         

<span style="background-color:#F5F5F5">**[해결: MatSciBERT]**</span>     
* <span style="background-color:#FFE6E6">**peer-reviewed materials science publications의 large corpus**에 대해 train</span>된 materials-aware language model이다.        
* 이 모델은 MatSciBERT가  science corpus에 대해 훈련된 언어 모델인 **[SciBERT](https://yerimoh.github.io/LAN22/)보다 성능이 좋다.**    
* downstream tasks, named entity recognition, relation classification, and abstract classification에서 SOTA성능이 난다.   


본 논문은 MatSciBERT의 pre-trained weights를 공개적으로 액세스할 수 있도록 한다.      

----


# INTRODUCTION
Discovering materials and utilizing them for practical applications
is an extremely time-consuming process that may span decades1,2
.
To accelerate this process, we need to exploit and harness the
knowledge on materials that has been developed over the
centuries through rigorous scientific procedure in a cohesive
fashion3–8
. Textbooks, scientific publications, reports, handbooks,
websites, etc., serve as a large data repository that can be mined
for obtaining the already existing information9,10. However, it is a
challenging task to extract useful information from these texts
since most of the scientific data is semi- or un-structured in the
form of text, paragraphs with cross reference, image captions, and
tables10–12. Extracting such information manually is extremely
time- and resource-intensive and relies on the interpretation of a
domain expert.

재료를 발견하고 실용적인 응용에 활용하는 것은 수십 년에 걸친 극도로 시간이 많이 걸리는 과정이다1,2. 이 과정을 가속화하기 위해서는 엄격한 과학적 절차를 통해 수세기에 걸쳐 개발된 재료에 대한 지식을 응집력 있는 방식으로 활용하고 활용해야 한다3-8, 과학 출판물, 보고서, 핸드북, 웹사이트 등은 이미 존재하는 정보를 얻기 위해 채굴할 수 있는 큰 데이터 저장소의 역할을 한다. 그러나 대부분의 과학 데이터는 텍스트, 상호 참조가 있는 단락, 이미지 캡션 및 표 10-12의 형태로 반구조화되거나 구조화되지 않았기 때문에 이러한 텍스트에서 유용한 정보를 추출하는 것은 어려운 작업이다. 이러한 정보를 수동으로 추출하는 것은 시간과 자원이 매우 많이 소요되며 도메인 전문가의 해석에 의존한다.



Natural language processing (NLP), a sub-domain in artificial
intelligence, presents an alternate approach that can automate
information extraction from text. Earlier approaches in NLP relied
on non-neural methods based on n-grams such as Brown et al.
(1992)13, structural learning framework by Ando and Zhang
(2005)14, or structural correspondence learning by Blitzer et al.
(2006)15, but these are no longer state of the art. Neural pretrained embeddings like word2vec16,17 and GloVe18 are quite
popular, but they lack domain-specific knowledge and do not
produce contextual embeddings. Recent progress in NLP has led
to the development of a computational paradigm in which a large,
pre-trained language model (LM) is finetuned for domain-specific
tasks. Research has consistently shown that this pretrain-finetune
paradigm leads to the best overall task performance19–23.
Statistically, LMs are probability distributions for a sequence of
words such that for a given set of words, it assigns a probability to
each word24. Recently, due to the availability of large amounts of
text and high computing power, researchers have been able to
pre-train these large neural language models. For example,
Bidirectional Encoder Representations from Transformers (BERT)25
is trained on BookCorpus26 and English Wikipedia, resulting in
state-of-the-art performance on multiple NLP tasks like question
answering and entity recognition, to name a few.

인공지능의 하위 도메인인 자연어 처리(NLP)는 텍스트에서 정보 추출을 자동화할 수 있는 대체 접근법을 제시한다. NLP의 이전 접근법은 브라운 외(1992)13, 안도와 장(2005)14의 구조 학습 프레임워크 또는 블리처 외의 구조적 대응 학습과 같은 n-gram에 기반한 비신경적 방법에 의존했다. (2006)15, 하지만 이것들은 더 이상 최첨단이 아니다. word2vec16,17 및 GloVe18과 같은 신경 사전 훈련된 임베딩은 꽤 인기가 있지만 도메인별 지식이 부족하고 상황별 임베딩을 생성하지 않는다. 최근 NLP의 발전은 도메인별 작업에 대해 사전 훈련된 대규모 언어 모델(LM)이 미세 조정되는 계산 패러다임의 개발로 이어졌다. 연구에 따르면 이러한 사전 훈련 미세 조정 패러다임은 19-23년 사이에 최고의 전반적인 작업 수행으로 이어진다. 통계적으로 LM은 주어진 단어 집합에 대해 각 단어 24에 확률을 할당하는 일련의 단어에 대한 확률 분포이다. 최근에, 많은 양의 텍스트와 높은 컴퓨팅 능력의 가용성 때문에, 연구자들은 이러한 큰 신경 언어 모델을 사전에 훈련시킬 수 있었다. 예를 들어, Transformers(BERT)25의 양방향 인코더 표현은 BookCorpus26 및 영어 위키피디아에서 훈련되어 질문 답변 및 엔티티 인식과 같은 여러 NLP 작업에서 최첨단 성능을 발휘한다.

Researchers have used NLP tools to automate database creation
for ML applications in the materials science domain. For instance,
ChemDataExtractor27, an NLP pipeline, has been used to create
databases of battery materials28, Curie and Néel temperatures of
magnetic materials29, and inorganic material synthesis routes30.
Similarly, NLP has been used to collect the composition and
dissolution rate of calcium aluminosilicate glassy materials31, and
zeolite synthesis routes to synthesize germanium containing
zeolites32, and to extract process and testing parameters of oxide
glasses, thereby enabling improved prediction of the Vickers
hardness11. Researchers have also made an automated NLP tool to
create databases using the information extracted from computational materials science research papers33. NLP has also been used
for other tasks such as topic modeling in glasses, that is, to group
the literature into different topics in an unsupervised fashion and
to find images based on specific queries such as elements present,
synthesis, or characterization techniques, and applications10

연구원들은 재료 과학 영역에서 ML 응용 프로그램을 위한 데이터베이스 생성을 자동화하기 위해 NLP 도구를 사용했다. 예를 들면, 전지 재료의 데이터베이스(28), 자성 재료의 퀴리 온도(29), 무기 재료 합성 경로(30)를 작성하기 위해서는, NLP 파이프라인인 ChemDataExtractor27이 이용되고 있다. 마찬가지로, NLP는 알루미늄 규산칼슘 유리물질(31)의 조성 및 용해율과 제올라이트 합성경로를 수집하여 제올라이트(32)를 함유하는 게르마늄을 합성하고, 산화물 유리의 공정 및 시험 파라미터를 추출하여 비커스 경도(11)의 향상된 예측을 가능하게 하였다. 연구자들은 또한 전산 재료 과학 연구 논문에서 추출한 정보를 사용하여 데이터베이스를 만드는 자동화된 NLP 도구를 만들었다. NLP는 또한 안경의 주제 모델링과 같은 다른 작업, 즉 감독되지 않은 방식으로 문학을 다른 주제로 그룹화하고 존재하는 요소, 합성 또는 특성화 기술과 같은 특정 쿼리를 기반으로 이미지를 찾는 데 사용되었다


A comprehensive review by Olivetti et al. (2019) describes
several ways in which NLP can benefit the materials science
community34. Providing insights into chemical parsing tools like
OSCAR435 capable of identifying entities and chemicals from text,
Artificial Chemist36, which takes the input of precursor information
and generates synthetic routes to manufacture optoelectronic
semiconductors with targeted band gaps, robotic system for
making thin films to produce cleaner and sustainable energy
solutions37, and identification of more than 80 million materials
science domain-specific named entities, researches have
prompted the accelerated discovery of materials for different
applications through the combination of ML and NLP techniques.
Researchers have shown the domain adaptation capability of
word2vec and BERT in the field of biological sciences as
BioWordVec38 and BioBERT19, other domain-specific BERTs like
SciBERT21 trained on scientific and biomedical corpus39, clinicalBERT40 trained on 2 million clinical notes in MIMIC-III v1.
database41, mBERT42 for multilingual machine translations tasks,
PatentBERT23 for patent classification and FinBERT for financial
tasks22. This suggests that a materials-aware LM can significantly
accelerate the research in the field by further adapting to
downstream tasks9,34. Although there were no papers on
developing materials-aware language models prior to this work43,
in a recent preprint44, Walker et al. (2021) emphasize the impact of
domain-specific language models on named entity recognition
(NER) tasks in materials science.



In this work, we train materials science domain-specific BERT,
namely MatSciBERT. Figure 1 shows the graphical summary of the
methodology adopted in this work encompassing creating the
materials science corpus, training the MatSciBERT, and evaluating
different downstream tasks. We achieve state-of-the-art results on
domain-specific tasks as listed below.

a. NER on SOFC, SOFC Slot dataset by Friedrich et al. (2020)45
and Matscholar dataset by Weston et al. (2019)9

b. Glass vs. Non-Glass classification of paper abstracts10

c. Relation Classification on MSPT corpus46


The present work, thus, bridges the gap in the availability of a
materials domain language model, allowing researchers to
automate information extraction, knowledge graph completion,
and other downstream tasks and hence accelerate the discovery
of materials. We have hosted the MatSciBERT pre-trained weights
at https://huggingface.co/m3rg-iitd/matscibert and codes for pretraining and finetuning on downstream tasks at https://github.
com/M3RG-IITD/MatSciBERT. Also, the codes with finetuned
models for the downstream tasks are available at https://doi.org/
10.5281/zenodo.6413296




