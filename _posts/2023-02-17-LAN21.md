---
title: "MatSciBERT: A materials domain language model for text mining and information extraction 정리"
date:   2023-02-15
excerpt: "Specializing Multi-domain NMT via Penalizing Low Mutual Information paper review"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---



materials science에 대한 knowledge가 많이 생성되며, 텍스트로 저장되어왔다.    

<span style="background-color:#F5F5F5">**[문제]**</span>         
Bidirectional Encoder Representations from Transformers (BERT)과 같은 NLP 모델은 유용한 정보 추출 도구이지만, 이러한 모델은 <span style="background-color:#fff5b1">**materials science 특정 표기법(materials science specific notations)**과 **전문 용어(jargons)**에 대해 train되지 않았기 때문에</span> materials science에 적용할 때 좋은 결과를 얻지 못한다.         

<span style="background-color:#F5F5F5">**[해결: MatSciBERT]**</span>     
* <span style="background-color:#FFE6E6">**peer-reviewed materials science publications의 large corpus**에 대해 train</span>된 materials-aware language model이다.        
* 이 모델은 MatSciBERT가  science corpus에 대해 훈련된 언어 모델인 **[SciBERT](https://yerimoh.github.io/LAN22/)보다 성능이 좋다.**    
* downstream tasks, named entity recognition, relation classification, and abstract classification에서 SOTA성능이 난다.   


본 논문은 MatSciBERT의 pre-trained weights를 공개적으로 액세스할 수 있도록 한다.      

----


# INTRODUCTION
<span style="background-color:#fff5b1">**materials를 발견하고 실용적인 응용에 활용**하는 것은 수십 년 단위로 걸려 **시간이 매우 많이든다**.</span>       
**이 과정을 가속화**하기 위해서는 엄격한 과학적 절차를 통해 
<span style="background-color:#fff5b1"**수세기에 걸쳐 개발된 재료에 대한 지식을 응집력 있는 방식으로 활용하고 활용**</span>해야 한다.       

Textbooks, scientific publications, reports, handbooks, websites 등은 이미 존재하는 정보를 얻기 위해 채굴할 수 있는 큰 데이터 저장소의 역할을 한다.     
그러나 대부분의 과학 데이터는 the form of text, paragraphs with cross reference, image captions, and tables의 형태로 **반구조화되거나 구조화되지 않았기 때문**에 이러한 텍스트에서 **유용한 정보를 추출하는 것은 어려운 작업**이다.       

⚠ 이러한 정보를 수동으로 추출하는 것은, **시간과 자원이 매우 많이 소요**되며 **domain 전문가의 해석에 의존**한다.   





<span style="background-color:#F5F5F5">**[NLP 모델 동향]**</span>    
인공지능의 하위 도메인인 자연어 처리(NLP)는 텍스트에서 **정보 추출을 자동화**할 수 있는 대체 접근법을 제시한다. * **word2vec 및 GloVe:** Neural pretrained embeddings으로, 꽤 인기가 있지만 **domain-specific knowledge이 부족**하고 **상황별 임베딩을 생성하지 않는다**.        
* 최근 NLP의 발전은 **domain-specific tasks에 대해 pre-trained language model (LM)이 finetuned되는 계산 패러다임의 개발**로 이어졌다.        
* 연구에 따르면 이러한 pretrain-finetune paradigm은 19-23년 사이에 SOTA였다.    
* 최근에, 많은 양의 텍스트와 높은 컴퓨팅 능력의 가용성 때문에, 연구자들은 이러한 **큰 신경 언어 모델을 pretrain시킬 수 있었다**.          
예를 들어, Bidirectional Encoder Representations from Transformers (BERT)은 BookCorpus 및 English Wikipedia에서 훈련되어  question answering과 entity recognition과 같은 여러 NLP 작업에서 SOTA를 발휘한다.         



<span style="background-color:#F5F5F5">**[materials science domain에서의 NLP applications]**</span>       
연구원들은 **materials science domain**에서 ML 응용 프로그램을 위한 **데이터베이스 생성을 자동화**하기 위해 **NLP tools를 사용**했다. 아래는 활용의 예시이다.          
아래 예시들은 모두 NLP 파이프라인인 [ChemDataExtractor](https://pubs.acs.org/doi/10.1021/acs.jcim.6b00207)를 사용했다.    
* [create databases of battery materials](https://www.nature.com/articles/s41597-020-00602-2)       
* [Curie and Néel temperatures of magnetic materials](https://www.nature.com/articles/sdata2018111)              
* [inorganic material synthesis routes](https://www.nature.com/articles/s41597-019-0224-1)       
* [composition and dissolution rate of calcium aluminosilicate glassy materials](https://ceramics.onlinelibrary.wiley.com/doi/10.1111/jace.17631)의 조성     
* [용해율과 zeolite 합성경로를 수집하여 zeolite](https://pubs.acs.org/doi/10.1021/acscentsci.9b00193)를 함유하는 게르마늄을 합성     
* [process and testing parameters of oxide glasses 및 testing parameters를 추출하여 Vickers
hardness의 향상된 예측을 가능하게 함](https://web.iitd.ac.in/~krishnan/publications/Krishnan/102_Extracting%20processing%20and%20test.html)              
* [computational materials science research papers에서 추출한 정보를 사용하여 데이터베이스를 만드는 자동화된 NLP](https://www.researchgate.net/publication/349533689_MatScIE_An_automated_tool_for_the_generation_of_databases_of_methods_and_parameters_used_in_the_computational_materials_science_literature) 도구를 만듦              
* [topic modeling in glasses](http://www.gstatic.com/generate_204): unsupervised fashion으로 문학을 다른 주제로 그룹화함, specific queries(elements present, synthesis, or characterization techniques, and applications)를 기반으로 이미지를 찾는 데 사용되었다.           




[Olivetti et al.(2019)](https://aip.scitation.org/doi/abs/10.1063/5.0021106)의 포괄적 검토는 NLP가 materials science에 이익을 줄 수 있는 몇 가지 방법을 설명한다.    
* [OSCAR4](https://jcheminf.biomedcentral.com/articles/10.1186/1758-2946-3-41): insights into chemical parsing tools제공, 화학적 구문 분석 도구에 대한 통찰력을 제공        
* [Artificial Chemis](https://onlinelibrary.wiley.com/doi/10.1002/adma.202001626): 전구체 정보의 입력을 받아 목표 대역 간격을 가진 광전자 반도체를 제조하기 위한 합성 경로를 생성          
* [robotic system](https://www.science.org/doi/10.1126/sciadv.aaz8867): 보다 깨끗하고 지속 가능한 에너지 솔루션을 생산하기 위해 thin films을 만듦     
➡ 37과 8천만 개 이상의 materials science domain-specific named entities를 식별하는 연구는 ML과 NLP 기술의 결합을 통해 **다양한 응용을 위한 재료의 가속화를 촉진**했다.      


<span style="background-color:#F5F5F5">**[Word2vec와 BERT의 확장]**</span>       
연구자들은 Word2vec와 BERT의 도메인 적응 능력을 아래와 같이 확장하였다.    
* [BioWordVec](https://www.nature.com/articles/s41597-019-0055-0), [BioBERT](https://arxiv.org/abs/1901.08746): 생물과학 분야에서 확장            
* [SciBERT](https://yerimoh.github.io/LAN22/): scientific and biomedical corpus으로 train        
* [clinicalBERT](https://arxiv.org/abs/1904.05342): 2 million clinical notes in [MIMIC-III v1.4 database](https://www.nature.com/articles/sdata201635)에서 훈련됨    
* [mBERT](https://aclanthology.org/2020.findings-emnlp.150/): multilingual machine translations tasks    
* [PatentBERT](http://www.digital.ntu.edu.tw/hsiang/pdf/WPI%20Patent%20classification%20by%20fine-tuning%20BERT.PDF): patent classification    
* [FinBERT](https://arxiv.org/abs/1908.10063): financial tasks        



✨ 이는 **materials-aware LM**이 downstream tasks에 추가로 적응함으로써 **해당 분야의 연구를 크게 가속화할 수 있음을 시사**한다.       


이 연구 이전에는 재료 인식 언어 모델 개발에 대한 논문이 없었지만, 최근 [a recent preprint](https://www.researchgate.net/publication/355846376_The_Impact_of_Domain-Specific_Pre-Training_on_Named_Entity_Recognition_Tasks_in_Materials_Science)은  materials science에서 domain-specific language models이 named entity recognition (NER) 작업에 미치는 영향을 강조한다.       


<span style="background-color:#F5F5F5">**[본 논문]**</span>       
이 연구에서, 우리는 materials science domain-specific BERT, 즉 **MatSciBERT**를 훈련시킨다.      
Fig. 1은 materials science 말뭉치 생성, MatSciBERT 훈련 및 다양한 downstream tasks 평가를 포함하는 본 연구에 채택된 방법론의 그래픽 요약을 보여준다.              
우리는 아래 나열된 것처럼 **domain-specific tasks에 대한 SoTA**를 달성한다.      
* **a.**  [NER on SOFC, SOFC Slot dataset by Friedrich et al. (2020)45     
 Matscholar dataset by Weston et al](https://pubmed.ncbi.nlm.nih.gov/31361962/)                    
* **b.** [Glass vs. Non-Glass classification of paper abstracts](https://www.sciencedirect.com/science/article/pii/S2666389921001239)             
* **c.**  [Relation Classification on MSPT corpus](https://aclanthology.org/W19-4007/)            


현재 연구는 materials domain language model의 가용성 격차를 해소하여 연구자가 정보 추출, knowledge graph completion 및 기타  downstream tasks을 자동화하여 **materials 발견을 가속화**할 수 있다.        


<span style="background-color:#F5F5F5">**[open source]**</span>       
* [huggingface/matscibert](https://huggingface.co/m3rg-iitd/matscibert): pre-trained weights을 제공    
[github/MatSciBERT](https://github.com/M3RG-IITD/MatSciBERT): MatSciBERT의 pretraining과 finetuning on downstream tasks 코드 공개            
* [zenodo](https://doi.org/ 10.5281/zenodo.6413296): downstream tasks을 위한finetuned
models이 있는 코드      


**[Fig. 1 Methodology for training MatSciBERT]**
![image](https://user-images.githubusercontent.com/76824611/225840065-ddbf1a52-fd41-4e8f-9f53-55d4f966fb06.png)   
* 우리는 관련 연구 논문을 선택한 후 query search을 통해 Materials Science Corpus (MSC)를 만든다.     
* MSC에서 pre-trained된 MatSciBERT는 다양한 downstream tasks에서 평가된다.       


---
---

# RESULTS AND DISCUSSION
## Dataset
Textual datasets are an integral part of the training of an LM. There
exist many general-purpose corpora like BookCorpus26 and
EnglishWikipedia, and domain-specific corpora like biomedical
corpus39, and clinical database41, to name a few. However, none of
these corpora is suitable for the materials domain. Therefore, with
the aim of providing a materials specific LM, we first create a
corpus spanning four important materials science families of
inorganic glasses, metallic glasses, alloys, and cement and
concrete. It should be noted that although these broad categories
are mentioned, several other categories of materials, including
two-dimensional materials, were also present in the corpus.
Specifically, we have selected ~150 K papers out of ~1 M papers
downloaded from the Elsevier Science Direct Database. The steps
to create the corpus are provided in the Methods section. The
details about the number of papers and words for each family are
given in Supplementary Table 1. We have also provided the list of
DOIs and PIIs of the papers used to pre-train MatSciBERT in the
GitHub repository for this work


The materials science corpus developed for this work has
~285 M words, which is nearly 9% of the number of words used to
pre-train SciBERT (3.17B words) and BERT (3.3B words). Since we
continue pre-training SciBERT, MatSciBERT is effectively trained on
a corpus consisting of 3.17 + 0.28 = 3.45B words. From Supplementary Table 1, one can observe that 40% of the words are from
research papers related to inorganic glasses and ceramics, and
20% each from bulk metallic glasses (BMG), alloys, and cement.
Although the number of research papers for “cement and
concrete” is more than “inorganic glasses and ceramics”, the
latter has higher words. This is because of the presence of a
greater number of full-text documents retrieved associated with
the latter category. The Supplementary Table 2 represents the
word count of important strings relevant to the field of materials
science. It should be noted that the corpus encompasses the
important fields of thermoelectric, nanomaterials, polymers, and
biomaterials. Also, note that the corpora used for training the
language model consists of both experimental and computational
works as both these approaches play a crucial role in understanding material response. The average paper length for this
corpus is ~1848 words, which is two-thirds of the average paper
length of 2769 words for the SciBERT corpus. The lower average
paper length can be attributed to two things: (a) In general,
materials science papers are shorter than biomedical papers. We
verified this by computing the average paper length of full-text
materials science papers. The number came out to be 2366. (b)
There are papers without full text also in our corpus. In that case,
we have used the abstracts of such papers to arrive at the final
corpus.


--

## Pre-training of MatSciBERT
For MatSciBERT pre-training, we follow the domain adaptive pretraining proposed by Gururangan et al. (2020). In this work,
authors continued pre-training of the initial LM on corpus of
domain-specific text20. They observed a significant improvement
in the performance on domain-specific downstream tasks for all
the four domains despite the overlap between initial LM
vocabulary and domain-specific vocabulary being less than
54.1%. BioBERT19 and FinBERT22 were also developed using the
similar approach where the vanilla BERT model was further pretrained on domain-specific text, and tokenization is done using
the original BERT vocabulary. We initialize MatSciBERT weights
with that of some suitable LM and then pre-train it on MSC. To
determine the appropriate initial weights for MatSciBERT, we
trained an uncased wordpiece47 vocabulary based on the MSC
using the tokenizers library48. The overlap of MSC vocabulary is
53.64% with the uncased SciBERT21 vocabulary and 38.90% with
the uncased BERT vocabulary. Because of the larger overlap with
the vocabulary of SciBERT, we tokenize our corpus using the
SciBERT vocabulary and initialize the MatSciBERT weights with that
of SciBERT as made publicly available by Beltagy et al. (2019)21. It is
worth mentioning that a materials science domain-specific
vocabulary would likely represent the corpus with a lesser number
of wordpieces and potentially lead to a better language model.
For e.g., “yttria-stabilized zirconia” is tokenized as [“yt”, “##tri”,
“##a”, “-”, “stabilized”, “zircon”, “##ia”] by the SciBERT vocabulary,
whereas a domain-specific tokenization might have resulted in
[“yttria”, “-”, “stabilized”, “zirconia”]. However, using a domainspecific tokenizer does not allow the use of SciBERT weights and
takes advantage of the scientific knowledge already learned by
SciBERT. Further, using the SciBERT vocabulary for the materials
domain is not necessarily detrimental since the deep neural
language models have the capacity to learn repeating patterns
that represent new words using the existing tokenizer. For
instance, when the wordpieces “yt”, “##tri”, and “##a” occur
consecutively, SciBERT indeed recognizes that some material
is being discussed, as demonstrated in the downstream tasks
This is also why most domain-specific BERT-based LMs like
FinBERT22, BioBERT19, and ClinicalBERT40 extend the pre-training
instead of using domain-specific tokenizers and learning from
scratch.


The details of the pre-training procedure are provided in the
Methods section. The pre-training was performed for 360 h, after
which the model achieved a final perplexity of 2.998 on the
validation set (see Supplementary Fig. 1a). Although not directly
comparable due to different vocabulary and validation corpus,
BERT25, and RoBERTa49 authors report perplexities as 3.99 and
3.68, respectively, which are in the same range. We also provide
graphs for other evaluation metrics like MLM loss and MLM
accuracy in Supplementary Fig. 1b, c. The final pre-trained LM was
then used to evaluate different materials science domain-specific
downstream tasks, details of which are described in the
subsequent sections. The performance of the LM on the downstream tasks was compared with that of SciBERT, BERT, and other
baseline models to evaluate the effectiveness of MatSciBERT to
learn the materials’ specific information.


In order to understand the effect of pre-training on the model
performance, a materials domain-specific downstream task, NER
on SOFC-slot, was performed using the model at regular intervals
of pre-training. To this extent, the pre-trained model was
finetuned on the training set of the SOFC-slot dataset. The choice
of the SOFC-slot dataset was based on the fact that the dataset
was comprised of fine-grained materials-specific information.
Thus, this dataset is appropriate to distinguish the performance
of SciBERT from the materials-aware LMs. The performance of
these finetuned models was evaluated on the test set. LM-CRF
architecture was used for the analysis since LM-CRF consistently
gives the best performance for the downstream task, as shown
later in this work. The macro-F1 averages across three seeds
exhibited an increasing trend (see Supplementary Fig. 2a),
suggesting the importance of training for longer durations. We
also show a similar graph for the abstract classification task
(Supplementary Fig. 2b).























