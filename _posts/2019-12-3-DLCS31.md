---
title: "[31] CS2241N: Lecture 1 Word Vectors 정리"
date:   2019-12-30
excerpt: "Lecture 1 | Word Vectors 요약"  
category: [Deep Learning]
layout: post
tag:
- Deep Learning
order: 0

comments: true
---




-----

# INTRO
강의 소개와 NLP에 대한 overview를 진행한다.     
오늘 우리의 목표인 word2vec과 별로 상관이 없기 때문에 생략하겠다.    

<details>
<summary>👀 강의 슬라이드 보기</summary>
<div markdown="1">
  
![image](https://user-images.githubusercontent.com/76824611/178132912-247dc377-09f2-481d-8902-b0833e42c745.png)
![image](https://user-images.githubusercontent.com/76824611/178132914-602a0d79-e2c8-48b3-a371-440a7f764f06.png)
![image](https://user-images.githubusercontent.com/76824611/178132917-f100eedf-06ba-4b3a-9390-8fe3d0530644.png)

  
</div>
</details>

---


# **Problems with NLP**
먼저 NLP task의 대표적인 문제점을 살펴보겠다.


## Problems resources like WordNet
* 1) 뉘앙스의 부족     
   * 단어들은 문장의 문맥에 따라 의미가 조금씩 달라진다.         
   * 이러한 단어의 뉘앙스를 표현하기가 힘들다.    
* 2) OOV문제     
   * out of vocabarary 문제로 학습 때 없던 새로운 단어를 봤을 때 컴퓨터가 무슨 단어인지 파악하기 힘들다는 것이다.        
   * 그리고 단어는 빠르게 새로운단어들이 출현하는데 이러한 새로운 단어에 대한 대응이 부족하다.     
* 3) 주관적    
   * 같은 문장이어도 사람에 따라 그 단어에 대해 느끼는 뉘앙스가 다름     
* 4) 인간의 노동    
   * 데이터 전처리와 같은 작업시 많은 인간의 노동이 필요하다.         
* 5) 단어 유사성을 정확하게 계산할 수 없다.        



여기서 5)의 문제를 자세하게 확인해보겠다.   



<details>
<summary>👀 강의 슬라이드 보기</summary>
<div markdown="1">
  
![image](https://user-images.githubusercontent.com/76824611/178133425-7ff3b02a-118f-451a-8636-cded1f948e5c.png)

  
</div>
</details>

------


## Representing words as discrete symbols
NLP task에서 우리는 단어표현을 [one-hot vectors](https://yerimoh.github.io/DL14/#%EC%A4%80%EB%B9%84-%EC%9B%90%ED%95%AB-one-hot-%ED%91%9C%ED%98%84)로 표현한다.       

즉, 예를 들어 hotel, motel 를  local한 단어 말뭉치에 있다고 가정해보고 이를 원핫 벡터로 표현해보면,    
아래와 같다.    
이와 같은 표현을 **localist representation**라고 한다.      


![image](https://user-images.githubusercontent.com/76824611/178133125-e5755b4f-9502-45e3-ada1-d1015a0db3a4.png)


여기서 원핫 벡터의 차원(Vector dimension)은 전체 말뭉치에서 단어의 개수이다.        


**[유사단어 표현 문제]**    
그런데 이렇게 원핫벡터로 표현하면 위 단어들이 전혀 연관성이 없는 단어가 아닌 "숙소"라는 공통점을 갖었음에도 불구하고 이를 단어 벡터에서 표현하지 못한다(두 단어에 대한 단어 벡터가 전혀 다르게 생겼다.)      
왜냐하면 하나의 단어에 하나만 1으로 표현되므로 그 단어의 존재 이외에 다른 정보를 표현할 방법이 없기 때문이다.       


<details>
<summary>👀 강의 슬라이드 보기</summary>
<div markdown="1">
  
![image](https://user-images.githubusercontent.com/76824611/178133438-316e73ee-33fe-4c16-9432-68d7d0b7397b.png)
![image](https://user-images.githubusercontent.com/76824611/178133445-841530bc-e80d-4c33-a454-f7df36fee750.png)

  
</div>
</details>




----
-----


# **Word2Vec**   



## Representing words by their context
그래서 위의 문제를 해결하기 위해 문맥을 표현할 수 있는 단어 벡터를 만들었다.    

**[Distributional semantics(분포 의미론)]**        
"단어의 의미를 그 단어의 가까이에 있는 단어의 분포를 통해 표현하자" (A word’s meaning is given by the words that frequently appear close-by)가 기본 모토이다.        

즉 표현하고자 하는 단어(tagret w) 주변에 있는 단어(context)를 통해 그 단어를 표현하자는 것이다.       
여기서 표현하고자 하는 단어를 기준으로 **주변 몇개의 단어를 볼 것인지**하는 주변 단어 고려 사이즈를 **window size**라고 한다.     

즉 아래에서 target이 banking일 때 그 주위의 회색이 문맥(context)단어인 것이다.    
![image](https://user-images.githubusercontent.com/76824611/178133627-9337520f-1b5f-49b8-adcd-1ecadc7987db.png)




**[Word vectors]**     
이 개념은 문맥에서 발생하는 단어를 벡터라고 본다.   
그리고ㅓ 각 단어에 대해 어떤 의미에서 **그 단어의 의미**를 나타내는 **조밀한 실수값 벡터**를 구축하는 것이 목표이다.     
즉 하나만 1이고 나머지 0인 raw한 표현에서 실수값을 사용함으로써 **단어에 대한 더 많은 정보**를 담는 것을 목표로 삼는 것이다.    
➡ 이를 만들기 위해 **vector dot (scalar) product**과 **similarity**를 측정하면서 **유사한 맥락에서 나타나는 단어의 벡터와 비슷**하도록 선택된 각 단어에 대해 조밀한 벡터를 만들 것이다.                


실제로는 3천차원의 벡터지만 요약해서 그려보면,    
![image](https://user-images.githubusercontent.com/76824611/178134129-3da62229-357c-4bf6-b7cb-f0ceeb7ae5b1.png)
위에서 0과 1로 나타내어진 벡터와 달리 실수값으로 나타러져 있다.        

이는 banking, monetary라는 단어의 의미가 벡터의 3천차원 모두에 퍼져있기 때문에      
**localist representation** 표현이 아닌 **distributed representation**이다.     



**[Word meaning as a neural word vector – visualization]**      
걱 던어를 위와 같이 표현하고,   
이렇게 표현한 word vector를 고차원 벡터 공간에 배치하여 해당 공간에 임베딩한다.     


그 결과를 확대하면(3처낵의 단어가 있개 때문에 전체를 보여주면 아이에 뭐가 뭔지 모른다) 아래과 같이 생겼다.       

확인해보면 비슷한 단어끼리 붙어있다는 것을 확인할 수 있다.     
즉 이렇게 밀집벡터를 이용하여 단어의 유사성르 표현할 수 있다는 것을 확인 할 수 있다.       
![image](https://user-images.githubusercontent.com/76824611/178134464-33f9f8d5-1c7a-445b-b165-a2dc876a6230.png)


-----


## Word2Vec Overview
Example windows and process for computing $$P(w_{t+j}|w_t)$$         
* $$w$$: 각 단어   
* $$t$$: 표현하고자 하는 타겟 단어     
* $$j$$: window size로 타겟 단어를 기준으로 주변 몇개의 단어를 고려할 것인지      

![image](https://user-images.githubusercontent.com/76824611/178134611-93188868-ce8e-4b56-ad03-9d5f9b225cc6.png)

위의 과정을 아래와 같이 모든 단어에 반복한다.        

![image](https://user-images.githubusercontent.com/76824611/178134663-7f833c73-70de-419f-b30a-aa33cad2e6ee.png)


-----


## Word2vec: objective function  

1️⃣ **Data likelihood**       
그럼 한단어를 표현하는 $$P(w_{t+j}|w_t)$$ 이 식으로 모든 단어를 표현해야 하므로,     
$$t= 1, … , T$$ 이와 같이 t를 확장해준다.       
목적: 중심단어 주변에서 볼 수 있는 문맥 단어의 가능성(Likelihood)을 최대화    

![image](https://user-images.githubusercontent.com/76824611/178136026-38a4a926-aea6-466a-b10f-e50fe349e369.png)
* $$m$$: 고정된 window size            
* $$w_t$$: 예측하고자 하는 주어진 타겟 단어(given center word)     
* $$θ$$: 최적화하고자하는 단어     


<details>
<summary>📜 수식 이해를 위한 추가 설명</summary>
<div markdown="1">
  
**[Optimization: Gradient Descent]**      
* We have a cost function $$J(θ)$$ we want to **minimize**       
* [Gradient Descent](https://yerimoh.github.io/DL7/) is an algorithm to minimize $$J(θ)$$     
* Idea: for current value of $$θ$$, calculate gradient of $$J(θ)$$, then take **small step** in d**irection of negative gradient**. Repeat.        
* 즉 현재 위치에서 기울기를 계산하여 기울기가 작은 곳으로 이동해 손실값이 최소화가 되는 지점을 찾아나가는 것이다.     
* 그러므로 이 과정에서 $$J(θ)$$는 최소화할 기울기를 찾아나가게 하는 기울기 계산 미분 함수라고 보면 된다.        
![image](https://user-images.githubusercontent.com/76824611/178135398-3fca9c39-d336-4820-babc-d0a53300d39f.png)
  
 
그리고 이 $$J(θ)$$를 계산하고, 계산한 것을 바탕으로 손실값이 낮은곳으로 θ를 업데이트 시켜줘야한다.       
  
![image](https://user-images.githubusercontent.com/76824611/178135716-0ff63dd3-f18a-4d0d-a080-b17dad8152c8.png)
  
  
  
</div>
</details>  




2️⃣3️⃣4️⃣5️⃣6️⃣












