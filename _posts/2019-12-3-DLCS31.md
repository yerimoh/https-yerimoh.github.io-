---
title: "[31] CS2241N: Lecture 1 Word Vectors 정리"
date:   2019-12-30
excerpt: "Lecture 1 | Word Vectors 요약"  
category: [Deep Learning]
layout: post
tag:
- Deep Learning
order: 0

comments: true
---




-----

# INTRO
강의 소개와 NLP에 대한 overview를 진행한다.     
오늘 우리의 목표인 word2vec과 별로 상관이 없기 때문에 생략하겠다.    

<details>
<summary>👀 강의 슬라이드 보기</summary>
<div markdown="1">
  
![image](https://user-images.githubusercontent.com/76824611/178132912-247dc377-09f2-481d-8902-b0833e42c745.png)
![image](https://user-images.githubusercontent.com/76824611/178132914-602a0d79-e2c8-48b3-a371-440a7f764f06.png)
![image](https://user-images.githubusercontent.com/76824611/178132917-f100eedf-06ba-4b3a-9390-8fe3d0530644.png)

  
</div>
</details>

---


# **Problems with NLP**
먼저 NLP task의 대표적인 문제점을 살펴보겠다.


## Problems resources like WordNet
* 1) 뉘앙스의 부족     
   * 단어들은 문장의 문맥에 따라 의미가 조금씩 달라진다.         
   * 이러한 단어의 뉘앙스를 표현하기가 힘들다.    
* 2) OOV문제     
   * out of vocabarary 문제로 학습 때 없던 새로운 단어를 봤을 때 컴퓨터가 무슨 단어인지 파악하기 힘들다는 것이다.        
   * 그리고 단어는 빠르게 새로운단어들이 출현하는데 이러한 새로운 단어에 대한 대응이 부족하다.     
* 3) 주관적    
   * 같은 문장이어도 사람에 따라 그 단어에 대해 느끼는 뉘앙스가 다름     
* 4) 인간의 노동    
   * 데이터 전처리와 같은 작업시 많은 인간의 노동이 필요하다.         
* 5) 단어 유사성을 정확하게 계산할 수 없다.        



여기서 5)의 문제를 자세하게 확인해보겠다.   



<details>
<summary>👀 강의 슬라이드 보기</summary>
<div markdown="1">
  
![image](https://user-images.githubusercontent.com/76824611/178133425-7ff3b02a-118f-451a-8636-cded1f948e5c.png)

  
</div>
</details>

------


## Representing words as discrete symbols
NLP task에서 우리는 단어표현을 [one-hot vectors](https://yerimoh.github.io/DL14/#%EC%A4%80%EB%B9%84-%EC%9B%90%ED%95%AB-one-hot-%ED%91%9C%ED%98%84)로 표현한다.       

즉, 예를 들어 hotel, motel 를  local한 단어 말뭉치에 있다고 가정해보고 이를 원핫 벡터로 표현해보면,    
아래와 같다.    
이와 같은 표현을 **localist representation**라고 한다.      


![image](https://user-images.githubusercontent.com/76824611/178133125-e5755b4f-9502-45e3-ada1-d1015a0db3a4.png)


여기서 원핫 벡터의 차원(Vector dimension)은 전체 말뭉치에서 단어의 개수이다.        


**[유사단어 표현 문제]**    
그런데 이렇게 원핫벡터로 표현하면 위 단어들이 전혀 연관성이 없는 단어가 아닌 "숙소"라는 공통점을 갖었음에도 불구하고 이를 단어 벡터에서 표현하지 못한다(두 단어에 대한 단어 벡터가 전혀 다르게 생겼다.)      
왜냐하면 하나의 단어에 하나만 1으로 표현되므로 그 단어의 존재 이외에 다른 정보를 표현할 방법이 없기 때문이다.       


<details>
<summary>👀 강의 슬라이드 보기</summary>
<div markdown="1">
  
![image](https://user-images.githubusercontent.com/76824611/178133438-316e73ee-33fe-4c16-9432-68d7d0b7397b.png)
![image](https://user-images.githubusercontent.com/76824611/178133445-841530bc-e80d-4c33-a454-f7df36fee750.png)

  
</div>
</details>




----
-----


# **Word2Vec**   



## Representing words by their context
그래서 위의 문제를 해결하기 위해 문맥을 표현할 수 있는 단어 벡터를 만들었다.    

**[Distributional semantics(분포 의미론)]**            
"단어의 의미를 그 단어의 가까이에 있는 단어의 분포를 통해 표현하자" (A word’s meaning is given by the words that frequently appear close-by)가 기본 모토이다.        

즉 표현하고자 하는 단어(tagret w) 주변에 있는 단어(context)를 통해 그 단어를 표현하자는 것이다.       
여기서 표현하고자 하는 단어를 기준으로 **주변 몇개의 단어를 볼 것인지**하는 주변 단어 고려 사이즈를 **window size**라고 한다.     

즉 아래에서 target이 banking일 때 그 주위의 회색이 문맥(context)단어인 것이다.    
![image](https://user-images.githubusercontent.com/76824611/178133627-9337520f-1b5f-49b8-adcd-1ecadc7987db.png)




**[Word vectors]****          
이 개념은 문맥에서 발생하는 단어를 벡터라고 본다.   
그리고 각 단어에 대해 어떤 의미에서 **그 단어의 의미**를 나타내는 **조밀한 실수값 벡터**를 구축하는 것이 목표이다.     
즉 하나만 1이고 나머지 0인 raw한 표현에서 실수값을 사용함으로써 **단어에 대한 더 많은 정보**를 담는 것을 목표로 삼는 것이다.    
➡ 이를 만들기 위해 **vector dot (scalar) product**과 **similarity**를 측정하면서 **유사한 맥락에서 나타나는 단어의 벡터와 비슷**하도록 선택된 각 단어에 대해 조밀한 벡터를 만들 것이다.                


실제로는 3천차원의 벡터지만 요약해서 그려보면,    
![image](https://user-images.githubusercontent.com/76824611/178134129-3da62229-357c-4bf6-b7cb-f0ceeb7ae5b1.png)
위에서 0과 1로 나타내어진 벡터와 달리 실수값으로 나타러져 있다.        

➡ 이는 banking, monetary라는 단어의 의미가 벡터의 3천차원 모두에 퍼져있기 때문에,      
localist representation 표현이 아닌 distributed representation 이다.     



**[Word meaning as a neural word vector – visualization]**          
각 단어를 위와 같이 표현하고,   
이렇게 표현한 word vector를 고차원 벡터 공간에 배치하여 해당 공간에 임베딩한다.     


그 결과를 확대하면(3처낵의 단어가 있개 때문에 전체를 보여주면 아이에 뭐가 뭔지 모른다) 아래과 같이 생겼다.       

확인해보면 비슷한 단어끼리 붙어있다는 것을 확인할 수 있다.     
즉 이렇게 밀집벡터를 이용하여 단어의 유사성르 표현할 수 있다는 것을 확인 할 수 있다.       
![image](https://user-images.githubusercontent.com/76824611/178134464-33f9f8d5-1c7a-445b-b165-a2dc876a6230.png)


-----


## Word2Vec Overview
Example windows and process for computing $$P(w_{t+j}|w_t)$$         
* $$w$$: 각 단어   
* $$t$$: 표현하고자 하는 타겟 단어     
* $$j$$: window size로 타겟 단어를 기준으로 주변 몇개의 단어를 고려할 것인지      

![image](https://user-images.githubusercontent.com/76824611/178134611-93188868-ce8e-4b56-ad03-9d5f9b225cc6.png)

위의 과정을 아래와 같이 모든 단어에 반복한다.        

![image](https://user-images.githubusercontent.com/76824611/178134663-7f833c73-70de-419f-b30a-aa33cad2e6ee.png)


-----


## Word2vec: objective function  

1️⃣ **Data likelihood**       
그럼 한단어를 표현하는 $$P(w_{t+j}|w_t)$$ 이 식으로 모든 단어를 표현해야 하므로,     
$$t= 1, … , T$$ 이와 같이 t를 확장해준다.       
목적: 중심단어 주변에서 볼 수 있는 문맥 단어의 가능성(Likelihood)을 최대화    

![image](https://user-images.githubusercontent.com/76824611/178136026-38a4a926-aea6-466a-b10f-e50fe349e369.png)
* $$m$$: 고정된 window size            
* $$w_t$$: 예측하고자 하는 주어진 타겟 단어(given center word)     
* $$θ$$: 최적화하고자하는 단어     


※ $$;θ$$는 모든 세타에 대해서, 즉 여기선 모든 최적화하고자하는 단어에 대해서 라는 뜻이라는데 강의에 정확하게 나오지 않아서 혹시 더 정확한 설명이 있으면 댓글로 남겨주시면 감사하겠습니다ㅠㅠ     

<details>
<summary>📜 수식 이해를 위한 추가 설명</summary>
<div markdown="1">
  
**[Optimization: Gradient Descent]**      
* We have a cost function $$J(θ)$$ we want to **minimize**       
* [Gradient Descent](https://yerimoh.github.io/DL7/) is an algorithm to minimize $$J(θ)$$     
* Idea: for current value of $$θ$$, calculate gradient of $$J(θ)$$, then take **small step** in d**irection of negative gradient**. Repeat.        
* 즉 현재 위치에서 기울기를 계산하여 기울기가 작은 곳으로 이동해 손실값이 최소화가 되는 지점을 찾아나가는 것이다.     
* 그러므로 이 과정에서 $$J(θ)$$는 최소화할 기울기를 찾아나가게 하는 기울기 계산 미분 함수라고 보면 된다.        
![image](https://user-images.githubusercontent.com/76824611/178135398-3fca9c39-d336-4820-babc-d0a53300d39f.png)
  
 
그리고 이 $$J(θ)$$를 계산하고, 계산한 것을 바탕으로 손실값이 낮은곳으로 θ를 업데이트 시켜줘야한다.       
  
![image](https://user-images.githubusercontent.com/76824611/178135716-0ff63dd3-f18a-4d0d-a080-b17dad8152c8.png)
  
  
  
</div>
</details>  


2️⃣ **유도식**      
3을 유도하는 유도식인데 강의에는 따로 설명이 되어있지 않아서 추가해봤다.          
유도식을 잘 따라가보면 1과 3이 같은 식이라는 것을 알 수 있을 것이다.     
* 먼저 확률 식을 보자
![image](https://user-images.githubusercontent.com/76824611/178136442-0e93be89-d421-4b1a-9e95-bb1b48f82b53.png)
* 여기서 맥락의 단어들 사이에 관련성이 없다고 가정 후 분해 (‘조건부 독립’이라고 가정).   
![image](https://user-images.githubusercontent.com/76824611/178136447-ced4c47d-254d-4bb8-ae4d-e2b16a9abc94.png)
* 손실함수 유도     
skip-gram에서 손실함수는 단순히 확률에 log를 취한 다음 마이너스를 붙이면 됨         
![image](https://user-images.githubusercontent.com/76824611/178136458-4d384fae-b378-44a3-ae3e-a3fe3b9f4583.png)
skip-gram 모델의 손실 함수는 맥락별 손실을 구한 다음 모두 더함.     
* 말뭉치 전체로 확장     
여기서의 예시는 j가 1로 고정되어있지만 이 j를 확장시켜 m으로 늘려 늘린 확률들을 계산해 주면 다음 단계인 3의 식이 나온다.     
![image](https://user-images.githubusercontent.com/76824611/178136588-27798f38-5091-45ee-b5c5-8979b5fda731.png)




3️⃣ **objective function**     
**(average) negative log likelihood:** 앞서 말했듯이 skip-gram에서 손실함수는 단순히 확률에 log를 취한 다음 마이너스를 붙이면 된다.            
그리고 손실값의 평균을 구해야하니까 전체 단어의 개수로 나눠($$1/T$$) nomalization을 해준다.       
즉 아래의 식으로 손실값을 구하고 이르를 최소화 시키는 것이다.   
**Minimizing objective function ⟺ Maximizing predictive accuracy**       
우리는 아래의 함수를 **minimize 하는 것이 목적**이다.     

![image](https://user-images.githubusercontent.com/76824611/178137032-751898f6-397d-4e7c-a36d-bd5304586a9d.png)
* $$J(θ)$$: sometimes called a cost or [loss function](https://yerimoh.github.io/DL3/#%EC%86%90%EC%8B%A4-%ED%95%A8%EC%88%98loss-function)      



-----

그럼 위의 식에서 $$P(w_{t+j}|w_t ; θ)$$를 계산해보자.     

본격적으로 계산에 들어가기 전에 각 단어 $$w$$에 대해 **2개의 vector**를 사용할 것이다.     
* $$v_w$$ when w is a center word    
* $$u_w$$ when w is a context word     



1️⃣ **식 재정의**       
그럼 $$P(w_{t+j}|w_t ; θ)$$ 이 식을 아래와 같이 바꿔쓸 수 있다.       
* $$c$$: center word      
* $$o$$: context word     
이므로 아래 식은 center word가 주어졌을 때 context word가 나올 확률이다.        

![image](https://user-images.githubusercontent.com/76824611/178138068-b70c03c9-cce4-443c-a633-563f0c39efca.png)


2️⃣ **식 설명 및 분석**       
아래 함수는 **softmax function**을 적용한 것이다.        
* **(1)** 내적(Dot product)을 이용하여 o와 c의 유사도를 계산한다.      
   * $$u^{T}v = u.v = \displaystyle\sum_{i=1}^{n}{u_{i}v_{i}}$$ ➡ 벡터의 차원들이 같으니 내적을 위해 $$T$$(차원 변경) 수행      
    ex) $$u = 300$$x$$1$$, $$v= 300$$x$$1$$ 이라면 $$u^{T}v = [1$$x$$300]$$ x $$[300$$x$$1]$$ 으로 바꿔 각 자리 $$i$$마다 계산하는 내적을 수행하는 것이다.        
* **(2)** exp(Exponentiation)라는 지수화를 사용하여 모든 값들을 양수값으로 바꾼다.     
   * 아래 식은 확률이기 때문에 값이 모두 양수로 나와야한다.      
* **(3)** 확률 분포를 나타내기위해 전체 합으로 나눠 정규화를 시킨다.          

![image](https://user-images.githubusercontent.com/76824611/178140131-6a4a85b0-739b-46f8-8b00-ccfb38719929.png)


       
<details>
<summary>📜 softmax function 설명 보기</summary>
<div markdown="1">

확률을 0에서 1사이로 정규화 하는 것으로  조금 특이한 이름의 뜻을 갖고있다.     
* **“soft”**: 조금이라도 유사하면 $$x_i$$에 확률을 부여하긴 한다.          
* **“max”** : $$x_i$$중에서 가장 큰 값을 반환한다.(그런데 소프트맥스 함수는 최대값이 아닌 분포를 반환하기 때문에 조금 이상한 이름이다)       
* 딥러닝에서 많이 사용한다.            
![image](https://user-images.githubusercontent.com/76824611/178140110-e2c928ce-f29b-4d2e-8458-fdec849f3704.png)

  
  
  
  
</div>
</details>  


-----


## more about objective function

위에서 본 목적함수(loss function) $$J(θ)$$의 미분값, $$J'(θ)$$를 **최대화(maximize)** 하는 것이 목표거나,      
![image](https://user-images.githubusercontent.com/76824611/178569682-be046386-6cb0-4447-a35f-22e59834e34a.png)




------
-------


# **train the model**   
To train the model: Optimize value of parameters to minimize loss     
위의 Loss function을 이용하여 모든 기울기를 계산한다.     



![image](https://user-images.githubusercontent.com/76824611/178561243-5c2bebf1-ea35-4954-8995-5f5056e0596e.png)
* $$θ$$: 우리의 모든 모델 매개변수를 하나의 긴 벡터로 나타낸다.    
* $$d$$: 우리의 경우 $$d$$(차원)는 300이다.     
* $$v$$: 또한 많은 $$v$$를 갖고있다.     
* 2: 그리고 모든 단어가 2개의 벡터를 갖고있다.        


그리고 우리의 목표는 모든 벡터의 기울기를 계산해서 손실값이 가장 낮은 곳으로 이동하는 것이다.      
















