---
title: "[00] Attention is All You Need "
date:   2021-07-12
excerpt: "The Illustrated Transformer"
category: [Language AI]
layout: post
tag:
- Java
order: 0

comments: true
---

[출처_원 링크](http://jalammar.github.io/illustrated-transformer/)

# intro
NIPS에서 Google이 소개했던 Transformer는 NLP 학계에서 정말 큰 주목을 받음  
CNN 과 RNN 이 주를 이뤘던 연구들에서 벗어나 아예 새로운 모델을 제안했기 때문    

## 핵심  
multi-head self-attention을 이용해 sequential computation 을 줄여 더 많은 부분을 병렬처리가 가능하게 만들면서 동시에 더 많은 단어들 간 dependency를 모델링 한다는 것

## 읽기 위해 필요한 지식
[attention](링크 삽입) 
[transformer 개요](링크 삽입)

## 원 논문
[Attention is All You Need](https://arxiv.org/abs/1706.03762)

----

# 도입 배경, 이유

## intro_RNN의 단점

어텐션을 갖춘 seq2seq의 구성에는 반드시 RNN이 등장했음.   

RNN은 병렬 처리 불가

### RNN
이전 시각에 계산한 결과를 이용하여 순서대로 계산   
따라서 RNN의 계산을 시간 방향으로 **병렬 계산**하기란 (기본적으로는) **불가능**       
* 딥러닝 학습이 GPU를 사용한 병렬 계산 환경에서 이뤄진다는 점을 생각하면 큰 단점임    
* 현재 RNN을 없애는 연구(혹은 병렬 계산할 수 있는 RNN 연구) 활발. 
* 그 중 유명한 것이 「Attention is all you need」라는 논문 [52 ] 에서 제안한 기법인 트랜스포머 Transformer 모델

---



# Transformer 모델의 시각화

Google Cloud는 이제 그들의 Cloud TPU를 쓸 때 Transfomer 모델을 기준으로 쓸 것을 추천.    


이 모델은 **‘RNN이 아닌’ 어텐션을 사용해 처리**   
트랜스포머는 어텐션으로 구성,     
* 그중 셀프어텐션 Self-Attention 이라는 기술을 이용     

## 모델의 전체 개요
기계 번역의 경우를 생각해본다면, 모델은 어떤 한 언어로 된 하나의 문장을 입력으로 받아 다른 언어로 된 번역을 출력으로 내놓음
![image](https://user-images.githubusercontent.com/76824611/117874990-9f4b6600-b2dc-11eb-924d-92f8fe54dfc0.png)

### encoding 과 decoding 확대
* encoding: encoder 을 쌓아 올려 만든 것   
* decoding: encoding 부분과 동일한 개수만큼의 decoder 을 쌓은 것   

encoder 들은 모두 정확히 똑같은 구조를 가지고 있음 (그러나 그들 간에 같은 ~~weight을 공유~~하진 않습니다)
![image](https://user-images.githubusercontent.com/76824611/117876759-b0957200-b2de-11eb-8944-a8c50db60a41.png)

### encoder 확대
인코더에 들어온 입력은,    
1) ```self-attention``` 을 지남[추후설명](링크삽입)             
  * encoder가 **하나의 특정한 단어**를 encode 하기 위해서 입력 내의 모든 **다른 단어들과의 관계를 살펴봄**    
2) 위의 출력은 ```feed-forward``` 신경망으로 들어감.    
  * 똑같은 feed-forward 신경망이 각 위치의 단어마다 독립적으로 적용돼 출력을 만듦     
![image](https://user-images.githubusercontent.com/76824611/117877743-e1c27200-b2df-11eb-937f-26a3130c5bec.png)
 
## decoder확대
encoder에 있는 두 layer 모두를 가지고 있음 
+)
1.5) ```encoder-decoder attention``` 추가 : 그 두 층 사이에 seq2seq 모델의**attention** 과 비슷한 이것이 끼워져있음   
*  decoder가 입력 문장 중에서 각 **타임 스텝에서 가장 관련 있는 부분**에 집중할 수 있도록 해줌    
![image](https://user-images.githubusercontent.com/76824611/117878868-30244080-b2e1-11eb-97b5-7ef6542d41ef.png)

----

# 백터들을 기준으로 그림 그려보기   
입력으로 들어와서 출력이 될 때까지,    
이 부분들 사이를 지나가며 변환될 **벡터/텐서**들을 기준으로 모델 분석.   

## 1단계 
입력 단어들을 embedding 알고리즘를 이용해 벡터로 바꿈(링크 삽입)         
* 각  단어들은 크기 512의 벡터 하나로 embed     
  * 이 벡터 리스트의 사이즈는 hyperparameter으로 우리가 마음대로 정할 수 있음
  * 주로 가장 긴 문장의 길이     
* 변환된 벡터들 박스로 표현    

![image](https://user-images.githubusercontent.com/76824611/117880292-e9cfe100-b2e2-11eb-8d45-ad2e5a4d763c.png)

이 embedding은 **가장 밑단**의 encoder 에서만 일어남     
모든 encoder 들은 크기 512의 벡터의 리스트를 입력으로 받음     

이 벡터는,    
* 가장 밑단의 encoder의 경우: word embedding이 된 것     
* 다른 encoder들: 바로 전의 encoder의 출력     



## 2단계
입력 문장의 단어들을 embedding 한 후에,   
각 단어에 해당하는 벡터들은 encoder 내의 두 개의 sub-layer으로 들어가게 됨    
![image](https://user-images.githubusercontent.com/76824611/117882477-59df6680-b2e5-11eb-8540-669c99792aed.png)

**[Transformer 모델의 주요 성질 파악 가능]***     
**각 위치**에 있는 **각 단어**가 **각각의 path**를 통해 encoder에서 흘러감
* ```Self-attention``` 층: 이 위치에 따른 path들 사이에 다 dependency 존재
* ```feed-forward``` 층: 이런 ~~dependency~~가 없음 
     * feed-forward layer 내의 이 다양한 path 들은 **병렬처리 가능**   

-----
 
# encoder의 각 sub-layer
무슨 일이 일어나는지를 자세히 보자    
단계 구체화     
1) encoder는 입력으로 벡터들의 리스트를 받음       
2) 이 리스트를 먼저 self-attention layer에,  
3) 그다음으로 feed-forward 신경망에 통과시키고  
4) 그 결과물을 그다음 encoder에게 전달   

각 위치의 단어들은 **각각 다른 self-encoding** 과정을 거침      
그다음으로는 모두에게 **같은 과정인 feed-forward** 신경망을 거침    
![image](https://user-images.githubusercontent.com/76824611/117884183-47fec300-b2e7-11eb-82a6-f0d8aad2ed04.png)


---

# sub-layer1 > Self-Attention
하나의 시계열 데이터를 대상으로 한 어텐션으로,        
**하나의 시계열 데이터 내에서** 각 원소가 다른 원소들과 어떻게 관련되는지를 살펴보자는 취지       

**[PROBLEM]**
다음 문장을 우리가 번역하고 싶은 문장:          
```“그 동물은 길을 건너지 않았다 왜냐하면 그것은 너무 피곤했기 때문이다”```

이 문장에서 “그것” 이 가리키는 것?      
* 길    
* 동물     
사람에게는 이것이 너무나도 간단한 질문이지만 신경망 모델에게는 그렇게 간단하지만은 않은 문제입니다.     

**[SOLUTION]**     
모델이 “그것은”이라는 단어를 처리할 때, 모델은 ```self-attention```을 이용하여 “그것”과 “동물”을 연결 가능          
* ```self-attention```: 현재 처리 중인 단어에 다른 연관 있는 단어들의 맥락을 불어 넣어주는 method
 
모델이 입력 문장 내의 각 단어를 처리해 나감에 따라,  
self-attention은 **입력 문장 내의 다른 위치에 있는 단어들**을 보고 거기서 **힌트를 받아** 현재 타겟 위치의 단어를 더 잘 encoding 할 수 있음

(SOLVE)   
가장 윗단에 있는 encoder에서 “그것”이라는 단어를 encoding 할 때,     
* attention 메커니즘은 입력의 여러 단어들 중에서 “그 동물”이라는 단어에 집중    
* 이 단어(“그 동물”)의 의미 중 일부를 “그것”이라는 단어를 encoding 할 때 이용
![image](https://user-images.githubusercontent.com/76824611/117885469-dc1d5a00-b2e8-11eb-9fe4-d2809610d60b.png)


## self-attention 원리      
여러 가지 벡터들을 통해서 어떻게 self-attention 을 계산할 수 있을까?     
* 행렬을 이용해서 이것이 실제로 어떻게 구현돼 있는지 확인   

### 벡터로 설명

**[1단계]**
encoder에 입력된 벡터들(각 단어의 embedding 벡터입니다)에서 부터 각 3개의 벡터를 만들어냄     
우리는 각 단어에 대해서   
* Query 벡터  
* Key 벡터  
* Value 벡터를 생성.   
이 벡터들은 입력 벡터에 대해서 세 개의 학습 가능한 행렬들을 각각 곱함으로써 만들어 짐     

 
주의)      
```기존의 벡터 사이즈 < 이 새로운 벡터 사이즈```       
* multi-head attention의 계산 복잡도를 일정하게 만들고자 내린 구조적인 선택        
![image](https://user-images.githubusercontent.com/76824611/117889352-60261080-b2ee-11eb-99f6-1a945103f665.png)


**[2 단계]**    
점수를 계산    
아래 예시의 첫 번째 단어인 “Thinking”에 대해서 self-attention 을 계산한다고 하면,    
우리는 이 단어와 입력 문장 속의 다른 모든 단어들에 대해서 각각 점수를 계산해야 함     
* **점수**: 현재 위치의 이 단어를 encode 할 때 **다른 단어들에** 대해서 **얼마나 집중**을 해야 할지를 결정     
  * 점수 계산: **현재 단어**의 ```query vector```와 점수를 매기려 하는 **다른** 위치에 있는 단어의 ```key vector```의 내적(얼마나 관련도가 높니?)         
  * 다시 말해, 우리가 위치에 있는 단어에 대해서 self-attention 을 계산한다 했을 때,     
     * 첫 번째 점수는 q1과 k1의 내적.    
     * 두 번째 점수는 q1과 k2의 내적.      

![image](https://user-images.githubusercontent.com/76824611/117890385-0888a480-b2f0-11eb-96fd-9080acff8a04.png)

**[3 단계]**     
1) $$(이 점수들)/{√key 벡터의 사이즈(key 벡터의 사이즈의 제곱근)}$$ = 더 안정적인 gradient    
2) 이 값(더 안정적인 gradient)을 ```softmax``` 계산을 통과시켜 모든 점수들을 **양수**로 만들고 **그 합을 1**으로 만들어 줌      
* softmax 점수: 현재 위치의 단어의 encoding에 있어서 **얼마나** **각 단어들의 표현**이 **들어갈 것**인지를 결정함
* 당연하게 **현재 위치의 단어**가 가장 **높은 점수**, 가장 많은 부분차지     
* 가끔은 **현재 단어에 관련이 있는 다른 단어**에 대한 정보가 들어가는 것이 도움이 됨       

**[4단계]**    
$$(입력의 각 단어들의 value 벡터)* (softmax 점수)$$    
* 우리가 집중을 하고 싶은 관련이 있는 단어들은 그래도 남겨두고,      
* 관련이 없는 단어들은 0.001 과 같은 작은 숫자 (점수)를 곱해 없애버리기 위함.     


**[5 단계]**      
이 점수로 곱해진 weighted value 벡터들을 다 더함.      
이 단계의 출력이 바로 현재 위치에 대한 **self-attention layer의 출력**  
* 이 결과로 나온 벡터를 ```feed-forward ``` 신경망으로 보냄     


![image](https://user-images.githubusercontent.com/76824611/117892522-e1cc6d00-b2f3-11eb-8d01-db64003bb264.png)


### 실제는 행렬 계산
실제 구현에서는 빠른 속도를 위해 이 모든 과정들이 ~~벡터~~가 아닌 **행렬**의 형태로 진행    

**[1단계]**     
입력 문장에 대해서 Query, Key, Value 행렬들을 계산    
* 이를 위해 입력 벡터들(embedding 벡터들)을 하나의 행렬 X로 쌓아 올리고,    
* 그것을 우리가 학습할 weight 행렬들인 WQ, WK, WV 로 곱함.      

행렬 X의 각 행: 입력 문장의 각 단어에 해당      
* 여기서 다시 한번 embedding 벡터들(크기4)와 query/key/value 벡터들(크기 3) 간의 크기 차이확인 가능     
![image](https://user-images.githubusercontent.com/76824611/117896262-227bb480-b2fb-11eb-995a-46ce940e68c5.png)


**[2단계]**         
현재 행렬을 이용하고 있으므로 앞의 self-attention 계산 단계 ```2~5 단계```를 하나의 식으로 압축 가능      

![image](https://user-images.githubusercontent.com/76824611/117896268-24457800-b2fb-11eb-9beb-9db5a01eea2e.png)

---


# The Beast With Many Heads
 
위의 ```self-attention layer```에 ``` “multi-headed” attention```이라는 메커니즘을 더해 더욱더 이를 개선      
이것은 두 가지 방법으로 **attention layer의 성능을 향상시킴**  

**[1]** 모델이 **다른 위치에 집중하는 능력**을 확장시킴     
z1 이 모든 다른 단어들의 encoding 을 조금씩 포함했지만,    
사실 이것은 실제 **자기 자신**에게만 **높은 점수**를 줘 자신만을 포함해도 됐을 것임    
* 이것은 “그 동물은 길을 건너지 않았다 왜냐하면 그것은 너무 피곤했기 때문이다” 과 같은 문장을 번역할 때,     
*  “그것”이 무엇을 가리키는지에 대해 알아낼 때 유용

**[2]** ```attention layer``` 가 여러 개의 “representation 공간”을 가지게 해줌        
 multi-headed attention을 이용함으로써 우리는 여러 개의 query/key/value weight 행렬들을 가지게 됨     
* 밑의 그림은 5개의 attention heads을 가지므로 각 encoder/decoder마다 이런 5개의 세트를 가지게 되는 것   
* 이 각각의 query/key/value set는 랜덤으로 초기화되어 학습됨    
  *  학습이 된 후 각각의 세트: 입력 벡터들에 곱해져 벡터들을 각 목적에 맞게 투영시키게 됨     
  *  이러한 세트가 여러개 있다는 것은 각 벡터들을 각각 다른 representation 공간으로 나타낸 다는 것을 의미 


## 각 head에 대한 Q/K/V 행렬 생성
```multi-headed attention```을 이용하기 위해서,    
* 각 head를 위해서 각각의 다른 query/key/value weight 행렬들을 모델에 가짐.    
* 이전에 설명한 것과 같이 우리는 입력 벡터들의 모음인 행렬 X를 WQ/WK/WV 행렬들로 곱해 각 head에 대한 Q/K/V 행렬들을 생성
![image](https://user-images.githubusercontent.com/76824611/117897183-2f99a300-b2fd-11eb-9cb0-3a45d889adb8.png)

![image](https://user-images.githubusercontent.com/76824611/117898459-f6aefd80-b2ff-11eb-8585-6c2b792e408c.png)

## 서로 다른 Z 행렬  
```self-attention``` 계산 과정을 5개의 다른 weight 행렬들에 대해 5번 거치게 되면, 우리는 5개의 서로 다른 Z 행렬을 가지게 됨    
![image](https://user-images.githubusercontent.com/76824611/117899236-a5a00900-b301-11eb-8cf3-bb26c697a066.png)

**[problem]**     
이 5개의 행렬을 바로 ```feed-forward layer```으로 보낼 수 없음     
* ```feed-forward layer``` 은 **한 위치**에 대해 오직 **한 개의 행렬**만을 **input**으로 받을 수 있음.     
* 그러므로 우리는 이 5개의 행렬을 하나의 행렬로 합치는 방법 필요     

**[solve]**       
1) 모두 이어 붙여서 하나의 행렬로 만들어버리고,     
2) 그다음 하나의 또 다른 weight 행렬인 W0을 곱함     




