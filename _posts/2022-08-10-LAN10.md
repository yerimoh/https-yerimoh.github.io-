---
title: "Imputing Out-of-Vocabulary Embeddings with LOVE Makes Language Models Robust with Little Cost 정리"
date:   2022-08-10
excerpt: "Imputing Out-of-Vocabulary Embeddings with LOVE Makes Language Models Robust with Little Cost"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---


# Abstract
State-of-the-art NLP systems represent inputs
with word embeddings, but these are brittle
when faced with Out-of-Vocabulary (OOV)
words. To address this issue, we follow
the principle of mimick-like models to generate vectors for unseen words, by learning
the behavior of pre-trained embeddings using only the surface form of words. We
present a simple contrastive learning framework, LOVE, which extends the word representation of an existing pre-trained language
model (such as BERT), and makes it robust to OOV with few additional parameters.
Extensive evaluations demonstrate that our
lightweight model achieves similar or even better performances than prior competitors, both
on original datasets and on corrupted variants.
Moreover, it can be used in a plug-and-play
fashion with FastText and BERT, where it significantly improves their robustness.

----

# 1 Introduction
Word embeddings represent words as vectors (Mikolov et al., 2013a,b; Pennington et al.,
2014). They have been instrumental in neural network approaches that brought impressive performance gains to many natural language processing
(NLP) tasks. These approaches use a fixed-size vocabulary. Thus they can deal only with words that
have been seen during training. While this works
well on many benchmark datasets, real-word corpora are typically much noisier and contain Out-ofVocabulary (OOV) words, i.e., rare words, domainspecific words, slang words, and words with typos,
which have not been seen during training. Model
performance deteriorates a lot with unseen words,
and minor character perturbations can flip the prediction of a model (Liang et al., 2018; Belinkov
and Bisk, 2018; Sun et al., 2020; Jin et al., 2020).
Simple experiments (Figure 1) show that the addition of typos to datasets degrades the performance
for text classification models considerably.
To alleviate this problem, pioneering work pretrained word embeddings with morphological features (sub-word tokens) on large-scale datasets (Wieting et al., 2016; Bojanowski et al., 2017; Heinzerling and Strube, 2017; Zhang et al., 2019). One
of the most prominent works in this direction is
FastText (Bojanowski et al., 2017), which incorporates character n-grams into the skip-gram model.
With FastText, vectors of unseen words can be imputed by summing up the n-gram vectors. However, these subword-level models come with great
costs: the requirements of pre-training from scratch
and high memory footprint. Hence, several simpler approaches have been developed, e.g., MIMICK (Pinter et al., 2017), BoS (Zhao et al., 2018)
and KVQ-FH (Sasaki et al., 2019). These use only
the surface form of words to generate vectors for
unseen words, through learning from pre-trained
embeddings.
Although MIMICK-like models can efficiently
reduce the parameters of pre-trained representa
tions and alleviate the OOV problem, two main
challenges remain. First, the models remain bound
in the trade-off between complexity and performance: The original MIMICK is lightweight but
does not produce high-quality word vectors consistently. BoS and KVQ-FH obtain better word
representations but need more parameters. Second, these models cannot be used with existing
pre-trained language models such as BERT. It is
these models, however, to which we owe so much
progress in the domain (Peters et al., 2018; Devlin
et al., 2019; Yang et al., 2019; Liu et al., 2020). To
date, these high-performant models are still fragile
when dealing with rare words (Schick and Schütze,
2020), misspellings (Sun et al., 2020) and domainspecific words (El Boukkouri et al., 2020).
We address these two challenges head-on: we design a new contrastive learning framework to learn
the behavior of pre-trained embeddings, dubbed
LOVE, Learning Out-of-Vocabulary Embeddings.
Our model builds upon a memory-saving mixed
input of character and subwords instead of n-gram
characters. It encodes this input by a lightweight
Positional Attention Module. During training,
LOVE uses novel types of data augmentation and
hard negative generation. The model is then able to
produce high-quality word representations that are
robust to character perturbations, while consuming only a fraction of the cost of existing models.
For instance, LOVE with 6.5M parameters can obtain similar representations as the original FastText
model with more than 900M parameters. What is
more, our model can be used in a plug-and-play
fashion to robustify existing language models. We
find that using LOVE to produce vectors for unseen
words improves the performance of FastText and
BERT by around 1.4-6.8 percentage points on noisy
text – without hampering their original capabilities
(As shown in Figure 2).
In the following, Section 2 discusses related
work, Section 3 introduces preliminaries, Section 4 presents our approach, Section 5 shows our
experiments, and Section 6 concludes. The appendix contains additional experiments and analyses. Our code and data is available at https:
//github.com/tigerchen52/LOVE

