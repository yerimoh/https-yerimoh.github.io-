---
title: "New Intent Discovery with Pre-training and Contrastive Learning 정리"
date:   2022-08-31
excerpt: "Between words and characters:A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---


   

# 원 논문
[Enriching Word Vectors with Subword Information](https://arxiv.org/abs/2112.10508)

**[소스코드]**     
* [git adress](https://github.com/zhang-yu-wei/MTP-CLNN)


**[사전 학습]**
* 읽기 전 아래의 포스트들을 읽어야 무슨소린지 알아듣기 편하다..!   
* [contrastive learning](https://daebaq27.tistory.com/97)    

---

# Abstract
새로운 의도(intent) 발견은 지원되는 **의도(intent) 클래스 세트**를 확장하기 위해,       
**사용자 발화에서 새로운 의도 범주를 발견**하는 것을 목표로 한다.       

이 발견은 실질적인 대화 시스템의 개발과 서비스 확대를 위한 중요한 과제이다.   

이러한 중요성에도 불구하고, 이 문제는 여전히 문헌에서 충분히 연구되지 않고 있다.  

**[기존 접근 방식의 문제]**        
* 일반적으로 레이블이 지정된 발화(labeled utterances)에 의존     
* 표현 학습 및 클러스터링을 위해 pseudo-labeling 방법을 사용 ➡ 이는 레이블 집약적이고 비효율적이며 부정확하다.     



**[본 논문의 해결책]**    
* 우리는 새로운 의도 발견을 위한 두 가지 중요한 연구 질문에 대한 새로운 해결책을 제공한다:    
* **(1)** 의미론적 발화 표현을 배우는 방법     
* **(2)** 발화를 더 잘 클러스터링하는 방법      
* 특히, 우리는 먼저 표현 학습을 위해 외부 레이블이 지정된 데이터와 함께 레이블이 지정되지 않은 풍부한 데이터를 활용하는 multi-task pre-training 전략을 제안한다.     
* 그런 다음 클러스터링을 위해 레이블이 지정되지 않은 데이터에서 self-supervisory 신호를 이용하기 위해 새로운 대조 손실(contrastive loss)을 설계한다.    
* 세 가지 의도 인식 벤치마크에 대한 광범위한 실험은 제안된 방법의 높은 효과를 입증하는데, 이는 비지도 및 준지도 시나리오 모두에서 최첨단 방법을 큰 폭으로 능가한다. 


---
---


# **1 Introduction**

## Why Study New Intent Discovery (NID)      
최근 몇 년 동안 대화형 AI 애플리케이션의 급속한 성장을 목격했다.   

**[문제]**      
자연어 이해 시스템(NLU)을 설계하기 위해, 의도 인식 모델(intent recognition model)을 훈련시키기 위해 **미리** **예상 고객의 의도 세트(predefined intents)를 수집**한다.    
그러나 미리 정의된 의도(predefined intents)는 고객의 요구를 완전히 충족시킬 수 없다.      
➡ 이는 레이블이 지정되지 않은 사용자 발화에서 발견된 새로운 의도를 반복적으로 통합하여 **의도 인식 모델을 확장할 필요성**을 시사한다
![image](https://user-images.githubusercontent.com/76824611/187633563-3b928c35-4da8-42d9-b82b-b4f7be06c397.png)


**[해결책]**      
다수의 발화에서 알려지지 않은 의도를 수동으로 식별하는 노력을 줄이기 위해, 이전 연구는 일반적으로 유사한 의도를 가진 발화를 그룹화하기 위해 clustering algorithms을 사용한다(Cheung and Li, 2012; Hakkani-Tür et al., 2015; Padmasundari, 2018).     
➡ 이후 클러스터 할당은 **새로운 의도 레이블로 직접 사용**하거나 **더 빠른 주석을 위한 휴리스틱**으로 사용가능함


**[개요]**    
아래의 2가지 방법으로 NID를 해결
1) multi-task pre-training method     
2) 클러스터링을 위한 contrastive learning      


----


## Research Questions (RQ) and Challenges

NID centers에 대한 현재 연구는 두 가지 기본 연구 질문을 중심으로 이루어진다.     

**1)** 클러스터링을 위한 적절한 단서를 제공하기 위해 **의미론적 발화 표현을 배우는** 방법?    
**2)** 발화를 **더 잘 클러스터링**하는 방법은 무엇인가?   

그 두 질문에 대한 연구는 종종 기존 연구에서 서로 얽혀 있다.    
발화는 다양한 요소에 따라 표현된다 (언어의 스타일, 관련된 주제 또는 심지어 문장의 길이와 같은 다양한 측면)     

**[사전 연구의 한계]**    
클러스터링에 대한 적절한 단서를 제공하기 위해 **의미론적 발성(semantic utterance) 표현을 배우는 것**이 중요하다.     
* **vanilla pre-trained language model:**     
    * 발성 표현을 생성하기 위해 vanilla pre-trained language model (PLM)을 단순히 적용하는 것은 실행 가능한 해결책이 아님      
    * 이는 4.2절의 실험 결과에서 알 수 있듯이 NID에서 성능이 저하된다.      
* **known intents의 labeled utterances 사용:**     
    * 일부 최근 연구는 표현 학습(representation learning)을 위해 known intents의 labeled utterances 사용할 것을 제안했지만,     
      특히 개발의 초기단계에서 그들은 상당한 양의 이미 알려진 의도와 항상 사용 가능한 각 의도의 **이미 충분히 레이블링된 발화**를 필요로 한다.          
      (Forman 등, 2015; Haponchyk 등, 2018; Lin 등, 2020; Zhang 등, 2021c; Haponchyk 및 Moschitti, 2021)
* **pseudo-labeling 접근 방식:**        
    * 또한 pseudo-labeling 접근 방식은 표현 학습 및 클러스터링을 위한 감독 신호(supervision signals)를 생성하기 위해 종종 사용된다.      
    * **Lin et al. (2020):** 레이블이 지정된 발화에 대한 **발화 유사성 예측 작업**(utterance similarity prediction task)으로,      
    vanilla pre-trained language model(PLM)을 미세 조정하여 **유사 레이블이 있는 레이블이 없는 데이터를 train**한다.      
    * **Zhang et al.(2021c):**은 k-means 클러스터링을 사용하여 **pseudo-labels을 생성하는 심층 클러스터링 방법**(Caron et al., 2018)을 채택한다.          
    ➡ 그러나 pseudo-labels은 종종 노이즈가 많고 오류 전파를 초래할 수 있다.      









---



## Our Solutions    

본 연구에서는 각 연구 질문에 대한 간단하면서도 효과적인 해결책을 제안한다.      

**[Solution to RQ 1: multi-task pre-training]**          
* representation learning을 위해 외부 데이터와 내부 데이터를 모두 활용하는 a multi-task pre-training 전략을 제안한다.      
* **방법**: NID에 대한 task-specific utterance representations을 학습하기 위해 pre-trained PLM을 fine-tune한다.      
   * **fine-tune 방법 1:** 본 논문에선 Zhang et al. (2021d)에 이어 공개적으로 사용 가능한 고품질 의도 탐지 데이터 세트( high-quality intent detection datasets) 이용          
   * **fine-tune 방법 2:** 현재 도메인에서 제공되는 레이블이 지정된 및 레이블이 지정되지 않은 발화를 활용 
* **효과:** multi-task 학습 전략은 일반적인 의도 탐지 작업에서 **특정 응용 프로그램 영역으로 지식을 전달**할 수 있도록 한다.     

**[Solution to RQ 2: contrastive learning with nearest neighbors]**         
* 본 논문에선 **대조적 손실을 사용하여 소형 클러스터(contrastive loss to produce compact clusters)를 생성**할 것을 제안한다.           
   * 이 학습은 컴퓨터 비전(Bachman et al., 2019; He et al., 2019; Chen et al., 2020; Khosla et al., 2020)과 자연어 처리(Gunel et al., 2021; Gao et al., 2021; Yan et al. 2021) 모두에서 최근 대조 학습의 성공에 동기를 부여할 정도로 유의미한 방법임.       
* **contrastive learning(대조 학습):**   
   * 사전에 정답 데이터를 구축하지 않는 판별 모델     
   * 학습된 표현 공간 상에서 **"비슷한" 데이터는 가깝게**, "다른" 데이터는 멀게 존재하도록 **표현 공간을 학습**         
   * 이후, classification 등 다양한 downstream task에 대해서 네트워크를 fine-tuning시키는 방향으로 활용함.    
   * **장점:**     
      * 데이터 구축 비용이 들지 않음     
      * 학습 과정에 있어서 보다 용이      
      * label이 없기 때문에 보다 일반적인 feature representation 가능      
      * 새로운 class가 들어와도 대응이 가능    
   * **단점:** 그러나 일반적으로 사용되는 인스턴스 식별 작업(instance discrimination task)은 false negatives을 밀어내고 클러스터링 성능을 해칠 수 있다.           
   * 아래 두 개의 방법 모두에 따라 **클러스터링을 위한 contrastive loss을 맞춤화**하기 위한 **neighborhood relationship를 소개**한다.
     * **방법 1:** unsupervised(즉, 알려진 intents의 labeled utterances없이)     
     * **방법 2:** semi-supervised scenarios 
   * 직관적으로, 의미론적 특징 공간에서, 이웃한 발화는 유사한 의도를 가져야 하며, 이웃한 샘플을 함께 당기면 클러스터가 더 콤팩트해진다.      
* **본 논문의 3가지  main contributions**    
   * 우리는 제안된 multi-task pretraining 방법이 이미 unsupervised NID와 semi-supervised NID 모두에 대한 최첨단 모델에 비해 큰 성능 향상으로 이어진다는 것을 보여준다.     
   * 우리는 이웃 관계를 contrastive 학습 목표에 통합하여 NID에 대한 self-supervised clustering 방법을 제안하여 성능을 더욱 향상시킨다.       
   * 우리는 방법의 효과를 검증하기 위해 세 가지 벤치마크 데이터 세트에 대한 광범위한 실험과 절제 연구를 수행한다.     




----
----

# **2. Related Works**

## New Intent Discovery.    

NID에 대한 연구는 아직 초기 단계에 있다. 선구적인 작업은 unsupervised clustering 방법에 중점을 둔다.     

**[선행연구]**    
* **Shi et al. (2018):** 자동 인코더를 활용하여 기능을 추출.   
* **Perkins and Yang (2019):** 대화에서 발언의 맥락을 고려            
* **Chatterjee and Sengupta (2020):** 밀도 기반 모델(density-based models)을 개선할 것을 제안             
* **일부 최근 연구(Haponchyk et al., 2018; Haponchyk and Moschitti, 2021):** intent labeling을 위한  supervised clustering 알고리즘을 연구했지만 **새로운 의도를 처리할 수 없다**.         
* **다른 작업 라인(Forman et al., 2015; Lin et al., 2020; Zhang et al., 2021c):** 종종 semisupervised NID로 불리는 **알려지지 않은 intents의 발견을 지원**하기 위해 알려진 일부 intents가 제공되는 보다 실용적인 사례를 조사했다.   
* **Lin et al.(2020)**:  semi-supervised NID를 다루기 위해 먼저 문장 **유사성 작업**으로 **알려진 intents**에 대한 지도 교육을 수행한 다음 **레이블이 지정되지 않은 발화**에 대한 pseudo labeling을 사용하여 더 나은 임베딩 공간을 학습할 것을 제안      
* **Zhang et al.(2021c)**: 먼저 알려진 intents를 pre-train한 다음 k-means clustering을 수행하여 Deep Clustering에 이어 representation learning을 위해 레이블이 지정되지 않은 데이터에 pseudo labeling을 할당할 것을 제안했다      
* **Caron et al., (2018):** 최상위 계층의 학습을 가속화하기 위해 **클러스터를 정렬**할 것을 제안했다.     
* **Vedula et al., 2020; Zhang et al., 2021b:** 먼저 발화를 알려진 것과 알려지지 않은 것으로 분류한 다음 알려지지 않은 발화로 새로운 의도를 밝히는 것이다. 따라서, 그것은 **첫 번째 단계에서 정확한 분류에 의존**한다.      


**[본 연구]**      
본 연구에서, 우리는 표현 학습을 위한 multi-task pre-training method와 클러스터링을 위한 contrastive learning 방법을 제안하여 NID를 해결한다.    
pre-train을 위해 현재 도메인의 주석이 달린 충분한 데이터에 의존하는 이전 방법과 달리, 우리의 방법은 unsupervised한 환경에서 사용될 수 있고 data-scarce scenarios에서 잘 작동할 수 있다(섹션 4.3).     



------



## Pre-training for Intent Recognition.    
**[문제]**    
large-scale pre-trained 언어 모델의 효과에도 불구하고,     
pre-trained 데이터 세트와 대화 사이의 언어 행동의 고유한 불일치의 문제가 지속되고있다.        


**[사전 연구]**    
대부분의 이전 연구는 selfsupervised 방식으로 개방형 도메인 대화에 대한 pre-train을 제안했다      
최근 여러 작품에서 **관련 과제를 가진 pre-train**이 **intent 인식에 효과적**일 수 있다는 지적이 나왔다.     
* Zhang et al. (2020): intent 인식을 문장 유사성 과제로 공식화하고 자연어 추론(NLI) 데이터 세트에 대해 pre-train을 받았다.       
* Vulic et al.(2021); Zhang et al.(2021e): intent 탐지 작업에서 contrastive loss로 사전 훈련을 받았다.      


**[본 논문]**     
본 논문의 multi-task pre-training method는 현재 **도메인의 레이블이 없는 데이터**와 **공개적으로 사용 가능한 intent 데이터 세트**를 활용하여 few-shot intent 탐지 성능을 향상시키는 Zhang 외(2021d)에서 영감을 받았다.      
✨ 그러나 **레이블이 없는 발화의 자연스러운 존재**로 인해 본 논문이 제안한 방법이 NID에 적용하기에 더 적합하다고 주장한다.






-----


## Contrastive Representation Learning.   
Contrastive learning은 컴퓨터 비전에서 유망한 결과를 보여주었고 자연어 처리에서 인기를 얻었다.      


**[사전 연구]**    
* **일부 최근 연구:** 문장 임베딩을 학습하기 위해 unsupervised Contrastive learning을 사용했다    
구체적으로, contrastive loss이 비등방성(anisotropic) 임베딩 공간을 피할 수 있음을 보여주었다.    
* **Kim et al.(2021)**: BERT 표현의 품질을 향상시키기 위한 self-guided contrastive training을 제안했다.    
* **Giorgi et al.(2021):** 인근 문장에서 무작위로 샘플링된 텍스트 세그먼트를 대조하여 universal sentence encoder를 사전 교육할 것을 제안했다.     
* **Zhang 외(2021e):** self-supervised contrastive pre-training과 supervised contrastive fine-tuning이 few-shot intent 인식에 도움이 될 수 있음을 입증했다.    
* **Zhang et al. (2021a):** contrastive loss을 클러스터링 목표와 결합하면 짧은 텍스트 클러스터링을 개선할 수 있음을 보여주었다.    


<details>
<summary>📜 anisotropic 설명 보기</summary>
<div markdown="1">
   
**Isotropic:** 벡터가 주어진 공간에서 모든 방향으로 존재하는 상황을 의미한다. 임베딩에 대입해보면 주어진 보캡의 임베딩 벡터들이 사방팔방으로 향해 있는 상황이다.


**anisotropic:** 벡터가 주어진 공간에서 특정 방향으로만 향해 있어서 일종의 cone 형태를 이루고 있는 것을 말한다  


**이상적인** 상황이라면 임베딩 벡터는 **Isotropic**해야 한다.    
그래야 각 단어의 의미가 명확히 구분되고, 유사한 단어는 서로 가깝게, 영 뚱딴지인 두 단어는 서로 멀리 분포하게 될 것이다. 만약 Anisotropic하게 임베딩 벡터가 분포해 있다면, 단어의 의미/문법적 유사도와 관계없이 모든 단어들이 가깝게 분포하기 때문에 각 단어의 의미가 명확히 구분되지 않을 우려가 있다.   
![image](https://user-images.githubusercontent.com/76824611/187747971-fbdb3733-a539-4858-a2ef-6c6dcc4a1488.png)

[출처](https://velog.io/@stapers/Contextual-Embedding-How-Contextual-are-Contextualized-Word-Representations)

  
</div>
</details>  

**[본 논문]**     
본 논문이 제안한 contrastive loss는 클러스터링에 맞게 조정되어,     
**유사한 의미론**을 가진 **발화**를 함께 **그룹화**하고     
**기존의 대조 손실**에서처럼 **false negatives을 밀어내는 것을 피한다**.    



-----
-----

# **3 Method**    



## Problem Statement

**[이미 라벨링 된 데이터를 준비]**     
* intent recognition model을 개발하기 위해,      
* 주석이 달린 몇 가지 발언(annotated utterances)과 함께       
* 각 intent에 대한 아래 식 준비.     
* $$D_{known}^{labeled}= {(x_i, y_i)|y_i ∈ C_k}$$    
   * $$C_{k}$$: a set of expected intents, predefined (known) intents    

**[라벨링 된 데이터와 함꼐 라벨링 되지 않은 데이터 준비]**     
* 배포(deployed)된 후,     
* 시스템은 predefined (known) intents $$C_k$$와 unknown intents $$C_{u}$$ 모두의 경우가 있는 발화를 준비한다      
* $$D^{unlabeled}= {x_i | y_i∈ {C_k, C_u}}$$     
   * $$C_{u}$$: unknown intents       
   * $$C_{k}$$: a set of expected intents, predefined (known) intents   


**[새로운 intent 발견(NID; new intent discovery)의 목적]**          
* $$D^{unlabeled}$$에서 새로운 intent $$C_{u}$$를 식별하는 것      


**[NID]**    
* OOD(Out-of-Distribution) 탐지의 직접적인 확장으로 볼 수 있음.      
* 여기서 OOD 예를 식별할 뿐만 아니라 기본 클러스터를 발견해야 한다.     
* NID는 또한 훈련 중에 **모든 class information에 대한 액세스를 가정하지 않는다**는 점에서 **제로샷 학습과 다르다**.     


이 연구에서 우리는  Zhang et al(2021c)에 이어 $$D_{known}^{labeled}$$의 존재로 구별되는 unsupervised NID와  semi-supervised NID를 모두 고려한다.      

----

## Overview of Our Approach  
아래 그림과 같이 본 논문은 Sec. 1에 언급된 연구 질문을 다루는 **2단계 프레임워크**를 제안한다.     

**[STAGE 1: MTP]**     
* **multi-task pre-training (MTP) 수행:** 외부 labeled data에 대한 crossentropy loss과 레이블링되지 않은 target 데이터에 대한 self supervised loss을 공동으로 최적화(3.1절).      

**[STAGE 2: MTP]**   
* **contrastive learning with nearest neighbors (CLNN) 수행:** 두 번째 단계에서는 임베딩 공간에서 각 훈련 인스턴스의 상위 K개의 가장 가까운 이웃을 먼저 발굴한 다음 가장 가까운 이웃과의 대조 학습을 수행한다(3.2장).    
* training 후, 우리는 clustering의 결과를 얻기 위해 간단한 **parametric clustering algorithm**을 사용한다.      

 ![image](https://user-images.githubusercontent.com/76824611/187759519-4f532a4b-8bed-43cc-996a-7eadfb0b6e85.png)


-----

## 3.1 Stage 1: Multi-task Pre-training (MTP)
**[아래의 1)과 2)를 결합한 multi-task pre-training 목표를 제안]**     
* 1) 공개적으로 사용 가능한 intent detection 데이터 세트의 외부 데이터에 대한 분류 작업     
* 2) 현재 도메인의 내부 데이터에 대한 self-supervised learning 작업     


**[본 논문의 차별점]**     
선행 연구(Lin et al., 2020; Zhang et al., 2021c)와 달리, 본 논문의 pre-training 방법은 현재 도메인의 **주석이 달린 데이터($$D_{known}^{labeled}$$)에 의존하지 않는다**.   
➡ unsupervised 않은 환경에서 적용될 수 있다.     


**[구제적인 과정]**     
* **1)** 먼저 [pre-trained BERT](https://yerimoh.github.io/Lan2/#1-pre-training-bert) 인코더로 모델을 초기화한다 (Devlin et al., 2019).     
* **2)** 그런 다음 joint pre-training loss을 사용한다 (Zhang et al. (2021d)).   



<details>
<summary>📜 joint pre-training loss 보기</summary>
<div markdown="1">

**[joint pre-training loss 값의 구성]**    
![image](https://user-images.githubusercontent.com/76824611/188257427-282551fa-58f4-48c0-926f-1e35a2604052.png)
   * θ: 모델 parameters    
   
**$$[L_{ce}$$]**   
* 외부 external labeled data의 [crossentropy loss](https://yerimoh.github.io/DL3/#%EA%B5%90%EC%B0%A8-%EC%97%94%ED%8A%B8%EB%A1%9C%ED%94%BC-%EC%98%A4%EC%B0%A8cross-entropy-error-cee)    
* **$$D_{known}^{labeled}$$**: supervised classification을 위해 외부 레이블로 표시된 다양한 도메인(예: CLINC150(Larson et al., 2019), 즉 $$D_{known}^{labeled}$$)을 가진 외부 공공 intent dataset를 활용한다.    
* **목표:** 직관적으로 외부 intent datasets에서 주석이 달린 발화를 사용하여 intent 인식에 대한 일반적인 지식(general knowledge)을 학습하는 것    

**$$[L_{mlm}]$$**     
* 현재 도메인에서 사용 가능한 모든 데이터의 masked language modelling(MLM) 손실로 구성된다.    
* **$$D_{internal}^{all}$$**: $$D_{internal}^{all}$$로 표시된 현재 도메인에서 사용 가능한 모든 데이터(labeled or unlabeled)를 사용한다.     
* **목표:** 현재 도메인에서 수집된 발화를 사용하여 도메인별 의미론을 학습   

**[공통]**        
* 두 작업은 모두 **의미론적 발화 표현**을 학습하여 후속 클러스터링 작업에 대한 적절한 단서를 제공      
* 두 작업 모두 NID에 필수적이다.      


**[semi-supervised NID]**    
* semi-supervised NID의 경우, 위의 식에서 $$D_{labeled}^{external}$$를 $$D_{known}^{labeled}$$로 대체하여 현재 도메인의 주석이 달린 데이터를 추가로 활용하여 지속적인 사전 훈련을 수행할 수 있다.    
* 이 단계는 unsupervised NID에 포함되지 않는다

   
 ![image](https://user-images.githubusercontent.com/76824611/187759519-4f532a4b-8bed-43cc-996a-7eadfb0b6e85.png)

</div>
</details>  



-----

##  Stage 2: Contrastive Learning with Nearest Neighbors (CLNN)

 
**[contrastive learning 제안]**    
clustering에 대한 **간결한 표현**을 학습하기 위해 **인접 인스턴스를 모으고** 임베딩 공간에서 **먼 인스턴스를 밀어냄**      

**[a simple example of CLNN]**
![image](https://user-images.githubusercontent.com/76824611/188260026-8c943b50-82c6-4e4e-9c50-e82e6a95f4b9.png)


 . A batch of four training instances {xi}
4
i=1 (solid
markers) and their respective neighborhoods {Ni}
4
i=1 are plotted (hollow markers within large circles). Since x2
falls within N1, x2 along with its neighbors are taken as positive instance for x1 (but not vice versa since x1 is not
in N2). We also show an example of adjacency matrix A0
and augmented batch B
0
. The pairwise relationships
with the first instance in the batch are plotted with solid lines indicating positive pairs and dashed lines indicating



**[구제적인 과정]**     
* **1)** 본 논문에선 먼저 1단계부터 pre-trained 모델로 utterance를 인코딩한다.      
* **2)** 그런 다음  임베딩 공간에서 가장 가까운 상위 K개의 이웃을 검색하여 이웃 $$N_i$$을 형성한다.     
    * 각 utterance $$x_i$$에 대해 inner product를  distance 메트릭으로 사용함   
    * $$N_i$$에서의 발화는 xi와 유사한 의도를 공유하도록 되어 있다.      
    * 훈련 중에, 우리는 발화의 미니 배치 B = {xi}Mi=1을 샘플링한다. 각 발성 xi b B에 대해 이웃 Ni에서 하나의 이웃 x 0 i를 균일하게 샘플링한 다음 데이터 확대를 사용하여 xi와 x 0 i에 대해 각각 x i i와 x 0 0 i를 생성한다. 여기서, 우리는 xii와 x 00i를 양의 쌍을 형성하는 xi의 두 개의 뷰로 취급한다. 그런 다음 생성된 모든 샘플과 함께 증강 배치 B 0 = {xpoti, xpot 0 i }M i = 1을 얻는다. 대조 손실을 계산하기 위해, 우리는 B0에 대한 인접 행렬 A0을 구성하는데, B0은 2M × 2M 이진 행렬이다. 1은 양의 관계(이웃이거나 준감독 NID에서 동일한 의도 레이블을 가지고 있음)를 나타내고 0은 음의 관계를 나타낸다. 따라서, 우리는 대조적인 손실을 다음과 같이 쓸 수 있다.


In the second stage, we propose a contrastive learning objective that pulls together neighboring instances and pushes away distant ones in the embedding space to learn compact representations
for clustering. Concretely, we first encode the utterances with the pre-trained model from stage 1.
Then, for each utterance xi
, we search for its topK nearest neighbors in the embedding space using
inner product as distance metric to form a neighborhood Ni
. The utterances in Ni are supposed to
share a similar intent as xi
. During training, we
sample a minibatch of utterances B = {xi}M
i=1. For
each utterance xi ∈ B, we uniformly sample one
neighbor x
0
i
from its neighborhood Ni
. We then
use data augmentation to generate x˜i and x˜
0
i
for xi
and x
0
i
respectively. Here, we treat x˜i and x˜
0
i
as
two views of xi
, which form a positive pair. We
then obtain an augmented batch B
0 = {x˜i
, x˜
0
i
}M
i=1
with all the generated samples. To compute contrastive loss, we construct an adjacency matrix A0
for B
0
, which is a 2M × 2M binary matrix where
1 indicates positive relation (either being neighbors
or having the same intent label in semi-supervised
NID) and 0 indicates negative relation. Hence, we
can write the contrastive loss as:







