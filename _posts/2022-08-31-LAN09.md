---
title: "New Intent Discovery with Pre-training and Contrastive Learning 정리"
date:   2022-08-31
excerpt: "Between words and characters:A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---


   

# 원 논문
[Enriching Word Vectors with Subword Information](https://arxiv.org/abs/2112.10508)





---

# Abstract
New intent discovery aims to uncover novel intent categories from user utterances to expand
the set of supported intent classes. It is a critical task for the development and service expansion of a practical dialogue system. Despite
its importance, this problem remains underexplored in the literature. Existing approaches
typically rely on a large amount of labeled
utterances and employ pseudo-labeling methods for representation learning and clustering,
which are label-intensive, inefficient, and inaccurate. In this paper, we provide new solutions
to two important research questions for new intent discovery: (1) how to learn semantic utterance representations and (2) how to better
cluster utterances. Particularly, we first propose a multi-task pre-training strategy to leverage rich unlabeled data along with external labeled data for representation learning. Then,
we design a new contrastive loss to exploit
self-supervisory signals in unlabeled data for
clustering. Extensive experiments on three intent recognition benchmarks demonstrate the
high effectiveness of our proposed method,
which outperforms state-of-the-art methods by
a large margin in both unsupervised and semisupervised scenarios. The source code will
be available at https://github.com/
zhang-yu-wei/MTP-CLNN.
