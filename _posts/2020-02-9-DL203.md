---
title: "[24] CS231N: Lecture 4 Neural Networks and Backpropagation"
date:   2020-02-9
excerpt: "Lecture 4 | Multi-layer Perceptron Backpropagation 요약"  
category: [Deep Learning]
layout: post
tag:
- Deep Learning
order: 0

comments: true
---



# 목차




------

👀 코드 보기 , 🤷‍♀️     
이 두개의 아이콘을 누르시면 코드, 개념 부가 설명을 보실 수 있습니다:)

------


[CS231N: Lecture 3](https://www.youtube.com/watch?v=d14TUNcbn1k&list=PLC1qU-LWwrF64f4QKQT-Vg5Wr4qEE1Zxk&index=4) 강의를 빠짐 없이 정리하였고, 어려운 개념에 대한 보충 설명까지 알기 쉽게 추가해 두었습니다.  


------

# **Intro**
[지난시간](https://yerimoh.github.io/DL201/)에 이어서,   
우리가 실제로 우리가 원하는것은 analytic gradient를 유도하고 사용하는 것이다            

하지만 동시에 analytic gradient를 사용한 응용에 대해서 수학적 검증도 필요하다
➡ 임의의 복잡한 함수를 통해 어떻게 **analytic gradient를 계산**하는지에 대해 이야기할 것이다.        
➡ computational graph(계산 그래프)라고 부르는 프레임워크를 사용할 것이다   


---
---

# **computational graph**
어떤 함수든지 computational graph(계산그래프)로 표현가능       
**노드**: 연산 단계를 나타냄        

**[EX]**   
* **식**: 예제는 우리가 지난시간에 배운 선형 classifier    
* $$x$$,$$W$$: input      
* **곱셈 노드**: 행렬 곱셈       
   * 파라미터 W와 데이터 x의 곱셈은 score vector를 출력    
* **hinge loss 노드**: 데이터 항  $$Lᵢ$$를 계산     
   * regularization 항을 계산    
   * 그 후, 우리의 최종 loss는 L은 regularization 항과 데이터 항의 합       
![image](https://user-images.githubusercontent.com/76824611/169669090-fc73415d-c119-47d0-afde-0bb56ea30f57.png)


**[효과]**     
* computational graph를 사용해서 함수를 표현하게 됨으로써, backpropagation가 사용가능해짐   
* backpropagation은 gradient를 얻기위해 computational graph 내부의 모든 변수에 대해 **chain rule을** **재귀적으로 사용**함    
* 이것은 우리가 매우 복잡한 함수를 이용하여 작업할때 아주 유용하게 사용됨      



**[EX: AlexNet(CNN)]**    
이 모델의 구성은 
* 가장 윗층(top): 입력 이미지가 들어감           
* 아래(bottom): loss가 있음         
➡ 입력 이미지는 loss function으로 가기까지 많은 layer를 거쳐 변형을 겪게 됨       
![image](https://user-images.githubusercontent.com/76824611/169677605-d3a95727-b18a-4c0a-b209-9f414035b992.png)



**[EX: neural turing machine(딥러닝 모델)]**    
* 미치게 복잡해보이지만 어짜피 계산 그래프기 때문에 결국 시간이 지남에 따라 이것을 풀게 될 것이다.
![image](https://user-images.githubusercontent.com/76824611/169678277-c59d95c6-4295-4abc-95bf-18be2cd363c1.png)


---
----


# Backpropagation
그럼 이제 계산그래프를 이용하여 역전파(Backpropagation)를 구해볼 것이다.    

**[기본 계산 원리]**     
그 전에 역전파의 원리에 대해서 간단하게 살펴보자.     
책에서는 그냥 넘어가 있어서 꼭 한번씩 읽어보고 포스트를 이어 읽는것이 이해에 많은 도움이 된다.    
* [덧셈 노드의 역전파](https://yerimoh.github.io/DL4/#%EB%8D%A7%EC%85%88-%EB%85%B8%EB%93%9C%EC%9D%98-%EC%97%AD%EC%A0%84%ED%8C%8C)     
* [곱셈 노드의 역전파](https://yerimoh.github.io/DL4/#%EA%B3%B1%EC%85%88-%EB%85%B8%EB%93%9C%EC%9D%98-%EC%97%AD%EC%A0%84%ED%8C%8C)     
* ➕ [예시로 완전히 이해하기](https://yerimoh.github.io/DL4/#%EC%98%88%EC%8B%9C-%EC%A0%81%EC%9A%A9)    






----


## 계산 


1️⃣ **computational grap로 나타내기**      
* 첫 번째 단계는 항상 함수 f를 이용해서 computational graph로 나타내는 것    
* $$x,y,z$$로 구성된 function $$f$$는 $$(x+y)* z$$라는 함수인 것 파악 가능     
* computational graph: 오른쪽 그림   
    * x,y에 대한 덧셈 노드 존재      
    * 다음 연산을 위한 곱셈 노드도 존재     
* 이 네트워크에 우리가 가지고 있는 값을 전달할 것임       
    * $$x= -2$$    
    * $$y= 5$$   
    * $$z= -4$$    
* 중간중간 값도 계산하여 computational graph에 값을 적어둠    
    * $$x+y = 3$$    
    * 마지막으로 최종 노드를 통과시켜 -12를 얻음         
![image](https://user-images.githubusercontent.com/76824611/169678807-19ef0469-e8d6-4fff-87b5-62f598e2fc70.png)


2️⃣ **gradient 표시**         
* 중간 변수에 이름붙임     
   * $$q$$: $$x+y$$ 덧셈 노드    
   * $$f$$: $$q* z$$ 곱셈 노드    
* 각각의 gradient를 표시       
   * 빨간색 네모   
       * $$x$$와 $$y$$에 각각에 대한 $$q$$의 gradient를 표시     
       * 단순 덧셈이기 때문에 1(원래 값 유지)     
   * 파란색 네모     
       *  $$q$$와 $$z$$에 대한 $$f$$의 gradient           
       *  곱셈규칙에 의해 각각 z와 q(서로 값이 바뀜)        
* 우리의 목표    
  *  $$x,y,z$$ 각각에 대한 f의 gradient 찾기
![image](https://user-images.githubusercontent.com/76824611/169679054-ecb9c7aa-095c-4fc5-b84a-48acf8748da6.png)



3️⃣ **backpropagation**       
* backpropagation은 [chain rule](https://yerimoh.github.io/DL4/#%EC%97%B0%EC%87%84%EB%B2%95%EC%B9%99-chain-rule)의 재귀적인 응용임        
    * chiain rule에 의해 뒤에서부터 gradient를 계산을 시작함      
* **3-1)** f 계산    
    * 우리는 출력의 $$f$$에 대한 gradient를 계산(가장 뒤)      
    * $$∂f/∂f$$=1      
    * $$f$$ gradient= 1      
    ![image](https://user-images.githubusercontent.com/76824611/169679373-e0f74d57-eb27-4cb8-ab52-f11a40aaa4d2.png)    
* **3-2)** z 계산     
   * $$q$$: $$z$$에 대한 $$f$$의 미분(위의 파란색 네모)([곱셈 법칙](https://yerimoh.github.io/DL4/#%EB%8D%A7%EC%85%88-%EB%85%B8%EB%93%9C%EC%9D%98-%EC%97%AD%EC%A0%84%ED%8C%8C)에 의해)      
   * $$∂f/∂z$$= $$q$$ = 3        
   * $$z$$ gradient= 3      
   ![image](https://user-images.githubusercontent.com/76824611/169679469-157869d0-877e-437c-bcb2-7662fe4aab31.png)
* **3-3)** q 계산      
   * $$z$$: $$q$$에 대한 $$f$$의 미분(위의 파란색 네모)([곱셈 법칙](https://yerimoh.github.io/DL4/#%EB%8D%A7%EC%85%88-%EB%85%B8%EB%93%9C%EC%9D%98-%EC%97%AD%EC%A0%84%ED%8C%8C)에 의해)      
   * $$∂f/∂q$$= $$z$$ = -4        
   * $$q$$ gradient= -4  
   ![image](https://user-images.githubusercontent.com/76824611/169680892-2982de2e-5506-40d1-bec1-cad1bbd473db.png)
* **3-4)** y 계산      
   * y에 대한 f의 미분값을 알고 싶지만 여기에서 y는 f와 바로 연결되어 있지 않음    
   * 구하기 위해서 chain rule을 이용함     
   * y에 대한 f의 미분은 q에 대한 f의 미분과 y에 대한 q의 미분의 곱으로 나타낼 수 있음   
   * 즉, q* f의 연산에서 q의 영향을 구할때와 동등함       
   * 즉,  y에 대한 q의 미분은 1이므로 q값 그대로 흘려보냄.       
       * 만약 우리가 y를 조금 변화시키면 q는 그것의 영향력 만큼 조금 변할것임.      
       * 이것은 내가 y를 조금 바꿨을때 그것이 q에 대해 1만큼의 영향력을 미치는 것을 의미함.       
   * $$∂f/∂y$$= $$z$$ = -4          
   * $$y$$ gradient= -4         
   ![image](https://user-images.githubusercontent.com/76824611/169681084-2a1d758c-1160-4a0d-bf91-efb85e7bea21.png)
* **3-5)** x 계산    
   * 3-4)와 똑같음      
   * f에 대한 x의 gradient는 -4 * 1 이므로 -4임.    
   ![image](https://user-images.githubusercontent.com/76824611/169681113-dfa75545-aba0-4919-87de-8f61e0315d52.png)






---


## 특징: local 계산  
위의 backpropagation에서 우리는 computational graph안에 모든 노드를 포함하고 있었다.    
하지만 각 노드는 오직 **주변에** 대해서만 관심이 있다.       
➡ 우리가 가지고 있는건 **각 노드**와 **각 노드의 local 입력**일 뿐 한 노드가 ~~전체~~를 보지 않는다.    


입력은 그 노드와 연결되어있고 값은 노드로 흘러들어가 이 노드로부터 출력을 얻게 된다.


그림을 통해 위의 특징을 이해해 보자,      
* 그래프 해석     
    * local 입력: x,y    
    * 출력: z          
* 이 노드에서 우리는 local gradient를 구할 수 있음      
    * z에서 x에 대한 gradient를 구할 수 있으며 y에 대한 gradient도 마찬가지이다.     
* 그것들은 쉽게 gradient를 구할 수 있고, 우리는 그것을 찾기 위해 복잡한 미적분을 할 필요가 없다.    
* 간단한 연산이다.      
![image](https://user-images.githubusercontent.com/76824611/169681387-d30d299a-dbb8-49e0-b660-67455d2e5bc4.png)
* chain rule을 사용해서, z에 대한 L의 gradient와 그리고 y에 대한 z의 gradient를 서로 곱한다.
     * chain rule에 의해 이 둘의 곱은 원하는 gradient를 준다.      
* 그리고 나서 우리가 gradient를 구하면 이 노드와 연결된 직전 노드로 전달가능하다.     
![image](https://user-images.githubusercontent.com/76824611/169681678-0b5670c2-52c8-43d2-8be7-fa2ea529ff49.png)


**[local 계산]**     
**각 노드**는 우리가 backpropagation을 통해 계산한 **local gradient**를 가지고 있다.     
이 값들은 상위 노드 방향으로 계속 전달되고,       
우리는 이것을 받아 local gradient와 곱하기만 하면 된다.     
➡ **우리는 노드와 연결된 노드 이외의 다른 어떤 값에 대하여 신경쓰지 않아도 된다.** 이 특징을 local 계산이라고 한다.               

----

## 복잡한 예제 
이번엔 조금 더 복잡한 예제로 계산해 보겠다.      
➡ backpropagation이 얼마나 유용한 방식인지를 알게 될 것이      



1️⃣ **computational grap로 나타내기**      
* 첫 번째 단계는 항상 함수 f를 이용해서 computational graph로 나타내는 것    
* $$w,x$$에 대한 함수 f는 $$1 / e^{-(w_0x_0 + w_1x_1 + w_2)}$$인 것 파악 가능     
* computational graph 분석       
    * $$w$$와 $$x$$에 대한 곱셈 노드 존재      
    * $$x_0$$과 $$x_0$$, $$x_1$$과 $$x_1$$ 그리고 $$w_2$$에 대한 덧셈 노드도 존재     
    * -1을 곱하고 exponential을 취하고, 1을 더함     
    * 마지막으로 모든 term을 역수로 뒤집음         
 * 우리에게 ws와 xs가 주어졌다고 했을때, 우리는 값을 이용해 모든 단계마다 계산을 통해서 앞으로(오른쪽 방향으로) 진행할 수 있음   
![image](https://user-images.githubusercontent.com/76824611/169682353-36f30652-1b64-4213-a641-37431fe1dc66.png)


2️⃣ **backpropagation**       
습니다.

187
00:19:37,091 --> 00:19:44,427
그리고 아래에다가 나중에 도움이 될 몇몇 유도식을 적어놓았습니다.

188
00:19:44,427 --> 00:19:49,339
이전에 단순한 예제에서 했던 것과 같이

189
00:19:49,339 --> 00:19:51,820
을 진행할 것입니다.

190
00:19:51,820 --> 00:19:53,327
다시한번 그래프의 뒤에서부터 시작합니다.

191
00:19:53,327 --> 00:20:00,736
그리고 여기에서 최종 변수에 대한 출력의 gradient는 1입니다.

192
00:20:00,736 --> 00:20:04,074


193
00:20:04,074 --> 00:20:13,405
이제 뒤로 한 스텝 가봅시다.
1/x 이전의 input에 대한 gradient는 얼마입니까?

194
00:20:13,405 --> 00:20:21,592
음.. 이 경우 우리는 upstream gradient를 알고 있습니다.
빨간색으로 쓰여있는 1입니다. 그렇죠?

195
00:20:21,592 --> 00:20:30,181
이것은 흐름에 따라 전파되는 gradient입니다. 이제 local gradient를 찾아야 합니다.
이 노드(1/x)에 대한 local gradient입니다.

196
00:20:30,181 --> 00:20:39,935
우리가 가지고 있는 f는 1/x 이고, 이것의 local gradient인
x에 대한 f의 미분은 -1/x^2와 같습니다. (빨간 박스 참고)

197
00:20:39,935 --> 00:20:45,845
그래서 여기에서 우리는 -1/x^2를 얻습니다.
그리고 x에 값을 대입합니다.

198
00:20:45,845 --> 00:20:57,075
이것(x)은 1.37이고,이 변수에 대한 우리의 최종 gradient는
1/-1.37^2 * 1 = -0.53과 같습니다.

199
00:21:04,382 --> 00:21:06,769
다음 노드로 다시 이동해봅시다.

200
00:21:06,769 --> 00:21:09,023
우리는 똑같은 과정을 거치게 될 것입니다, 맞습니까?

201
00:21:09,023 --> 00:21:16,007
여기에서 upstream gradient는 -0.53입니다. 그렇죠?

202
00:21:16,007 --> 00:21:20,365
그리고 현재 노드는 +1입니다.

203
00:21:20,365 --> 00:21:25,203
아래의 유도식을 다시 살펴보겠습니다.

204
00:21:25,203 --> 00:21:31,729
(x+상수)에 대한 local gradient는 1입니다. 그렇죠?

205
00:21:31,729 --> 00:21:37,376
이제 chain rule을 이용하면 이 변수에 대한 gradient는 얼마입니까?

206
00:21:42,883 --> 00:21:51,592
upstream gradient는 -0.53이고, 우리의 local gradient는 1입니다.
그렇기 때문에 (gradient 는) -0.53이 됩니다.

207
00:21:55,849 --> 00:21:59,604
한 단계 더 진행해봅시다.

208
00:21:59,604 --> 00:22:05,022
여기에서 우리는 exponential을 가지고 있습니다.
upstream gradient는 얼마입니까?

209
00:22:05,022 --> 00:22:08,536
[학생이 마이크로 말하고 있다]

210
00:22:08,536 --> 00:22:11,775
네, upstream gradient는 -0.53이죠.

211
00:22:11,775 --> 00:22:18,002
여기에서 local gradient는 얼마일까요?
이것은 x에 대한 e의 local gradient가 될 것입니다.

212
00:22:18,002 --> 00:22:28,301
exponential 노드에서, chain rule에 의해 gradient가 -0.53*e^x임을 알 수 있습니다.

213
00:22:30,869 --> 00:22:37,587
(x의) 값이 -1인 경우 (exponential 노드에서) 최종 gradient는 -0.2가 됩니다.

214
00:22:40,215 --> 00:22:47,560
좋아요, 노드가 하나 더 있습니다.
다음 노드는 -1과 곱하는 노드입니다. 그렇죠?

215
00:22:48,912 --> 00:22:52,729
여기에서 upstream gradient는 얼마인가요?

216
00:22:52,729 --> 00:22:54,090
[학생] -0.2

217
00:22:54,090 --> 00:22:56,565
네 맞아요, 그리고 local gradient는 무엇이 될까요

218
00:22:56,565 --> 00:23:01,510
유도식을 한번 살펴 보세요.

219
00:23:01,510 --> 00:23:03,049
얼마인가요?

220
00:23:03,049 --> 00:23:03,889
들은것 같네요.

221
00:23:03,889 --> 00:23:05,205
[학생] -1

222
00:23:05,205 --> 00:23:09,680
네 -1 맞아요.

223
00:23:09,680 --> 00:23:19,220
local gradient는 x에 대한 f의 미분입니다.
그러므로 이 노드의 결과는 -1이 됩니다. (1 * -1이 되므로)

224
00:23:19,220 --> 00:23:26,825
그렇기 때문에 gradient는 -1 * -0.2이므로 0.2가 됩니다.

225
00:23:29,169 --> 00:23:37,269
좋아, 이제 우리는 덧셈 노드에 도달했습니다.
그리고 여기에서 우리는 두 개의 노드와 연결됩니다.

226
00:23:37,269 --> 00:23:43,286
일단 여기에서 upstream gradient는 얼마인가요?
그것은 0.2일거에요, 그리고

227
00:23:43,286 --> 00:23:50,122
각 브랜치에 대한 gradient를 추가해봅시다.

228
00:23:50,122 --> 00:23:56,226
이전의 간단한 예제에서 덧셈 노드를 가지고 있을 때 덧셈 노드에서,
각 입력에 대한 local gradient는 1이었습니다.

229
00:23:56,226 --> 00:24:06,406
여기에서 local gradient는 upstream gradient 0.2와
한번 곱해질 것입니다. 맞죠? 1 * 0.2 = 0.2입니다.

230
00:24:06,406 --> 00:24:16,957
그리고 우리는 아래(bottom) 브랜치에 대해서도 똑같이 할 것입니다.
같은 들어오는 gradient 0.2를 이용해서

231
00:24:18,400 --> 00:24:23,277
그리고 총 gradient는 0.2입니다.
여기까지 이해가 되시나요?

232
00:24:23,277 --> 00:24:26,110
좋아요.

233
00:24:27,581 --> 00:24:35,413
몇가지 gradient를 더 채워봅시다.
w₀과 x₀이 있는곳까지 가면 여기에서는 곱셈 노드를 가지고 있습니다.

234
00:24:37,648 --> 00:24:45,698
이전에 보았던 곱셈 노드에서 input에 대한 gradient는
한 인풋에 대하여 다른 인풋의 값이었습니다.

235
00:24:45,698 --> 00:24:49,506
이 경우 w₀에 대한 gradient는 얼마일까요?

236
00:24:49,506 --> 00:24:53,088


237
00:24:56,927 --> 00:24:58,795
[학생] -0.2

238
00:24:58,795 --> 00:25:04,366
네 -0.2 맞습니다.
w0의 관점에서 upstream gradient 0.2를 가지고 있죠.

239
00:25:04,366 --> 00:25:11,900
아래에 있는 다른 하나(x0)는 -1이기 때문에
-1과 0.2를 곱해 -0.2를 얻게 됩니다.

240
00:25:13,781 --> 00:25:22,008
x0에 대해서도 같은 과정을 반복하면 0.2 * 2를 하게 되므로 0.4를 얻게 됩니다.

241
00:25:22,008 --> 00:25:24,425
모든 gradient를 채워 넣었습니다.

242
00:25:26,525 --> 00:25:30,692
그리고 이전에 질문 중에 왜 이 방식이 특정 변수에 대해서

243
00:25:32,200 --> 00:25:43,071
analytic gradient를 유도해 값을 계산하는 것보다 단순한지가 있었습니다.

244
00:25:43,071 --> 00:25:50,164
우리가 써야만 하는 local gradient에 대한 표현을 여기에서 볼 수 있습니다.

245
00:25:50,164 --> 00:25:54,034
 우리가 가진 각각의 값을 결합했습니다.

246
00:25:54,034 --> 00:26:01,532
곱셈으로 나타내기 위해 chain rule을 사용했고 뒤에서부터
시작하여 모든 변수에 대한 gradients를 구했습니다.

247
00:26:01,532 --> 00:26:11,644
그리고 우리는 w0과 x0에 대한 gradient 또한 같은 방법을 사용해 구했습니다.

248
00:26:11,644 --> 00:26:14,490


249
00:26:14,490 --> 00:26:21,996
그리고 주목하기 바라는 것은 우리가 computational graph를 만들 때,
computational 노드에 대해 우리가 원하는 세분화된 정의를 할 수 있는 것입니다.

250
00:26:21,996 --> 00:26:29,813
우리는 이번 케이스에서 덧셈과 곱셈, 절대적으로 작은 단위로 쪼갰습니다.
알다시피 덧셈과 곱셈은 더 단순해질 수 없습니다.

251
00:26:29,813 --> 00:26:38,883
 실제로 원한다면 이 노드들을 더
복잡한 그룹으로 묶을 수 있습니다.

252
00:26:38,883 --> 00:26:44,600
우리는 그 노드에 대한 local gradient를 적어두기만 하면 됩니다



