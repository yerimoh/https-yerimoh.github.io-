---
title: "[24] CS231N: Lecture 4 Neural Networks and Backpropagation"
date:   2020-02-9
excerpt: "Lecture 4 | Multi-layer Perceptron Backpropagation 요약"  
category: [Deep Learning]
layout: post
tag:
- Deep Learning
order: 0

comments: true
---



# 목차




------

👀 코드 보기 , 🤷‍♀️     
이 두개의 아이콘을 누르시면 코드, 개념 부가 설명을 보실 수 있습니다:)

------


[CS231N: Lecture 3](https://www.youtube.com/watch?v=d14TUNcbn1k&list=PLC1qU-LWwrF64f4QKQT-Vg5Wr4qEE1Zxk&index=4) 강의를 빠짐 없이 정리하였고, 어려운 개념에 대한 보충 설명까지 알기 쉽게 추가해 두었습니다.  


------

# **Intro**
[지난시간](https://yerimoh.github.io/DL201/)에 이어서,   
우리가 실제로 우리가 원하는것은 analytic gradient를 유도하고 사용하는 것이다            

하지만 동시에 analytic gradient를 사용한 응용에 대해서 수학적 검증도 필요하다
➡ 임의의 복잡한 함수를 통해 어떻게 **analytic gradient를 계산**하는지에 대해 이야기할 것이다.        
➡ computational graph(계산 그래프)라고 부르는 프레임워크를 사용할 것이다   


---
---

# **computational graph**
어떤 함수든지 computational graph(계산그래프)로 표현가능       
**노드**: 연산 단계를 나타냄        

**[EX]**   
* **식**: 예제는 우리가 지난시간에 배운 선형 classifier    
* $$x$$,$$W$$: input      
* **곱셈 노드**: 행렬 곱셈       
   * 파라미터 W와 데이터 x의 곱셈은 score vector를 출력    
* **hinge loss 노드**: 데이터 항  $$Lᵢ$$를 계산     
   * regularization 항을 계산    
   * 그 후, 우리의 최종 loss는 L은 regularization 항과 데이터 항의 합       
![image](https://user-images.githubusercontent.com/76824611/169669090-fc73415d-c119-47d0-afde-0bb56ea30f57.png)


**[효과]**     
* computational graph를 사용해서 함수를 표현하게 됨으로써, backpropagation가 사용가능해짐   
* backpropagation은 gradient를 얻기위해 computational graph 내부의 모든 변수에 대해 **chain rule을** **재귀적으로 사용**합니다.


그리고 우리는 이것이 어떻게 동작하는지 볼 것입니다.

53
00:05:09,830 --> 00:05:13,413
이것은 우리가 매우 복잡한 함수를 이용하여 작업할때 아주 유용하게 사용됩니다.

54
00:05:13,413 --> 00:05:20,900
예를들면 우리가 이 수업에서 이야기하게 될 CNN은 가장 윗층(top)에
입력 이미지가 들어가고 아래(bottom)에는 loss가 있습니다.

55
00:05:20,900 --> 00:05:29,102
입력 이미지는 loss function으로 가기까지 많은 layer를 거쳐 변형을 겪게 됩니다.

56
00:05:30,896 --> 00:05:33,165
이것은 심지어 상상하는만큼 미칠(crazier) 수 있습니다.

57
00:05:33,165 --> 00:05:38,869
여러분은 딥러닝의 다른 종류인 neural turing machine을 알고있을텐데요
이 경우의 computational graph를 볼 수 있습니다.

58
00:05:38,869 --> 00:05:45,797
이것은 아주 미쳤지만 특별합니다.
우리는 결국 시간이 지남에 따라 이것을 풀게 될 것입니다.

59
00:05:45,897 --> 00:05:53,234
만약 여러분이 어떤 중간 변수의 gradient를
계산하고 싶으면 이것은 기본적으로 아주 비실용적입니다.

60
00:05:55,560 --> 00:05:59,139
자 어떻게 backpropagation이 동작할까요?
간단한 예제부터 시작해봅시다.

61
00:05:59,139 --> 00:06:01,838
일단, 우리의 목표는 우리가 함수를 가지는 것입니다.

62
00:06:01,838 --> 00:06:08,614
이 경우 x,y,z로 구성된 function f는 (x+y)*z와 같습니다.

63
00:06:08,614 --> 00:06:13,572
그리고 우리는 function f의 출력에 대한 어떤 변수의 gradient를 찾기 원합니다.

64
00:06:13,572 --> 00:06:17,329
그래서 첫 번째 단계는 항상 함수 f를 이용해서 computational graph로 나타내는 것입니다.

65
00:06:17,329 --> 00:06:20,623
computational graph가 오른쪽에 있습니다.

66
00:06:20,623 --> 00:06:30,044
x,y에 대한 덧셈 노드를 보실 수 있으며
오른쪽에는 다음 연산을 위한 곱셈 노드도 가지고 있습니다.

67
00:06:30,044 --> 00:06:34,060
그리고 이 네트워크에 우리가 가지고 있는 값을 전달할 것입니다.

68
00:06:34,060 --> 00:06:42,944
x는 -2, y는 5, z는 -4입니다.

69
00:06:42,944 --> 00:06:49,417
또한 중간중간 값도 계산하여 computational graph에 값을 적어 놓았습니다.

70
00:06:49,417 --> 00:06:56,990
x+y는 3입니다. 그리고 마지막으로 최종
노드를 통과시켜 -12를 얻습니다.

71
00:06:56,990 --> 00:07:00,823
여기에서 모든 중간 변수에 이름을 줘봅시다.

72
00:07:04,310 --> 00:07:10,438
그래서 나는 x+y 덧셈 노드를 q라고 부를 것입니다.

73
00:07:10,438 --> 00:07:14,997
또한 여기에서 f는 q*z입니다.

74
00:07:14,997 --> 00:07:24,763
그리고 저는 x와 y에 각각에 대한 q의 gradient를 표시해 놓았습니다.
그것은 단순 덧셈이기때문에 1입니다.

75
00:07:24,763 --> 00:07:34,607
그리고 q와 z에 대한 f의 gradient는 곱셈규칙에 의해 각각 z와 q입니다.

76
00:07:34,607 --> 00:07:40,431
그리고 우리가 찾기 원하는것은 x,y,z 각각에 대한 f의 gradient입니다.

77
00:07:43,356 --> 00:07:49,182
backpropagation은 chain rule의 재귀적인 응용입니다.
chiain rule에 의해 우리는 뒤에서부터 시작합니다.

78
00:07:49,182 --> 00:07:56,373
computational graph의 가장 끝,
우리는 뒤에서부터 gradient를 계산합니다.

79
00:07:56,373 --> 00:08:04,674
끝에서부터 시작해봅시다. 우리는 출력의 f에
대한 gradient를 계산하길 원합니다.

80
00:08:04,674 --> 00:08:08,591
그리고 이 gradient는 1입니다, 쉽죠.

81
00:08:10,065 --> 00:08:18,187
이번에는 뒤로 이동해서 z에 대한 gradient를 계산해봅시다.
여러분은 z에 대한 f의 미분이 q라는 것을 알고 있습니다.

82
00:08:19,993 --> 00:08:26,386
그리고 q의 값은 3이죠. 그래서 여기에서
z에 대한 f의 미분값은 3을 갖습니다.

83
00:08:28,819 --> 00:08:33,855
그리고 다음으로 q에 대한 f의 미분값을 알아봅시다.
값이 얼마일까요?

84
00:08:36,732 --> 00:08:37,957
q에 대한 f의 미분값은 얼마인가요?

85
00:08:37,957 --> 00:08:42,040
q에 대한 f의 미분값은 z와 같네요.

86
00:08:46,012 --> 00:08:54,130
그리고 z는 여기에서 -4입니다. 여기에서 우리는
q에 대한 f의 미분값이 -4라는 것을 알았습니다.

87
00:08:57,485 --> 00:09:03,793
자 이제 그래프의 뒤로 이동해봅시다.
여러분은 y에 대한 f의 미분값을 알고 싶습니다.

88
00:09:03,793 --> 00:09:08,986
하지만 여기에서 y는 f와 바로 연결되어 있지 않습니다.

89
00:09:08,986 --> 00:09:17,998
f는 z와 연결되어있습니다.
이것을 구하는 방법은 chain rule을 이용합니다.

90
00:09:17,998 --> 00:09:21,748
y에 대한 f의 미분은 q에 대한 f의 미분과
y에 대한 q의 미분의 곱으로 나타낼 수 있습니다.

91
00:09:22,746 --> 00:09:30,707
이것은 직관적으로 y가 f에 미치는
영향을 구하기 위한 것을 알수 있죠.

92
00:09:30,707 --> 00:09:37,416
실제로 이것은 우리가 q*f의 연산에서
q의 영향을 구할때와 동등합니다.

93
00:09:37,416 --> 00:09:45,497
q에 대한 f의 미분은 -4이고, q에 대한 y의
영향력 즉 y에 대한 q의 미분과 합성(곱)합니다.

94
00:09:46,604 --> 00:09:50,986
이 예제에서 y에 대한 q의 미분은 얼마인가요?

95
00:09:50,986 --> 00:09:51,819
[학생] 1

96
00:09:51,819 --> 00:09:52,980
맞아요. 정확합니다.

97
00:09:52,980 --> 00:09:55,916
y에 대한 q의 미분은 1이죠, 아시다시피.

98
00:09:55,916 --> 00:10:01,627
만약 우리가 y를 조금 변화시키면 q는
그것의 영향력 만큼 조금 변할것입니다.

99
00:10:01,627 --> 00:10:07,474
그리고 이것은 내가 y를 조금 바꿨을때 그것이 q에
대해 1만큼의 영향력을 미치는 것을 의미합니다.

100
00:10:07,474 --> 00:10:16,186
그리고 f에 대한 q의 영향력은 정확하게 -4의 입니다.

101
00:10:18,628 --> 00:10:26,470
이것들을 곱해보면 우리는 f에 대한 y의 영향력으로 -4를 얻습니다.
(1 곱하기 -4 이므로)

102
00:10:30,887 --> 00:10:41,191
만약 우리가 똑같이 x의 gradient를 알고싶다고 해봅시다. 여러분은
이전과 같은 방법으로 구할 수 있습니다. 어떻게 할 수 있을까요?

103
00:10:41,191 --> 00:10:42,711
[학생이 마이크로 대답하고 있다]

104
00:10:42,711 --> 00:10:44,746
들었습니다.

105
00:10:44,746 --> 00:10:51,051
정확합니다, 이 경우 우리는 다시한번 chain rule을 적용할 수 있습니다.

106
00:10:51,051 --> 00:10:55,882
우리는 f에 대한 q의 영향력이 -4인것을 알죠

107
00:10:55,882 --> 00:11:01,958
그리고 다시 우리는 같은 덧셈 노드를 가지고 있기
때문에 x에 대한 q의 영향력은 다시한번 1입니다.

108
00:11:01,958 --> 00:11:08,760
f에 대한 x의 gradient는 -4 * 1 이므로 -4입니다.

109
00:11:11,467 --> 00:11:15,389
지금 우리가 하고 있는 backpropagation에서 우리는
computational graph안에 모든 노드를 포함하고 있었습니다.

110
00:11:15,389 --> 00:11:20,724
하지만 각 노드는 오직 주변에 대해서만 알고 있습니다. 그렇죠?

111
00:11:20,724 --> 00:11:23,874
우리가 가지고 있는건 각 노드와 각 노드의 local 입력입니다.

112
00:11:23,874 --> 00:11:32,432
입력은 그 노드와 연결되어있고 값은 노드로 흘러들어가
이 노드로부터 출력을 얻게 됩니다.

113
00:11:32,432 --> 00:11:36,599
여기에서 local 입력은 x,y이고 출력은 z입니다.

114
00:11:39,777 --> 00:11:43,469
그리고 이 노드에서 우리는 local gradient를 구할 수 있습니다. 그렇죠?

115
00:11:43,469 --> 00:11:48,905
우리는 z에서 x에 대한 gradient를 구할 수 있으며 y에 대한 gradient도 마찬가지입니다.

116
00:11:48,905 --> 00:11:51,217
그리고 이것들은 대부분 간단한 연산입니다. 그렇죠?

117
00:11:51,217 --> 00:11:54,821
각 노드는 우리가 이전에 보았던 것처럼 덧셈 혹은 곱셈입니다.

118
00:11:54,821 --> 00:12:04,665
 그것들은 쉽게 gradient를 구할 수 있고, 우리는
그것을 찾기 위해 복잡한 미적분을 할 필요가 없습니다.

119
00:12:04,665 --> 00:12:15,313
[학생] 이전으로 돌아가서 설명해 주실수 있으십니까? 어떻게
마지막 슬라이드에서 이 기본적인 미적분으로 넘어왔는지..

120
00:12:15,313 --> 00:12:18,683
네, 다시 돌아가보죠

121
00:12:18,683 --> 00:12:20,183
잠시만요..

122
00:12:21,740 --> 00:12:26,565
여기로 돌아와보면 우리는 미적분학만을 사용해 모든 것을 쓸 수 있습니다.

123
00:12:26,565 --> 00:12:32,572
아시다시피 우리는 x에 대한 f의 미분을 알고 싶었고 우리는 이 식을 사용해서 표현했습니다.

124
00:12:32,572 --> 00:12:42,667
그리고 보시면 이것은 z입니다. 이것은 단순합니다
하지만 정말로 복잡한 표현을 보게 된다면

125
00:12:42,667 --> 00:12:47,487
당신은 무언가의 gradient를
유도하기위해 미적분을 쓰기 싫을 것입니다.

126
00:12:47,487 --> 00:12:52,094
 대신 만약 당신이 이 식을 사용하게 되면

127
00:12:52,094 --> 00:13:01,612
당신은 계산 노드를 나눌 수 있으며 이는 gradient를 구하기 위해 아주 단순한 계산만을 사용합니다.

128
00:13:01,612 --> 00:13:10,038
알다시피 덧셈, 곱셈, 지수 같은 것들은 원하는 만큼 단순하고
이 모든 것들을 곱하기 위해 chain rule만 있으면 됩니다.

129
00:13:10,038 --> 00:13:16,571
그러면 gradient를 구할 수 있습니다. 필요한 다른 식은 없습니다.

130
00:13:18,562 --> 00:13:20,508
이해가 되셨나요?

131
00:13:20,508 --> 00:13:21,367
[학생: 웅성웅성]

132
00:13:21,367 --> 00:13:25,034
나중에 이것에 대한 예제를 보겠습니다.

133
00:13:28,751 --> 00:13:30,449
또 다른 질문이 있었습니까?

134
00:13:30,449 --> 00:13:32,240
[학생이 마이크로 말하고 있다]

135
00:13:32,240 --> 00:13:36,146
[학생] z가 나타내는 -4는 무엇입니까?

136
00:13:36,146 --> 00:13:38,408
음.. 네 -4는

137
00:13:38,408 --> 00:13:43,831
위의 초록색 값은 function을 통해 전달되는 값입니다. 그렇죠?

138
00:13:43,831 --> 00:13:49,835
 여기에서 x는 -2, y는 5이고, z는 -4입니다.

139
00:13:49,835 --> 00:13:55,621
 그리고 우리가 원하는 계산된 다른 값들을 채웠습니다.

140
00:13:55,621 --> 00:14:09,289
 우리는 q를 x+y라고 할 수 있습니다.
이것은 -2 + 5이기 때문에 3이 될 것이고 z는 -4죠

141
00:14:09,289 --> 00:14:16,886
최종 값 f는 z와 q를 곱해서 얻을 수 있고 그것은 -4 * 3입니다. 그렇죠?

142
00:14:16,886 --> 00:14:23,790
그리고 아래에 있는 붉은색 값은 gradient에 대해서 적어놓은 것입니다.
우리가 뒤에서부터 계산한.

143
00:14:29,418 --> 00:14:38,969
그래요 우리가 말했던 것처럼 우리가 가지고 있는 각 노드는 local 입력을 받고

144
00:14:38,969 --> 00:14:46,302
보이는 것처럼 다음 노드로 출력값을 보냅니다.
그리고 우리가 계산한 local gradient가 있는데

145
00:14:46,302 --> 00:14:51,175
이는 들어오는 입력에 대한 출력의 기울기입니다.

146
00:14:51,175 --> 00:14:55,155
backpropagation이 어떻게 동작하는지 알아봅시다.

147
00:14:55,155 --> 00:14:56,750
우리는 그래프의 뒤에서 부터 시작합니다.

148
00:14:56,750 --> 00:14:58,471
뒤에서부터 시작 부분까지 끝까지 진행됩니다.

149
00:14:58,471 --> 00:15:08,980
각 노드에 도달하면 출력과 관련한 노드의
gradient가 상류의(입력과 가까운) 노드로 전파됩니다.

150
00:15:08,980 --> 00:15:17,808
그래서 우리는 backpropagation에서 이 노드에 도달할 때까지
z에 대한 최종 loss L은 이미 계산되어있습니다. 그렇죠?

151
00:15:17,808 --> 00:15:26,675
이제 우리는 x와 y의 값에 대한 바로 직전 노드의 gradient를 찾고자합니다.

152
00:15:27,679 --> 00:15:33,053
그리고 우리가 이미 보았듯이, 우리는 chain rule을 사용합니다.

153
00:15:33,053 --> 00:15:34,857


154
00:15:34,857 --> 00:15:44,987
x의 gradient는 z에 대한 gradient와, x에 대한 z의 local gradient로 합성됩니다.

155
00:15:44,987 --> 00:15:48,422
그렇습니다. chain rule에서는 항상 위쪽으로 gradient가 전파됩니다.

156
00:15:48,422 --> 00:15:55,071
그리고 이것(gradient)을 local gradient와 곱해서 (노드의)입력에 대한 gradient를 구합니다.

157
00:15:55,071 --> 00:16:01,949
[학생] 죄송하지만 이 방법은 gradient를 구하는 상징적인 예시일 뿐이지
실제 gradient를 계산하는 일반적인 수식이 아닌것 같습니다.

158
00:16:01,949 --> 00:16:11,896
[학생] 이 방법은 그때그때의 값에 따라서만 동작합니다.
콜록콜록, 또는 약간의 상수 값과 함께요.

159
00:16:11,896 --> 00:16:22,579
질문은 이 방법이 현재 값의 여부와 관계없이 동작하는지에 관한 것이죠?

160
00:16:23,842 --> 00:16:29,819
우리가 연결했던 함수에 현재 값이 주어지면
우리는 이것에 대해 변수에 관한 식으로 나타낼 수 있습니다. 그렇죠?

161
00:16:29,819 --> 00:16:36,357
z에 대한 L의 gradient를 보면 이것은 식으로 나타나 있고,

162
00:16:36,357 --> 00:16:48,708
x에 관한 z의 gradient를 보면 이것은 또 다른 수식입니다 그렇죠?
우리가 이것들에 값을 대입하여 x에 대한 gradient를 얻습니다.

163
00:16:48,708 --> 00:16:55,203
당신이 할 수 있는 것은 이 모든 식을 재귀적으로 사용할 수 있다는 것입니다. 맞죠?

164
00:16:55,203 --> 00:17:01,442
z에 대한 gradient, 그리고 x에 대한 gradient 이것들은 단순한 수식으로 나타납니다.

165
00:17:01,442 --> 00:17:08,612
만약 우리가 곱셈 노드를 가지고 있다고 가정하면 이 경우 x에 대한 z의 gradient는 y가 될 것입니다.

166
00:17:08,612 --> 00:17:12,811
하지만 z에 대한 L의 gradient는 이 그래프 내에서 아주 복잡한 부분입니다.

167
00:17:12,811 --> 00:17:20,748
그렇기 때문에 여기에서 우리는 이 숫자들만 가지고 합니다.

168
00:17:20,748 --> 00:17:30,719
학생이 말했던 것처럼, 이것은 값이 전달되는 기본적인 부분입니다.
우리는 그것을 수식에 따라 곱하고 local gradient를 갖습니다.

169
00:17:30,719 --> 00:17:35,647
나는 이 방식이 더 몇몇 슬라이드의 복잡한 예제보다 명확하다고 생각합니다.

170
00:17:38,225 --> 00:17:44,284
그래서 지금 y에 대한 L의 gradient는 다시 한번 완전히 같은 방식입니다.

171
00:17:44,284 --> 00:17:54,411
chain rule을 사용해서, z에 대한 L의 gradient와 그리고 y에 대한 z의 gradient를 서로 곱합니다.
chain rule에 의해 이 둘의 곱은 원하는 gradient를 줍니다.

172
00:17:55,848 --> 00:18:00,816
그리고 나서 우리가 gradient를 구하면 이 노드와 연결된 직전 노드로 전달할 수 있습니다.

173
00:18:00,816 --> 00:18:09,047
이것으로부터 얻을 수 있는 중요한 것은
각 노드는 우리가 계산한 local gradient를 가지고 있습니다.

174
00:18:09,047 --> 00:18:16,127
우리는 backpropagation을 통해 그것을 얻습니다.
값들은 상위 노드 방향으로 계속 전달되고

175
00:18:16,127 --> 00:18:20,077
우리는 이것을 받아 local gradient와 곱하기만 하면 됩니다.

176
00:18:20,077 --> 00:18:31,664
우리는 노드와 연결된 노드 이외의 다른 어떤 값에 대하여 신경쓰지 않아도 됩니다.

177
00:18:31,664 --> 00:18:35,353
이제부터는 다른 예제에 대해서 살펴보겠습니다. 이것은 이전보다는 조금 복잡합니다.

178
00:18:35,353 --> 00:18:39,239
그렇기 때문에 backpropagation이 얼마나 유용한 방식인지를 알게 될 것입니다.

179
00:18:39,239 --> 00:18:49,576
이 경우 함수 w,x에 대한 함수 f는 1 / e^ - (w0x0 + w1x1 + w2)와 같습니다.

180
00:18:49,576 --> 00:18:52,619


181
00:18:52,619 --> 00:18:57,525
다시 말하지만 첫 번째 과정은 이것을 computational graph로 나타내는 것입니다.

182
00:18:57,525 --> 00:19:02,863
이 경우의 graph를 볼 수 있습니다.
먼저 우리는 w와 x를 곱합니다. x0과 x0, x1과 x1 그리고

183
00:19:02,863 --> 00:19:10,388
w2와는 이것들을 모두 더하게됩니다. 그렇죠?

184
00:19:10,388 --> 00:19:20,318
그리고 이것에 -1을 곱하고 exponential을 취하고, 1을 더합니다.
마지막으로 모든 term을 역수로 뒤집습니다. (1/x)

185
00:19:22,533 --> 00:19:25,170
여기서도 값을 채워 놓았습니다.

186
00:19:25,170 --> 00:19:35,171
우리에게 ws와 xs가 주어졌다고 했을때, 우리는 값을 이용해 모든
단계마다 계산을 통해서 앞으로(오른쪽 방향으로) 진행할 수 있습니다.

187
00:19:37,091 --> 00:19:44,427
그리고 아래에다가 나중에 도움이 될 몇몇 유도식을 적어놓았습니다.

188
00:19:44,427 --> 00:19:49,339
이전에 단순한 예제에서 했던 것과 같이

189
00:19:49,339 --> 00:19:51,820
backpropagation을 진행할 것입니다.

190
00:19:51,820 --> 00:19:53,327
다시한번 그래프의 뒤에서부터 시작합니다.

191
00:19:53,327 --> 00:20:00,736
그리고 여기에서 최종 변수에 대한 출력의 gradient는 1입니다.

192
00:20:00,736 --> 00:20:04,074


193
00:20:04,074 --> 00:20:13,405
이제 뒤로 한 스텝 가봅시다.
1/x 이전의 input에 대한 gradient는 얼마입니까?

194
00:20:13,405 --> 00:20:21,592
음.. 이 경우 우리는 upstream gradient를 알고 있습니다.
빨간색으로 쓰여있는 1입니다. 그렇죠?

195
00:20:21,592 --> 00:20:30,181
이것은 흐름에 따라 전파되는 gradient입니다. 이제 local gradient를 찾아야 합니다.
이 노드(1/x)에 대한 local gradient입니다.

196
00:20:30,181 --> 00:20:39,935
우리가 가지고 있는 f는 1/x 이고, 이것의 local gradient인
x에 대한 f의 미분은 -1/x^2와 같습니다. (빨간 박스 참고)

197
00:20:39,935 --> 00:20:45,845
그래서 여기에서 우리는 -1/x^2를 얻습니다.
그리고 x에 값을 대입합니다.

198
00:20:45,845 --> 00:20:57,075
이것(x)은 1.37이고,이 변수에 대한 우리의 최종 gradient는
1/-1.37^2 * 1 = -0.53과 같습니다.

199
00:21:04,382 --> 00:21:06,769
다음 노드로 다시 이동해봅시다.

200
00:21:06,769 --> 00:21:09,023
우리는 똑같은 과정을 거치게 될 것입니다, 맞습니까?

201
00:21:09,023 --> 00:21:16,007
여기에서 upstream gradient는 -0.53입니다. 그렇죠?

202
00:21:16,007 --> 00:21:20,365
그리고 현재 노드는 +1입니다.

203
00:21:20,365 --> 00:21:25,203
아래의 유도식을 다시 살펴보겠습니다.

204
00:21:25,203 --> 00:21:31,729
(x+상수)에 대한 local gradient는 1입니다. 그렇죠?

205
00:21:31,729 --> 00:21:37,376
이제 chain rule을 이용하면 이 변수에 대한 gradient는 얼마입니까?

206
00:21:42,883 --> 00:21:51,592
upstream gradient는 -0.53이고, 우리의 local gradient는 1입니다.
그렇기 때문에 (gradient 는) -0.53이 됩니다.

207
00:21:55,849 --> 00:21:59,604
한 단계 더 진행해봅시다.

208
00:21:59,604 --> 00:22:05,022
여기에서 우리는 exponential을 가지고 있습니다.
upstream gradient는 얼마입니까?

209
00:22:05,022 --> 00:22:08,536
[학생이 마이크로 말하고 있다]

210
00:22:08,536 --> 00:22:11,775
네, upstream gradient는 -0.53이죠.

211
00:22:11,775 --> 00:22:18,002
여기에서 local gradient는 얼마일까요?
이것은 x에 대한 e의 local gradient가 될 것입니다.

212
00:22:18,002 --> 00:22:28,301
exponential 노드에서, chain rule에 의해 gradient가 -0.53*e^x임을 알 수 있습니다.

213
00:22:30,869 --> 00:22:37,587
(x의) 값이 -1인 경우 (exponential 노드에서) 최종 gradient는 -0.2가 됩니다.

214
00:22:40,215 --> 00:22:47,560
좋아요, 노드가 하나 더 있습니다.
다음 노드는 -1과 곱하는 노드입니다. 그렇죠?

215
00:22:48,912 --> 00:22:52,729
여기에서 upstream gradient는 얼마인가요?

216
00:22:52,729 --> 00:22:54,090
[학생] -0.2

217
00:22:54,090 --> 00:22:56,565
네 맞아요, 그리고 local gradient는 무엇이 될까요

218
00:22:56,565 --> 00:23:01,510
유도식을 한번 살펴 보세요.

219
00:23:01,510 --> 00:23:03,049
얼마인가요?

220
00:23:03,049 --> 00:23:03,889
들은것 같네요.

221
00:23:03,889 --> 00:23:05,205
[학생] -1

222
00:23:05,205 --> 00:23:09,680
네 -1 맞아요.

223
00:23:09,680 --> 00:23:19,220
local gradient는 x에 대한 f의 미분입니다.
그러므로 이 노드의 결과는 -1이 됩니다. (1 * -1이 되므로)

224
00:23:19,220 --> 00:23:26,825
그렇기 때문에 gradient는 -1 * -0.2이므로 0.2가 됩니다.

225
00:23:29,169 --> 00:23:37,269
좋아, 이제 우리는 덧셈 노드에 도달했습니다.
그리고 여기에서 우리는 두 개의 노드와 연결됩니다.

226
00:23:37,269 --> 00:23:43,286
일단 여기에서 upstream gradient는 얼마인가요?
그것은 0.2일거에요, 그리고

227
00:23:43,286 --> 00:23:50,122
각 브랜치에 대한 gradient를 추가해봅시다.

228
00:23:50,122 --> 00:23:56,226
이전의 간단한 예제에서 덧셈 노드를 가지고 있을 때 덧셈 노드에서,
각 입력에 대한 local gradient는 1이었습니다.

229
00:23:56,226 --> 00:24:06,406
여기에서 local gradient는 upstream gradient 0.2와
한번 곱해질 것입니다. 맞죠? 1 * 0.2 = 0.2입니다.

230
00:24:06,406 --> 00:24:16,957
그리고 우리는 아래(bottom) 브랜치에 대해서도 똑같이 할 것입니다.
같은 들어오는 gradient 0.2를 이용해서

231
00:24:18,400 --> 00:24:23,277
그리고 총 gradient는 0.2입니다.
여기까지 이해가 되시나요?

232
00:24:23,277 --> 00:24:26,110
좋아요.

233
00:24:27,581 --> 00:24:35,413
몇가지 gradient를 더 채워봅시다.
w₀과 x₀이 있는곳까지 가면 여기에서는 곱셈 노드를 가지고 있습니다.

234
00:24:37,648 --> 00:24:45,698
이전에 보았던 곱셈 노드에서 input에 대한 gradient는
한 인풋에 대하여 다른 인풋의 값이었습니다.

235
00:24:45,698 --> 00:24:49,506
이 경우 w₀에 대한 gradient는 얼마일까요?

236
00:24:49,506 --> 00:24:53,088


237
00:24:56,927 --> 00:24:58,795
[학생] -0.2

238
00:24:58,795 --> 00:25:04,366
네 -0.2 맞습니다.
w0의 관점에서 upstream gradient 0.2를 가지고 있죠.

239
00:25:04,366 --> 00:25:11,900
아래에 있는 다른 하나(x0)는 -1이기 때문에
-1과 0.2를 곱해 -0.2를 얻게 됩니다.

240
00:25:13,781 --> 00:25:22,008
x0에 대해서도 같은 과정을 반복하면 0.2 * 2를 하게 되므로 0.4를 얻게 됩니다.

241
00:25:22,008 --> 00:25:24,425
모든 gradient를 채워 넣었습니다.

242
00:25:26,525 --> 00:25:30,692
그리고 이전에 질문 중에 왜 이 방식이 특정 변수에 대해서

243
00:25:32,200 --> 00:25:43,071
analytic gradient를 유도해 값을 계산하는 것보다 단순한지가 있었습니다.

244
00:25:43,071 --> 00:25:50,164
우리가 써야만 하는 local gradient에 대한 표현을 여기에서 볼 수 있습니다.

245
00:25:50,164 --> 00:25:54,034
 우리가 가진 각각의 값을 결합했습니다.

246
00:25:54,034 --> 00:26:01,532
곱셈으로 나타내기 위해 chain rule을 사용했고 뒤에서부터
시작하여 모든 변수에 대한 gradients를 구했습니다.

247
00:26:01,532 --> 00:26:11,644
그리고 우리는 w0과 x0에 대한 gradient 또한 같은 방법을 사용해 구했습니다.

248
00:26:11,644 --> 00:26:14,490


249
00:26:14,490 --> 00:26:21,996
그리고 주목하기 바라는 것은 우리가 computational graph를 만들 때,
computational 노드에 대해 우리가 원하는 세분화된 정의를 할 수 있는 것입니다.

250
00:26:21,996 --> 00:26:29,813
우리는 이번 케이스에서 덧셈과 곱셈, 절대적으로 작은 단위로 쪼갰습니다.
알다시피 덧셈과 곱셈은 더 단순해질 수 없습니다.

251
00:26:29,813 --> 00:26:38,883
 실제로 원한다면 이 노드들을 더
복잡한 그룹으로 묶을 수 있습니다.

252
00:26:38,883 --> 00:26:44,600
우리는 그 노드에 대한 local gradient를 적어두기만 하면 됩니다



