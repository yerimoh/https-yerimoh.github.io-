---
title: "[24] CS231N: Lecture 4 Neural Networks and Backpropagation"
date:   2020-02-9
excerpt: "Lecture 4 | Multi-layer Perceptron Backpropagation 요약"  
category: [Deep Learning]
layout: post
tag:
- Deep Learning
order: 0

comments: true
---



# 목차




------

👀 코드 보기 , 🤷‍♀️     
이 두개의 아이콘을 누르시면 코드, 개념 부가 설명을 보실 수 있습니다:)

------


[CS231N: Lecture 3](https://www.youtube.com/watch?v=d14TUNcbn1k&list=PLC1qU-LWwrF64f4QKQT-Vg5Wr4qEE1Zxk&index=4) 강의를 빠짐 없이 정리하였고, 어려운 개념에 대한 보충 설명까지 알기 쉽게 추가해 두었습니다.  


------

# **Intro**
[지난시간](https://yerimoh.github.io/DL201/)에 이어서,   
우리가 실제로 우리가 원하는것은 analytic gradient를 유도하고 사용하는 것이다            

하지만 동시에 analytic gradient를 사용한 응용에 대해서 수학적 검증도 필요하다
➡ 임의의 복잡한 함수를 통해 어떻게 **analytic gradient를 계산**하는지에 대해 이야기할 것이다.        
➡ computational graph(계산 그래프)라고 부르는 프레임워크를 사용할 것이다   


---
---

# **computational graph**
어떤 함수든지 computational graph(계산그래프)로 표현가능       
**노드**: 연산 단계를 나타냄        

**[EX]**   
* **식**: 예제는 우리가 지난시간에 배운 선형 classifier    
* $$x$$,$$W$$: input      
* **곱셈 노드**: 행렬 곱셈       
   * 파라미터 W와 데이터 x의 곱셈은 score vector를 출력    
* **hinge loss 노드**: 데이터 항  $$Lᵢ$$를 계산     
   * regularization 항을 계산    
   * 그 후, 우리의 최종 loss는 L은 regularization 항과 데이터 항의 합       
![image](https://user-images.githubusercontent.com/76824611/169669090-fc73415d-c119-47d0-afde-0bb56ea30f57.png)


**[효과]**     
* computational graph를 사용해서 함수를 표현하게 됨으로써, backpropagation가 사용가능해짐   
* backpropagation은 gradient를 얻기위해 computational graph 내부의 모든 변수에 대해 **chain rule을** **재귀적으로 사용**함    
* 이것은 우리가 매우 복잡한 함수를 이용하여 작업할때 아주 유용하게 사용됨      



**[EX: AlexNet(CNN)]**    
이 모델의 구성은 
* 가장 윗층(top): 입력 이미지가 들어감           
* 아래(bottom): loss가 있음         
➡ 입력 이미지는 loss function으로 가기까지 많은 layer를 거쳐 변형을 겪게 됨       
![image](https://user-images.githubusercontent.com/76824611/169677605-d3a95727-b18a-4c0a-b209-9f414035b992.png)



**[EX: neural turing machine(딥러닝 모델)]**    
* 미치게 복잡해보이지만 어짜피 계산 그래프기 때문에 결국 시간이 지남에 따라 이것을 풀게 될 것이다.
![image](https://user-images.githubusercontent.com/76824611/169678277-c59d95c6-4295-4abc-95bf-18be2cd363c1.png)


---
----


# Backpropagation
그럼 이제 계산그래프를 이용하여 역전파(Backpropagation)를 구해볼 것이다.    

**[기본 계산 원리]**     
그 전에 역전파의 원리에 대해서 간단하게 살펴보자.     
책에서는 그냥 넘어가 있어서 꼭 한번씩 읽어보고 포스트를 이어 읽는것이 이해에 많은 도움이 된다.    
* [덧셈 노드의 역전파](https://yerimoh.github.io/DL4/#%EB%8D%A7%EC%85%88-%EB%85%B8%EB%93%9C%EC%9D%98-%EC%97%AD%EC%A0%84%ED%8C%8C)     
* [곱셈 노드의 역전파](https://yerimoh.github.io/DL4/#%EA%B3%B1%EC%85%88-%EB%85%B8%EB%93%9C%EC%9D%98-%EC%97%AD%EC%A0%84%ED%8C%8C)     
* ➕ [예시로 완전히 이해하기](https://yerimoh.github.io/DL4/#%EC%98%88%EC%8B%9C-%EC%A0%81%EC%9A%A9)    






----


## 계산 


1️⃣ **computational grap로 나타내기**      
* 첫 번째 단계는 항상 함수 f를 이용해서 computational graph로 나타내는 것    
* $$x,y,z$$로 구성된 function $$f$$는 $$(x+y)* z$$라는 함수인 것 파악 가능     
* computational graph: 오른쪽 그림   
    * x,y에 대한 덧셈 노드 존재      
    * 다음 연산을 위한 곱셈 노드도 존재     
* 이 네트워크에 우리가 가지고 있는 값을 전달할 것임       
    * $$x= -2$$    
    * $$y= 5$$   
    * $$z= -4$$    
* 중간중간 값도 계산하여 computational graph에 값을 적어둠    
    * $$x+y = 3$$    
    * 마지막으로 최종 노드를 통과시켜 -12를 얻음         
![image](https://user-images.githubusercontent.com/76824611/169678807-19ef0469-e8d6-4fff-87b5-62f598e2fc70.png)


2️⃣ **gradient 표시**         
* 중간 변수에 이름붙임     
   * $$q$$: $$x+y$$ 덧셈 노드    
   * $$f$$: $$q* z$$ 곱셈 노드    
* 각각의 gradient를 표시       
   * 빨간색 네모   
       * $$x$$와 $$y$$에 각각에 대한 $$q$$의 gradient를 표시     
       * 단순 덧셈이기 때문에 1(원래 값 유지)     
   * 파란색 네모     
       *  $$q$$와 $$z$$에 대한 $$f$$의 gradient           
       *  곱셈규칙에 의해 각각 z와 q(서로 값이 바뀜)        
* 우리의 목표    
  *  $$x,y,z$$ 각각에 대한 f의 gradient 찾기
![image](https://user-images.githubusercontent.com/76824611/169679054-ecb9c7aa-095c-4fc5-b84a-48acf8748da6.png)



3️⃣ **backpropagation**       
* backpropagation은 [chain rule](https://yerimoh.github.io/DL4/#%EC%97%B0%EC%87%84%EB%B2%95%EC%B9%99-chain-rule)의 재귀적인 응용임        
    * chiain rule에 의해 뒤에서부터 gradient를 계산을 시작함      
* **3-1)** f 계산    
    * 우리는 출력의 $$f$$에 대한 gradient를 계산(가장 뒤)      
    * $$∂f/∂f$$=1      
    * $$f$$ gradient= 1      
    ![image](https://user-images.githubusercontent.com/76824611/169679373-e0f74d57-eb27-4cb8-ab52-f11a40aaa4d2.png)    
* **3-2)** z 계산     
   * $$q$$: $$z$$에 대한 $$f$$의 미분(위의 파란색 네모)([곱셈 법칙](https://yerimoh.github.io/DL4/#%EB%8D%A7%EC%85%88-%EB%85%B8%EB%93%9C%EC%9D%98-%EC%97%AD%EC%A0%84%ED%8C%8C)에 의해)      
   * $$∂f/∂z$$= $$q$$ = 3        
   * $$z$$ gradient= 3      
   ![image](https://user-images.githubusercontent.com/76824611/169679469-157869d0-877e-437c-bcb2-7662fe4aab31.png)
* **3-3)** q 계산      
   * $$z$$: $$q$$에 대한 $$f$$의 미분(위의 파란색 네모)([곱셈 법칙](https://yerimoh.github.io/DL4/#%EB%8D%A7%EC%85%88-%EB%85%B8%EB%93%9C%EC%9D%98-%EC%97%AD%EC%A0%84%ED%8C%8C)에 의해)      
   * $$∂f/∂q$$= $$z$$ = -4        
   * $$q$$ gradient= -4  
   ![image](https://user-images.githubusercontent.com/76824611/169680892-2982de2e-5506-40d1-bec1-cad1bbd473db.png)
* **3-4)** y 계산      
   * y에 대한 f의 미분값을 알고 싶지만 여기에서 y는 f와 바로 연결되어 있지 않음    
   * 구하기 위해서 chain rule을 이용함     
   * y에 대한 f의 미분은 q에 대한 f의 미분과 y에 대한 q의 미분의 곱으로 나타낼 수 있음   
   * 즉, q* f의 연산에서 q의 영향을 구할때와 동등함       
   * 즉,  y에 대한 q의 미분은 1이므로 q값 그대로 흘려보냄.       
       * 만약 우리가 y를 조금 변화시키면 q는 그것의 영향력 만큼 조금 변할것임.      
       * 이것은 내가 y를 조금 바꿨을때 그것이 q에 대해 1만큼의 영향력을 미치는 것을 의미함.       
   * $$∂f/∂y$$= $$z$$ = -4          
   * $$y$$ gradient= -4         
   ![image](https://user-images.githubusercontent.com/76824611/169681084-2a1d758c-1160-4a0d-bf91-efb85e7bea21.png)
* **3-5)** x 계산    
   * 3-4)와 똑같음      
   * f에 대한 x의 gradient는 -4 * 1 이므로 -4임.    
   ![image](https://user-images.githubusercontent.com/76824611/169681113-dfa75545-aba0-4919-87de-8f61e0315d52.png)






---


## 특징: local 계산  
위의 backpropagation에서 우리는 computational graph안에 모든 노드를 포함하고 있었다.    
하지만 각 노드는 오직 **주변에** 대해서만 관심이 있다.       
➡ 우리가 가지고 있는건 **각 노드**와 **각 노드의 local 입력**일 뿐 한 노드가 ~~전체~~를 보지 않는다.    


입력은 그 노드와 연결되어있고 값은 노드로 흘러들어가 이 노드로부터 출력을 얻게 된다.


그림을 통해 위의 특징을 이해해 보자,      
* 그래프 해석     
    * local 입력: x,y    
    * 출력: z          
* 이 노드에서 우리는 local gradient를 구할 수 있음      
    * z에서 x에 대한 gradient를 구할 수 있으며 y에 대한 gradient도 마찬가지이다.     
* 그것들은 쉽게 gradient를 구할 수 있고, 우리는 그것을 찾기 위해 복잡한 미적분을 할 필요가 없다.    
* 간단한 연산이다.      
![image](https://user-images.githubusercontent.com/76824611/169681387-d30d299a-dbb8-49e0-b660-67455d2e5bc4.png)
* chain rule을 사용해서, z에 대한 L의 gradient와 그리고 y에 대한 z의 gradient를 서로 곱한다.
     * chain rule에 의해 이 둘의 곱은 원하는 gradient를 준다.      
* 그리고 나서 우리가 gradient를 구하면 이 노드와 연결된 직전 노드로 전달가능하다.     
![image](https://user-images.githubusercontent.com/76824611/169681678-0b5670c2-52c8-43d2-8be7-fa2ea529ff49.png)


**[local 계산]**     
**각 노드**는 우리가 backpropagation을 통해 계산한 **local gradient**를 가지고 있다.     
이 값들은 상위 노드 방향으로 계속 전달되고,       
우리는 이것을 받아 local gradient와 곱하기만 하면 된다.     
➡ **우리는 노드와 연결된 노드 이외의 다른 어떤 값에 대하여 신경쓰지 않아도 된다.** 이 특징을 local 계산이라고 한다.               

----

## 복잡한 예제 
이번엔 조금 더 복잡한 예제로 계산해 보겠다.      
➡ backpropagation이 얼마나 유용한 방식인지를 알게 될 것이      



1️⃣ **computational grap로 나타내기**      
* 첫 번째 단계는 항상 함수 f를 이용해서 computational graph로 나타내는 것    
* $$w,x$$에 대한 함수 f는 $$1 / e^{-(w_0x_0 + w_1x_1 + w_2)}$$인 것 파악 가능     
* computational graph 분석       
    * $$w$$와 $$x$$에 대한 곱셈 노드 존재      
    * $$x_0$$과 $$x_0$$, $$x_1$$과 $$x_1$$ 그리고 $$w_2$$에 대한 덧셈 노드도 존재     
    * -1을 곱하고 exponential을 취하고, 1을 더함     
    * 마지막으로 모든 term을 역수로 뒤집음         
 * 우리에게 ws와 xs가 주어졌다고 했을때, 우리는 값을 이용해 모든 단계마다 계산을 통해서 앞으로(오른쪽 방향으로) 진행할 수 있음   
![image](https://user-images.githubusercontent.com/76824611/169682353-36f30652-1b64-4213-a641-37431fe1dc66.png)


2️⃣ **backpropagation**       
* 다시한번 그래프의 뒤에서부터 시작         
* **3-1)** 최종 출력 f 계산    
    * $$1/x$$ 이전의 input에 대한 gradient       
    * 우리는 출력의 $$f$$에 대한 gradient를 계산(가장 뒤)      
    * $$∂f/∂f$$=1      
    * $$f$$ gradient= 1      
    ![image](https://user-images.githubusercontent.com/76824611/169682944-a0bd84bc-1b9b-4138-8834-42e1ace0726c.png)   
* **3-2)** $$1/x$$에 대한 local gradient 계산     
   * $$x$$에 대한 $$f$$의 미분은 $$-1/(x^2)$$$     
   * x = 1.37      
   * $$∂f/∂x$$= $$-1/(x^2)$$             
   * $$x$$ gradient= $$1/(-1.37^{2}) * 1$$ $$= -0.53$$        
   ![image](https://user-images.githubusercontent.com/76824611/169683242-c96e20ae-a404-483c-a79a-72157744f82c.png)
* **3-3)** +1 에 대한 local gradient 계산     
   * upstream gradient는 -0.53          
   * (x+상수)에 대한 local gradient는 1(덧셈노드니까)      
   * $$∂f/∂x$$= 1            
   * $$x$$ gradient= $$1 * -0.53$$ = -0.53     
   ![image](https://user-images.githubusercontent.com/76824611/169683265-b8946e6e-e7db-46ff-a472-bf735bdd7a3a.png)  
* **3-4)** exponential 에 대한 local gradient 계산     
   * upstream gradient는 -0.53          
   * local gradient는 chain rule에 의해 gradient가 $$-0.53* e^x$$임      
   * x = -1  
   * $$∂f/∂x$$= $$e^x$$         
   * $$x$$ gradient=  $$-0.53* e^x$$ = -0.2      
   ![image](https://user-images.githubusercontent.com/76824611/169683510-3e3af4b3-0e2c-4d90-8972-83ae89c21aa9.png)
* **3-5)** $$* -1$$ 에 대한 local gradient 계산     
   * upstream gradient는 -0.2          
   * local gradient는 x에 대한 f의 미분이므로 $$1 * -1 = - 1$$이다.           
   * $$∂f/∂x$$= $$-1$$  
   * $$x$$ gradient=  = $$-1 * -0.2$$  = 0.2             
   ![image](https://user-images.githubusercontent.com/76824611/169683701-6102e315-e3d6-4017-b461-b8a66f03cbf8.png)
* **3-6)** 덧셈노드에 대한 local gradient 계산     
   * upstream gradient는 0.2          
   * local gradient는 1이다(덧셈노드는 그래도 보내주면 된다)        
   * $$∂f/∂x$$= $$1$$     
   * $$x$$ gradient= $$1 * 0.2$$ = 0.2             
   ![image](https://user-images.githubusercontent.com/76824611/169683753-0dca688e-2439-49e3-a67a-2140e417be0c.png)
* **3-7)** $$w₀$$과 $$x₀$$에 대한 local gradient 계산     
   * upstream gradient는 0.2          
   *  곱셈 노드에서 input에 대한 gradient는 한 인풋에 대하여 다른 인풋의 값임             
   * $$∂f/∂w₀$$= $$x₀$$, $$∂f/∂x₀$$= $$w₀$$           
   * $$w₀$$ gradient= -1 x 0.2 = -0.2   
   * $$x₀$$ gradient= 2 x 0.2 = 0.4           
   ![image](https://user-images.githubusercontent.com/76824611/169683906-07fb7c8e-0d20-418b-9a76-ecbfb30b206d.png)



**[왜 analytic gradient를 유도해 값을 계산하는 것보다 단순할까?]**     
* [전 챕터](https://yerimoh.github.io/DL202/#%ED%95%B4%EC%84%9D%EC%A0%81-%EB%AF%B8%EB%B6%84-analytic-gradient)에서 나온 질문이다.     
* 우리가 써야만 하는 local gradient에 대한 표현을 여기에서 볼 수 있다.     
* 우린 곱셈으로 나타내기 위해 chain rule을 사용했고 뒤에서부터 시작하여 모든 변수에 대한 gradients를 구했다. * w0과 x0에 대한 gradient 또한 같은 방법을 사용해 구했다.     
* 여기서 중요한건, 우리가 computational graph를 만들 때, computational 노드에 대해 **우리가 원하는 세분화된 정의**를 할 수 있다.        
    * 위의 예제들에서 덧셈과 곱셈, 절대적으로 작은 단위로 쪼갰다.   
    * 덧셈과 곱셈은 더 단순해질 수 없다.         
    * 필요하다면 이 노드들을 더 복잡한 그룹으로 묶는 것도 가능하다.          
    * 우리는 그 노드에 대한 local gradient를 적어두기만 하면 된다.          


253
00:26:44,600 --> 00:26:53,950
예를 들어 sigmoid 함수를 보겠습니다.
sigmoid 함수에 대한 정의를 오른쪽 상단에 적어놓았습니다.

254
00:26:55,691 --> 00:27:03,404
이는 1 / (1 + e^-x)와 같습니다.
그리고 이것은 남은 수업에서 많이 보게 될 함수입니다.

255
00:27:03,404 --> 00:27:12,686
그리고 우리는 이것에 대한 gradient를 계산할 수 있습니다.
우리는 이것을 사용할 수 있으며, 분석적으로 이를 살펴본다면

256
00:27:12,686 --> 00:27:16,427
마지막에 좋은 표현을 얻을 수 있습니다.

257
00:27:16,427 --> 00:27:26,609
따라서 이 예제에서 이것은 (1- sigma(x)) * sigma(x)와 같습니다.

258
00:27:26,609 --> 00:27:33,210
그리고 이 경우에 우리는 sigmoid를 만들기 위한 모든 계산을 그래프에 가지고 있습니다.

259
00:27:33,210 --> 00:27:37,038
그리고 우리는 이것을 하나의 big 노드, sigmoid로 바꿀 수 있습니다.

260
00:27:37,038 --> 00:27:43,655
우리는 이 gate의 local gradient, expression
그리고 x에 대한 sigmoid의 미분을 알고 있기 때문이죠. 그렇죠?

261
00:27:43,655 --> 00:27:54,795
여기에서 가장 중요한 사실은 local gradient를 적을
수 있는한 더 복잡한 노드 그룹을 만들 수 있다는 것입니다.

262
00:27:54,795 --> 00:28:01,645
 그리고 이 모든것은 trade-off의 관계에 있습니다.

263
00:28:01,645 --> 00:28:06,433
간결하고 단순한 그래프를 얻기 위해 당신이 많은 수학을 알고 싶어하는 것과

264
00:28:06,433 --> 00:28:13,793
각 노드에서 단순한 gradient를 얻고 싶어하는 것 사이에서.

265
00:28:14,913 --> 00:28:19,290
그 결과 당신은 원하는 만큼 복잡한 computational graph를 쓸 수 있습니다.

266
00:28:19,290 --> 00:28:21,616
질문 있나요?

267
00:28:21,616 --> 00:28:28,149
[학생] 이것은 그래프 자체에 대한 질문입니다. 처음의 두 곱셈 노드의 weight들은 덧셈 노드와 연결되어 있지 않습니다.
특별한 이유가 있습니까? (y = wx + b의 느낌으로 질문을 한 듯)

268
00:28:28,149 --> 00:28:36,771
그것들은 하나의 덧셈 노드와 연결되어야 할 것 같은데 질문은
왜 w0와 x0이 w2와 연결이 되어 있지 않냐는 것이죠?

269
00:28:36,771 --> 00:28:46,179
 모든 추가 노드들은 서로 연결되어 있기 때문에 제 대답은
당신이 원한다면 그렇게 할 수 있다는 것입니다.

270
00:28:46,179 --> 00:28:50,479
그리고 실제로 당신은 그것을 원할수도 있습니다.
그것은 매우 단순한 노드이기 때문에.

271
00:28:50,479 --> 00:28:52,821
그래서 이 경우 저는 최대한 단순하게 썼습니다.

272
00:28:52,821 --> 00:29:04,499
각 노드는 오직 2개의 입력만 받게, 하지만 당신은 분명 그것을 할 수 있습니다.
이 질문에 대한 다른 질문이 있으십니까?

273
00:29:04,499 --> 00:29:13,806
computational graph로 생각하는 것은 저를 아주 편안하게 만듭니다.
gradient를 구할때나 다른 무언가의 gradient를 찾아야만 할 때,

274
00:29:15,406 --> 00:29:22,960
sigmoid 처럼 설사사 그 식이 정말로 복잡하고, 두려워도

275
00:29:22,960 --> 00:29:33,214
우리가 알다시피 필요하다면 이것을 유도해냈습니다.
computational graph의 관점에서 나는 적어나갔고

276
00:29:33,214 --> 00:29:44,058
나는 단순하게 진행할 수 있었다. backpropagation과 chain rule을 통해.
그리고 나는 필요한 gradient를 계산할 수 있었다.

277
00:29:44,058 --> 00:29:53,438
그리고 이것은 당신이 과제를 할 때 생각해야 합니다.
gradient를 찾기 어려울 때 언제든지 이 방식을 사용해서 알아낼 수 있습니다.

278
00:29:53,438 --> 00:29:57,285
모든 부분들을 쪼개고 chain rule을 적용할 수 있습니다.

279
00:29:57,285 --> 00:30:09,292
아시다시피 우리는 이 노드들을 sigmoid로 그룹화 하는 것을 보았습니다.
실제로 이것이 동등한지 확인했습니다.

280
00:30:10,558 --> 00:30:21,717
우리는 이것을 연결할 수 있습니다.
그러면 우리는 입력이 1 (초록색으로 쓰여 있음) 출력은 0.73을 갖습니다. 그렇죠?

281
00:30:21,717 --> 00:30:25,800
이것은 sigmoid 함수에 연결하면 될 것입니다.

282
00:30:26,745 --> 00:30:36,315
 그리고 gradient를 얻기 원한다면 하나의 완전한 sigmoid 노드를 이용할 수 있습니다.

283
00:30:36,315 --> 00:30:41,219
앞에서 이것의 local gradient를 유도했었습니다. 그렇죠?

284
00:30:41,219 --> 00:30:45,962
그것은 (1-sigmoid(x)) * sigmoid(x) 였죠.
즉 이것을 여기에 대입해봅시다.

285
00:30:45,962 --> 00:30:57,543
알고있는 값은 x는 0.73이고 이 값을 위의 식에
대입하면 gradient로 0.2를 얻게 됩니다.

286
00:30:57,543 --> 00:31:09,267
우리는 이것을 upstream gradient인 1과 곱합니다. 그러면 우리는 sigmoid
gate로 바꾸기 이전의 작은 노드들로 계산된 값과 정확히 같은 값을 얻을 수 있습니다.

287
00:31:09,267 --> 00:31:19,957
좋아요, 우리가 gradient를 뒤에서부터 가져오는 동안 이제 어떤 일이 일어나는지 봅시다.

288
00:31:23,714 --> 00:31:28,790
직관적 해석으로 알 수 있는 패턴이 있습니다.

289
00:31:28,790 --> 00:31:37,214
 덧셈 게이트는 gradient를 나눠줍니다.

290
00:31:37,214 --> 00:31:48,586
두 개의 브랜치와 연결되는 덧셈 게이트에서는 upstream
gradient를 연결된 브랜치에 정확히 같은 값으로 나눠줍니다.

291
00:31:48,586 --> 00:31:55,156
여기에는 생각해볼만한 것이 더 있습니다.
그렇다면 max 게이트 모양은 어떤 모양입니까?

292
00:31:55,156 --> 00:32:03,350
아래에 max 게이트가 있습니다.
입력으로 z와 w가 들어오고 z는 2, w는 -1입니다.

293
00:32:04,825 --> 00:32:11,444
그리고 최대값을 취한 다음에 그것을 통과시킵니다.

294
00:32:11,444 --> 00:32:20,068
이것에 대한 gradient를 구해보면, upstream gradient는 2네요 그렇죠.

295
00:32:20,068 --> 00:32:26,890
이 것(max 노드)의 local gradient는 무엇일까요?

296
00:32:30,057 --> 00:32:31,753
[학생] 하나는 0이되고 다른 하나는

297
00:32:31,753 --> 00:32:35,011
맞습니다.

298
00:32:35,011 --> 00:32:35,862
[학생이 마이크로 말하고 있다]

299
00:32:35,862 --> 00:32:45,873
맞습니다. z는 gradient 2를 가질 것이고
w는 gradient 0을 가질 것입니다.

300
00:32:45,873 --> 00:32:50,931
이것은 gradient가 단지 통과하는 효과입니다.

301
00:32:50,931 --> 00:32:57,140
하나는 전체 값이, 다른 하나에는 0의 gradient가 향하게 됩니다.

302
00:32:57,140 --> 00:33:04,844
우리는 이런 종류의 gradient 라우터에 대해 생각할 수 있습니다.
덧셈 노드에서는 같은 gradient를 들어오는 브랜치에게 전달해주기 때문에

303
00:33:05,949 --> 00:33:12,630
max 게이트는 그것을 받고 하나의 브랜치를 지정합니다.
그리고 이것은 forward pass를 볼 때 의미가 있습니다.

304
00:33:12,630 --> 00:33:19,393
값(value)은 무슨일이 일어날까요. 최대값이 남은 computational graph로 통과됩니다.

305
00:33:19,393 --> 00:33:27,647
그래서 결국 함수 계산에 실제로 영향을 주는 값은 유일한 값이므로

306
00:33:27,647 --> 00:33:32,610
따라서 gradient를 다시 전달할 때

307
00:33:33,544 --> 00:33:40,705
무엇을 조정하고 계산의 해당 지점을 통해 흐르게 하는 것이 바람직합니다.

308
00:33:40,705 --> 00:33:44,872
[학생이 마이크로 말하고 있다]

309
00:33:46,028 --> 00:33:56,627
네 답은 local gradient는 기본적으로 다른 변수의 값입니다.

310
00:33:59,408 --> 00:34:01,407
네 정확합니다.

311
00:34:01,407 --> 00:34:06,409
그래서 이것을 gradient switcher라고 생각할 수 있습니다.
맞습니까? Switcher, 저는 scaler라고 추측합니다.

312
00:34:06,409 --> 00:34:15,788
upstream gradient를 받아 다른 브랜치의 값으로 scaling 합니다.
네 그리고 다른 주목할만한 것은 우리가 여러 노드와 연결되어 있는 하나의 노드를 가지고 있을 때

313
00:34:20,205 --> 00:34:22,592
gradient는 이 노드에서 합산됩니다. 맞습니까?

314
00:34:22,592 --> 00:34:31,520
다 변수(multivariate) chain rule을 사용하는 노드들에서 우리는
단지 각 노드들로부터 들어오는 upstream gradient 값을 취하고

315
00:34:31,520 --> 00:34:35,274
그리고 이것들을 합합니다.

316
00:34:35,274 --> 00:34:43,004
그러면 당신은 이것들로부터 다 변수 chain rule을 볼 수 있습니다.
그리고 이것에 대해 생각해보세요,

317
00:34:43,005 --> 00:34:54,948
만약 당신이 이 노드를 조금 변경시킨다면
그래프를 따라 forward pass를 할 때 연결된 노드들에게 영향을 미칠 것입니다.

318
00:34:54,949 --> 00:35:03,803
그리고 당신이 backpropagation을 할 때 이 두개의
gradient가 돌아오면 이 노드에 영향을 미치게 될 것입니다.

319
00:35:03,803 --> 00:35:07,872
따라서 이 값을 더해야 총 upstream gradient가 됩니다.

320
00:35:07,872 --> 00:35:12,725
backpropagation에 대한 질문이 있습니까?

321
00:35:12,725 --> 00:35:15,892


322
00:35:18,439 --> 00:35:23,429
[학생] 우리는 실제 가중치에 대해 아무런 업데이트도 하지 않았습니다.

323
00:35:23,429 --> 00:35:25,490
[마이크로 말하는 중]

324
00:35:25,490 --> 00:35:31,418
네 질문은 우리는 아직 이 가중치 값에 대해서 아무런 업데이트를 하지 않았다는 것입니다.

325
00:35:31,418 --> 00:35:35,994
우리는 변수에 대한 gradient만 찾았습니다. 맞습니다.

326
00:35:35,994 --> 00:35:41,206
이 강의에서 이야기 한 것은 어떻게 함수에서 변수에 대한 gradient를 찾는가였습니다.

327
00:35:41,206 --> 00:35:48,210
찾은 다음 마지막 최적화 강의에서 배운 모든 것을 적용할 수 있습니다.

328
00:35:48,210 --> 00:35:51,258


329
00:35:51,258 --> 00:35:57,363
그래서 gradient가 주어지면, 우리는 가중치를 업데이트하기 위한 다음 스텝을 밟습니다.

330
00:35:57,363 --> 00:36:04,331
 당신은 이 모든 구조를 우리가 마지막
최적화강의에서 배운 것에 적용할 수 있습니다.

331
00:36:04,331 --> 00:36:11,196
여기에서 우리가 한 것은 임의의 복잡한 함수에 대해 gradient를 계산하는 것을 배운 것입니다.

332
00:36:11,196 --> 00:36:20,039
따라서 나중에 신경망과 같은 복잡한 함수에 대해 이야기 할 때 유용할 수 있습니다.

333
00:36:20,039 --> 00:36:24,453
[학생] 모든 변이에 대해서 적어 주실 수 있습니까?
이 슬라이드를 이해하는데 도움이 되기 위해

334
00:36:24,453 --> 00:36:31,069
그래요 저는 이것을 보드에 쓸 수 있습니다.

335
00:36:32,851 --> 00:36:39,544
만약 우리가 함수 f의 x에 대한 gradient를 가지고 있으면

336
00:36:39,544 --> 00:36:45,821
 변수 i에 의해 연결되어있다고 가정해 보겠습니다.

337
00:36:49,203 --> 00:37:03,311
 x는 여러 요소들과 연결되어 있다고 할 수 있습니다.

338
00:37:03,311 --> 00:37:15,217
이 경우에는 ∂qi이고, 그리고 chain rule은 모든것을
취하고 있으며 각각의 중간 변수의 효과를 취할 것입니다.

339
00:37:16,453 --> 00:37:26,447
우리의 최종 출력 f, 그리고 다음 변수 x가 해당 중간 값에 미치는 영향에 대해.

340
00:37:26,447 --> 00:37:29,127


341
00:37:29,127 --> 00:37:39,742
이것은 기본적으로 모든 것들을 합쳐 놓은 것입니다.
좋아요, 이제 우리는 스칼라 값에 대한 모든 예제를 완료했습니다.

342
00:37:42,203 --> 00:37:50,250
이제 벡터가있을 때 어떻게 될까요?
우리의 변수 x,y,z에 대해서 숫자 대신에 vector를 가지고 있다고 합시다.

343
00:37:50,250 --> 00:37:59,428
모든 흐름은 정확히 같습니다.
차이점이라면 우리의 gradient는 Jacobian 행렬이 될 것입니다.

344
00:37:59,428 --> 00:38:10,574
그래서 이것들은 각 요소의 미분을 포함하는 행렬이 될 것입니다.
예를들어 x의 각 원소에 대해 z에 대한 미분을 포함하는.

345
00:38:14,237 --> 00:38:24,049
좋아요, 예제를 하나 봅시다.
우리의 입력은 4096차원의 벡터입니다.

346
00:38:24,049 --> 00:38:29,797
그리고 이것은 CNN에서 흔히 볼 수 있는 사이즈입니다.

347
00:38:29,797 --> 00:38:37,918
 그리고 이 노드는 요소별로(elementwise) 최대값을 취합니다.

348
00:38:37,918 --> 00:38:45,295
우리는 f를 가지고 있습니다.
요소별로 0과 비교해 최대 값을 같는 x를 취하는,

349
00:38:46,875 --> 00:38:54,052
그리고 출력 또한 4096 차원의 벡터입니다.
좋아요, 이 경우에 Jacobian 행렬의 사이즈는 얼마인가요?

350
00:38:56,969 --> 00:39:10,396
이전에 말했던 것을 기억해보세요
Jacobian 행렬의 각 행은 입력에 대한 출력의 편미분이 될 것입니다.

351
00:39:11,705 --> 00:39:17,572
제가 들은 답은 4096의 제곱인데요 맞습니다.

352
00:39:17,572 --> 00:39:21,405
그리고 4096*4096은 매우 큽니다.

353
00:39:22,261 --> 00:39:25,203
그리고 실질적으로 이것은 더 커질 수 있습니다.

354
00:39:25,203 --> 00:39:30,692
예로들면 100개의 인풋을 동시에 입력으로 같는 배치를 이용해 작업할 수 있습니다.

355
00:39:30,692 --> 00:39:38,739
그것은 우리의 노드를 더욱 효율적으로 만들지만 그것은 100배 커지게 만듭니다.

356
00:39:38,739 --> 00:39:45,082
실제로 Jacobian은 4096000 * 4096000입니다.

357
00:39:45,082 --> 00:39:50,750
이것은 너무 거대해서 작업에 실용적이지 않습니다.

358
00:39:50,750 --> 00:39:57,507
실제로는, 우리는 이 거대한 Jacobian을 계산할 필요가 없습니다.

359
00:39:57,507 --> 00:40:00,902
그러면 이 Jacobian 행렬이 어떻게 생겨나게 되었을까요?

360
00:40:00,902 --> 00:40:04,509
만약 우리가 요소별로 최대값을 같는 여기에서 어떤 일이 일어나는지 생각해 본다면

361
00:40:04,509 --> 00:40:08,312
 우리는 각각의 편미분에 대해 생각해볼 수 있습니다.

362
00:40:08,312 --> 00:40:13,888
 입력의 어떤 차원이 출력의 어떤 차원에 영향을 줍니까?

363
00:40:13,888 --> 00:40:19,686
 이 Jacobian 행렬에서 어떤
종류의 구조를 볼 수 있습니까?

364
00:40:19,686 --> 00:40:21,198
[학생 대답 중]

365
00:40:21,198 --> 00:40:23,869
네 들었습니다. 대각선, 정확합니다.

366
00:40:23,869 --> 00:40:27,703
이것은 요소별 이기 때문에, 입력의 각 요소, 첫번째 차원은

367
00:40:27,703 --> 00:40:31,109
오직 출력의 해당 요소에만 영향을 줍니다.

368
00:40:31,109 --> 00:40:38,014
 그렇기 때문에 우리의 Jacobian
행렬은 대각행렬이 될 것입니다.

369
00:40:38,014 --> 00:40:47,198
 실제로 이 전체 Jacobian 행렬을 작성하고 공식화 할 필요는 없습니다.

370
00:40:47,198 --> 00:40:51,365
우리는 출력에 대한 x의 영향에 대해서 그리고 이 값을 사용하는 것에 대해서만 알면 됩니다.

371
00:40:54,986 --> 00:41:01,800
 그리고 우리가 계산한 gradient의
값을 채워 넣으면 됩니다.

372
00:41:04,100 --> 00:41:06,716
좋아요, 이제 computational graph보다 구체적인 벡터화 된 예제를 보겠습니다.

373
00:41:06,716 --> 00:41:11,724
 보겠습니다. x와 W에 대한 함수 f는

374
00:41:11,724 --> 00:41:19,232
 x에 의해 곱해진 W의 L2와 같습니다.

375
00:41:22,407 --> 00:41:26,763
그리고 이 경우 x를 n-차원이라고
하고, W는 n*n이라고 합시다.

376
00:41:29,076 --> 00:41:30,563
네 처음부터 해 봅시다.

377
00:41:30,563 --> 00:41:32,635
computational graph 쓰기.. 맞죠?

378
00:41:32,635 --> 00:41:35,303
우리는 x에 의해 곱해진 W를 가지고 있고

379
00:41:35,303 --> 00:41:37,886
그리고 L2가 뒤따릅니다.

380
00:41:39,520 --> 00:41:42,822
이제 값을 채워 넣읍시다.

381
00:41:42,822 --> 00:41:45,382
W는 2x2 행렬인 것을 볼 수 있고

382
00:41:45,382 --> 00:41:47,560
x는 2차원의 벡터입니다. 맞죠?

383
00:41:47,560 --> 00:41:53,949
 우리는 중간 노드를 다시 쓸 수 있습니다.

384
00:41:53,949 --> 00:41:56,653
그것은 q로 할 것이고, W와 x의 곱입니다.

385
00:41:56,653 --> 00:42:02,666
 이것은 요소별 방식으로 쓸 수 있습니다.

386
00:42:02,666 --> 00:42:05,632
W1,1 * x1 + W1,2 * x2 등등

387
00:42:05,632 --> 00:42:12,611
그러면 우리는 이제 f를 q에 대한 표현으로 나타낼 수 있습니다. 맞죠?

388
00:42:12,611 --> 00:42:15,508
따라서 q의 두 번째 노드를 보면

389
00:42:15,508 --> 00:42:18,175
이것은 q의 L2 norm과 같습니다.

390
00:42:19,260 --> 00:42:24,218
이것은 q1의 제곱과 q2의 제곱을 합친 것과 같습니다.

391
00:42:24,218 --> 00:42:25,923
네 이것을 채워넣었습니다.

392
00:42:25,923 --> 00:42:29,423
우리는 q를 구했고 최종 출력을 구했습니다.

393
00:42:30,491 --> 00:42:33,218
좋아요, 이제 이것에 대해 backpropagation을 해봅시다.

394
00:42:33,218 --> 00:42:40,500
출력에 대한 gradient를 가지고 있고 이것은 1입니다. 이것은 항상 첫 단계입니다.

395
00:42:44,084 --> 00:42:47,505
이제 한 노드 뒤로 이동합시다.

396
00:42:47,505 --> 00:42:50,902
우리는 L2 이전의 중간 변수인 q에 대한 gradient를 찾기 원합니다.

397
00:42:50,902 --> 00:42:58,719
 그리고 q는 2차원의 벡터입니다.

398
00:42:58,719 --> 00:43:04,232
그리고 우리가 진짜 원하는것은 q의 각각의 요소가 f의 최종 값에 어떤 영향을 미치는지 입니다.

399
00:43:07,918 --> 00:43:09,586
그리고아래에 f를 위한 식을 적어놓았습니다.

400
00:43:09,586 --> 00:43:11,955
여기서 우리는 바닥에 f를 위해
를 써 넣었습니다.  우리는 f

401
00:43:11,955 --> 00:43:14,705
qi에 대한 f의 gradient를 볼 수 있습니다.

402
00:43:15,644 --> 00:43:19,293
이것을 q1이라고 부르겠습니다.

403
00:43:19,293 --> 00:43:23,136
이것은 qi * 2가 될 것입니다. 그렇죠?

404
00:43:23,136 --> 00:43:26,569
이것은 도함수를 구하는 것이고, 우리는 이것에 대한 식을 가지고 있습니다.

405
00:43:26,569 --> 00:43:32,453
 각각의 요소 qi에 대해서.

406
00:43:32,453 --> 00:43:34,606
우리는 또한 이것을 벡터의 형태로 쓸 수 있습니다.

407
00:43:34,606 --> 00:43:39,869
 이것은 우리의 벡터 q에 대해서 2를 곱한 것이었습니다.
그렇죠?

408
00:43:39,869 --> 00:43:41,719
그리고 우리는 이 벡터에서 gradient를 0.44와 0.52를 얻었습니다.

409
00:43:41,719 --> 00:43:50,656
 그리고 여러분은 그것이 q를 취해 단순히 2로 곱한것임을 보았습니다.

410
00:43:50,656 --> 00:43:55,192
 각 요소는 2로 곱해졌습니다.

411
00:43:55,192 --> 00:43:58,772
벡터의 gradient는 항상 원본 벡터의 사이즈와 같습니다.

412
00:43:58,772 --> 00:44:05,015
 그리고 gradient의 각 요소는 의미합니다.

413
00:44:06,530 --> 00:44:12,549
함수의 최종 출력에 얼마나 특별한 영향을 미치는지.

414
00:44:16,126 --> 00:44:18,920
좋아, 그럼 이제 한 걸음 뒤로 이동해봅시다.

415
00:44:18,920 --> 00:44:22,523
W의 gradient는 무엇입니까?

416
00:44:22,523 --> 00:44:25,639
그리고 여기에서 우리는 다시 chain rule을 사용할 것입니다.

417
00:44:25,639 --> 00:44:29,129
 우리가 W에 대한 q의 local
gradient를 계산하기 원한다면

418
00:44:29,129 --> 00:44:31,328
요소별 연산을 다시 해야할 것입니다.

419
00:44:31,328 --> 00:44:37,884
 각각의 q에 대한 영향을 봅시다.

420
00:44:37,884 --> 00:44:41,636
W의 각 요소에 대한 q의 각 요소 그리고 이것은

421
00:44:41,636 --> 00:44:43,106
이전에 말했던 Jacobian일 것입니다.

422
00:44:43,106 --> 00:44:47,592
그리고 우리는 곱셈의 형태로 볼  수 있습니다.

423
00:44:47,592 --> 00:44:49,509
여기에서 q는 W*x와 같습니다.

424
00:44:49,509 --> 00:44:53,092
q의 첫번째 요소의 gradient는 무엇입니까?

425
00:44:56,236 --> 00:44:59,197
도함수 또는 첫 번째 요소의 기울기이므로

426
00:44:59,197 --> 00:45:03,962
W11에 대한 첫 번째 요소가 위로 올라갑니다.

427
00:45:03,962 --> 00:45:07,470
그래서 W11에 대한 q1

428
00:45:07,470 --> 00:45:09,157
그 값은 무엇입니까?

429
00:45:09,157 --> 00:45:10,851
x1, 정확합니다.

430
00:45:10,851 --> 00:45:14,068
우리는 이것이 x1인 것을 알고있죠.

431
00:45:14,068 --> 00:45:21,826
그리고 우리는 이것을 Xj와 동일한 Wi,j에 대한 qk의 식으로 일반화 할 수 있습니다.

432
00:45:24,313 --> 00:45:30,278
그리고 우리는 f에 대한 gradient를 찾기 원합니다. 각 Wij에 대해서

433
00:45:31,523 --> 00:45:35,332
이것들의 도함수를 봅시다.

434
00:45:35,332 --> 00:45:38,339
우리는 chain rule을 사용할 수 있습니다.

435
00:45:38,339 --> 00:45:41,672
여기에서 기본적으로 qk에 대한 f의 미분을 합성 할 때

436
00:45:43,770 --> 00:45:47,270
Wij의 각 요소인 dqk/Wij의 각 요소에 대해합니다. 그렇죠?

437
00:45:48,934 --> 00:45:53,563
 그래서 우리는 W의 각 요소들의
영향을 찾을 수 있습니다.

438
00:45:53,563 --> 00:45:57,649
q의 각 요소들에 대해서, 그리고 모든 q에 대해 이것들을 모두 더합니다.

439
00:45:57,649 --> 00:46:03,236
그리고 만약 당신이 이것을 적다보면 이것은 결국
2 * qi * xj와 같은 식을 나타냅니다.

440
00:46:05,898 --> 00:46:08,744
좋아요, 우리가 구한 W에 대한 gradient를 채우면

441
00:46:08,744 --> 00:46:14,270
 각 요소들에 대해 다시 계산할 수 있습니다.

442
00:46:14,270 --> 00:46:20,943
또는 우리가 유도한 식을 벡터화된 형식으로 작성할 수 있습니다.

443
00:46:22,028 --> 00:46:24,827
좋아요, 중요한 것은 변수에 대해 gradient를 항상 체크하는 것이에요.

444
00:46:24,827 --> 00:46:31,137
 이것들은 항상 변수와 같은
모양(shape)을 가지고 있습니다.

445
00:46:31,137 --> 00:46:40,709
gradient가 무엇인지 계산을 했으면 이것이 변수의 모양과 동일한지 확인하세요.
이것은 검사하는데 있어서 매우 유용합니다.

446
00:46:42,710 --> 00:46:53,343
다시 말해, gradient의 요소는 정량화합니다, 얼마나 최종 출력에 영향을 미치는지를.

447
00:46:54,177 --> 00:46:55,010
응?

448
00:46:55,010 --> 00:46:57,489
[학생이 마이크로 말하는 중]

449
00:46:57,489 --> 00:47:04,177
양쪽 모두 지시 함수(indicator function)입니다.
이것은 k와 i가 같으면 1임을 의미합니다.

450
00:47:08,276 --> 00:47:11,571
좋아요, 봅시다.

451
00:47:11,571 --> 00:47:14,738
이제 또 하나의 예제를 보겠습니다.

452
00:47:16,647 --> 00:47:21,031
이제 우리가 찾아야 할 마지막 것은
qI에 대한 gradient입니다.

453
00:47:21,031 --> 00:47:28,574
dqk/dxi가 Wki와 같은 것을 볼 수 있습니다.

454
00:47:29,535 --> 00:47:32,702
우리가 W에 했던 것과 같은 방식으로

455
00:47:34,833 --> 00:47:38,702
그리고 다시 우리는 체인 규칙을 사용할 수 있습니다.

456
00:47:38,702 --> 00:47:44,902
그리고 그것에 대한 총 표현식을 얻을 수 있습니다. 아시겠습니까? 그리고
이것은 x와 같은 모양을 가진 x에 대한 gradient가 될 것입니다.

457
00:47:44,902 --> 00:47:52,022
 또한 이것을 벡터 형식으로 쓸 수 있습니다.

458
00:47:53,529 --> 00:47:56,442
좋아요, 그럼 이것에 대한 질문은 없습니까?

459
00:47:56,442 --> 00:47:59,100
[학생이 마이크로 말하는 중]

460
00:47:59,100 --> 00:48:01,850
우리는 Jacobian을 계산하는 중입니다.

461
00:48:03,194 --> 00:48:05,694
뒤로 한번 돌아가보죠.

462
00:48:07,414 --> 00:48:10,774
계산하는 중에 우리는 xi에 대한 qk의 편도함수를 가지고 있습니다.

463
00:48:10,774 --> 00:48:16,439
 그리고 이것들은 Jacobian 행렬을 구성합니다.
맞죠?

464
00:48:16,439 --> 00:48:26,306
 그리고 실제로 우리가 할 일은 이것을 가져와서

465
00:48:26,306 --> 00:48:29,222
x에 대한 gradient의 벡터화된 표현으로써 chain rule에서 사용하는 것입니다.

466
00:48:29,222 --> 00:48:33,293
 여기에 Jacobian 행렬이 있습니다.
이것은 전치된 값입니다.

467
00:48:33,293 --> 00:48:38,926
 당신은 이것을 벡터화된 형식으로 쓸 수 있습니다.

468
00:48:38,926 --> 00:48:43,489
[학생이 마이크로 말하는 중]

469
00:48:43,489 --> 00:48:51,803
이 행렬의 경우 W와 크기와 같습니다.
그리고 이 행렬은 큰 경우는 아닙니다.

470
00:48:55,878 --> 00:48:58,941
그래서 우리가 생각한 방식은 computational graph에서 실제로 모듈화 된 구현과 같습니다.

471
00:48:58,941 --> 00:49:10,965
 우리는 각 노드를 local하게 보았고 upstream gradient와 함께
chain rule을 이용해서 local gradient를 계산했습니다.

472
00:49:10,965 --> 00:49:19,186
이것을 forward pass, 그리고 backward pass의 API로 생각할 수 있습니다.
forward pass에서는 노드의 출력을 계산하는 함수를 구현하고

473
00:49:19,186 --> 00:49:25,011
 backward pass에서는
gradient를 계산합니다.

474
00:49:25,011 --> 00:49:27,295
그래서 우리가 실제로 코드에서 이것을 구현한다면,

475
00:49:27,295 --> 00:49:30,592
우리는 똑같은 방식으로 이 작업을 수행 할 것입니다.

476
00:49:30,592 --> 00:49:34,865
각각의 gate에 대해 생각해 볼 수 있습니다.

477
00:49:34,865 --> 00:49:40,908
forward, backward 함수를 구현할 때
backward 함수는 chain rule을 이용해 계산하고

478
00:49:40,908 --> 00:49:45,572
만약 우리가 완전한 그래프를 가지고 있으면

479
00:49:45,572 --> 00:49:48,833
그래프의 모든 노드를 반복함으로써 전체 그래프를 forward pass로 통과시킬 수 있습니다.

480
00:49:48,833 --> 00:49:54,768
 여기에서는 게이트와 노드라는 단어를 사용하려고 합니다. 이
모든 게이트를 반복하여 각 게이트를 호출할 수 있습니다.

481
00:49:54,768 --> 00:50:01,349
 그리고 우리는 이것을 위치적으로
정렬 된 순서로 수행하기를 원합니다.

482
00:50:01,349 --> 00:50:03,357
우리는 노드를 처리하기 전에 이 전에 노드로 들어오는 모든 입력을 처리합니다.

483
00:50:03,357 --> 00:50:15,482
 그리고 backward로 보면, 역순서로 모든 게이트를
통과한 다음에 게이트 각각을 거꾸로 호출합니다.

484
00:50:16,564 --> 00:50:22,285
좋아요, 그러면 특정 게이트에 대한 구현
살펴보면 여기에 곱셈 게이트가 있습니다.

485
00:50:22,285 --> 00:50:26,960
 이것의 forward pass를 구현하려고 합니다.

486
00:50:26,960 --> 00:50:31,127
이것은 x,y를 입력으로 받고 z를 리턴합니다.

487
00:50:32,520 --> 00:50:40,167
그리고 backward로 진행 할 때는 입력으로 upstream gradient인
dz를 받고 출력으로 입력인 x와 y에 gradient를 전달할 것입니다.

488
00:50:40,167 --> 00:50:45,190
 그래서 여기서 출력은 dx와 dy입니다.

489
00:50:46,426 --> 00:50:51,567
그리고 이 예제의 경우 모든 것은 scalar 입니다.

490
00:50:51,567 --> 00:50:57,215
그리고 forward pass를 보면 중요한 것은

491
00:50:57,215 --> 00:51:03,156
forward pass의 값을 저장(cache)해야 하는 것입니다. 왜냐하면 이것(forward
pass)이 끝나고 backward pass에서 더 많이 사용하기 때문입니다.

492
00:51:03,156 --> 00:51:06,061
그리고 forward pass에서 x와 y도 저장하는데

493
00:51:06,061 --> 00:51:10,594
backward pass에서 chain rule을 사용하여

494
00:51:10,594 --> 00:51:12,930
upstream gradient 값을 이용해 다른 브랜치의 값과 곱합니다.

495
00:51:12,930 --> 00:51:20,294
 self.y 와 dz를 곱해 얻은
dx를 보관하게 될 것입니다.

496
00:51:20,294 --> 00:51:25,877
 dy에 대해서도 마찬가지입니다.

497
00:51:28,799 --> 00:51:32,879
네 이제 딥러닝 프레임워크 그리고 라이브러리를 보면

498
00:51:32,879 --> 00:51:35,358
그들은 이러한 종류의 모듈화를 따릅니다.

499
00:51:35,358 --> 00:51:42,040
 예를들면 Caffe는 유명한 딥러닝 프레임워크입니다.

500
00:51:43,284 --> 00:51:46,047
그리고 당신은 Caffe의 소스 코드를 볼 수 있습니다.

501
00:51:46,047 --> 00:51:48,351
당신은 layer라고 부르는 폴더를 구할 수 있으며

502
00:51:48,351 --> 00:51:54,400
레이어는 기본적으로 computational node이고 보통 레이어는
sigmoid 처럼 당신이 생각하는 것보다 좀 더 복잡합니다.

503
00:51:54,400 --> 00:52:01,555
 computational 노드의 전체 목록 볼 수 있습니다.
그렇죠?

504
00:52:01,555 --> 00:52:06,132
 당신은 sigmoid를 가질 수
있고 여기에 있을것 같네요,

505
00:52:06,132 --> 00:52:09,205
convolution 같은 것도 여기에 있네요

506
00:52:09,205 --> 00:52:11,925
Argmax도 다른 레이어로 있습니다.

507
00:52:11,925 --> 00:52:16,340
당신은이 모든 레이어들을 가지게 될 것입니다. 만약 당신이 이것들을 탐구하면 정확하게
forward pass, backward pass 등을 구현할 수 있습니다.

508
00:52:16,340 --> 00:52:20,929
 이 모든 것들은 호출됩니다.

509
00:52:20,929 --> 00:52:27,372
우리가 만든 전체 네트워크를 통해 forward/backward pass 할 때,
그래서 우리의 네트워크는 기본적으로

510
00:52:27,372 --> 00:52:30,393
우리가 네트워크에 사용하기위해 선택한 레이어들을 쌓게 됩니다.

511
00:52:30,393 --> 00:52:37,560
 이 경우 우리는 sigmoid
레이어를 살펴볼 수 있습니다.

512
00:52:37,560 --> 00:52:44,658
 우리는 sigmoid 함수에 대해서 이야기 했었습니다.

513
00:52:44,658 --> 00:52:47,140
그것의 forward pass를 볼 수 있습니다.

514
00:52:47,140 --> 00:52:53,084
sigmoid 식과 똑같은 계산을 합니다.
그리고나서 backward pass로 전달합니다.

515
00:52:53,084 --> 00:52:57,251
여기서 upstream gradient인 top_diff를 취합니다.

516
00:53:00,031 --> 00:53:06,325
 그리고 우리가 계산한 local
gradient와 그것을 곱합니다.

517
00:53:08,267 --> 00:53:10,539
그래서 과제 하나에서는 이런 종류의 연습을 하게 될 것입니다.

518
00:53:10,539 --> 00:53:16,829
 computational graph 방식으로
SVM과 Softmax 클래스를 작성하고

519
00:53:16,829 --> 00:53:21,041
 이들의 gradient를 구할 수 있습니다.

520
00:53:21,041 --> 00:53:24,005
항상 기억하세요 첫번째 스텝은 comput
