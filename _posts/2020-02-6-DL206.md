---
title: "[27] CS231N: Lecture 7 Training Neural Networks Part II"
date:   2020-02-6
excerpt: "Lecture 7 | Training Neural Networks Part II 요약"  
category: [Deep Learning]
layout: post
tag:
- Deep Learning
order: 0

comments: true
---



# 목차
------

👀 코드 보기 , 🤷‍♀️     
이 두개의 아이콘을 누르시면 코드, 개념 부가 설명을 보실 수 있습니다:)

------


[CS231N: Lecture 7](https://www.youtube.com/watch?v=_JB0AO7QxSA&list=PLC1qU-LWwrF64f4QKQT-Vg5Wr4qEE1Zxk&index=7)강의를 빠짐 없이 정리하였고, 어려운 개념에 대한 보충 설명까지 알기 쉽게 추가해 두었습니다.  





---
----






# Overview

**1. Fancier optimization**   

**2. Regularization**    
* 네트워크의 Train/Test Error 간의 격차를 줄이고자 사용하는 추가적인 기법     
* Neural Network에서 실제로 사람들이 사용하고 있는 Regularization 전략에 대해서 다뤄보도록 하겠음    


**3. Transfer Learning**    
* 원하는 양 보다 더 적은 데이터만을 가지고 있을때 사용할 수 있는 방법       

---


# **Fancier Optimization**    
지난 강의를 돌이켜보면 Neural network에서 가장 중요한 것은 바로 최적화 문제 였다는 것을 알 수 있다.       
Nerwork의 가중치에 대해서 **손실 함수를** 정의해 놓으면 이 손심 함수는 그 **가중치가 얼마나 좋은지 나쁜지**를 알려준다.       
그리고 우리는 손심 함수가 가중치에 대한 "산(landscape)"이라고 상상해 볼 수 있을 것이다.        

-----

## SGD의 문제


**[예제]**      
* X/Y축: 두 개의 가중치를 의미       
* 각 색: Loss의 값        
* 목표: 가장 붉은색인 지점을 찾는 것(가장 낮은 Loss를 가진 가중치를 찾는 것)        
![image](https://user-images.githubusercontent.com/76824611/176171196-7f84e041-2737-49ea-81a2-7ff76bfd07fd.png)

**[Solution: Stochastic Gradient Descent(SGD)]**    
* 지금까지 배운 것 중 위의 예제를 푸는 간단한 최적화 알고리즘     
* 코드는 아래 참조

<details>
<summary>👀 SGD 코드 & 코드 해석 보기</summary>
<div markdown="1">
  
* 미니 배치 안의 데이터에서 Loss를 계산      
* Gradient 의 반대 방향 을 이용해서 파라미터 벡터를 업데이트   
![image](https://user-images.githubusercontent.com/76824611/176171331-6fbc8e45-d027-478c-9742-48e679d99fa0.png)
  
**반대 방향인 이유**: 손실 함수는 내려가는 방향 이어야하기 때문      
 
 
 이 단계를 계속 반복하면 결국 붉은색 지역으로 수렴할 것이고 Loss가 낮을 것이다.
 
</div>
</details>


**[SGD의 문제]**     
**1. 속도 저하 문제**     
<details>
<div markdown="1">
  
* 우리의 손실함수가 아래와 같이 생겼고 여기에 똑같이 $$W_1$$과 $$W_2$$가 있다고 가정해보자      
* 둘중 어떤 하나는 업데이트를 해도 손실 함수가 아주 느리게 변한다.(수평 축의 가중치는 변해도 Loss가 아주 천천히 줄어든다)    
* Loss는 수직 방향의 가중치 변화에 훨씬 더 민감하게 반응      
* 현재 지점에서 Loss는 **bad condition number**를 지니고 있음      
* 이 지점의 Hessian maxrix의 **최대/최소 singular values 값의 비율이 매우 안좋음**    
* 이 문제는 고차원 공간에서 훨씬 더 빈번하게 발생    
* 예시는 2차원 밖에 안되지만 실제로는 가중치가 수천 수억개(수억개의 방향으로 움직임)     
➡ 수억개의 방향중에 불균형한 방향이 존재한다면 SGD는 잘 동작하지 않을 것     
![image](https://user-images.githubusercontent.com/76824611/176173285-940c6dea-57c3-4bf3-a32d-df2b4aa01b5e.png)

</div>
</details>  


**2. local minima**      

<details>
<div markdown="1">
  
* X축은 어떤 하나의 가중치를 나타내고 Y축은 Loss를 나타낸다고 하면 휘어진 손실함수는 중간에 "valley"가 하나 있다.     
* 이 valley에서 가중치가 0이 되어버려서 학습을 중단해버린다.(이 valley를 목적지라고 착각하는 것이다)     
* locally falt하다.      
![image](https://user-images.githubusercontent.com/76824611/176175402-d79d65f4-a2e6-47c1-af51-8678e69e3278.png)
  

</div>
</details>  

**3. saddle points**     

<details>
<div markdown="1">
  
* local minima는 아니지만, 한쪽 방향으로는 증가하고 있고 다른 한쪽 방향으로는 감소하고 있는 지역     
* 이런 곳에서도 gradient는 0이 되어 학습 중단    
* 이 지점 근처도 기울기가 매우 작기 떄문에 비슷한 문제를 일으킴       
* local minima보다 훨씬 더 많은 문제를 일으킴    
![image](https://user-images.githubusercontent.com/76824611/176175415-48c34b40-7f6a-44ac-b1b6-b0bdd2ad123e.png)

</div>
</details>  
  
**4. 미니배치 학습의 부정확성**    

<details>
<div markdown="1">
  
* 손실함수를 계산할 때는 엄청 엄청 많은 Traing set 각각의  loss를 전부 계산해야하므로 Loss를 계산할 때 마다 매번 전부를 계산하는 것은 어렵다.       
* 그래서 실제로는 **미니배치**의 데이터들만 가지고 실제 Loss를 추정하기만 한다.(매번 정확한 gradient를 얻을 수가 없다)        
gradient의 부정확한 추정값(noisy estimate) 만을 구할 뿐이다.        
* 아래 그림은 이 문제를 조금 과장해서 그린 것인데 각 지점의 gradient에 random uniform noise를 추가하고 SGD를 수행하게 만든 것이다.(gradient에 noise가 들어가면 어떻게 되는지 알 수 있다)         
* 손실함수 공간을 이런식으로 비틀거리면서 돌아다니게 되면 minima까지 도달하는데 시간이 더 오래 걸린다.      
![image](https://user-images.githubusercontent.com/76824611/176348752-6a5d1e8a-5781-459c-a2da-7cf7299477a5.png)
  
</div>
</details>  
   
  
  
<details>
<summary>📜 학생 질문: SGD를 쓰지 않고 그냥 GD를 쓰면 이런 문제가 전부 해결되는지</summary>
<div markdown="1">
  
답변 전, GD란? [더 알아보러 가기](https://yerimoh.github.io/DL7/)   

**[GD(gradient descent)와 SGD(stochastic gradient descent)의 차이]**    
두가지 모두 parameter set을 error function이 가장 작아지도록 수정하는 것          
* GD(gradient descent): 전체 샘플들을 반복적으로 업데이트(수정)한다.      
* SGD(stochastic gradient descent): 훈련 셋에서 하나의 샘플만을 반복적으로 업데이트 한다.      
  
  
**[답변]**   
자 이전의 taco shell에서의 문제를 다시한번 살펴보면
full batch gradient descent에서도 같은 문제가 발생합니다

Noise의 문제도 한번 볼까요. Noise는 미니배치이기 때문에서만이
아니라 네트워크의  explicit stochasticity 로도 발생합니다.

  
이는 나중에 더 살펴 볼 것이지만
이는 여전히 문제가 됩니다.  
  
Saddle points 또한 full batch GD에서 문제가 됩니다.
전체 데이터를 사용한다고 해도 여전히 나타날 수 있겠죠  
  
기본적으로 full batch gradient descent를 사용한다
하더라도 이런 문제들이 해결되지는 않습니다.
  

  
  
  
</div>
</details>  
  
----

## SGD + Momentum 
바로 SGD에 momentum term을 추가하는 아주 간단한 방법으로 SGD를 최적화 하여 위의 문제를 해결 할 수 있다.      

**[classic 버전의 SGD (왼쪽])**    
* 오로지 gradient 방향으로만 움직임    

**[SGD + momentum (오른쪽)]**      
* gradient 를 계산할 때 **velocity**를 이용(현재 미니배치의 gradient 방향만 고려하는 것이 아니라 velocity를 같이 고려하는 것)      
* 하이퍼 파라미터 **rho**가 추가
    * momemtum의 비율을 나타냄      
    * **velocity의 영향력**을 rho의 비율로 맞춰줌       
    * 보통 **0.9**와 같은 높은 값으로 맞춰줌     
* velocity에 일정 비율 rho를 곱해주고 현재 gradient를 더함     
* 이를 통해서 우리는 이제 gradient vector 그대로의 방향이 아닌 velocity vector의 방향으로 나아가게 됨     
![image](https://user-images.githubusercontent.com/76824611/176351554-687a11f6-67c5-4986-9a41-376166ca59a0.png)


**[작동 방식]**      
비유해보면 물리적으로 공이 굴러내려오는 것을 상상해 볼 수 있다.    
이 공은 떨어지면 속도가 점점 빨라진다.          
이 공은 local minima에 도달해도 여전히 velocity(속도)를 가지고 있기 때문에 gradient = 0 이라도 움직일 수 있다.       
때문에 local minima, saddle points를 극복할 수 있게 되고 계속해서 내려갈 수 있다.      
![image](https://user-images.githubusercontent.com/76824611/176352335-fcf43cee-d004-4e65-850e-932d9d25660a.png)

업데이트가 잘 안되는 경우(poor conditioning) 를 다시 한번 살펴보겠다.    
아래와 같이 지그재그로 움직이는 상황이라면 momentum이 이 변동을 서로 상쇄시켜 버린다.         
이를 통해서 loss에 만감한 수직 방향의 변동은 줄여주고 수평방향의 움직임은 점차 가속화 될 것이다.        
momentum을 추가하게 되면 high condition number problem을 해결하는 데 도움이 되는 것이다.   
아래 그림에선 검은색이 일반 SGD이고, 파란색이 Momentum SGD이다.    
![image](https://user-images.githubusercontent.com/76824611/176352390-9d52910c-0788-467a-bce0-e83b1aae127e.png)


Momentum을 추가해서 velocity가 생기면 **결국 noise가 평균화**되버림.          



**[그림으로 이해]**      
* **빨간 점:** 현재 지점     
* **Red Vector:** 현재 지점에서의 gradient의 방향을 나타냄     
* **Green vector:** Velocity vector     
* 실제 업데이트는(autual step) 이 둘의 가중평균으로 구할 수 있다.    
* 이는 gradient의 noise를 극복할 수 있게 해준다.        
![image](https://user-images.githubusercontent.com/76824611/176352949-2620cfff-c603-4c0d-8e5d-c3e21dd753d6.png)


------


## Nesterov momentum     
= Nesterov accelerated       
Momentum의 변형   
계산하는 순서를 조금 바꾼 것임 


**[기본 SGD momentum]**     
* "현재 지점" 에서의 gradient를 계산한 뒤에 velocity와 섞어준다.       


**[Nesterov momentum]**    
* 빨간 점에서 시작해서 우선은 Velocity방향으로 움직임    
* 그리고 그 지점에서의 gradient를 계산(다시 원점으로 돌아가서 둘을 합치는 것)      
* 완벽한 설명은 아니지만 **두 정보를 약간 더 섞어준다**고 생각해 볼 수 있을 것임     
* velocity의 방향이 잘못되었을 경우에 현재 **gradient의 방향을 좀 더 활용**할 수 있도록 해줌        
![image](https://user-images.githubusercontent.com/76824611/176355083-16e006c3-0266-4b4c-9074-20e3010179b7.png)




**[성능]**     
* Nesterov는 Convex optimization 문제에서는 뛰어난 성능을 보임.     
* 하지만 Neural network와 같은 non-convex problem에서는 성능이 보장되지는 않음      



**[수식]**     
* velocity를 업데이트하기 위해서 이전의 velocity와 (x + pv)에서의 gradient를 계산함.       
* 그리고 step update는 앞서 계산한 velocity를 이용해서 구해줌.       
* 그리고 더 최근의 gradients에 가중치가 더 크게 부여된다.        
* 매 스텝마다 **이전 velocity에  rho(0.9 or 0.99) 를 곱하**고 **현재 gradient를 더해**준다.       
➡ 이를 **moving average**라고 볼 수 있다.(exponentially weighted moving average)    
* 그리고 시간이 지날수록 **이전의 gradient**들은 **exponentially하게 감소**한다.      
![image](https://user-images.githubusercontent.com/76824611/176356928-cf61751b-fade-400b-81a9-400f047e60d9.png)



<details>
<summary>📜 수식 더 자세히 이해하기</summary>
<div markdown="1">
  



**[기존 수식]**    
기존에는 Loss와 Gradient를 같은 점($$x_t$$)에서 구했었다.
![image](https://user-images.githubusercontent.com/76824611/176356714-c499795a-ac16-4cb4-9150-2d0b956a09ca.png)

  

**[Nesterov 수식]**    
Nesterov는 이 규칙을 조금 비틀어 버렸다.      
이는 상당히 성가시지만 다행이도 쉽게 해결할 수 있는 변형 공식이 있다.

변수들을 적절히 잘 바꿔주면 Nesterov를 조금 다르게 표현할 수 있으며 Loss와 Gradient를 같은 점에서 계산할 수 있게 된다.
그리고 수정된 수식(아래 박스)을 통해서 우리는 Nesterov를 새롭게 이해해보자        
![image](https://user-images.githubusercontent.com/76824611/176356953-eb3eb317-35db-4c50-949e-d08361f968d1.png)
  
  
**첫 번째 수식**     
* 기존의 momentum과 동일     
* 기존과 동일하게 velocity 와 계산한 gradient를 일정 비율로 섞어주는 역할을 함        
![image](https://user-images.githubusercontent.com/76824611/176357697-cf3fc74a-32fb-4c36-89a8-268a75197b71.png)


**맨 밑의 수식**     
* 우선 현재 점과 velocity를 더해줌(여기까지는 기존과 동일)     
* 그리고 여기에 "현재 velocity - 이전 velocity" 를 계산해서 일정 비율(rho)을 곱하고 더해줌.        
➡ Nesterov momentum는 현재/이전의 velocity간의 에러보정(error-correcting term)이 추가된 것임           
![image](https://user-images.githubusercontent.com/76824611/176357762-e6870b9f-5876-4d4d-9ba9-87e6d02beda9.png)
 
  

  
</div>
</details>  


<details>
<summary>📜 학생질문: velocity의 초기값을 구하는 좋은 방법이 있는지</summary>
<div markdown="1">
  
velocity의 초기값은 항상 0이다.    
이는 하이퍼파라미터가 아닙니다. 그저 0으로 둔다.  
  
velocity은 이전 gradients의 weighted sum이다.  
  
</div>
</details>  









**[학습경항성 비교]**      
* **기본 SGD(검정색)**: 매우 느리게 학습되고 있음      
* **momentum(파란색)**: minima를 그냥 지나쳐 버리는 경향이 있지만(이전의 velocity의 영향을 받기 때문) 스스로 경로를 수정하고는 결국 minima에 수렴     
* **Nesterov(초록색)**: 일반 momentum에 비해서 overshooting이 덜하다.( Nesterov에서 추가된 수식 때문)        
![image](https://user-images.githubusercontent.com/76824611/176363053-2e1ebbd4-c02b-4b90-bff9-e9098557185e.png)




---

## AdaGrad
AdaGrad는 훈련도중 계산되는 gradients를 활용하는 방법      
Adagrad는 velocity term 대신에 **grad squared term**을 이용       


**[수식]**    
학습 도중에 계산되는 gradient에 제곱을 해서 계속 더해줌              
Update를 할때 **Update term**을 앞서 계산한 **gradient 제곱 항으로 나눠줌**       
Condition number인 경우 빨간색 박스 안의 값은 점점 작아지게된다.      
![image](https://user-images.githubusercontent.com/76824611/176391321-d62d2c48-e065-40a8-b26c-cef1013728b9.png)





<details>
<summary>📜  빨간색 박스 안의 값이 점점 작아지는 이유</summary>
<div markdown="1">

2차원 좌표가 있다고 해 봅시다. 그 중 한 차원은 항상 gradient가
높은 차원입니다. 그리고 다른 하나는 항상 작은 gradient를 가집니다.


Small dimension에서는 gradient의 제곱 값 합이 작습니다.
이 작은 값이 나눠지므로 가속도가 붙게 됩니다.



Large dimension에서는 gradient가 큰 값 이므로 큰 값이
나눠지게 되겠죠. 그러므로 속도가 점점 줄어듭니다.


</div>
</details>  



**[❌ 문제]**    
학습이 계속 진행되면 값이 점점 작아진다.      
update 동안 gradient의 제곱이 계속해서 더해지면, 값(estimate)은 서서히(monotonically) 증가하게 된다.       
➡ 이는 **Step size를 점점 더 작은 값**이 되게 한다.      

손실함수가 convex(볼록)한 경우에 점점 작아지는 것은 정말 좋은 특징이 될 수 있다. (convex case에서는 minimum에 근접하면 서서히 속도를 줄여서 수렴할 수 있기 때문) 

하지만 **non-convex case에서는 문제**가 될 수 있다 (가령 **saddle point**에 걸려버렸을 때 AdaGrad는 **멈춰**버릴 수 있다.)       

**[⭕ 해결]**     
AdaGrad의 변형인 RMSProp으로 해결        







-----


## RMSProp
앞서 언급한 문제를 개선시킨 방법입


**[수식]**     
* RMSProp에서는 AdaGrad의 gradient  제곱 항을 그대로 사용       
* 하지만 이 값들을 그저 누적만 시키는 것이 아니라 **기존의 누적 값에 decay_rate를 곱해줌**(gradients의 제곱을 계속해서 누적해 나감)
➡ 점점 속도가 줄어드는 문제를 해결          
* RMSProp에서는 gradient 제곱 항에 쓰는 decay rate는 보통 0.9 또는 0.99정도를 자주 사용함           
* 그리고 '현재 gradient의 제곱'은 $$(1 - decay rate)$$ 를 곱해줘서 더해줌       
* RMSProp은 gradient 제곱을 계속 나눠준다는 점에서 AdaGrad와 유사함     
* 이를 통해 step의 속도를 가속/감속 시킬 수 있음        
![image](https://user-images.githubusercontent.com/76824611/176371587-9612912a-76cd-4c01-a10b-559be70d68c5.png)



**[학습경항성 비교]**      
* **기본 SGD(검정색)**: 매우 느리게 학습되고 있음      
* **momentum(파란색)**: overshoots한 뒤에 다시 minima로 돌아옴         
* **RMSProp(빨간색)**: 각 차원마다의 상황에 맞도록 적절하게 궤적(trajectory)을 수정시킴           
* **Adagrad(초록색)**: RMSProp에 가려서 잘 보이지 않겠지만 학습하다가 죽어버렸다.      
![image](https://user-images.githubusercontent.com/76824611/176363053-2e1ebbd4-c02b-4b90-bff9-e9098557185e.png)



**[평가]**    
실제로 AdaGrad는 잘 쓰이지는 않음    
그리고 위의 케이스는 convex case이지만 learning rates가 서로 상이하여 Adagrad에게 불리함     
아마도 AdaGrad의 Learning rate를 늘리게 되면 RMSProp과 비슷한 동작을 할 것임    
➡ 때문에 여러 알고리즘 간에 '같은 Learning rates' 를 가지고 AdaGrad 를 visualization하는 것은 공정하지 못함      
➡ visualization을 하고자 한다면 알고리즘 별로 learning rates를 조정하는 것이 좋음          







----


## Adam
momentum 계열 vs Ada계열을 조합    
* momentum 계열: velocity를 이용해서 step을 조절    
* Ada계열: gradients의 제곱을 나눠주는 방식으로 step을 조절(목적지에 갈수록 이동량을 줄임)       



그럼 아담과 유사한 알고리즘부터 출발하여 완성형 아담을 만들어보겠다.    



**[유사 Adam]**    
* Adam은 first moment와 second moment을 이용해서 **이전의 정보(estimate)를 유지**시킴.       
* 빨간색 박스 안 ```first_moment```: gradient의 가중 합 (velocity를 담당)      
* 파란색 박스 안 ```second_moment```: AdaGrad이나 RMSProp처럼 gradients의 제곱을 이용하는 방법 (sqrt(second moment)를 나눠주는데 second moment는 gradient의 제곱 항임)   
* Adam ≈ RMSProp + momentum (momentum +  second squared gradients)     
![image](https://user-images.githubusercontent.com/76824611/176373764-20ce190b-e939-4d8f-a0a7-2e8f41552629.png)


**[❌ 문제]**            
* 초기에 ```second_moment```를 0으로 초기화한다.   
이렇게 되면, ```second_moment```를 1회 Update 하고 난 후엔 ```beta2```는 ```decay_rate```로 .9또는 .99로 1에 가까운 값이 된다.(모멘텀의 velocity는 거의 .9에 가까운 큰 값으로 설정하니까)      
* 그렇기 때문에 1회 업데이트 이후에도 ```second_moment```는 여전히 **0에 가깝다**.      
➡ 이는 update step에서 ```second_moment```로 **나누게 되는데** 나눠주는 값이 크기 때문에 **초기 step이 엄청나게 커지**게 된다.       
✨ 중요한 것은 이 커진 step이 실제로 손실함수가 가파르기 때문(geometry)이 아니라는 것이다. (second moment을 0으로 초기화 시켰기 때문에 발생하는 '인공적인' 현상임)            


**[⭕ 해결: Adam 완성]**      
* Adam은 초기 Step이 엄청 커져 버릴 수 있고 이로 인해 잘못될 수도 있다고 했다.       
* Adam을 이를 해결하기 위해 보정하는 항을 추가한다.(bias correction term)      
* first/second moments를 Update하고 난 후 현재 Step에 맞는 적절한 unbiased term 을 계산해줌     
* 실제 Adam은 first/second moment만 계산하는 것이 아니라 이렇게 unbiased term을 넣어줘야 함.     
![image](https://user-images.githubusercontent.com/76824611/176380839-02d2c106-7f56-4006-ac5c-9d053e970f69.png)

**[하이퍼파라미터 설정]**     
* beta_1 = 0.9, beta_2 = 0.999로 설정    
* Learning rate를 E-3나 E-4 정도로 설정     
➡ 거의 모든 아키텍쳐에서 잘 동작한다.      

**[평가]**      
* Adam은 엄청 좋다. 다양한 문제들에도 정말 잘 동작한다.      
* 일반적으로 Adam으로 시작해 보는 것은 정말 좋다.



**[학습경항성 비교]**      
* **기본 SGD(검정색)**: 매우 느리게 학습되고 있음      
* **momentum(파란색)**: overshoots한 뒤에 다시 minima로 돌아옴         
* **RMSProp(빨간색)**: 각 차원마다의 상황에 맞도록 적절하게 궤적(trajectory)을 수정시킴           
* **Adagrad(초록색)**: RMSProp에 가려서 잘 보이지 않겠지만 학습하다가 죽어버렸다.     
* **Adam(보라색)**: momentum과 RMSProp을 합쳐놓은듯한 모습(momentum처럼 overshoots하긴 하지만 momentum만큼 엄청 심하지는 않음)  
![image](https://user-images.githubusercontent.com/76824611/176382735-3f3df3be-8325-401f-bc48-72dd3f6bacfd.png)


<details>
<summary>📜 학생 질문: Adam이 해결하지 못하는 것은 무엇인지</summary>
<div markdown="1">
  
Neural networks는 여전이 엄청 크고 학습은 오래걸립니다.   
Adam을 쓰더라도 여전히 문제점들이 있습니다.    
  
가령 손실함수가 타원형일 경우를 생각해 봅시다      
Adam을 이용하면 각 차원마다 적절하게 속도를 높히고
줄이면서 '독립적으로' step을 조절할 것입니다   
  
하지만 이 타원이 축 방향으로 정렬되어 있지 않고
기울어져 있다고 생각해 봅시다.   
  

이 경우에도 Adam은 차원에 해당하는 축 만을
조절할 수 있습니다.  
  

이는 차원을 회전시킨 다음에 수평/수직 축으로만 늘렸다 줄였다
하는 것입니다. 회전을 시킬수는 없죠


이러한 회전된 타원(poor conditioning) 문제는 Adam을 비롯한
다른 여러 알고리즘들도 다룰 수 없는 문제입니다.

  
</div>
</details>  


----

## learning rates dacay


**[❌ 문제]**   
지금까지의 모든 Opimization 알고리즘은
learning rate 이라는 하이퍼파라미터를 가지고 있었습니다.


Learning rate가 지나치게 높으면
노란색 처럼 솟구치게 되겠죠


파란색 처럼 너무 낮으면
수렴하는데 너무 오래걸립니다.

하지만 learning rate를 잘 고르는 것은 상당히 까다롭습니다.


우리는 학습 과정에서 learning rate 하나를 정해놓고
시작해야 하기 때문에 잘 고르는게 참 어렵습니다.


 learning rates dacay 전략을 사용해 볼 수 있습니다.
각각의 learning rates의 특성을 적절히 이용하는 것이죠

417
00:45:29,705 --> 00:45:39,366
처음에는 learning rates를 높게 설정한 다음에 학습이 진행될수록
learning rates를 점점 낮추는 것입니다.

418
00:45:39,366 --> 00:45:46,795
다양한 전략을 시도해 볼 수 있지만 가령 100,000 iter에서
learning rates를 낮추고 학습시키는 것입니다.(step decay)

419
00:45:46,795 --> 00:45:52,579
혹은 exponential decay 처럼 학습과정 동안에
꾸준히 learning rate를 낮출 수도 있습니다.

420
00:45:52,579 --> 00:45:57,598
꾸준히 learning rate를 감소시키는 것에는
다양한 전략이 있을 수 있습니다.

421
00:45:57,598 --> 00:46:04,347
오른쪽의 그림을 보시기 바랍니다. Resnet 논문에 있는
그림입니다. Loss가 계속 내려가고 있습니다.

422
00:46:04,347 --> 00:46:07,898
그리고 어느순간 평평해 지는 듯 하더니 다시 또 내려가는
것을 반복하는 것을 알 수 있습니다.

423
00:46:07,898 --> 00:46:11,312
Resnet 논문에서는 step decay learning rate 전략을
사용한 것입니다.

424
00:46:11,312 --> 00:46:18,401
평평해지다가 갑자기 내려가는 구간은
Learning rate를 낮추는 구간입니다.

425
00:46:18,401 --> 00:46:26,243
learning rate를 언제 낮춰야 하는지를 생각해보면, 현재 수렴을
잘 하고 있는 상황에서 gradient가 점점 작아지고 있는 것입니다.

426
00:46:26,243 --> 00:46:28,066
learning rate가 너무 높아서 더 깊게 들어가지 못합니다.
(bouncing around too much)

427
00:46:28,066 --> 00:46:32,745
이 상황에서  learning rate를 낮추게 되면 속도가 줄어들 것이고
지속해서 Loss를 내려갈 수 있을 것입니다.

428
00:46:32,745 --> 00:46:36,475
실제로 상당히 도움이 되는 방법입니다.

429
00:46:36,475 --> 00:46:44,973
그리고 한가지 말씀드릴 것은 learning rate decay는
Adam 보다는 SGD Momentum을 사용할 때 자주 씁니다.

430
00:46:44,973 --> 00:46:50,458
또 한가지 말씀드릴 것은 learning rate decay는
부차적인 (second-order ) 하이퍼파라미터 라는 것입니다.

431
00:46:50,458 --> 00:46:53,324
일반적으로 learning-rate decay를 학습 초기부터
고려하지는 않습니다.

432
00:46:53,324 --> 00:47:00,877
보통 학습 초기에는 learning rate decay가 없다고 생각하고
learning rate를 잘 선택하는 것이 중요합니다.

433
00:47:00,877 --> 00:47:06,068
learning rate와 dacay 등을 cross-validate 하려고
한다면 문제가 너무 복잡해 집니다.

434
00:47:06,068 --> 00:47:10,581
learning rate decay를 설정하는 순서는
우선 decay 없이 학습을 시켜 봅니다.

435
00:47:10,581 --> 00:47:15,427
그리고 Loss curve를 잘 살피고 있다가 decay가 필요한 곳이
어디인지 고려해 보는 것이 좋습니다.

436
00:47:16,860 --> 00:47:24,948
지금까지 배운 Optimization 알고리즘들을 모두
1차미분을 활용한(first-order) 방법이었습니다.

437
00:47:24,948 --> 00:47:33,064
그림처럼 1차원의 손실함수가 있다고 생각해 봅시다.
우리는 지금 빨간색 점에 있는 것입니다.

438
00:47:33,064 --> 00:47:36,057
이 점에서 gradient 를 계산하겠죠

439
00:47:36,057 --> 00:47:40,722
이 gradient 정보를 이용해서 우리는 손실함수를
선형함수로 근사시킵니다.

440
00:47:40,722 --> 00:47:44,208
이는 일종의 1차 테일러 근사입니다.
(first-order Taylor apporximation)

441
00:47:44,208 --> 00:47:51,814
우리는 이 1차 근사함수를 실제 손실함수라고 가정하고
Step을 내려갈 것입니다.

442
00:47:51,814 --> 00:47:57,353
하지만 이 근사함수로는 멀리갈 수 없습니다.

443
00:47:57,353 --> 00:48:04,509
현재 우리가 사용하는 정보는 1차 미분값일 뿐입니다.
그리고 우리는 조금 더 괜찮은 방법을 생각해 볼 수도 있을 것입니다.

444
00:48:04,509 --> 00:48:11,248
2차 근사 (second-order approximation)의 정보를
추가적으로 활용하는 방법이 있습니다.

445
00:48:11,248 --> 00:48:18,449
이는 2차 테일러 근사 함수가 될 것이고 이 함수는
2차함수의 모양입니다.

446
00:48:18,449 --> 00:48:22,281
2차 근사를 이용하면 minima에 더 잘 근접할 수 있습니다.
you're really happy

447
00:48:22,281 --> 00:48:25,769
이것이 바로 2nd-order optimization의 기본 아이디어입니다.

448
00:48:25,769 --> 00:48:30,489
방금 예시는 다차원으로 확장시켜보면 이를
'Newton step' 이라고 합니다.

449
00:48:30,489 --> 00:48:35,066
Hessian matrix를 계산합니다. 2차 미분값들로 된 행렬입니다.

450
00:48:35,066 --> 00:48:43,689
 Hessian matrix의 역행렬을  이용하게 되면 실제 손실함수의
2차 근사를 이용해 minima로 곧장 이동할 수 있을 것입니다.

451
00:48:43,689 --> 00:48:48,910
혹시 이 알고리즘이 다른 Optimization 알고리즘에 비해
특이한 것이 무엇인지 아시겠습니까?

452
00:48:48,910 --> 00:48:51,107
[학생이 대답]

453
00:48:51,107 --> 00:48:54,328
네 맞습니다. Learning rate가 없습니다.
That's kind of cool.

454
00:48:56,463 --> 00:49:00,664
지금 우리는 2차근사 함수를 만들었고 이 2차근사 함수의
minima로 이동하는 것입니다.

455
00:49:00,664 --> 00:49:04,681
적어도 "기본적인 Newton's method" 에서는
learning rate는 불필요합니다.

456
00:49:04,681 --> 00:49:07,849
매 step마다 항상 minma를 향해 이동합니다.

457
00:49:07,849 --> 00:49:13,265
하지만 실제로는 learning rate가 필요합니다.
왜냐하면 2차 근사도 사실상 완벽하지 않기 때문이죠

458
00:49:13,265 --> 00:49:21,055
minima로 이동하는게 아니라 'minima의 방향'으로 이동하기 때문이죠
어쨌든 기본버전에는 learning rate가 없습니다.

459
00:49:23,994 --> 00:49:27,366
하지만 불행하게도 Deep learning에서는 사용할 수 없습니다.

460
00:49:27,366 --> 00:49:34,519
왜냐하면 Hessian matrix는 N x N 행렬입니다.
N은 Network의 파라미터 수입니다.

461
00:49:34,519 --> 00:49:38,498
N이 1억이면 1억의 제곱만큼 존재할 것입니다.

462
00:49:38,498 --> 00:49:42,046
이를 메모리에 저장할 방법은 없으며
또한 역행렬계산도 불가능 할 것입니다.

463
00:49:42,046 --> 00:49:46,486
그래서 실제로는 'quasi-Newton methods' 를 이용합니다.

464
00:49:46,486 --> 00:49:52,725
Full Hessian을 그대로 사용하기 보다 근사시킵니다.
Low-rank approximations 하는 방법입니다.

465
00:49:52,725 --> 00:49:57,092
여러분도 앞으로 간혹 접하게 될 것입니다.

466
00:49:57,092 --> 00:50:03,487
L-BFGS도 second-order optimizer입니다. 이 방법도
Hassian을 근사시켜서 사용하는 방법입니다.

467
00:50:03,487 --> 00:50:11,205
L-BFGS도 여러분이 가끔씩 보게될텐데 사실상
DNN에서는 잘 사용하지 않습니다.

468
00:50:11,205 --> 00:50:16,410
왜냐하면 L-BFGS에서 2차근사가 stochastic case에서
잘 동작하지는 않기 때문입니다.

469
00:50:16,410 --> 00:50:20,616
그리고 L-BFGS는 non-convex problems에도
적합하지 않습니다.

470
00:50:20,616 --> 00:50:23,142
더 깊게는 들어가지 않겠습니다.

471
00:50:23,142 --> 00:50:29,022
실제로는 Adam을 제일 많이 씁니다.

472
00:50:29,022 --> 00:50:38,974
하지만 full batch update가 가능하고 stochasticity이
적은 경우라면  L-BFGS가 좋은 선택이 될 수 있습니다.

473
00:50:38,974 --> 00:50:43,181
L-BFGS가 Neural network를 학습시키는데 그렇게
많이 사용되지는 않지만

474
00:50:43,181 --> 00:50:47,251
앞으로 보게될 Sytle tranfer와 같은 알고리즘에
L-BFGS을 종종 사용할 수 있습니다.

475
00:50:47,251 --> 00:50:54,356
Sytle tranfer같은 stochasticity와 파라미터가
적은 경우에서 Optimization을 해야할 경우에 말이죠

476
00:50:55,834 --> 00:51:00,992
지금까지 이야기했던 모든 것들을 전부 training error를
줄이기 위한 방법들이었습니다.

477
00:51:02,344 --> 00:51:07,452
optimization 알고리즘들은 training error를 줄이고
손실함수를 최소화시키기 위한 역할을 수행합니다.

478
00:51:07,452 --> 00:51:10,403
하지만 사실 우리는 Training error에 크게 신경쓰지 않습니다.

479
00:51:10,403 --> 00:51:13,203
대신에 "한번도 보지 못한 데이터"에 대한 성능이 더 중요하죠

480
00:51:13,203 --> 00:51:16,817
우리가 원하는 것은 train/test error의 격차를
줄이는 것입니다.

481
00:51:16,817 --> 00:51:21,228
질문은 바로 우리가 손실함수 최적화를
이미 모두 끝마친 상황에서

482
00:51:21,228 --> 00:51:25,535
'한번도 보지 못한 데이터' 에서의 성능을
올리기 위해서는 어떻게 해야 할까요?

483
00:51:28,497 --> 00:51:33,617
가장 빠르고 쉬운 길은 바로 모델 앙상블입니다.

484
00:51:33,617 --> 00:51:36,767
Machine learning 분야에서 종종 사용하는 기법입니다.

485
00:51:36,767 --> 00:51:44,588
아이디어는 아주 간단합니다. 모델을 하나만 학습시키지 말고
10개의 모델을 독립적으로 학습시키는 것입니다.

486
00:51:44,588 --> 00:51:51,333
결과는 10개 모델 결과의 평균을 이용합니다.

487
00:51:53,562 --> 00:52:01,555
모델의 수가 늘어날수록 overfitting 줄어들고
성능이 조금씩 향상됩니다. 보통 2%정도 증가하죠

488
00:52:01,555 --> 00:52:05,302
엄청나게 큰 변화는 아니지만 상당히
일관적으로 이 정도 늘어납니다.

489
00:52:05,302 --> 00:52:13,263
Imagenet 같은 대회에서는 모델의 성능을 최대화 시키기 위해서
이러한 앙상블 기법을 사용하는 모습을 볼 수 있습니다.

490
00:52:14,488 --> 00:52:20,482
조금 더 창의적인 방법도 있습니다. 모델을 독립적으로
학습시키는 것이 아니라

491
00:52:20,482 --> 00:52:25,928
학습 도중 중간 모델들을 저장(sanpshots)하고
앙상블로 사용할 수 있습니다.

492
00:52:25,928 --> 00:52:29,804
그리고 Test time에는 여러 snapshots에서 나온 예측값들을
평균을 내서 사용합니다.

493
00:52:29,804 --> 00:52:33,244
이런 snapshots은 Training 과정 중간에 저장하는 것입니다.

494
00:52:34,133 --> 00:52:43,210
이번주에 ICLR에 좀 더 향상된 앙상블 알고리즘이 발표되었습니다.
이 방법은 아주 독특한 Learning rate 스케줄을 이용합니다.

495
00:52:43,210 --> 00:52:47,996
Learning rate를 엄청 낮췄다가 다시 엄청 높혔다가를 반복합니다.

496
00:52:47,996 --> 00:52:57,631
이 논문의 아이디어는 이런 방식으로 손실함수에 다양한 지역에
수렴할 수 있도록 만들어 줍니다.

497
00:52:58,717 --> 00:53:05,532
이런 앙상블 기법으로 모델을 한번만 Train시켜도 좋은
성능을 얻을 수 있게 하는 방법입니다.

498
00:53:05,532 --> 00:53:11,198
질문있나요?

499
00:53:25,388 --> 00:53:33,413
질문은 모델간의 Loss 차이가 크면 한쪽이 overfiting 일 수 있으니
별로 안좋고, 또 차이가 작아도 안좋지 않냐는 것입니다.

500
00:53:33,413 --> 00:53:37,446
그러니 좋은 앙상블 결과를 위해서라면 모델 간의 최적의 갭을 찾는 것이
중요하지 않느냐는 것입니다.

501
00:53:37,446 --> 00:53:39,132
사실 갭이 중요한 것이 아닙니다.

502
00:53:39,132 --> 00:53:44,019
중요한 것은 validation set의
성능을 최대화시키는 것입니다.

503
00:53:44,019 --> 00:53:54,995
하지만 우선 갭을 신경쓰지 않고 모델을 조금 더 Overfitting 시킬 수
있다면 아마 더 좋은 성능을 낼 수 있을 것입니다.

504
00:53:54,995 --> 00:54:02,720
Validation set 성능과 이 갭 사이에는 묘한 관계가 있긴 하지만
오로지 Validation set 성능만 신경쓰면 됩니다.

505
00:54:02,720 --> 00:54:03,735
질문있나요?

506
00:54:03,735 --> 00:54:07,004
[학생이 질문]

507
00:54:07,004 --> 00:54:09,528
질문은 앙상블 모델마다 하이퍼파라미터를 동일하게
줘야 하는지 입니다.

508
00:54:09,528 --> 00:54:12,234
좋은 질문이군요. 그렇지 않을 수도 있습니다.

509
00:54:12,234 --> 00:54:19,614
다양한 "모델 사이즈", "learning rate", 그리고 "다양한
regularization 기법" 등을 앙상블 할 수 있습니다.

510
00:54:19,614 --> 00:54:22,614
실제로 그렇게 사용하기도 합니다.

511
00:54:23,496 --> 00:54:31,769
또 다른 방법으로는 학습하는 동안에 파라미터의
exponentially decaying average를 계속 계산합니다.

512
00:54:31,769 --> 00:54:35,778
이 방법은 학습중인 네트워크의
smooth ensemble 효과를 얻을 수 있습니다.

513
00:54:35,778 --> 00:54:41,649
checkpoints에서의 파라미터를 그대로 쓰지 않고
smoothly decaying average를 사용하는 방법입니다.

514
00:54:41,649 --> 00:54:45,262
이를 Polyak averaging라고 합니다. 때때로
조금의 성능향상을 보일 수 있습니다.

515
00:54:45,262 --> 00:54:50,838
이 또한 시도해볼만한 방법이긴 하지만
실제로 자주 사용하지는 않습니다.

516
00:54:50,838 --> 00:54:55,778
그렇다면 앙상블이 아닌 단일 모델의 성능을
향상시키기 위해서는 어떻게 해야 할까요?

517
00:54:57,229 --> 00:55:02,503
앙상블을 하려 한다면 test time에 10개의 모델을
돌려야 할 수도 있습니다. 그렇게 좋은 방법은 아니죠

518
00:55:02,503 --> 00:55:06,219
단일 모델의 성능을 올리는 것이
우리가 정말 원하는 것입니다.

519
00:55:06,219 --> 00:55:08,237
 바로 regularization입니다.

520
00:55:08,237 --> 00:55:11,954
우리가 모델에 어떤 것을 추가할 텐데
모델이 training data에 fit하는 것을 막아줄 것입니다.

521
00:55:11,954 --> 00:55:16,203
그리고 한번도 보지 못한 데이터에서의
성능을 향상시키는 방법입니다.

522
00:55:16,203 --> 00:55:23,515
우리는 이미 몇 가지 regularization 기법을 살펴봤습니다.
Loss에 추가적인 항을 삽입하는 방법이었습니다.

523
00:55:23,515 --> 00:55:29,738
손실함수에서 기존의 항은 training data에 fit하려 하고
다른 하나는 regularization term 이죠

524
00:55:29,738 --> 00:55:33,032
과제에서도 있었죠  L2 regularization이
대표적인 예입니다.

525
00:55:34,804 --> 00:55:43,001
앞서 강의에서도 말씀드렸듯이 L2 regularization은
Neural networks에는 잘 어울리지 않습니다.

526
00:55:43,922 --> 00:55:47,982
때문에 조금 다른 것을 사용하죠

527
00:55:47,982 --> 00:55:53,376
Neural network에서 가장 많이 사용하는 regularization은
바로 dropout입니다.

528
00:55:53,376 --> 00:55:55,080
Dropout은 정말 간단합니다.

529
00:55:55,080 --> 00:56:02,264
forward pass 과정에서 임의로 일부 뉴런을
0으로 만드는 것입니다.

530
00:56:02,264 --> 00:56:08,688
forward pass 할때마다 0이 되는 뉴런이 바뀝니다.
Dropout은 한 레이어씩 진행하게 됩니다.

531
00:56:08,688 --> 00:56:15,193
한 레이어의 출력을 전부 구합니다. 그리고 임의로 일부를 0으로
만듭니다. 그리고 다음 레이어로 넘어가는 식입니다.

532
00:56:15,193 --> 00:56:22,445
왼쪽에는 dropout이 없고 오른쪽은 dropout이
적용된 경우 입니다.

533
00:56:22,445 --> 00:56:30,400
오른쪽 모델은 동일한 네트워크의 더 작아진 버전 처럼 생겼습니다.
오로지 뉴런의 일부만 사용하고 있습니다.

534
00:56:30,400 --> 00:56:35,746
forward pass iteration 마다 그 모양은 계속 바뀝니다.

535
00:56:35,746 --> 00:56:36,732
질문 있나요?

536
00:56:36,732 --> 00:56:40,899
[학생이 질문]

537
00:56:43,694 --> 00:56:46,375
질문은 지금 무엇을 0으로 놓는 것인지 입니다.
activations을 0으로 설정하는 것입니다.

538
00:56:46,375 --> 00:56:51,731
각 레이어에서
next activ= prev activ * weight 죠

539
00:56:51,731 --> 00:57:01,592
현재 activations의 일부를 0으로 만들면
다음 레이어의 일부는 0과 곱해질 것입니다.

540
00:57:01,592 --> 00:57:03,155
질문 있나요?

541
00:57:03,155 --> 00:57:06,702
[학생이 질문]

542
00:57:06,702 --> 00:57:08,751
질문은 어떤 종류의 레이어에서 이를 사용하는지 입니다.

543
00:57:08,751 --> 00:57:14,454
Dropout은 fc layer에서 흔히 사용합니다.
하지만 conv layers에서도 종종 볼 수 있습니다.

544
00:57:14,454 --> 00:57:23,423
Conv net의 경우에서는 전체 feature map에서
dropout을 시행합니다.

545
00:57:24,455 --> 00:57:30,117
conv layer의 경우에 여러 channels이 있기 때문에
일부 channel 자체를 dropout 시킬 수도 있겠습니다.

546
00:57:32,059 --> 00:57:38,480
실제로 Dropout 구현은 아주 쉽습니다.
두 줄이면 충분합니다.

547
00:57:38,480 --> 00:57:41,572
여기 3-layer neural network 예제가 있습니다.
여기에 Dropout을 추가했습니다.

548
00:57:41,572 --> 00:57:49,460
임의로 0으로 설정하는 부분만 추가하면 됩니다.
구현하기 아주 쉽습니다.

549
00:57:49,460 --> 00:57:52,138
그렇다면 dropout이 도대체 왜 좋을까요?

550
00:57:52,138 --> 00:57:58,067
일부 값들을 0으로 만들면서 training time의 네트워크를
심각하게 훼손시키고 있습니다.

551
00:57:58,067 --> 00:58:00,988
이런 일이 어떻게 가능할까요?

552
00:58:00,988 --> 00:58:08,532
대략적으로 말씀드리면, 특징들 간의
상호작용(co-adaptation)을 방지한다고 볼 수 있습니다.

553
00:58:09,622 --> 00:58:15,853
자 우선 우리에게 고양이를 분류하는 어떤 네트워크가 있다고 해봅시다.

554
00:58:15,853 --> 00:58:21,066
어떤 뉴런은 눈에대해, 어떤 뉴런은 꼬리에대해, 또 어떤 뉴런은
고양이의 털에 대해 학습하는 것을 상상해 볼 수 있을 것입니다.

555
00:58:21,066 --> 00:58:24,751
그리고 이 이미지가 고양이인지 아닌지를 이 정보들을
모두 취합해서 결정을 내리는 것입니다.

556
00:58:24,751 --> 00:58:32,831
Dropout을 적용하게 되면 네트워크가 어떤 일부
features에만 의존하지 못하게 해줍니다.

557
00:58:32,831 --> 00:58:37,725
대신에 모델이 "고양이다" 라고 예측할 때 다양한
features를 골고루 이용할 수 있도록 합니다.

558
00:58:37,725 --> 00:58:42,205
따라서 Dropout이 Overfitting을 어느정도
막아준다고 할 수 있겠습니다.

559
00:58:42,205 --> 00:58:50,347
그리고 최근 Dropout에 대한 새로운 해석이 나왔는데
단일 모델로 앙상블 효과를 가질 수 있다는 것입니다.

560
00:58:51,690 --> 00:58:58,745
자 여기 왼쪽의 Dropout을 적용한 네트워크를 살펴보면
뉴런의 일부만 사용하는 서브네트워크라는 것을 알 수 있습니다.

561
00:58:58,745 --> 00:59:03,391
Dropout으로 만들 수 있는 서브네트워크의
경우의 수가 정말 다양하다는 것을 알 수 있습니다.

562
00:59:03,391 --> 00:59:09,145
따라서 Dropout은 서로 파라미터를 공유하는 서브네트워크
앙상블을 동시에 학습시키는 것이라고 생각할 수 있습니다.

563
00:59:09,145 --> 00:59:13,790
하지만 뉴런의 수에 따라서 앙상블 가능한 서브네트워크의 수가
기하급수적으로 증가하기 때문에

564
00:59:13,790 --> 00:59:17,152
가능한 모든 서브네트워크를 사용하는것은 사실상 불가능합니다.

565
00:59:18,089 --> 00:59:24,788
Dropout은 아주 거대한 앙상블 모델을 동시에
학습 시키는 것이라고 볼 수 있겠습니다.

566
00:59:25,622 --> 00:59:29,128
그렇다면 Dropout을 사용하면 Test time에
어떤 일이 일어날까요?

567
00:59:29,128 --> 00:59:34,158
Dropout을 사용하게되면 기본적으로 Neural network의
동작 자체가 변하게 됩니다.

568
00:59:34,158 --> 00:59:42,850
기존의 Neural network는 가중치 w와 입력 x에
대한 함수(f) 였습니다.

569
00:59:42,850 --> 00:59:48,268
하지만 Dropout을 사용하면 Network에 z라는 입력이 추가됩니다.
z는 "random dropout mask" 입니다.

570
00:59:48,268 --> 00:59:52,732
z는 random입니다. 하지만 test time에 임의의 값을
부여하는 것은 좋지 않습니다.

571
00:59:52,732 --> 00:59:57,444
가령 여러분이 Facebook에서 일하고 있고 사람들이 업로드하는
이미지를 분류하려 한다고 해봅시다.

572
00:59:57,444 --> 01:00:03,092
오늘은 고양이라고 분류했던 이미지를 내일이 되서는
다른 이미지라고 분류한다고 하면 그닥 좋지 않은 상황이겠죠

573
01:00:03,092 --> 01:00:09,323
따라서 네트워크가 이미 학습된 네트워크의 Test time에는
이러한 임의성(stochasticity)은 적절하지 않습니다.

574
01:00:09,323 --> 01:00:12,093
대신 그 임의성(randomness)을 average out 시킵니다.

575
01:00:12,093 --> 01:00:18,131
이는 적분을 통해 randomness를 marginalize out시키는
것으로 한번 생각해 볼 수 있을 것입니다. 하지만 실제로는-

576
01:00:18,131 --> 01:00:24,368
이 적분을 다루기는 상당히 까다롭습니다.

577
01:00:24,368 --> 01:00:28,073
이 문제를 해결할 수 있는 간단한 방법 중 하나는
샘플링을 통해서 적분을 근사시키는 것인데요

578
01:00:28,073 --> 01:00:31,484
z를 여러번 샘플링해서 test time에 이를
average out 시키는 것입니다.

579
01:00:31,484 --> 01:00:36,040
하지만 이 방법도 Test time에서의 randomness을
만들어 내기 때문에 좋지 않은 방법입니다.

580
01:00:36,040 --> 01:00:41,423
고맙게도 dropout의 경우에는 일종의 locally cheap한 방법을
이용해서 이 적분식을 근사화시킬 수 있습니다.

581
01:00:41,423 --> 01:00:47,228
가령 여기 하나의 뉴런이 있습니다. 출력은 a 이고
입력 x, y가 있고 가중치 w_1, w_2가 있습니다.

582
01:00:47,228 --> 01:00:52,622
Test time에서 a는 w_1x + w_2y 입니다.

583
01:00:53,590 --> 01:01:00,645
자 그럼 이 네트워크를 dropout(p = 0.5) 를 적용해서
train 시킨다고 생각해 봅시다.

584
01:01:00,645 --> 01:01:06,317
train time에서의 기댓값은 아래와 같은 경우로
(analytically) 계산해 볼 수 있습니다.

585
01:01:07,712 --> 01:01:12,249
dropout mask에는 4가지 경우의 수가 존재합니다.
이제 그 값들을 4개의 마스크에 대해 평균화 시켜 줍니다.

586
01:01:12,249 --> 01:01:18,204
Train time에서 a의 기댓값은
1/2(w_1x + w_2y) 입니다.

587
01:01:19,075 --> 01:01:29,000
이 부분에서 test/train time 간의 기댓값이 서로 상이합니다.
train time의 기댓값은 test time의 절반밖에 안되죠

588
01:01:29,000 --> 01:01:34,883
Test time에서 stochasticity를 사용하지 않고
할 수 있는 값 싼(cheap) 방법 중 하나는

589
01:01:34,883 --> 01:01:40,736
대신에 dropout probability를 네트워크의 출력에 곱합니다.
자 이제 둘의 기댓값이 같아졌습니다.

590
01:01:40,736 --> 01:01:44,733
이 방법은 이전의 복잡한 적분식을 아주 저렴하게(cheap)
local approximation시킬 수 있는 방법입니다.

591
01:01:44,733 --> 01:01:48,576
실제로 많은 사람들이 Dropout을 사용할 때
이 방법을 많이 사용합니다.

592
01:01:49,715 --> 01:01:56,269
Dropout을 사용하게 되면 네트워크 출력에
dropout probability를 곱해줘야 합니다.

593
01:01:56,269 --> 01:01:59,393
Dropout에 대해 요약해보자면 Forward pass에 Dropout
을 추가시키는 것은 상당히 간단합니다.

594
01:01:59,393 --> 01:02:03,807
일부 노드를 무작위로 0으로 만들어주는데는
2줄이면 충분하죠

595
01:02:03,807 --> 01:02:10,209
그리고 Test time에서는 그저 값 하나만 곱해주면 됩니다.
(probability)

596
01:02:10,209 --> 01:02:16,613
Dropout은 상당히 간단합니다. 그리고 Neural network의
Regularization에 상당히 효과적이죠

597
01:02:16,613 --> 01:02:21,454
그리고 Dropout을 사용할 때 한가지 트릭을 생각해 볼 수 있습니다.
Dropout을 역으로 계산하는 것입니다 (inverted dropout)

598
01:02:22,665 --> 01:02:28,735
 Test time에는 곱하기 연산이 하나 추가되는 것은 상당히
신경쓰이는 일입니다. Test time에서는 계산효율이 중요합니다

599
01:02:28,735 --> 01:02:37,677
Test time에는 기존의 연산을 그대로 사용하고 대신 Train time
에서 p를 나눠줍니다. 보통 train은 GPU로 하기 때문이죠

600
01:02:37,677 --> 01:02:44,733
Train time에서는 곱하기 몇 번 추가되는 것에 별로 신경쓰지 않지만
Test time에서는 가능한 효율적으로 동작하길 원합니다.

601
01:02:44,733 --> 01:02:45,566
질문 있나요?

602
01:02:46,416 --> 01:02:56,777
[학생이 질문]

603
01:02:57,678 --> 01:03:02,212
질문은 dropout을 사용하게 되면 Train time에서
gradient에는 어떤 일이 일어나는지 입니다.

604
01:03:02,212 --> 01:03:06,583
Dropout이 0으로 만들지 않은 노드에서만
Backprop이 발생하게 됩니다.

605
01:03:06,583 --> 01:03:15,356
때문에 dropout을 사용하게 되면 전체 학습시간이 늘어납니다.
각 스텝마다 업데이드되는 파라미터의 수가 줄어들기 때문이죠

606
01:03:15,356 --> 01:03:22,287
다시 말해 Dropout을 사용하게 되면 전체 학습시간은 늘어나지만
모델이 수렴한 후에는 더 좋은 일반화 능력을 얻을 수 있습니다.

607
01:03:24,409 --> 01:03:32,810
사실 Dropout은 여기 보이는 일반적인 regularization 전략을
구체화시킨 하나의 예시에 불과합니다. 이 전략은 -

608
01:03:32,810 --> 01:03:37,482
Train time에는 네트워크에 무작위성(randomness)을 추가해
training data에 너무 fit하지 않게 합니다.

609
01:03:37,482 --> 01:03:41,037
네트워크를 마구잡이로 흩뜨려 놓으므로써
training data에 fit하는 것을 방해하는 것입니다.

610
01:03:41,037 --> 01:03:46,160
그리고 Test time에서는 randomness을 평균화 시켜서
generalization 효과를 주는 것입니다.

611
01:03:46,160 --> 01:03:53,927
Dropout이 Regularization에 가장 대표적인 예이긴 하지만
Batch normalization 또한 이와 비슷한 동작을 할 수 있습니다.

612
01:03:53,927 --> 01:04:00,755
Train time의 BN를 상기해보면 mini batch로 하나의 데이터가
샘플링 될 때 매번 서로 다른 데이터들과 만나게 됩니다.

613
01:04:00,755 --> 01:04:07,200
Train time에서는 각 데이터에 대해서 이 데이터를 얼마나 어떻게
정규화시킬 것인지에 대한 stochasticity이 존재했습니다.

614
01:04:07,200 --> 01:04:14,735
하지만 test time에서는 정규화를 mini batch 단위가 아닌
global 단위로 수행함으로써 stochasticity를 평균화 시킵니다.

615
01:04:14,735 --> 01:04:20,223
이런 특성 때문에 BN 은 Dropout과 유사한
Regularization 효과를 얻을 수 있었습니다.

616
01:04:20,223 --> 01:04:25,478
Train time에는 stochasticity(noise)가 추가되지만
Test time에서는 전부 평균화 되기 때문입니다.

617
01:04:25,478 --> 01:04:35,744
실제로 BN을 사용할 때는 Dropout을 사용하지 않습니다. BN에도
충분히 regularization 효과가 있기 때문이죠

618
01:04:35,744 --> 01:04:43,833
하지만 여전히 Dropout은 쓸모 있습니다. 우리가 자유롭게 조절할
수 있는 파라미터 p가 있기 때문이죠. BN에는 없는 것이죠

619
01:04:43,833 --> 01:04:48,928
이러한 Regularization 패러다임에 부합하는 또 한가지
전략은 바로 data augmentation 입니다.

620
01:04:48,928 --> 01:04:57,078
기본 버전의 학습과정에서는 데이터가 있고 레이블이 있고
이를 통해 매 스텝 CNN을 업데이트했습니다.

621
01:04:57,078 --> 01:05:03,555
하지만 그 대신 train time에 이미지를 무작위로 변환시켜 볼 수
있겠습니다. 레이블은 그대로 놔둔 채로 말입니다.

622
01:05:03,555 --> 01:05:09,418
이제 우리는 원본 이미지를 사용하는 것이 아니라
무작위로 변환시킨 이미지로 학습시키게 되는 것입니다.

623
01:05:09,418 --> 01:05:16,153
가령 이미지가 horizontal flips 할 수 있겠습니다.
이미지가 반전되도 고양이는 여전히 고양이죠

624
01:05:17,690 --> 01:05:23,763
혹은 이미지를 임의의 다양한 사이즈로 잘라서(crop)
사용할 수도 있습니다. 그래도 여전히 이미지는 고양이죠

625
01:05:25,188 --> 01:05:30,317
자 그럼 test time에서 stochasticity를 average out
시키는 것을 한번 생각해봅시다.

626
01:05:30,317 --> 01:05:34,309
"네개의 각 코너" 와 "중앙" 에서 잘라낸 이미지와
이들의 "반전 이미지"를 사용합니다.

627
01:05:34,309 --> 01:05:38,041
ImageNet 관련 모델들의 논문을 읽어보면 저자들이 보통
하나의 이미지를 그대로 사용했을 때의 성능과

628
01:05:38,041 --> 01:05:47,308
이미지 한장에서 10개를 잘라내서의 성능을 비교하곤 합니다.
*10개 = (4개코너 + 중앙) x (그냥 + 반전)

629
01:05:48,238 --> 01:05:56,345
Data augmentation으로 color jittering도 있습니다.
학습 시 이미지의 contrast 또는 brightness를 바꿔줍니다.

630
01:05:56,345 --> 01:06:04,642
color jittering에는 조금 더 복잡한 방법도 있습니다. PCA의
방향을 고려하여 color offset을 조절하는 방법입니다.

631
01:06:04,642 --> 01:06:11,456
이 방법은 color jittering을 좀 더 data-dependent한
방법으로 진행하는 것입니다. 자주 사용하는 방법은 아닙니다.

632
01:06:12,492 --> 01:06:18,037
일반적으로 data augmentation는 어떤 문제에도 적용해 볼 수 있는
아주 "일반적인 방법" 이라고 할 수 있습니다.

633
01:06:18,037 --> 01:06:24,940
어떤 문제를 풀려고 할 때, 이미지의 label을 바꾸지 않으면서
이미지를 변환시킬 수 있는 많은 방법들을 생각해 볼 수 있습니다.

634
01:06:24,940 --> 01:06:31,218
Train time에 입력 데이터에 임의의 변환을 시켜주게 되면
일종의 regularization 효과를 얻을 수 있습니다.

635
01:06:31,218 --> 01:06:38,954
그 이유를 다시 말씀드리면 train time에는 stochasticity가
추가되고 test time에는 marginalize out 되기 때문이죠

636
01:06:40,055 --> 01:06:45,232
지금까지 세 가지의 예시를 살펴보았습니다. dropout,
batch normalization, data augmentation 이죠

637
01:06:45,232 --> 01:06:47,154
하지만 또 다른 많은 방법들이 존재합니다.

638
01:06:47,154 --> 01:06:53,049
여러분들도 Regularization의 패턴을 잘 숙지하고 논문을 읽다보면
그런 방법들이 눈에 잘 들어올 것입니다.

639
01:06:53,049 --> 01:06:56,722
Dropout과 유사한 방법이 하나 더 있습니다.
DropConnect라는 방법이죠

640
01:06:56,722 --> 01:07:06,265
DropConnect은 activation이 아닌 weight matrix를
임의적으로 0으로 만들어주는 방법입니다.

641
01:07:06,265 --> 01:07:09,652
Dropout과 동작도 아주 비슷합니다.

642
01:07:09,652 --> 01:07:16,281
그리고 또 한가지 방법이 있습니다. 사람들이 자주 쓰지는 않지만
개인적으로 아주 좋은 아이디어라고 생각합니다.

643
01:07:16,281 --> 01:07:19,400
fractional max pooling 이라는 방법입니다.

644
01:07:19,400 --> 01:07:29,067
보통 2x2 maxpooling 연산은 고정된 2x2 지역에서 수행합니다.
하지만 fractional max pooling에서는 그렇게 하지 않고

645
01:07:29,067 --> 01:07:35,851
pooling연산을 수행 할 지역이 임의로 선정됩니다.

646
01:07:35,851 --> 01:07:43,070
여기 오른쪽 예시를 보시면 Train time에 샘플링될 수 있는
임의의 pooling region을 보실 수 있습니다.

647
01:07:43,070 --> 01:07:48,857
그리고 test time에 stochasticity를
average out 시키려면

648
01:07:48,857 --> 01:07:54,704
pooling regions를 고정시켜 버리거나 혹은 여러개의
pooling regions을 만들고 averaging over시킵니다.

649
01:07:54,704 --> 01:07:59,027
많이 사용하는 방법은 아니지만 참 좋은 아이디어입니다.

650
01:07:59,027 --> 01:08:05,890
Regularization pattern이라는 패러다임에서 봤을때 아주
놀랄만한 논문이 작년(2016)에 나왔습니다.

651
01:08:05,890 --> 01:08:09,911
올해 강의에 새롭게 소개시켜 드리는 논문입니다.
stochastic depth에 관련한 논문이죠

652
01:08:09,911 --> 01:08:15,490
왼쪽의 네트워크를 한번 보시죠 우리에게 아주
깊은 네트워크가 있다고 해봅시다.

653
01:08:15,490 --> 01:08:18,530
train time에 네크워크의 레이어를
randomly drop합니다.

654
01:08:18,530 --> 01:08:24,113
Train time에는 layer 중 일부를 제거해 버리고
일부만 사용해서 학습합니다.

655
01:08:24,114 --> 01:08:26,854
그리고 test time에는 전체 네트워크를 다 사용합니다.

656
01:08:26,854 --> 01:08:30,251
정말 놀라운 연구가 아닐 수 없습니다.

657
01:08:30,251 --> 01:08:35,310
하지만 이 방법의  regularization 효과는 dropout과 같은
다른 방법들과 유사합니다.

658
01:08:35,310 --> 01:08:42,041
그렇지만 아주 놀랍고 아주 최신의 연구이며(cutting-edge)
실제로 잘 사용하진 않지만 아주 좋은 아이디어입니다.

659
01:08:44,694 --> 01:08:52,673
regularization에 대해서 질문 있으신가요?

660
01:08:52,673 --> 01:08:57,046
[학생이 질문]

661
01:08:57,046 --> 01:09:01,184
질문은 보통 하나 이상의 regularization 방법을
사용하는지 입니다.

662
01:09:04,325 --> 01:09:09,751
일반적으로는  batch normalization를 많이 사용합니다.
대부분의 네트워크에서 보통 잘 동작하기 때문이죠

663
01:09:09,752 --> 01:09:12,650
아주 깊은 네트워크에서도 수렴을 잘 하도록 도와줍니다.

664
01:09:12,650 --> 01:09:25,204
대게는 BN만으로 충분합니다만 overfitting이 발생한다 싶으면
Dropout과 같은 다양한 방법을 추가해 볼 수 있습니다.

665
01:09:25,204 --> 01:09:28,526
보통은 이를 가지고 blind cross-validation
를 수행하지는 않습니다.

666
01:09:28,526 --> 01:09:33,942
대신에 네트워크에 overfit의 조짐이 보일때
하나씩 추가시켜 보는 것입니다.

667
01:09:36,400 --> 01:09:38,981
모델을 빠르게 학습시킬 수 있는 방법 중에는
transfer learning이 있습니다.

668
01:09:38,981 --> 01:09:47,018
지금까지는 regularization를 배웠습니다. 다양한 전략으로
train/test error간의 격차를 줄여보려는 것이었죠

669
01:09:48,903 --> 01:09:53,012
overfitting이 일어날 수 있는 상황중 하나는 바로
충분한 데이터가 없을 때 입니다.

670
01:09:53,012 --> 01:10:00,444
우리는 엄청 크고 파워풀한 모델을 원할지 모르겠지만 그 모델은
아주 작은 데이터셋을 지나치게 overfit할 수 있습니다.

671
01:10:00,444 --> 01:10:05,909
Regularization이 이를 해결할 수 있는 전략 중 하나입니다만
Transfer learning 이라는 방법도 있습니다.

672
01:10:05,909 --> 01:10:12,730
Transfer learning은 "CNN 학습에는 엄청많은 데이터가 필요함"
이라는 미신을 무너뜨려버립니다.

673
01:10:12,730 --> 01:10:15,300
아이디어는 정말 간단합니다.

674
01:10:15,300 --> 01:10:20,798
우선 여기 CNN모델이 있다고 해봅시다.
VGG스럽게 생긴 모델입니다.

675
01:10:20,798 --> 01:10:25,031
이 CNN을 가지고 우선은 ImageNet과 같은 아주 큰
데이터셋으로 학습을 한번 시킵니다.

676
01:10:25,031 --> 01:10:28,039
이정도 데이터셋이면 전체 네트워크를 학습시키기에
충분한 양입니다.

677
01:10:28,039 --> 01:10:34,596
자 이제 할 일은 Imagenet에서 학습된 features를
우리가 가진 작은 데이터셋에 적용하는 것입니다.

678
01:10:34,596 --> 01:10:42,864
이제는 1000개의 ImageNet 카테고리를 분류하는 것이 아니라
10종의 강아지를 분류하는 문제입니다. 데이터는 엄청 적습니다.

679
01:10:42,864 --> 01:10:45,917
이 작은 데이터셋은 C개의 클래스만 가지고있습니다.(10개)

680
01:10:45,917 --> 01:10:58,135
일반적인 절차는, 우선 가장 마지막의 FC Layer는 최종
feature와 class scores간의 연결인데 이를 초기화시킵니다.

681
01:10:59,651 --> 01:11:02,952
기존에 ImageNet을 학습시킬 때는
4,096 x 1,000 차원의 행렬이었습니다.

682
01:11:02,952 --> 01:11:09,182
하지만 우리의 새로운 문제를 풀기 위해서
4,096 x 10(C)으로 바꿔주게 됩니다.

683
01:11:09,182 --> 01:11:13,985
그리고 방금 정의한 가중치 행렬은 초기화시킵니다. 그다음
나머지 이전의 모든 레이어들의 가중치는 freeze시킵니다.

684
01:11:13,985 --> 01:11:21,947
그렇게 되면  linear classifier를 학습시키는 것과 같습니다.
오로지 마지막 레이어만 가지고 우리 데이터를 학습시키는 것입니다.

685
01:11:23,788 --> 01:11:28,756
이 방법은 사용하면 아주 작은 데이터셋일지라도
아주 작 동작하는 모델을 만들 수 있습니다.

686
01:11:28,756 --> 01:11:35,166
만일 데이터가 조금 더 있다면 전체 네트워크를
fine-tuning 할 수 있습니다.

687
01:11:35,166 --> 01:11:44,935
최종 레이어들을 학습시키고 나면, 네트워크의 일부만이 아닌
네트워크 전체의 학습을 고려해 볼 수도 있을 것입니다.

688
01:11:44,935 --> 01:11:49,434
데이터가 더 많이 있다면 네트워크의 더 많은 부분을
업데이트 시킬 수 있을지도 모릅니다.

689
01:11:49,434 --> 01:11:56,143
이 부분에서는 보통 기존의 Learning rate보다는 낮춰서 학습시킵니다.

690
01:11:56,143 --> 01:12:02,973
왜냐하면 기존의 가중치들이 이미 ImageNet으로 잘 학습되어 있고
이 가중치들이 대게는 아주 잘 동작하기 때문입니다.

691
01:12:02,973 --> 01:12:08,605
우리가 가진 데이터셋에서의 성능을 높히기 위해서라면
그 가중치들을 아주 조금씩만 수정하면 될 것입니다.

692
01:12:08,605 --> 01:12:15,490
따라서 transfer learning을 수행 함에 있어서 이렇게 생긴
2 x 2의 격자 시나리오를 예상해 볼 수 있을 것입니다.

693
01:12:15,490 --> 01:12:20,113
우선 아주 적은 양의 데이터셋이 있는 경우와
아주 많은 양의 데이터셋이 있는 경우가 있겠죠

694
01:12:21,188 --> 01:12:28,780
그리고 이전에 학습된 데이터셋과 현재 데이터셋이
얼마나 유사한지의 경우도 생각해 볼 수 있습니다.

695
01:12:28,780 --> 01:12:35,335
기존의 데이터와 유사한 경우라면 아주 순조로운 출발입니다.

696
01:12:35,335 --> 01:12:48,861
가령 현재의 데이터셋이 ImageNet와 유사하지만 소량의 경우라면,
기존 모델의 마지막 레이어만 학습시켜 볼 수 있겠습니다

697
01:12:48,861 --> 01:12:54,786
데이터가 그보다는 조금 많다고 생각이 되면
모델 전체를 fine tuning해 볼 수도 있을 것입니다.

698
01:12:54,786 --> 01:12:58,755
하지만 ImageNet과 다소 다르게 생긴 데이터셋을 가지고있다면
문제가 될 수 있습니다.

699
01:12:58,755 --> 01:13:06,781
가령 X-rays나 CAT scans와 같은 의료영상들은 대게
ImageNet의 데이터와는 많이 다릅니다.

700
01:13:06,781 --> 01:13:09,072
이 경우에는 좀 더 창의적인 방법이 필요할 수 있습니다.

701
01:13:09,072 --> 01:13:14,408
기존의 전략이 잘 동작할 수도 있겠지만 어쩌면 최종 레이어만
학습시키는 전략이 쓸모없어 질 수도 있을 것입니다.

702
01:13:14,408 --> 01:13:21,507
어쩌면 네트워크의 더 많은 부분은 다시 초기화시켜야 하거나 하는
더 창의적이고 경험적인 부분이 필요합니다.

703
01:13:21,507 --> 01:13:29,015
다만 이 문제는 여러분의 데이터셋이 아주 크다면 조금 완화됩니다.
이 경우에는 더 많은 레이어를 fine-tune 해 볼수 있기 때문이죠

704
01:13:29,015 --> 01:13:32,587
한가지 말씀드리고 싶은 것은
transfer learning이 아주 보편적이라는 것입니다.

705
01:13:32,587 --> 01:13:35,660
사실 transfer learning은 거의 일상적인 수준이 되었습니다.

706
01:13:35,660 --> 01:13:40,562
Computer vision 관련 논문들을 읽어보면 다양한 task에 대한
다음과 같은 시스템 다이어그램을 보실 수 있을 것입니다.

707
01:13:40,562 --> 01:13:44,706
왼쪽은 Object Detection과 관련된 다이어그램입니다.
오른쪽은 Image Captioning과 관련된 것이죠

708
01:13:44,706 --> 01:13:48,387
두 모델 모두 CNN구조를 가지고 있고
기본적으로 이미지를 처리합니다.

709
01:13:48,387 --> 01:13:53,913
요즘에는 거의 모든 computer vision 관련 응용 알고리즘들이
모델들을 밑바닥부터(from scrtch) 학습시키지 않습니다.

710
01:13:53,913 --> 01:13:59,973
대부분은 ImageNet pretrained-model을 사용하고 현재 본인의
task에 맞도록 fine tune 합니다.

711
01:13:59,973 --> 01:14:07,089
captioning의 경우 word vectors를
pretrain하기도 합니다.

712
01:14:07,089 --> 01:14:14,143
pretrained CNN 뿐만 아니라 규모가 큰 말뭉치로부터 학습된
pretrained word vectors도 함께 이용할 수 있겠습니다.

713
01:14:14,143 --> 01:14:22,278
captioning task에서는 pretrained word vectors을
사용하는 경우가 그닥 많지 않고 크게 중요하지 않습니다.

714
01:14:22,278 --> 01:14:33,225
여러분에게 어떤 문제가 있는데
이 문제에 대한 데이터셋이 크지 않은 경우라면

715
01:14:33,225 --> 01:14:41,673
우선 여러분의 task와 유사한 데이터셋으로 학습된
pretrained model 다운로드 받습니다.

716
01:14:41,673 --> 01:14:44,859
그리고 이 모델의 일부를 초기화시키고
여러분 데이터로 모델을 fine-tune 합니다.

717
01:14:44,859 --> 01:14:50,676
여러분의 Training data가 적당히만 있다면
아마 아주 잘 동작할 것입니다.

718
01:14:50,676 --> 01:14:57,738
이런 경우가 너무 흔하기 때문에 대부분의 딥러닝 소프트웨어
패키지들은 model zoo를 제공합니다.

719
01:14:57,738 --> 01:15:01,099
단지 접속만 하면 다양한 모델들의 pretrained 버전을
손쉽게 다운로드 받을 수 있습니다.

720
01:15:01,099 --> 01:15:06,043
오늘 한 내용을 요약해 보자면 우선 optimization을 배웠습니다.
training loss를 개선하는 방법이었죠.

721
01:15:06,043 --> 01:15:10,884
regularization에 대해서도 배웠습니다. test data에서의
성능을 향상시키는 방법이었습니다.

722
01:15:10,884 --> 01:15:12,838
Model ensembling도 regularization에 속했었죠

723
01:15:12,838 --> 01:15:17,440
그리고 transfer learning에 대해서도 배웠습니다.
데이터가 적을때 할 수 있는 아주 좋은 방법이었습니다.

724
01:15:17,440 --> 01:15:21,940
이들은 모두 아주 좋은 방법들입니다. 여러분들의 프로젝트 등에
사용해야만 하는 것들입니다.

725
01:15:21,940 --> 01:15:25,238
다음 시간에는 다양한 딥러닝 소프트웨어 패키지들이 대해서
조금 더 자세히 배워보도록 하겠습니다.
