---
title: "Deep learning: BERT 구현, 코드 설명"
date:   2020-01-17
excerpt: ""
category: [Deep Learning]
layout: post
tag:
- Deep Learning
order: 0

comments: true
---


# 목차



----
---

👀, 🤷‍♀️ , 📜    
이 아이콘들을 누르시면 코드, 개념 부가 설명을 보실 수 있습니다:)

---
----


# INRTO


**[목표]**              
사전 트레이닝된 대규모 고성능 Transformer 기반 언어 모델흘 사용하여 NLP 작업,   
* **0) 데이터 살펴보기**: NCBI 질병 언어 자료에서 가져온 데이터세트를 사용     
* **1) pretraining:** 텍스트 분류        
* **2) fine tuning:** NER(명명된 엔터티 인식)구축   





**[읽기 위해 필요한 지식]**    
* [attention](https://yerimoh.github.io/DL19/)             
* [transformer](https://yerimoh.github.io/Lan/): 정말 무조건무조건 정독하자 이거 읽고오면 이번 포스트는 껌이다.            
* [bert](https://yerimoh.github.io/Lan2/)    

**[원 논문]**          
[Attention is All You Need](https://arxiv.org/abs/1706.03762)              
[bert](https://arxiv.org/pdf/1810.04805.pdf)              
  
      



---
---

# **0) 데이터 살펴보기**
**[Corpus Annotated Data]**     
  

<details>
<summary>📜 </summary>
<div markdown="1">

 
  
</div>
</details> 




















