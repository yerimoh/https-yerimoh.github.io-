---
title: "[12] Deep learning 2: 추론기반 기법 word2vec 이용한 분산표현"
date:   2020-02-26
excerpt: "추론기반 기법 word2vec"
category: [Deep Learning]
layout: post
tag:
- Deep Learning
order: 0

comments: true
---

# 목차


------

# 자연어처리의 의미
Natural Language Processing (NLP)      
**자연어**: 한국어와 영어 등 우리가 평소에 쓰는 말     
**자연어 처리**: 컴퓨터가 우리 말을 알아듣게(이해하게)만드는 것 으로 **‘단어의 의미’를 이해**시키는 게 중요      
단어의 의미를 잘 파악하는 법      
* **1)** 시소러스를 활용한 기법     
* **2)** 통계 기반 기법     
* **3)** 추론 기반 기법(word2vec)     

지난 포스트에서 [1) 과 2)](https://yerimoh.github.io/DL13/)를 봤으니 이번엔 **3)**을 보겠다!    


----


# 추론기반 기법 word2vec
신경망(word2vec)을 이용하여 추론을 하는 기법          

## 추론 기반 기법과 신경망
- 통계 기반 기법의 문제점
: 주변 단어의 빈도를 기초로 단어 표현
-> 대규모 말뭉치를 다룰 때 문제가 발생.
: 말뭉치 전체의 통계(동시발생 행렬과 PPMI 등)를 이용해 단 
1회의 처리 (SVD 등)만에 단어의 분산 표현을 얻음(배치학습)

  [추론 기반 기법에서는] 
: 신경망을 이용하는 경우는 미니배치로 학습하는 것이 일반적
-> 신경망이 한번에 소량(미니배치)의 학습 샘플씩 반복해서 
학습하며 가중치를 갱신. 
-> 말뭉치의 어휘 수가 많아 SVD 등 계산량이 큰 작업을 처리
하기 어려운 경우에도 신경망을 학습 가능
 

- 추론 기반 기법 개요
: 추론 기반 기법_ 주변 단어 (맥락)가 주어졌을 때 “?”에 
무슨 단어가 들어가는지를 추측하는 작업입니다.
 
 
 [추론기반, 통계기반 공통점]
분포 가설에 기초_ 단어의 의미는 주변 단어에 의해 형성된다
                  단어의 동시발생 가능성

원핫 one-hot 표현 (또는 원핫 벡터)
: 신경망에서의 단어 처리
: 신경망은 “you”와 “say” 등의 단어를 있는 그대로 처리할 
수 없으니 단어를 ‘고정 길이의 벡터’로 변환 필요
-> 단어를 one-hot 표현으로 변환.

원핫 표현_ 벡터 원소 중 하나만 1이고 나머지는 모두 0인 벡터
  [ex]
 “You say goodbye and I say hello.”
   
1) 총 어휘 수만큼의 원소를 갖는 벡터를 준비
2) 인덱스가 단어 ID와 같은 원소를 1로, 나머지는 모두 0으로
3) 이처럼 단어를 고정 길이 벡터로 변환하면 우리 신경망의 입력
층은 밑처럼 뉴런의 수를 ‘고정’가능
 
-> 단어를 벡터로 나타내기 가능
-> 신경망을 구성하는 ‘계층’들은 벡터 처리 가능.
=> 단어를 신경망으로 처리 가능
 
-> 원핫 표현으로 된 단어 하나를 완전연결계층을 통해 변환
-> 완전연결계층_각각의 노드가 이웃 층의 모든 노드와 화살표
로 연결
   이 화살표에는 가중치(매개변수)가 존재_
 입력층 뉴런과의 가중합 weighted sum 이 은닉층 뉴런
 + 편향을 생략(뒤에 설명)

WARNING_ 편향을 이용하지 않는 완전연결계층
:‘행렬 곱’ 계산에 해당. 
그래서 이 책의 경우, 완전연결계층은 1장에서 구현한 MatMul 계
층과 같아짐.
참고로 딥러닝 프레임워크들은 일반적으로 완전연결계층을 생성
할 때 편향을 이용할지 선택할 수 있도록 해줌.
  
 

-> 뉴런 사이의 결합을 화살표-> 위 그림처럼 표현
   가중치를 명확하게 보여주기 위해

 [구현]

import numpy as np
c = np.array([[1, 0, 0, 0, 0, 0, 0]]) # 입력/원핫표현
W = np.random.randn(7, 3) # 가중치
h = np.matmul(c, W) # 중간 노드
print(h)
# [[-0.70012195 0.25204755 -0.79774592]]


WARNING_ 이 코드에서 입력 데이터(c )의 차원 수(ndim)는 2입니다. 이는 미니배치 처리를 고려한 것으로, 최초의 차원(0번째 차원)에 각 데이터를 저장.

 
[MatMul 계층으로 구현]

import sys
sys.path.append('..')
import numpy as np from common.layers import MatMul

c = np.array([[1, 0, 0, 0, 0, 0, 0]]) 
W = np.random.randn(7, 3) 
layer = MatMul(W)
h = layer.forward(c)
print(h)
# [[-0.70012195 0.25204755 -0.79774592]]

-> common 디렉터리에 있는 MatMul 계층을 임포트해 사용.
-> MatMul 계층에 가중치 W를 설정하고
-> forward ( ) 메서드를 호출해 순전파를 수행합니다.

 단순한 word2vec
앞은 준비
준비는 끝! 드디어 word2vec을 구현할 차례.
 
: ‘모델’을 신경망으로 구축이 목표
: word2vec에서 제안하는 CBOW continuous bag-of-words 신경망 
사용
(CBOW 모델과 skip-gram 모델은 word2vec에서 사용되는 신경망)

- CBOW 모델의 추론 처리
: CBOW 모델은 맥락으로부터 타깃 target을 추측하는 용도
‘타깃’은 중앙 단어이고 그 주변 단어들이 ‘맥락’
: CBOW 모델의 입력은 맥락입니다
1)  이 맥락을 원핫 표현으로 변환
 CBOW 모델이 처리할 수 있도록 준비
       
-> 입력층이 2개 있고, 은닉층을 거쳐 출력층에 도달.
-> 두 입력층에서 은닉층으로의 변환은 똑같은 완전연결계층
(가중치는 W_in )이 처리
-> 그리고 은닉층에서 출력층 뉴런으로의 변환은 다른 완전연
결계층(가중치는 W out )이 처리

WARNING_ 이 그림에서 입력층이 2개인 이유
맥락으로 고려할 단어를 2개로 정했기 때문.
즉, 맥락에 포함시킬 단어가 N개라면 입력층도 N개.



-> 은닉층_ 은닉층의 뉴런은 입력층의 완전연결계층에 의해 변
환된 값이 되는데, 
입력층이 여러 개이면 전체를 ‘평균’하면 됩니다. 

-> 출력층_ 출력층의 뉴런은 총 7개인데, 
이 뉴런 하나하나가 각각의 단어에 대응.
그리고 출력층 뉴런은 각 단어의 ‘점수’를 뜻함
값이 높을수록 대응 단어의 출현 확률도 높아짐
+ 점수_ 확률로 해석되기 전의 값,
점수에 소프트맥스 함수를 적용해서 ‘확률’획득 가능.

	입력층에서 은닉층으로의 변환은 완전연결계층(가중치는 W in )에 의해서 이뤄짐
	이때 완전연결계층의 가중치 W in 은 7×3 행렬이며, 
(미리 밝히자면) 이 가중치가 바로 단어의 분산 표현의 정체.
      
-> 가중치 W_in 의 각 행 = 해당 단어의 분산 표현
-> 학습을 진행할수록 맥락에서 출현하는 단어를 잘 추측하는 
방향으로 이 분산 표현들이 갱신

[핵심] 은닉층의 뉴런 수 < 입력층의 뉴런 수
: 은닉층 에는 단어 예측에 필요한 정보를 ‘간결하게’ 담음, 
-> 결과적으로 밀집벡터 표현을 얻기 가능.
인코딩_ 은닉층의 정보는 인간은 이해할 수 없는 코드로 쓰임. 
디코딩_ 은닉 층의 정보로부터 원하는 결과를 얻는 작업
인코딩 된 정보를 이해 가능한 표현으로 복원하는 작업

-> 지금까지 우리는 CBOW 모델을 ‘뉴런 관점’에서 그림
-> ‘계층 관점’에서 그림(밑)
 

WARNING_ 편향을 사용하지 않는 완전연결계층의 처리는 MatMul 
계층의 순전파와 같음
계층은 내부에서 행렬 곱을 계산.



[CBOW 모델의 추론 처리를 구현]
(추론 처리란 ‘점수’를 구하는 처리를 말합니다).

import sys
sys.path.append('..')
import numpy as np from common.layers import MatMul

# 샘플 맥락 데이터
c0 = np.array([[1, 0, 0, 0, 0, 0, 0]]) 
c1 = np.array([[0, 0, 1, 0, 0, 0, 0]])

# 가중치 초기화
W _in = np.random.randn(7, 3) 
W _out = np.random.randn(3, 7)

# 계층 생성
# 처리하는 MatMul 계층을 맥락 수만큼 (여기에서는 2개) 생성
# 출력층 측의 MatMul 계층은 1개만 생성합니다
in _layer0 = MatMul(W _in) 
in _layer1 = MatMul(W _in) 
out _layer = MatMul(W _out)

# 순전파
# 입력층 측의 MatMul 계층들(in_layer0과 in_layer1 )의 
forward ( )메서드를 호출해 중간 데이터를 계산하고, 
# 출력층 측의 MatMul 계층(out_layer)을 통과시켜 각 단어의 
점수 구함
h0 = in _layer0.forward(c0) 
h1 = in _layer1.forward(c1) 
h = 0.5 * (h0 + h1) 
s = out _layer.forward(h)

print(s)
# [[ 0.30916255 0.45060817 -0.77308656 0.22054131 0.15037278
# -0.93659277 -0.59612048]]

-> CBOW 모델은 활성화 함수를 사용 하지 않는 간단한 신경망 

[결과] 
확률은 맥락(전후 단어) 이 주어졌었을 때 그 중앙에 어떤 단어가 출현하는지를 나타냄
위의 예에서 맥락은 “you”와 “goodbye”이고, 
정답 레이블(신경망이 예측해야 할단어)은 “say”.
이때 ‘가중치가 적절히 설정된’ 신경망이라면 ‘확률’을 나타내는 뉴런들 중 정답에 해당하는 뉴런의 값이 클 것이라 기대
 

-> CBOW 모델의 학습에서는 올바른 예측을 할 수 있도록 가중치
를 조정하는 일을 함
-> 그 결과로 가중치 W in 에(정확하게는 W in 과 W out 모두에) 
단어의 출현 패턴을 파악한 벡터가 학습


NOTE_ 말뭉치가 다르면 학습 후 얻게 되는 단어의 분산 표현도 
달라짐
예컨대 말뭉치로 ‘스포츠’ 기사만을 사용하는 경우와 ‘음악’ 
관련 기사만을 사용하는 경우는 얻게 되는 단어의 분산 표현이 
크게 다름.

-> 지금 다루고 있는 모델_ 다중 클래스 분류를 수행하는 신경망 
따라서 이 신경망을 학습하려면 소프트맥스와 교차 엔트로피 
오차만 이용. 
[과정]
1) 소프트맥스 함수를 이용해 점수를 확률로 변환하고, 
2) 그 확률과 정답 레이블로부터 교차 엔트로피 오차를 구함 
3) 값을 손실로 사용해 학습을 진행.
 

-> 여기선, Softmax 계층과 Cross Entropy Error 계층을 사용,
But, 우리는 이 두 계층을 Softmax with Loss라는 하나의 계
층으로 구현. 
     
-> Softmax 계층과 Cross Entropy Error 계층을 Softmax with 
Loss 계층으로 합침

- word2vec의 가중치와 분산 표현
: word2vec에서 사용되는 신경망에는 두 가지 가중치 존재_
 입력 측 완전연결계층의 가중치(W_in )
 출력 측 완전연결계층의 가중치(W_out )
-> 입력 측 가중치 W_in 의 각 행= 각 단어의 분산 표현
   출력 측 가중치 W_out = 단어 의미가 인코딩된 벡터가 저장
다만, 출력측가중치는 분산표현이 열 방향(수직방향)으로 저장
    


최종적으로 이용하는 단어의 분산 표현으로 선택할 가중치는?
선택지는 세 가지입니다.
A 입력 측의 가중치만 이용한다.
B 출력 측의 가중치만 이용한다.
C 양쪽 가중치를 모두 이용한다.

: A와 B 안은 어느 한쪽 가중치만 이용한다는 것.
: 마지막 C 안에서는 두 가중치를 어떻게 조합하느냐에 따라 다
시 몇 가지 방법을 생각 가능 
-> 그중 하나는 두 가중치를 단순히 합치는 것.

word2vec (특히 skip-gram 모델)_
 A 안인 ‘입력 측의 가중치만 이용한다’가 가장 대중적인 선택. 

학습 데이터 준비
: word2vec 학습에 쓰일 학습 데이터를 준비. 

- 맥락과 타깃
: 신경망의 입력은 ‘맥락’입니다. 
: 정답 레이블은 맥락에 둘러 싸인 중앙의 단어, 즉 ‘타깃’
(목표) 신경망에 ‘맥락’을 입력했을때 ‘타깃’이 출현할 확
률을 높이는 것(그렇게 되도록 학습시킬 겁니다).

[EX] 말뭉치에서 맥락과 타깃을 만드는 예
 
+ 각 샘플 데이터에서 맥락의 수는 여러 개(이 예에서는 2개)가 
될 수 있으나, 타깃은 오직 하나 뿐.
+ 그래서 맥락을 영어로 쓸 때는 끝에 ‘s’를 붙여 복수형임을 
명시하는 게 좋음

말뭉치로부터 맥락과 타깃을 만드는 함수를 구현
1) 우선 말뭉치 텍스트를 단어 ID로 변환
preprocess ( ) 함수를 사용합니다.

import sys
sys.path.append('..') 
from common.util import preprocess

text = 'You say goodbye and I say hello.'
corpus, word _to _id, id _to _word = preprocess(text)
print(corpus)
# [0 1 2 3 4 1 5 6]

print(id _to _word) 
# {0:'you', 1:'say', 2:'goodbye', 3:'and', 4:'i', 5: 'hello', 6: '.'}


2) 단어 ID의 배열인 corpus로부터 맥락과 타깃을 만들어 냄.
 
-> 맥락은 2차원 배열
맥락의 0번째 차원: 각 맥락 데이터가 저장
_정확하게 말하면, contexts[0]에는 0번째 맥락이 저장되고, 
contexts[1 ]에는 1번째 맥락이 저장되는 식
_마찬가지로 타깃에서도 target[0 ]에는 0번째 타깃이, 
target[1 ]에는 1번째 타깃이 저장.
[구현]
create _contexts _target (corpus, window _size)

# 인수를 두 개 받음.
# 하나는 단어 ID의 배열(corpus),
   다른 하나는 맥락의 윈도우 크기(window _size)
def create _contexts _target(corpus, window _size=1):
target = corpus[window _size:-window _size] 
contexts = []

for idx in range(window _size, len(corpus)-window _size):
cs = [] 
for t in range(-window _size, window _size + 1):
if t == 0:
continue
cs.append(corpus[idx + t]) 
contexts.append(cs)

# 맥락과 타깃을 각각 넘파이 다차원 배열로 돌려줌
return np.array(contexts), np.array(target)

# 실제로 사용
contexts, target = create _contexts _target(corpus, window _size=1)

print(contexts)
# [[0 2] 
 [1 3] 
[2 4] 
 [3 1] 
 [4 5] 
 [1 6]]

print(target)
# [1 2 3 4 1 5]

3) 이를 원핫 표현으로 변환
 
: 다차원 배열의 형상에 주목_ 
이 그림에서는 단어 ID를 이용했을 때의 맥락의 형상은 (6, 2)
-> 이를 원핫 표현으로 변환하면 (6, 2, 7 )이 됩니다.
convert _one _hot ( ) 함수를 사용(책에서 제공) 
common/utill.py에 구현

 CBOW 모델 구현
전체 신경망 SimpleCBOW라는 이름으로 구현할 겁니다
  
 [구현]

import sys
sys.path.append('..')
import numpy as np 
from common.layers import MatMul, SoftmaxWithLoss

# 이 초기화 메서드는 인수로 
# 어휘 수(vocab _size )와 
은닉층의 뉴런 수(hidden _size )받음
class SimpleCBOW:
def _ _init _ _(self, vocab _size, hidden _size):
V, H = vocab _size, hidden _size

# 가중치 초기화
  두 가중치는 각각 작은 무작위 값으로 초기화
# 이때 넘파이 배열의 데이터 타입을 astype ('f ')
즉, 32비트 부동소수점 수로 초기화
W _in = 0.01 * np.random.randn(V, H).astype('f')
W _out = 0.01 * np.random.randn(H, V).astype('f')

# 계층 생성
# 입력 측의 MatMul 계층을 2개,
_입력 측의 맥락 처리계층은 맥락에서 사용하는 
단어의 수(윈도우 크기)만큼 만들어야 함
- 입력 측 MatMul 계층들은 모두 같은 가중치를 
이용하도록 초기화
# 출력 측의 MatMul 계층을 하나, 
# Softmax with Loss 계층을 하나 생성
self.in _layer0 = MatMul(W _in) 
self.in _layer1 = MatMul(W _in) 
self.out _layer = MatMul(W _out) 
self.loss _layer = SoftmaxWithLoss()

# 모든 가중치와 기울기를 리스트에 모은다
 이 신경망에서 사용되는 매개변수와 기울기를 
인스턴스 변수 params와 grads 리스트에 각각 모아둠
layers = [self.in _layer0, self.in _layer1, self.out _layer] 
self.params, self.grads = [], [] 
for layer in layers:
self.params += layer.params 
self.grads += layer.grads

# 인스턴스 변수에 단어의 분산 표현을 저장한다 .
self.word _vecs = W _in

# 신경망의 순전파인 forward ( ) 메서드를 구현
# 인수로 맥락 (contexts )과 타깃(target )을 받아 
손실(loss )을 반환
def forward(self, contexts, target):
h0 = self.in _layer0.forward(contexts[:, 0]) 
h1 = self.in _layer1.forward(contexts[:, 1]) 
h = (h0 + h1) * 0.5
score = self.out _layer.forward(h) 
loss = self.loss _layer.forward(score, target) 
return loss

+ 인수 contexts는 3차원 넘파이 배열이라고 가정. 
[그림 3-18 ]의 예에서라면 이배열의 형상은 (6, 2, 7 )
_0번째 차원의 원소 수: 미니배치의 수
_1번째 차원의 원소 수: 맥락의 윈도우 크기
_2번째 차원: 원핫 벡터
또한, target의 형상은 2차원, 
예컨대 (6, 7)과 같은 형상이 됩니다.




# 마지막으로 역전파인 backward ( )를 구현 
def backward(self, dout=1):
ds = self.loss _layer.backward(dout) 
da = self.out _layer.backward(ds) 
da *= 0.5
self.in _layer1.backward(da)
 self.in _layer0.backward(da) 
return None

   

-> 이미 각 매개변수의 기울기를 인스턴스 변수 grads에 모아둠
따라서 forward ( ) 메서드를 호출한 다음 backward ( ) 메서드
를 실행하는 것만으로 grads 리스트의 기울기가 갱신 

학습 코드 구현
1) 학습 데이터를 준비해 신경망에 입력
2) 기울기를 구하고 
3) 가중치 매개변수를 순서대로 갱신해 감
Trainer 클래스(1장)이용(☞ ch03/train.py ).

import sys
sys.path.append('..') 
from common.trainer import Trainer 
from common.optimizer import Adam 
from simple _cbow import SimpleCBOW 
from common.util import preprocess, create _contexts _target, 
convert _one _hot

window _size = 1
hidden _size = 5
batch _size = 3
max _epoch = 1000

text = 'You say goodbye and I say hello.'
corpus, word _to _id, id _to _word = preprocess(text)

vocab _size = len(word _to _id) 
contexts, target = create_contexts_target(corpus, window _size) 
target = convert _one _hot(target, vocab _size) 
contexts = convert _one _hot(contexts, vocab _size)

model = SimpleCBOW(vocab _size, hidden _size) 
optimizer = Adam() 
#신경망을 학습
 학습 데이터로부터 미니배치를 선택한 다음, 신경망에 입력해 기울기를 구하고, 그 기울기를 Optimizer 에 넘겨 매개변수를 갱신하는 일련의 작업을 수행
trainer = Trainer(model, optimizer)

trainer.fit(contexts, target, max _epoch, batch _size) 
trainer.plot()

매개변수 갱신 방법으로는 SGD와 AdaGrad 같은 유명 알고리즘 몇 개가 common/optimizer.py에 구현.
-> 이 코드에서는 그중 Adam. 
결과
     

-> 학습을 거듭할수록 손실이 줄어듦

[학습이 끝난 후의 가중치 매개변수]
앞의 코드 바로 뒤에 다음 코드를 추가합니다.

word _vecs = model.word _vecs 
for word _id, word in id _to _word.items():
print(word, word _vecs[word _id])
    
     
 단어를 밀집벡터로 나타냄

> word2vec 보충
: CBOW 모델을 ‘확률’ 관점에서 다시 살펴봄.

CBOW 모델과 확률
‘확률’의 표기법: 
- A라는 현상이 일어날 확률은 P (A )
- 동시 확률 P (A, B): ‘A와 B가 동시에 일어날 확률’
- 사후 확률 P (A|B) : ‘ 사건 이 일어난 후 의 확률 ’
‘B 가 주어졌을 때 A가 일어날 확률’

CBOW 모델을 확률 표기법으로 기술:
맥락을 주면 타깃 단어가 출현할 확률을 출력
 
[EX] 맥락 w_t-1 과 w_t+1 이 주어질 때, 타깃이 w t 가 될 확률? 
P(w_t | w_t−1 ,w_t+1 )
->‘w t -1 과 w t +1 이 일어난 후 w t 가 일어날 확률’
->‘w t -1 과 w t +1 이 주어졌을 때 w t 가 일어날 확률’
CBOW 모델의 손실 함수도 간결하게 표현:
[교차 엔트로피 오차]
 ->교차 엔트로피 오차
y_k:‘k번째에 해당하는 사건이 일어날 확률’
t_k : 정답 레이블(원핫 벡터로 표현)
여기서 문제의 정답: ‘w_t 가 발생’
-> w_t 해당 원소만 1, 나머지는 0
->즉, w t 이외의 일이 일어날 경우에 대
해서는 해당 원핫 레이블의 요소는 0
[적용] 음의 로그 가능도(negative log likelihood)
 
-> CBOW 모델의 손실 함수는 단순히 확률에 log를 취한 다음 
마이너스를 붙이면 됨
[말뭉치 전체로 확장]
 
-> CBOW 모델의 학습이 수행하는 일:
 이 손실 함수의 값을 가능한 한 작게 만드는 것.
 이때의 가중치 매개변수_ 우리가 얻고자 하는 단어의 분산 표현.

skip-gram 모델
word2vec은 2개의 모델 제안_ CBOW 모델, skip-gram 모델
: skip-gram은 CBOW에서 다루는 맥락과 타깃을 역전시킨 모델
  
: CBOW 모델_ 맥락이 여러 개 존재, 이로부터 타깃 추측.
: skip-gram 모델_ 타깃으로부터 주변의 여러 단어 (맥락)를 추측
 
-> skip-gram 모델의 입력층은 하나,
 출력층은 맥락의 수만큼 존재 
1) 각 출력층에서는 (Softmax with Loss 계층 등을 이용해) 개별
적으로 손실을 구함
2) 이 개별 손실들을 모두 더한 값이 최종 손실.

[skip-gram 모델을 확률로]
1)
 
2) 여기서 맥락의 단어들 사이에 관련성이 없다고 가정 후 분해
(‘조건부 독립’이라고 가정).
  
3) 손실함수 유도
  
-> skip-gram 모델의 손실 함수는 맥락별 손실을 구한 다음 
모두 더함.
[말뭉치 전체로 확장]
  

	단어 분산 표현의 정밀도 면에서 skip-gram 모델의 결과가 더 좋은 경우가 많아 앞의 모델보다 이걸 쓰는 것을 추천(특히 말뭉치가 커질수록 저빈도 단어나 유추 문제의 성능 면에서 skip-gram 모델이 더 뛰어남)
	속도는 CBOW가 더 빠름
> 통계 기반 vs. 추론 기반
- 학습하는 틀:
통계 기반 기법_ 말뭉치의 전체 통계로부터 1회 학습하여 단어의 
분산 표현을 얻음
추론 기반 기법_ 말뭉치를 일부분씩 여러 번 보며 학습(미니배치
학습). 

- 단어의 분산 표현을 갱신해야 하는 상황(새 어휘 추가):
통계 기반 기법_ 계산을 처음부터 다시
                단어의 분산 표현을 조금만 수정하고 싶어도, 
동시발생 행렬을 다시 만들고 SVD를 수행하는
일련의 작업을 다시 해야 함.
추론 기반 기법_ 기존에 학습한 경험을 해치지 않으면서 단어의 
분산 표현을 효율적으로 갱신 가능.

- 단어의 분산 표현의 성격이나 정밀도
통계 기반 기법_ 분산표현이 주로 단어의 유사성이 인코딩됨.
추론 기반 기법_ 분산표현이 복잡한 단어 사이의 패턴까지도 파
악되어 인코딩. 

+ 단어의 유사성을 정량 평가해본 결과, 
의외로 추론 기반과 통계 기반 기법의 우열을 가릴 수 없음
+ 두 방법은 (특정 조건 하에서) ‘서로 연결되어 있다’
+ GloVe [27 ] 기법이 등장
: 추론 기반 기법과 통계 기반 기법을 융합

