---
title: "[01] BERT 정리 "
date:   2021-03-10
excerpt: "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---

# **intro**
최근 BERT 라는 이름의 모델이 많은 자연어처리 분야에서 가장 주목 받고 있음     
특정 분야에 국한된 기술이 아니라 모든 자연어 응용 분야에서 좋은 성능을 내는 범용 모델    
사람 성능까지 뛰어넘음    

**[핵심]**       
**encode**r의 개선 모델으로 Token을 가리고 학습하는 masking **(MLM)**,    
두 문장의 연관성을 파악하는 **NSP**를 통해 학습 전확도를 높임     
 
**[읽기 위해 필요한 지식]**    
* [attention](https://yerimoh.github.io/DL19/)           
* [transformer](https://yerimoh.github.io//Lan/)

**[원 논문]**     
[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)

---

# 목차 
- [**BERT**](#--bert--)
- [**전체 개요**](#--전체-개요--)
  * [Token Embeddings](#token-embeddings)
  * [Sentence Embeddings](#sentence-embeddings)
  * [Position Embedding](#position-embedding)
  * [임베딩 취합](#임베딩-취합)
- [**BERT의 2가지 구성**](#**bert의-2가지-구성**)
- [**언어 모델링 구조(Pre-training BERT)**](#**언어-모델링-구조-pre-training-bert-**)
  * [BERT 모델 구조](#bert-모델-구조)
- [**결과 비교**](#**결과-비교**)




----



👀, 🤷‍♀️ , 📜    
이 아이콘들을 누르시면 코드, 개념 부가 설명을 보실 수 있습니다:)



---


# BERT
BERT는 Bidirectional Encoder Representations from Transformers의 약자      
즉, "[트렌스포머](https://yerimoh.github.io//Lan/)의 양방향 엔코더" 라는 뜻이다      
BERT는 **양방향성**을 포함하여 **문맥**을 더욱 자연스럽게 파악가능   

------
-----


# **전체 개요**
**BERT의 구성**    
![image](https://user-images.githubusercontent.com/76824611/118171515-427bb700-b466-11eb-9924-7293b7d525ae.png)

3가지의 **입력** 임베딩의 합    
* Token 임베딩    
* Position 임베딩  
* Segment 임베딩     
  

[임베딩 더 알아보러 가기](https://yerimoh.github.io/DL15/#1%EF%B8%8F%E2%83%A3-embedding-%EA%B3%84%EC%B8%B5-%EB%8F%84%EC%9E%85)

----

## Token Embeddings

![image](https://user-images.githubusercontent.com/76824611/132776279-8e42d622-4c59-469a-a2c8-a0ff7a263956.png)
[CLS] 토큰은 Next sentence prediction, NSP에서 추후 설명    
  
**[기존의 워드 임베딩]**        
단어를 ~~희소 표현~~이 아닌 **밀집 표현**으로 표현하는 방법     
* 희소표현: [원-핫 인코딩](https://yerimoh.github.io/DL14/#%EC%A4%80%EB%B9%84-%EC%9B%90%ED%95%AB-one-hot-%ED%91%9C%ED%98%84)을 통해 표현     
   * 고양이 = \[ 0 1 0 1 0 1 1 0 0 0 0 ]   
* 밀집 표현: 벡터의 차원을 단어 집합의 크기로 상정하지 않소 사용자가 설정 ➡ 실수값 가짐    
   * 고양이 = \[ 0.2 1.8 1.1 -2.1 1.1 2.8 ]    

⛔ **PROBLEM**      
기존 워드 임베딩 방법은 Out-of-vocabulary (**OOV**) 문제 존재 (**모르는 단어 처리 못함**)      
* 희귀 단어, 이름, 숫자나 단어장에 없는 단어에 대한 학습, 번역에 어려움 존재      

⭕ **SOLUTION**     
**BERT의 임베딩**         
기존 흔히 사용되는 ~~워드 임베딩 방식~~ 사용 안함 ->  **Word Piece 임베딩 방식을 사용**     
이 방식은 **자주 등장**하면서 **가장 긴 길이**의 **sub-word**를 하나의 단위로 만듦.              
* 자주 등장하는 sub-word: 그 자체가 단위가 되고,    
* 자주 등장하지 않는 단어(rare word): sub-word로 쪼개짐      


<details>
<summary>📜 Word Piece 임베딩, 워드 임베딩, 캐릭터 임베딩 자세히 보기 </summary>
<div markdown="1">
  
**자연어처리 딥러닝**은 **임베딩(embedding)**을 해야 한다는 점에서 이미지 딥러닝과 큰 차이가 있음.      

임베딩: 문장을 신경망이 이해할 수 있는 벡터로 변환하는 것을 말한다(차원 수에 무리가 가지 않게)     
* **워드 임베딩**: 단어를 기준으로 변환   
  * 워드 임베딩인 Seq2Seq에서 "강아지"는 하나의 출력이다       
  * 더 높은 성능을 보이는 경우가 많음      
  * 하지만 OOV(Out Of Vocabulary) 문제 존재       
* **캐릭터 임베딩**: 글자를 기준으로 변환       
   * "강","아","지" 이렇게 케릭터 단위로 출력한다.          
   * 학습에 더 어려움     
   * 하지만 OOV(Out Of Vocabulary) 문제가 없음   
 
**Word Piece 임베딩**: 워드 임베딩 + 캐릭터 임베딩    
* 1) 캐릭터 단위로 말뭉치 분리     
* 2) 자주 나오는 캐릭터들을 병합하여 하나의 토큰으로 만듦      
* 의미가 있는 캐릭터들이 묶여지기 때문에 캐릭터 임베딩과 워드 임베딩의 장점이 합쳐짐      
 
 
</div>
</details>  

<details>
<summary>📜 ex 자세히 보기 </summary>
<div markdown="1">
  
**(ex1)**     
```" 내 강아지는 귀엽다"```를 이해할 때,  
'강아지'라는 명사를 정해두고 '귀엽다'를 쓰지 않음     
'귀엽다'라는 표현을 쓰고 싶어서 '강아지'라는 명사를 선택했을 수 있음    
즉, 앞뒤를 모두 봐야 문맥 파악 가능      
**(ex2)**    
문장의 ```"잘했다"```**긍정 부정** 의미 파악    
* A: 나 칭찬받았어!   B: "잘했다"     
* A: 나 혼났어!   B: "잘~했다"      
즉, 동의어를 파악하기 위해선 앞뒤 단어의 관계성을 파악해야 함   
 
 
</div>
</details>  

**[기존 모델 개선점]**    
모든 언어에 적용 가능    
sub-word 단위로 단어를 분절하므로 OOV 처리에 효과적    
정확도 상승효과    
* Question and Answering: 주어진 질문에 적합하게 대답해야하는 매우 대표적인 문제(KoSQuAD, Visual QA etc)      
* Machine Translation: 구글 번역기, 네이버 파파고.     
* 문장 주제 찾기 또는 분류하기: 기존 NLP에서도 해결할 수 있는 문제 해결 가능    
* 사람처럼 대화하기: 이와 같은 주제에선 매우 강력함       

-----


## Position Embedding
![image](https://user-images.githubusercontent.com/76824611/132776320-29e508a3-053d-450e-a844-20ef1ee4989b.png)
BERT는 [Transformer](https://yerimoh.github.io//Lan/) 모델을 착용.     
* Transformer은 주로 사용하는 CNN, RNN 모델을 사용하지 않고 Self-Attention 모델을 사용.      
* Self-Attention은 ~~입력의 위치~~에 대해 고려하지 못하므로 **입력 토큰의 위치 정보**를 줘야함     
   * **Transformer**:  Sinusoid 함수를 이용한 [Positional encoding](https://yerimoh.github.io//Lan/#positional-encoding)을 사용      
   * **BERT**: 이를 변형하여 Position encoding을 사용           

**Position encoding**: 단순하게 Token 순서대로 0, 1, 2, ...와 같이 순서대로 인코딩       
ex)    
* 첫번째 단어의 임베딩 벡터 + 0번 포지션 임베딩 벡터   
* 두번째 단어의 임베딩 벡터 + 1번 포지션 임베딩 벡터   
* 세번째 단어의 임베딩 벡터 + 2번 포지션 임베딩 벡터   
* 네번째 단어의 임베딩 벡터 + 3번 포지션 임베딩 벡터    

➡ Segment Embeddings에서  문장의 최대 길이를 512로 하고 있으므로, 총 512개의 포지션 임베딩 벡터가 학습된다    

---


## Segment Embeddings
![image](https://user-images.githubusercontent.com/76824611/132776332-b264eae2-5d0f-4d22-9725-7cbce65dca17.png)

BERT는 **두 개**의 문장을 **문장 구분자**([SEP])와 함께 합쳐 넣음.    
* 입력 길이의 제한: 두 문장은 합쳐서 512 subword 이하          
* 입력의 길이가 길어질수록 학습시간은 제곱으로 증가하기 때문   ->    적절한 입력 길이를 설정 필요      

BERT는 Q&A 등과 같은 두 개의 문장 입력이 필요한 테스크를 풀기도 함.   
➡ 이러한 문제에 문장 구분은 위해서 이 임베딩을 사용     
➡ sentence 0 임베딩, 두번째 문장에는 Sentence 1 임베딩을 더해주는 방식이며 임베딩 벡터는 두 개만 사용됨    

**[사용하는 이유]**    
Next sentence prediction, NSP에 쓰인다(추후 설명)


----


## 임베딩 취합     
BERT는 위에서 소개한 3가지의 입력 임베딩(Token, Segment, Position 임베딩)을 취합하여 **하나의 임베딩** 값으로 만듦.     
![프레젠테이션3](https://user-images.githubusercontent.com/76824611/132776179-cdba6875-8ec7-4dc0-a720-2531b53eb44a.gif)

즉, 위의 3가지 임베딩이 어떤식으로 작용하는지 보면,    
* **WordPiece Embedding** : 실질적인 **입력**이 되는 워드 임베딩. 임베딩 벡터의 종류는 단어 집합의 크기 30,522개.          
* **Segment Embedding** : **두 개의 문장을 구분**하기 위한 임베딩. 임베딩 벡터의 종류는 문장의 최대 개수인 2개.        
* **Position Embedding** : **위치** 정보를 학습하기 위한 임베딩. 임베딩 벡터의 종류는 문장의 최대 길이인 512개.    


그리고 이 합에 **Layer Normalization** 과 **[Dropout](https://yerimoh.github.io/DL5/#%EA%B0%80%EC%A4%91%EC%B9%98-%EA%B0%90%EC%86%8Cweight-decay)**을 적용하여 입력으로 사용      


-----
-----

# **BERT의 2가지 구성**
![image](https://user-images.githubusercontent.com/76824611/118174862-51fcff00-b46a-11eb-8502-259855d20f59.png)     
출처: [bert paper](https://arxiv.org/pdf/1810.04805.pdf)    

BERT를 이용한 자연어처리는 2단계로 나뉜다         

---

## 1) Pre-training BERT    
![image](https://user-images.githubusercontent.com/76824611/132776663-88cd65cb-24bb-40c4-bebb-9c05b578e45c.png)
출처: [bert paper](https://arxiv.org/pdf/1810.04805.pdf)    
* **Trm**: 트랜스포머를 의미  


거대 Encoder가 입력 문장들을 임베딩 하여 언어를 모델링하는 언어 모델링 구조 과정을 보여준다.    

    
ELMo와 GPT-1, 그리고 BERT의 구조적인 차이를 보여준다.     
**[단방향(➡)으로 설계]**     
* **ELMo**: 정방향 LSTM과 역방향 LSTM을 각각 훈련시키는 방식으로 양방향 언어 모델을 만듦       
* GPT-1: 트랜스포머의 디코더를 이전 단어들로부터 다음 단어를 예측하는 방식으로 단방향 언어 모델을 만듦     

**[양방향(⬅➡)으로 설계]**   
* BERT: 화살표가 양방향으로 뻗어나가는 모습을 볼 수 있다.(자신보다 정의 노드들에게 뻗어나가는 화살표 들이 존재)     
   * 이는 마스크드 언어 모델(Masked Language Model)을 통해 양방향성을 얻었기 때문이다     

**[BERT의 사전 훈련 방법]**   
크게 두 가지로 나뉜다      
* [1] 마스크드 언어 모델(Masked Language Model)             
* [2] 다음 문장 예측(Next sentence prediction, NSP)      

논문에 따르면 BERT는 BookCorpus(8억 단어)와 위키피디아(25억 단어)로 학습되었다.      



<details>
<summary>📜 [1] Masked Language Model, MLM </summary>
<div markdown="1">
 
**1)** 사전 훈련을 위해서 인공 신경망의 입력으로 들어가는 입력 텍스트의 **15%의 단어**를 랜덤으로 마스킹(Masking)한다     
**2)** 모델에게 이 가려진 단어들을(Masked words) **예측**하도록 한다.     
**3)** 중간에 단어들에 구멍을 뚫어놓고, 구멍에 들어갈 단어들을 예측하게 한다   
 

랜덤으로 선택된 **15%의 단어들**은 다시 다음과 같은 비율로 규칙이 적용된다         

1️⃣ 80%의 단어들은 [MASK]로 변경한다.   
Ex) The man went to the store → The man went to the **[MASK]**    

2️⃣ 10%의 단어들은 랜덤으로 단어가 변경된다.   
Ex) The man went to the **store** → The man went to the **dog**    
 
3️⃣ 10%의 단어들은 동일하게 둔다.   
Ex) The man went to the store → The man went to the store   
 

**[랜덤으로 선택된 **15%의 단어들**에 모두 [MASK]를 적용하지 않는 이유]**    
➡ [MASK]만 사용할 경우에는 **[MASK] 토큰이 파인 튜닝 단계에서는 나타나지 않으므로** 사전 학습 단계와 파인 튜닝 단계에서의 불일치가 발생     
(왜?)  미세 조정 시 올바른 예측을 돕도록 마스킹에 **노이즈**를 섞는 것.    
 
**[ex]**   
'I love my cat. she like runing'라는 문장에 대해서 Masked Language Model를 적용해보자      

전처리와 BERT의 서브워드 토크나이저에 의해,    
이 문장은 ['i', 'love', 'my', 'cat', 'she', 'like', 'run', '##ing']로 토큰화가 된다.    
 
1️⃣ 80%의 단어들은 [MASK]로 변경한다. 
![image](https://user-images.githubusercontent.com/76824611/132779735-4d484ba1-17b3-401d-8814-6ad768fae821.png)
'cat' 토큰이 [MASK]로 변경되어서 BERT 모델이 원래 단어를 맞추려고 하는 것이 보인다      
 
출력층에 있는 다른 위치의 벡터들은 예측과 학습에 사용되지 않고 **'cat'의 출력층 위치** 에만 **Masked Language Model, MLM **이 쓰인다    
➡ BERT의 손실 함수에서 다른 위치에서의 예측은 무시 

2️⃣ 10%의 단어들은 랜덤으로 단어가 변경된다.     
![image](https://user-images.githubusercontent.com/76824611/132779741-d1291ba6-0ef1-4ead-84fc-699144de287c.png)
이제 BERT는 랜던 담어 'child'으로 변경된 토큰에 대해서도 원래 단어가 무엇인지 예측을 해야한다.      
그러니 이제  **'child'의 출력층 위치** 에도 **Masked Language Model, MLM **가 쓰였다    
 
 
3️⃣ 10%의 단어들은 동일하게 둔다.    
![image](https://user-images.githubusercontent.com/76824611/132779746-c7ff7e63-e4fb-4d9f-9b03-7ed62445d646.png)
그런데 그림을 보면 동일하게 둔 'run'은 바뀌지 않았는데도 **Masked Language Model, MLM **가 적용이 되었다.     
왜 그럴까?  
왜 변경되지 않은 단어 'run'에 대해서도 원래 단어가 무엇인지를 예측해야 하는걸까?     

바로 2️⃣ 때문이다.   
'run'은 변경되지 않았지만 BERT 입장에서는 이것이 변경된 단어인지 아닌지 모르므로 마찬가지로 원래 단어를 예측해야 하는 것 이다.     


 
 
</div>
</details>  


<details>
<summary>📜 [2] Next sentence prediction, NSP </summary>
<div markdown="1">
 
1) 두 개의 문장을 줌     
2) 이 문장이 **이어지는 문장인지 아닌지**를 맞추는 방식으로 훈련시킴      
 
 
**[훈련방법]**   
이를 위해서 50:50 비율로 실제 이어지는 두 개의 문장과 랜덤으로 이어붙인 두 개의 문장을 주고 훈련시킨다.    

이를 각각 Sentence A와 Sentence B라고 할 때,    
**(이어지는 문장의 경우)**   
Sentence A : I love my cat   
Sentence B : she like running        
Label = **IsNextSentence**    

**(이어지는 문장이 아닌 경우)**   
Sentence A : I love my cat         
Sentence B : the man is working         
Label = **NotNextSentence**   

![image](https://user-images.githubusercontent.com/76824611/132780876-c66dada1-3d99-4e35-b730-487355070363.png)

✔ **[SEP] 토큰**      
BERT의 입력으로 넣을 때에, **[SEP]** 토큰을 사용해서 문장을 구분한다.    
* Sentence A의 끝에 [SEP] 토큰이 있다    
* Sentence B이 끝나면 역시 [SEP] 토큰을 확인할 수 있다     

✔ **[CLS] 토큰**       
이 두 문장이 실제 이어지는 문장인지 아닌지를 **[CLS] 토큰의 위치의 출력층**에서 **이진 분류 문제**를 풀도록 한다.     
[CLS] 토큰은 BERT가 **분류 문제를 풀기 위해 추가된 특별 토큰**이다.     
그리고 위의 그림에서 나타난 것과 같이 **마스크드 언어 모델**과 다음 **문장 예측**은 따로 학습하는 것이 아닌 loss를 합하여 학습이 **동시에 이루어**짐       

**[Next sentence prediction, NSP를 하는 이유]**    
BERT가 풀고자 하는 태스크 중에서는 QA(Question Answering)나 NLI(Natural Language Inference)와 같이 **두 문장의 관계**를 이해하는 것이 중요한 태스크들이 있기 때문      

 
</div>
</details>  



----


## 2) fine-tuning
이를 fine-tuning 하여 여러 자연어 처리 Task를 수행하는 과정.       
[fine-tuning 더 알아보기](https://yerimoh.github.io/DL12/#%ED%8C%8C%EC%9D%B8-%ED%8A%9C%EB%8B%9D)    

**fine-tuning**      
* 사전 학습 된 BERT에 우리가 풀고자 하는 태스크의 데이터를 추가로 학습 시켜서 테스트하는 단계   
* 학습한 언어 모델을 이용하여 실제 자연어처리 문제를 푸는 과정          
➡ 언어 모델이 제대로 학습되야 [전이학습](https://yerimoh.github.io/DL15/#word2vec-%EB%82%A8%EC%9D%80-%EC%A3%BC%EC%A0%9C)시 좋은 성능이 나옴    
    

**[fine-tuning의 4가지 케이스]**    
![image](https://user-images.githubusercontent.com/76824611/118200416-f47ba900-b48f-11eb-98e2-a460c352d3cc.png)
 
**(a)**: 문장 쌍 분류 문제로 두 문장을 하나의 입력으로 넣고 두 문장간 관계 구함.    
**(b)**: 한 문장을 입력으로 넣고 문장의 종류를 분류하는 문제.    
**(c)**: 문장이나 문단 내에서 원하는 정답 위치의 시작과 끝을 구함.     
**(d)**: 입력 문장 Token들의 개체명(Named entity recognigion)을 구하거나 품사(Part-of-speech tagging) 를 구하는 문제     
* 다른 Task들과 다르게 입력의 모든 Token들에 대해 결과를 구함   


----
----


# **언어 모델링 구조(Pre-training BERT)**    

**[기존의 방법들]**     
보통 좌-우(left-to-right)로 학습하거나 우-좌(right-to-left)로 학습
이 방식들은 입력의 다음단어를 예측하는데 좋은 성능을 발휘      

**[BERT]**      
이 방식들과 다르게 언어의 특성을 잘 학습하도록 2가지 방식 사용      
* MLM(Masked Language Model)      
* NSP(Next Sentence Prediction)     

➡ 위의 설명 참조     

기존 방식인 좌-우 언어모델과 비교하여 훨씬 좋은 성능을 기록.         
![image](https://user-images.githubusercontent.com/76824611/118175342-f54e1400-b46a-11eb-93d4-bffaf504be61.png)


----

## BERT 모델 구조       
![image](https://user-images.githubusercontent.com/76824611/132781428-2e6c0728-25b8-4902-809c-cd7ee9497bc0.png)     위는 [Transformer](https://yerimoh.github.io/Lan2)의 구조이다          


BERT는 Transformer의 인코더-디코더 중 **인코더만 사용**하는 모델이다.    
**incoder**: input에서 text의 표현을 학습하고     
**decoder**: 우리가 원하는 task의 결과물을 만드는 방식      

**(BERT는 2가지 버전 존재)**           
BERT는 decoder를 사용하지 않고, 두 가지 대표적인 학습 방법으로 encoder를 학습시킨 후에      
특정 task의 fine-tuning을 활용하여 결과물을 얻는 방법으로 사용됨    
* BERT-base(L=12, H=768, A=12)     
* BERT-large(L=24, H=1024, A=16)     

(설명)    
* **L**: Transformer 블록의 숫자    
* **H**: hidden size     
* **A**: Transformer의 Attention block 숫자         
즉 L, H, A가 크다는 것은 블록을 많이 쌓았고, 표현하는 은닉층이 크며 Attention 개수를 많이 사용하였다는 뜻.   

<details>
<summary>📜 BERT 이후의 변종</summary>
<div markdown="1">

sub-word 단위로 쪼개진 Token을 마스킹 하는게 아니라, 한 단어를 통째로 마스킹 하는 whole word masking 방법을 사용하기도 함.     
**한국어**는 subword 단위로 쪼개는 방식보다 **형태소 단위**로 쪼개서 마스킹하는 방식이 더 효과가 좋음      

</div>
</details> 


----
---

# **결과 비교**
BERT의 2가지 학습방법(MLM, NSP)을 제거해보며 결과를 비교해봄.      
* No NSP: MLM만 적용한 방식     
* LTR & No NSP: MLM 대신 left-to-right 언어모델 방식으로 학습한 방식     
left-to-right 방식보다 MLM 방식이 엄청난 성능 향상을 가져옴       
NSP 유무도 NLI 계열의 문제에서 성능이 많이 하락-> NSP가 **문장간의 논리 구조**를 학습하는데 중요함을 알 수 있음    
![image](https://user-images.githubusercontent.com/76824611/118200563-5b995d80-b490-11eb-801c-c6d750e1eb35.png)

 
