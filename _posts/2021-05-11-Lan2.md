---
title: "[01] BERT 정리 "
date:   2021-03-10
excerpt: "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
category: [Language AI]
layout: post
tag:
- Java
order: 0

comments: true
---

# intro
최근 BERT 라는 이름의 모델이 많은 자연어처리 분야에서 가장 주목 받고 있음     
특정 분야에 국한된 기술이 아니라 모든 자연어 응용 분야에서 좋은 성능을 내는 범용 모델    
사람 성능까지 뛰어넘음    

## 핵심  
**encode**r의 개선 모델으로 Token을 가리고 학습하는 masking **(MLM)**,    
두 문장의 연관성을 파악하는 **NSP**를 통해 학습 전확도를 높임     
 
## 읽기 위해 필요한 지식
[attention](링크 삽입) 
[transformer](https://yerimoh.github.io//Lan/)

## 원 논문
[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)

---

# 목차 
- [BERT](#bert)
- [전체 개요](#전체-개요)
  * [Token Embeddings](#token-embeddings)
  * [Sentence Embeddings](#sentence-embeddings)
  * [Position Embedding](#position-embedding)
  * [임베딩 취합](#임베딩-취합)
- [BERT의 2가지 구성](#bert의-2가지-구성)
- [언어 모델링 구조(Pre-training BERT)](#언어-모델링-구조-pre-training-bert-)
  * [기존의 방법들](#기존의-방법들)
  * [BERT](#bert-1)
  * [모델 구조](#모델-구조)
  * [MLM(Masked Language Model)](#mlm-masked-language-model-)
  * [NSP(Next Sentence Prediction)](#nsp-next-sentence-prediction-)
- [학습된 언어모델 전이학습(fine-tuning)](#학습된-언어모델-전이학습-fine-tuning-)
  * [전이학습의 4가지 케이스](#전이학습의-4가지-케이스)
- [결과 비교](#결과-비교)





---


# BERT
BERT는 Bidirectional Encoder Representations from Transformers의 약자      
즉, "[트렌스포머](https://yerimoh.github.io//Lan/)의 양방향 엔코더" 라는 뜻이다      
BERT는 **양방향성**을 포함하여 **문맥**을 더욱 자연스럽게 파악가능   
 
# 전체 개요
BERT의 구성    
![image](https://user-images.githubusercontent.com/76824611/118171515-427bb700-b466-11eb-9924-7293b7d525ae.png)

3가지의 **입력** 임베딩의 합    
* Token 임베딩    
* Segment 임베딩     
* Position 임베딩    

## Token Embeddings
  
### 기존의 워드 임베딩  
단어를 ~~희소 표현~~이 아닌 **밀집 표현**으로 표현하는 방법     
* [희소표현](링크 삽입): 원-핫 인코딩을 통해 표현     
   * 고양이 = \[ 0 1 0 1 0 1 1 0 0 0 0 ]   
* 밀집 표현: 벡터의 차원을 단어 집합의 크기로 상정하지 않소 사용자가 설정 -> 실수값 가짐    
   * 고양이 = \[ 0.2 1.8 1.1 -2.1 1.1 2.8 ]    

**[문제]**     
기존 워드 임베딩 방법은 Out-of-vocabulary (OOV) 문제 존재(모르는 단어)      
* 희귀 단어, 이름, 숫자나 단어장에 없는 단어에 대한 학습, 번역에 어려움 존재      

### BERT의 임베딩
B기존 흔히 사용되는 ~~워드 임베딩 방식~~ 사용 안함 ->  **Word Piece 임베딩 방식을 사용**     
이 방식은 **자주 등장**하면서 **가장 긴 길이**의 **sub-word**를 하나의 단위로 만듦.              
* 자주 등장하는 sub-word: 그 자체가 단위가 되고,    
* 자주 등장하지 않는 단어(rare word): sub-word로 쪼개짐    
**(ex1)**     
```" 내 강아지는 귀엽다"```를 이해할 때,  
'강아지'라는 명사를 정해두고 '귀엽다'를 쓰지 않음     
'귀엽다'라는 표현을 쓰고 싶어서 '강아지'라는 명사를 선택했을 수 있음    
즉, 앞뒤를 모두 봐야 문맥 파악 가능      
**(ex2)**    
문장의 ```"잘했다"```**긍정 부정** 의미 파악    
* A: 나 칭찬받았어!   B: "잘했다"     
* A: 나 혼났어!   B: "잘~했다"      
즉, 동의어를 파악하기 위해선 앞뒤 단어의 관계성을 파악해야 함   

**[기존 모델 개선]**    
모든 언어에 적용 가능    
sub-word 단위로 단어를 분절하므로 OOV 처리에 효과적    
정확도 상승효과    
Question and Answering: 주어진 질문에 적합하게 대답해야하는 매우 대표적인 문제(KoSQuAD, Visual QA etc)      
Machine Translation: 구글 번역기, 네이버 파파고.     
문장 주제 찾기 또는 분류하기: 기존 NLP에서도 해결할 수 있는 문제 해결 가능    
사람처럼 대화하기: 이와 같은 주제에선 매우 강력함       



## Sentence Embeddings
BERT는 **두 개**의 문장을 **문장 구분자**([SEP])와 함께 합쳐 넣음.    
* 입력 길이의 제한: 두 문장은 합쳐서 512 subword 이하          
* 입력의 길이가 길어질수록 학습시간은 제곱으로 증가하기 때문   ->    적절한 입력 길이를 설정 필요      

## Position Embedding
BERT는 [Transformer](https://yerimoh.github.io//Lan/) 모델을 착용.     
* Transformer은 주로 사용하는 CNN, RNN 모델을 사용하지 않고 Self-Attention 모델을 사용.      
* Self-Attention은 ~~입력의 위치~~에 대해 고려하지 못하므로 **입력 토큰의 위치 정보**를 줘야함     
   * **Transformer**:  Sinusoid 함수를 이용한 Positional encoding을 사용      
   * **BERT**: 이를 변형하여 Position encoding을 사용           
**Position encoding**: 단순하게 Token 순서대로 0, 1, 2, ...와 같이 순서대로 인코딩       


## 임베딩 취합     
BERT는 위에서 소개한 3가지의 입력 임베딩(Token, Segment, Position 임베딩)을 취합하여 **하나의 임베딩** 값으로 만듦.      
그리고 이 합에 **Layer Normalization** 과 **[Dropout](링크 삽입)**을 적용하여 입력으로 사용      

-----

# BERT의 2가지 구성
![image](https://user-images.githubusercontent.com/76824611/118174862-51fcff00-b46a-11eb-8502-259855d20f59.png)

BERT를 이용한 자연어처리는 2단계로 나뉨     
1) **Pre-training BERT**     
거대 Encoder가 입력 문장들을 임베딩 하여 언어를 모델링하는 언어 모델링 구조 과정    
2) **fine-tuning**
이를 fine-tuning 하여 여러 자연어 처리 Task를 수행하는 과정.     



# 언어 모델링 구조(Pre-training BERT)    

## 기존의 방법들
보통 좌-우(left-to-right)로 학습하거나 우-좌(right-to-left)로 학습
이 방식들은 입력의 다음단어를 예측하는데 좋은 성능을 발휘      

## BERT
이 방식들과 다르게 언어의 특성을 잘 학습하도록 2가지 방식 사용      
* MLM(Masked Language Model)      
* NSP(Next Sentence Prediction)     
기존 방식인 좌-우 언어모델과 비교하여 훨씬 좋은 성능을 기록.         
![image](https://user-images.githubusercontent.com/76824611/118175342-f54e1400-b46a-11eb-93d4-bffaf504be61.png)

## 모델 구조   
[참고_transformer[https://yerimoh.github.io/Lan/]           
![image](https://user-images.githubusercontent.com/76824611/118175800-8ae9a380-b46b-11eb-97fb-b123b436d929.png)

BERT는 Transformer의 인코더-디코더 중 **인코더만 사용**하는 모델이다.    
**incoder**: input에서 text의 표현을 학습하고     
**decoder**: 우리가 원하는 task의 결과물을 만드는 방식      

**(BERT는 2가지 버전 존재)**           
BERT는 decoder를 사용하지 않고, 두 가지 대표적인 학습 방법으로 encoder를 학습시킨 후에      
특정 task의 fine-tuning을 활용하여 결과물을 얻는 방법으로 사용됨    
* BERT-base(L=12, H=768, A=12)     
* BERT-large(L=24, H=1024, A=16)     

(설명)    
* **L**: Transformer 블록의 숫자    
* **H**: hidden size     
* **A**: Transformer의 Attention block 숫자         
즉 L, H, A가 크다는 것은 블록을 많이 쌓았고, 표현하는 은닉층이 크며 Attention 개수를 많이 사용하였다는 뜻.     

## MLM(Masked Language Model)   
입력 문장에서 임의로 Token을 가리고(masking), 그 Token을 맞추는 방식인 MLM 학습을 진행        
* 문장의 빈칸 채우기 문제를 학습        

```
BERT 이후의 변종들에서 sub-word 단위로 쪼개진 Token을 마스킹 하는게 아니라, 한 단어를 통째로 마스킹 하는 whole word masking 방법을 사용하기도 함.     
**한국어**는 subword 단위로 쪼개는 방식보다 **형태소 단위**로 쪼개서 마스킹하는 방식이 더 효과가 좋음      
```
### MASK 생성 방식 
cf) 생성 모델 계열은(예를들어 GPT) 입력의 다음 단어를 예측               
**MLM**: 문장 내 **랜덤한 단어**를 마스킹 하고 이를 예측                
아래 그림과 같이 입력의 15% 단어를 [MASK] Token으로 바꿔주어 마스킹               
* 이 때 80%는 [MASK]로 바꿔주지만,      
* 나머지 10%는 다른 랜덤 단어로,    
* 또 남은 10%는 바꾸지 않고 그대로 둠            
(왜?)  미세 조정 시 올바른 예측을 돕도록 마스킹에 **노이즈**를 섞는 것.      
 ![image](https://user-images.githubusercontent.com/76824611/118177604-de5cf100-b46d-11eb-9e34-91b127b34321.png)

### MLM의 학습 과정
1) 입력 단어의 15%가 [MASK]로 대체된 입력이 들어가고,     
2) MLM은 [MASK]가 어떤 단어인지를 예측.     
* BERT의 Token 임베딩은 Word Piece 임베딩 방식을 사용,     
* Word piece의 단어수는 30522 단어.      
   * 그러므로 3만 단어 중 [MASK]에 들어갈 단어를 찾는 것이므로 MLM의 출력인 Softmax의 클래스는 3만개    
![image](https://user-images.githubusercontent.com/76824611/118199332-d3b25400-b48d-11eb-9f63-e2bf9175f711.png)
 
 
 
## NSP(Next Sentence Prediction)
두 문장이 주어졌을 때 두 번째 문장이 첫 번째 문장의 바로 다음에 오는 문장인지 여부를 예측하는 방식.     
두 문장 간 관련이 고려되어야 하는 NLI와 QA의 파인튜닝을 위해 **두 문장의 연관성**여부를 학습.           
ex) 설명한 MLM과 동시에 NSP도 적용된 문장    
* 첫 번째 문장과 두 번째 문장은 [SEP]로 구분.    
* 두 문장이 실제로 연속하는지는 50% 비율로 참인 문장과, 50%의 랜덤하게 추출된 상관 없는 문장으로 구성.     
* 이 학습을 통해 문맥과 순서를 언어모델이 학습 가능     
![image](https://user-images.githubusercontent.com/76824611/118199760-c2b61280-b48e-11eb-96ad-dcdf8b988371.png)

아래 그림은 NSP의 학습 방법.      
* 연속 문장인지, 아닌지만 판단하면 되므로 Softmax의 출력은 2개      
* 3만개의 출력을 갖는 MLM에 비해 빠르게 학습됨     
![image](https://user-images.githubusercontent.com/76824611/118199892-fdb84600-b48e-11eb-8a9b-c0f66e2096c7.png)


----

# 학습된 언어모델 전이학습(fine-tuning)
미세조정을 하는 단계: 학습한 언어 모델을 이용하여 실제 자연어처리 문제를 푸는 과정       
언어 모델이 제대로 학습되야 전이학습 시 좋은 성능이 나옴    
전이학습은 라벨이 주어지므로 지도학습(Supervised learning)      

## 전이학습의 4가지 케이스
**(a)**: 문장 쌍 분류 문제로 두 문장을 하나의 입력으로 넣고 두 문장간 관계 구함.    
**(b)**: 한 문장을 입력으로 넣고 문장의 종류를 분류하는 문제.    
**(c)**: 문장이나 문단 내에서 원하는 정답 위치의 시작과 끝을 구함.     
**(d)**: 입력 문장 Token들의 개체명(Named entity recognigion)을 구하거나 품사(Part-of-speech tagging) 를 구하는 문제     
* 다른 Task들과 다르게 입력의 모든 Token들에 대해 결과를 구함   
![image](https://user-images.githubusercontent.com/76824611/118200416-f47ba900-b48f-11eb-98e2-a460c352d3cc.png)

---

# 결과 비교
BERT의 2가지 학습방법(MLM, NSP)을 제거해보며 결과를 비교해봄.      
* No NSP: MLM만 적용한 방식     
* LTR & No NSP: MLM 대신 left-to-right 언어모델 방식으로 학습한 방식     
left-to-right 방식보다 MLM 방식이 엄청난 성능 향상을 가져옴       
NSP 유무도 NLI 계열의 문제에서 성능이 많이 하락-> NSP가 **문장간의 논리 구조**를 학습하는데 중요함을 알 수 있음    
![image](https://user-images.githubusercontent.com/76824611/118200563-5b995d80-b490-11eb-801c-c6d750e1eb35.png)

 
