---
title: "[01] BERT 정리 "
date:   2021-05-10
excerpt: "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
category: [Language AI]
layout: post
tag:
- Java
order: 0

comments: true
---

# intro
최근 BERT 라는 이름의 모델이 많은 자연어처리 분야에서 가장 주목 받고 있음     
특정 분야에 국한된 기술이 아니라 모든 자연어 응용 분야에서 좋은 성능을 내는 범용 모델    
사람 성능까지 뛰어넘음    

## 핵심  
#multi-head self-attention을 이용해 sequential computation 을 줄여 더 많은 부분을 병렬처리가 가능하게 만들면서 동시에 더 많은 단어들 간 dependency를 모델링 한다는 것

## 읽기 위해 필요한 지식
#[attention](링크 삽입) 
[transformer](https://yerimoh.github.io//Lan/)

## 원 논문
[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)

---

# BERT
BERT는 Bidirectional Encoder Representations from Transformers의 약자      
즉, "[트렌스포머](https://yerimoh.github.io//Lan/)의 양방향 엔코더" 라는 뜻이다
 
# 전체 개요
BERT의 구성    
![image](https://user-images.githubusercontent.com/76824611/118171515-427bb700-b466-11eb-9924-7293b7d525ae.png)

3가지의 **입력** 임베딩의 합    
* Token 임베딩    
* Segment 임베딩     
* Position 임베딩    

## Token Embeddings
  
### 기존의 워드 임베딩  
단어를 ~~희소 표현~~이 아닌 **밀집 표현**으로 표현하는 방법     
* [희소표현](링크 삽입): 원-핫 인코딩을 통해 표현     
   * 고양이 = \[ 0 1 0 1 0 1 1 0 0 0 0 ]   
* 밀집 표현: 벡터의 차원을 단어 집합의 크기로 상정하지 않소 사용자가 설정 -> 실수값 가짐    
   * 고양이 = \[ 0.2 1.8 1.1 -2.1 1.1 2.8 ]    

**[문제]**     
기존 워드 임베딩 방법은 Out-of-vocabulary (OOV) 문제 존재(모르는 단어)      
* 희귀 단어, 이름, 숫자나 단어장에 없는 단어에 대한 학습, 번역에 어려움 존재      

### BERT의 임베딩
B기존 흔히 사용되는 ~~워드 임베딩 방식~~ 사용 안함 ->  **Word Piece 임베딩 방식을 사용**     
이 방식은 **자주 등장**하면서 **가장 긴 길이**의 **sub-word**를 하나의 단위로 만듦.              
* 자주 등장하는 sub-word: 그 자체가 단위가 되고,    
* 자주 등장하지 않는 단어(rare word): sub-word로 쪼개짐    

**[기존 모델 개선]**    
모든 언어에 적용 가능    
sub-word 단위로 단어를 분절하므로 OOV 처리에 효과적    
정확도 상승효과    


## Sentence Embeddings
BERT는 **두 개**의 문장을 **문장 구분자**([SEP])와 함께 합쳐 넣음.    
* 입력 길이의 제한: 두 문장은 합쳐서 512 subword 이하          
* 입력의 길이가 길어질수록 학습시간은 제곱으로 증가하기 때문   ->    적절한 입력 길이를 설정 필요      

## Position Embedding
BERT는 [Transformer](https://yerimoh.github.io//Lan/) 모델을 착용.     
* Transformer은 주로 사용하는 CNN, RNN 모델을 사용하지 않고 Self-Attention 모델을 사용.      
* Self-Attention은 ~~입력의 위치~~에 대해 고려하지 못하므로 **입력 토큰의 위치 정보**를 줘야함     
   * **Transformer**:  Sinusoid 함수를 이용한 Positional encoding을 사용      
   * **BERT**: 이를 변형하여 Position encoding을 사용           
**Position encoding**: 단순하게 Token 순서대로 0, 1, 2, ...와 같이 순서대로 인코딩       


## 임베딩 취합     
BERT는 위에서 소개한 3가지의 입력 임베딩(Token, Segment, Position 임베딩)을 취합하여 **하나의 임베딩** 값으로 만듦.      
그리고 이 합에 **Layer Normalization** 과 **[Dropout](링크 삽입)**을 적용하여 입력으로 사용      

-----

# BERT의 2가지 구성
![image](https://user-images.githubusercontent.com/76824611/118174862-51fcff00-b46a-11eb-8502-259855d20f59.png)

BERT를 이용한 자연어처리는 2단계로 나뉨     
1) **Pre-training BERT**     
거대 Encoder가 입력 문장들을 임베딩 하여 언어를 모델링하는 언어 모델링 구조 과정    
2) **fine-tuning**
이를 fine-tuning 하여 여러 자연어 처리 Task를 수행하는 과정.     



# 언어 모델링 구조(Pre-training BERT)    

## 기존의 방법들
보통 좌-우(left-to-right)로 학습하거나 우-좌(right-to-left)로 학습
이 방식들은 입력의 다음단어를 예측하는데 좋은 성능을 발휘      

## BERT
이 방식들과 다르게 언어의 특성을 잘 학습하도록 2가지 방식 사용      
* MLM(Masked Language Model)      
* NSP(Next Sentence Prediction)     
기존 방식인 좌-우 언어모델과 비교하여 훨씬 좋은 성능을 기록.         
![image](https://user-images.githubusercontent.com/76824611/118175342-f54e1400-b46a-11eb-93d4-bffaf504be61.png)

## 모델 구조   
<참고_transformer>
![image](https://user-images.githubusercontent.com/76824611/118175800-8ae9a380-b46b-11eb-97fb-b123b436d929.png)

BERT는 Transformer의 인코더-디코더 중 **인코더만 사용**하는 모델이다.    

BERT는 2가지 버전 존재      
* BERT-base(L=12, H=768, A=12)     
* BERT-large(L=24, H=1024, A=16)     

(설명)    
* **L**: Transformer 블록의 숫자    
* **H**: hidden size     
* **A**: Transformer의 Attention block 숫자         
즉 L, H, A가 크다는 것은 블록을 많이 쌓았고, 표현하는 은닉층이 크며 Attention 개수를 많이 사용하였다는 뜻.     

## MLM(Masked Language Model)   
입력 문장에서 임의로 Token을 가리고(masking), 그 Token을 맞추는 방식인 MLM 학습을 진행        
* 문장의 빈칸 채우기 문제를 학습        

```
BERT 이후의 변종들에서 sub-word 단위로 쪼개진 Token을 마스킹 하는게 아니라, 한 단어를 통째로 마스킹 하는 whole word masking 방법을 사용하기도 함.     
**한국어**는 subword 단위로 쪼개는 방식보다 **형태소 단위**로 쪼개서 마스킹하는 방식이 더 효과가 좋음      
```
### MASK 생성 방식 
cf) 생성 모델 계열은(예를들어 GPT) 입력의 다음 단어를 예측               
**MLM**: 문장 내 **랜덤한 단어**를 마스킹 하고 이를 예측                
아래 그림과 같이 입력의 15% 단어를 [MASK] Token으로 바꿔주어 마스킹               
* 이 때 80%는 [MASK]로 바꿔주지만,      
* 나머지 10%는 다른 랜덤 단어로,    
* 또 남은 10%는 바꾸지 않고 그대로 둠            
(왜?)  미세 조정 시 올바른 예측을 돕도록 마스킹에 **노이즈**를 섞는 것.      
 ![image](https://user-images.githubusercontent.com/76824611/118177604-de5cf100-b46d-11eb-9e34-91b127b34321.png)

 ### MLM의 학습 과정
 입력 단어의 15%가 [MASK]로 대체된 입력이 들어가고, MLM은 [MASK]가 어떤 단어인지를 예측한다. BERT의 Token 임베딩은 Word Piece 임베딩 방식을 사용하고, Word piece의 단어수는 30522 단어이다. 그러므로 3만 단어 중 [MASK]에 들어갈 단어를 찾는 것이므로 MLM의 출력인 Softmax의 클래스는 3만개 이다.
 
 
 
 
