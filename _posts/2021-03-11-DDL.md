---
title: "[10] [INDEX] Deep learning 2"
date:   2021-03-11
excerpt: "Deep learning starting from the bottom 2"
category: [Deep Learning]
layout: post
tag:
- Deep Learning
order: 0

comments: true
---

# 자연어와 단어의 분산 표현법   

## [11] 시소러스, 통계기반 기법   [learn [11]](https://yerimoh.github.io/DL13/){: .btn}
- [자연어와 단어의 분산 표현](#자연어와 단어의 분산 표현)
- [자연어처리의 의미](#자연어처리의 의미)
- [1) 시소러스](#1-시소러스)
- [2) 통계 기반 기법](#2-통계 기반 기법)
  * [말뭉치 전처리 구현](#말뭉치 전처리 구현)
  * [단어의 분산 표현](#단어의 분산 표현)
  * [통계기반 기법 개선](#통계기반 기법 개선)
    + [상호정보량 PMI](#상호정보량-pmi)
    + [차원 감소(dimensionality reduction)](#차원 감소-dimensionality-reduction-)

## [12] word2vec 기법 [learn [12]](https://yerimoh.github.io/DL14/){: .btn}
- [자연어처리의 의미](#자연어처리의-의미)
- [추론기반 기법 word2vec](#-추론기반-기법-word2vec)
- [준비: 원핫 one-hot 표현](#준비--원핫-one-hot-표현)
- [단순한 word2vec 구현](#-단순한-word2vec-구현)
  * [CBOW 모델 사용](#cbow-모델-사용)
    + [학습 데이터 준비](#학습-데이터-준비)
    + [CBOW 모델 구현](#cbow-모델-구현)
- [word2vec 보충](#word2vec-보충)
  * [CBOW 모델과 확률](#cbow-모델과-확률)
  * [skip-gram 모델](#skip-gram-모델)
    + [skip-gram 모델을 확률로](#skip-gram-모델을-확률로)
- [통계 기반 vs. 추론 기반](#통계 기반-vs--추론-기반)

## [13] word2vec 기법 개선 [learn [13]](https://yerimoh.github.io/DL15/){: .btn}
- [word2vec 속도 개선](#word2vec-속도-개선)
- [word2vec 개선 2가지 방법](#word2vec-개선-2가지-방법)
  * [1️⃣ Embedding 계층 도입](#1---embedding-계층-도입)
    + [Embedding 계층 구현](#embedding--계층-구현)
  * [2️⃣ 네거티브 샘플링이란 손실 함수 도입](#2--네거티브-샘플링이란-손실-함수-도입)
    + [다중 분류에서 이진 분류로](#다중-분류에서-이진-분류로)
    + [시그모이드 함수와 교차 엔트로피 오차](#시그모이드-함수와-교차-엔트로피-오차)
    + [Embedding Dot 계층의 구현](#embedding-dot-계층의-구현)
    + [네거티브 샘플링](#네거티브-샘플링)
- [개선판 word2vec 학습](#개선판-word2vec-학습)
  * [CBOW 모델 구현](#cbow-모델-구현)
  * [CBOW 모델 평가](#cbow-모델-평가)
- [word2vec 남은 주제](#word2vec-남은-주제)
  * [word2vec을 사용한 애플리케이션의 예](#word2vec을-사용한-애플리케이션의-예)


-------

# RNN

## 순환신경망 RNN   [learn [14]](https://yerimoh.github.io/DL16/){: .btn}
- [확률과 언어 모델](#확률과 언어 모델)
  * [word2vec의 CBOW 모델 복습](#word2vec의-cbow-모델 복습)
  * [CBOW 모델 ➡ 언어 모델의 문제점](#cbow-모델 ➡ 언어 모델의 문제점)
- [RNN(순환 신경망)이란](#rnn(순환 신경망)이란)
  * [순환 구조 펼치기](#순환 구조 펼치기)
  * [BPTT](#bptt)
  * [Truncated BPTT](#truncated-bptt)
- [RNN 구현](#rnn-구현)
  * [1) RNN 계층 구현](#1--rnn-계층-구현)
  * [2) Time RNN 계층 구현](#2--time-rnn-계층-구현)
- [시계열 데이터 처리 계층 구현 RNNLM](#시계열 데이터 처리 계층-구현-rnnlm)
- [RNNLM 학습과 평가](#rnnlm-학습과 평가)
  * [RNNLM 구현](#rnnlm-구현)
- [언어 모델의 평가 perplexity](#언어 모델의 평가-perplexity)
  * [RNNLM의 학습 코드](#rnnlm의 학습 코드)
- [RNNLM의 Trainer 클래스](#rnnlm의-trainer-클래스)
- [정리](#정리)
