---
title: "[10] [INDEX] Deep learning 2"
date:   2021-03-11
excerpt: "Deep learning starting from the bottom 2"
category: [Deep Learning]
layout: post
tag:
- Deep Learning
order: 0

comments: true
---

# 자연어와 단어의 분산 표현법   

## [11] 시소러스, 통계기반 기법  

[learn [11]](https://yerimoh.github.io/DL13/){: .btn}  
- [자연어와 단어의 분산 표현](#자연어와 단어의 분산 표현)
- [자연어처리의 의미](#자연어처리의 의미)
- [1) 시소러스](#1-시소러스)
- [2) 통계 기반 기법](#2-통계 기반 기법)
  * [말뭉치 전처리 구현](#말뭉치 전처리 구현)
  * [단어의 분산 표현](#단어의 분산 표현)
  * [통계기반 기법 개선](#통계기반 기법 개선)
    + [상호정보량 PMI](#상호정보량-pmi)
    + [차원 감소(dimensionality reduction)](#차원 감소-dimensionality-reduction-)

## [12] word2vec 기법 

[learn [12]](https://yerimoh.github.io/DL14/){: .btn}  
- [자연어처리의 의미](#자연어처리의-의미)
- [추론기반 기법 word2vec](#-추론기반-기법-word2vec)
- [준비: 원핫 one-hot 표현](#준비--원핫-one-hot-표현)
- [단순한 word2vec 구현](#-단순한-word2vec-구현)
  * [CBOW 모델 사용](#cbow-모델-사용)
    + [학습 데이터 준비](#학습-데이터-준비)
    + [CBOW 모델 구현](#cbow-모델-구현)
- [word2vec 보충](#word2vec-보충)
  * [CBOW 모델과 확률](#cbow-모델과-확률)
  * [skip-gram 모델](#skip-gram-모델)
    + [skip-gram 모델을 확률로](#skip-gram-모델을-확률로)
- [통계 기반 vs. 추론 기반](#통계 기반-vs--추론-기반)

## [13] word2vec 기법 개선  

[learn [13]](https://yerimoh.github.io/DL15/){: .btn}    
- [word2vec 속도 개선](#word2vec-속도-개선)
- [word2vec 개선 2가지 방법](#word2vec-개선-2가지-방법)
  * [1️⃣ Embedding 계층 도입](#1---embedding-계층-도입)
    + [Embedding 계층 구현](#embedding--계층-구현)
  * [2️⃣ 네거티브 샘플링이란 손실 함수 도입](#2--네거티브-샘플링이란-손실-함수-도입)
    + [다중 분류에서 이진 분류로](#다중-분류에서-이진-분류로)
    + [시그모이드 함수와 교차 엔트로피 오차](#시그모이드-함수와-교차-엔트로피-오차)
    + [Embedding Dot 계층의 구현](#embedding-dot-계층의-구현)
    + [네거티브 샘플링](#네거티브-샘플링)
- [개선판 word2vec 학습](#개선판-word2vec-학습)
  * [CBOW 모델 구현](#cbow-모델-구현)
  * [CBOW 모델 평가](#cbow-모델-평가)
- [word2vec 남은 주제](#word2vec-남은-주제)
  * [word2vec을 사용한 애플리케이션의 예](#word2vec을-사용한-애플리케이션의-예)


-------

# RNN

## [15] 순환신경망 RNN   

[learn [14]](https://yerimoh.github.io/DL16/){: .btn}    
- [확률과 언어 모델](#확률과 언어 모델)
  * [word2vec의 CBOW 모델 복습](#word2vec의-cbow-모델 복습)
  * [CBOW 모델 ➡ 언어 모델의 문제점](#cbow-모델 ➡ 언어 모델의 문제점)
- [RNN(순환 신경망)이란](#rnn(순환 신경망)이란)
  * [순환 구조 펼치기](#순환 구조 펼치기)
  * [BPTT](#bptt)
  * [Truncated BPTT](#truncated-bptt)
- [RNN 구현](#rnn-구현)
  * [1) RNN 계층 구현](#1--rnn-계층-구현)
  * [2) Time RNN 계층 구현](#2--time-rnn-계층-구현)
- [시계열 데이터 처리 계층 구현 RNNLM](#시계열 데이터 처리 계층-구현-rnnlm)
- [RNNLM 학습과 평가](#rnnlm-학습과 평가)
  * [RNNLM 구현](#rnnlm-구현)
- [언어 모델의 평가 perplexity](#언어 모델의 평가-perplexity)
  * [RNNLM의 학습 코드](#rnnlm의 학습 코드)
- [RNNLM의 Trainer 클래스](#rnnlm의-trainer-클래스)
- [정리](#정리)

## [16] RNN에 게이트 추가하기(LSTM)  

[learn [15]](https://yerimoh.github.io/DL17/){: .btn}
- [게이트가 추가된 RNN](#게이트가 추가된-rnn)
- [RNN의 문제점](#rnn의 문제점)
  * [기울기 소실 또는 기울기 폭발](#기울기 소실 또는 기울기 폭발)
- [기울기 소실과 LSTM](#기울기 소실과-lstm)
  * [LSTM의 인터페이스](#lstm의 인터페이스)
    + [output 게이트](#output-게이트)
    + [forget 게이트](#forget-게이트)
    + [새로운 기억 셀](#새로운 기억 셀)
    + [input 게이트](#input-게이트)
- [LSTM의 기울기 흐름, 구현](#lstm의 기울기 흐름, 구현)
- [LSTM을 사용한 언어 모델](#lstm을 사용한 언어 모델)
- [RNNLM 추가 개선](#rnnlm-추가 개선)
    + [LSTM 계층 다층화](#lstm-계층 다층화)
    + [드롭아웃에 의한 과적합 억제](#드롭아웃에 의한 과적합 억제)
    + [가중치 공유 weight tying](#가중치 공유-weight-tying)
  * [개선된 RNNLM 구현](#개선된-rnnlm-구현)

## [17] RNN을 사용한 문장 생성   

[learn [16]](https://yerimoh.github.io/DL18/){: .btn}
- [언어 모델을 사용한 문장 생성](#언어-모델을-사용한-문장-생성)
- [seq2seq](#seq2seq)
  * [seq2seq의 구조](#seq2seq의-구조)
- [시계열 데이터 변환용 장난감 문제](#시계열-데이터-변환용-장난감-문제)
  * [가변 길이 시계열 데이터](#가변-길이-시계열-데이터)
- [seq2seq 구현](#seq2seq-구현)
  * [[1단계] Encoder 클래스](#-1단계--encoder-클래스)
  * [[2단계] Decoder 클래스](#-2단계--decoder-클래스)
  * [[3단계] Seq2seq 클래스](#-3단계--seq2seq-클래스)
  * [seq2seq 평가](#seq2seq-평가)
- [seq2seq 개선](#seq2seq-개선)
  * [입력 데이터 반전(Reverse)](#입력 데이터 반전-reverse-)
  * [엿보기 Peeky](#엿보기-peeky)
- [seq2seq를 이용하는 애플리케이션](#seq2seq-이용하는-애플리케이션)
- [정리](#정리)

----

# [18] 어텐션 Attention   

[learn [17]](https://yerimoh.github.io/DL19/){: .btn}
- [INTRO](#intro)
- [어텐션의 구조](#어텐션의-구조)
  * [Encoder 개선](#encoder-개선)
    + [Decoder 개선 ①](#decoder-개선--)
    + [Decoder 개선 ②](#decoder-개선--)
    + [Decoder 개선 ③](#decoder-개선--)
- [어텐션을 갖춘 seq2seq 구현](#어텐션을 갖춘--seq2seq-구현)
- [어텐션 평가](#어텐션-평가)
- [어텐션에 관한 남은 이야기](#어텐션에-관한-남은-이야기)
  * [양방향 RNN](#양방향-rnn)
  * [Attention 계층 사용 방법](#attention-계층-사용-방법)
- [정리](#정리)


-----


# 추가 학습

## Hyperparameter와 Parameters  

[learn](https://yerimoh.github.io/DL100/){: .btn}  

## Training, validation, test datasets 비교 

[learn](https://yerimoh.github.io/DL101/){: .btn}    

## 오토인코더(Auto Encoder)

[learn](https://yerimoh.github.io/DL104/){: .btn}      

## 강화학습 (Reinforcement Learning) 

[learn](https://yerimoh.github.io/DL105/){: .btn} 

## NeMo를 통한 BERT 구현, 코드 설명 

[learn](https://yerimoh.github.io/DL107/){: .btn}    

## Transformer 구현, 코드 설명  

[learn](https://yerimoh.github.io/DL106/){: .btn}    

## 프레임워크, 라이브러리  

[learn](https://yerimoh.github.io/DL108/){: .btn}    
