---
title: "[03] Deep learning 1: 신경망 학습"
date:   2020-03-10
excerpt: "데이터 주도 학습,손실 함수(loss function),오차 제곱 합(sum of squares for error, SSE),교차 엔트로피 오차(cross entropy error, CEE), 미니배치, 수치 미분, 학습 알고리즘 구현하기/ 경사하강법"
category: [Deep Learning]
layout: post
tag:
- Deep Learning
order: 0

comments: true
---


# 신경망 학습 목차

- [INTRO](#intro)
- [데이터 주도 학습](#데이터-주도-학습)
  * [훈련 데이터와 시험 데이터](#훈련-데이터와-시험-데이터)
- [손실 함수(loss function)](#손실-함수(loss-function))
  * [오차 제곱 합(sum of squares for error, SSE)](#오차-제곱-합(sum-of-squares-for-error--sse))
  * [교차 엔트로피 오차(cross entropy error, CEE)](#교차-엔트로피-오차(cross-entropy-error--cee))
  * [미니배치 학습](#미니배치-학습)
    + [(배치용) 교차 엔트로피 오차 구현하기](#(배치용)-교차-엔트로피-오차-구현하기)
  * [손실함수 설정 이유](#손실함수-설정-이유)
- [수치 미분](#수치-미분)
  * [미분](#미분)
  * [구현시 주의](#구현시-주의)
  * [중심 차분 or 중앙 차분](#중심-차분-or-중앙-차분)
  * [편미분](#편미분)
  * [경사법(경사 하강법)](#경사법(경사-하강법))
    + [수식](#수식)
  * [신경망에서의 기울기](#신경망에서의-기울기)
- [학습 알고리즘 구현하기](#학습-알고리즘-구현하기)




---

# 신경망 학습

# INTRO

**손실 함수**: 신경망이 학습할 수 있도록 해주는 지표   

**목표** 
손실 함수의 결괏값을 **가장 작게** 만드는 **가중치 매개변수**를 찾는 것  
* 경사법 사용: 함수의 기울기를 활용



# 데이터 주도 학습

## 특징 feature   
입력 데이터(입력 이미지)에서 본질적인 데이터(중요한 데이터)를 정확하게 추출할 수 있도록 설계된 변환기   
Ex) 손글씨‘5’를 인식하기 위한 특징    

**[기술법]**
* ```벡터```: 이미지 특징
* ```SIFT, SURF, HOG```: 컴퓨터 비전 분야

**[과정]**
1) 이런 특징을 사용하여 이미지 데이터를 벡터로 변환(여전히 사람 설계)                                     
2) 변환된 벡터로 지도 학습 방식의 대표 분류 기법인 SVM, KNN 등으로 학습     
   * 이와 같은 기계학습에서는 모아진 데이터로부터 **규칙을 찾아내는 역할**을 **‘기계’**가 담당
 ![image](https://user-images.githubusercontent.com/76824611/117395510-065ec880-af33-11eb-9742-469579968b21.png)
 
 즉 딥러닝= 신경망 =  종단간 기계학습 end-to-end machine learning


## 훈련 데이터와 시험 데이터
1) 우선 **훈련 데이터**만 사용하여 학습하면서 **최적의 매개변수**를 찾음   
2) **시험 데이터**를 사용하여 앞서 훈련한 모델의 실력을 **평가**   

[훈련데이터와 시험데이터를 나누는 이유]       
**범용 능력**(아직 보지 못한 데이터(훈련 데이터에 포함되지 않는 데이터)로도 문제를 올바르게 풀어내는 능력) 을 제대로 평가하기 위해
 
+ **오버피팅 overfitting**(한데이터셋에만 지나치게 최적화된 상태)를 피해야 함


-----

# 손실 함수(loss function)
## 목표
신경망은 ‘하나의 지표(loss function)’를 기준으로 **최적의 매개변수** 값을 탐색    

## 정의
임의의 함수를 사용할 수도 있지만 일반적으로는 **오차제곱합**과 **교차 엔트로피 오차**를 사용
     
손실함수는 신경망 성능의 ‘나쁨’을 나타내는 지표     
* (-)를 곱해주면 ‘좋음’을 나타내는 지표     
* <나쁨을 최소로 하는 것-> 좋음을 최대로 하는 것>

## 오차 제곱 합(sum of squares for error, SSE)

![image](https://user-images.githubusercontent.com/76824611/117395832-b2081880-af33-11eb-8e6c-49097e40d2d5.png)

* **$$y_k$$**: 신경망의 출력(신경망이 추정한 값)   
* **$$t_k$$**: 정답 레이블    
* **$$k$$**: 데이터의 차원 수     

ex)     
```python
>>> y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0] 
>>> t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
```
**[해석]**
* **$$y$$**:  소프트맥스 함수의 출력 => 확률   
* 이미지가 ‘0’일 확률은 0.1, ‘1’일 확률은 0.05    
* 정답 레이블인 **$$t$$** => 정답    
* 답을 가리키는 위치의 원소는 1로, 그 외에는 0으로 처리  
* **원-핫 인코딩**: 한 원소만 1로 하고 그 외는 0으로 나타내는 표기법

**[구현]**
```python
def sum _squares _error(y, t):
  return 0.5 * np.sum((y-t)**2) 
```
값이 작으면 오차일 확률 낮음






## 교차 엔트로피 오차(cross entropy error, CEE)
![image](https://user-images.githubusercontent.com/76824611/117400995-ac640000-af3e-11eb-80c7-314bafd39798.png)

* **$$log$$**: 밑이 e인 자연로그($$log_e$$)  
* **$$y_k$$**: 신경망의 출력   
* **$$t_k$$**: 정답 레이블(원-핫 인코딩)   

실질적으로 t=1 (정답)일 때인 자연로그 계산 (정답일 때의 출력이 전체 값을 정함)  
  
![image](https://user-images.githubusercontent.com/76824611/117401257-3613cd80-af3f-11eb-9202-e98ae331f5ff.png)

* x가 1일 때 y는 0
* x가 0에 가까워질수록 y의 값은 점점 작아짐
* **정답**에 해당하는 출력이 커질수록 **0**에 다가감

 
[구현]
```python
def cross _entropy _error(y, t):
  delta = 1e-7
  return -np.sum(t * np.log(y + delta))
```
* y와 t는 넘파이 배열
* 코드 마지막 **+ delta** 이유
    * np.log ( ) 함수에 0을 입력하면 마이너스 
    * 무한대를 뜻하는 -inf가 되어 더 이상 계산을 진행할 수 없게 되기 때문
아주 작은 값을 더해서 절대 0이 되지 않도록, 즉 마이너스 무한대가 발생하지 않도록 한 것  

## 미니배치 학습
앞의 방법을 활용하면 **모든 훈련 데이터**를 대상으로 **손실 함수** 값을 구해야 함    
이제 훈련 데이터 모두에 대한 손실 함수의 합을 구하는 방법   

![image](https://user-images.githubusercontent.com/76824611/117402048-a3742e00-af40-11eb-9f47-e527caf5058c.png)

* **$$N$$**: 데이터의 개수   
* **$$t_nk$$**: n번째 데이터의 k번째 값   

**[해석]**       
데이터 하나에 대한 손실 함수인 위 식을 단순히 N개의 데이터로 확장  
마지막에 N으로 나누어 정규화(‘**평균** 손실 함수’를 구함): 훈련 데이터 개수와 관계없이 언제든 통일된 지표를 얻을 수 있음  

**[미니배치 mini-batch]**    
많은 데이터 모두 손실 함수를 계산 힘듦  
데이터 **일부**를 추려 전체의 **‘근사치’**로 이용 가능  
신경망 학습에서도 훈련 데이터로부터 일부만 골라학습을 수행 //이 일부를 뜻함    

훈련 데이터에서 지정한 수의 데이터를 무작위로 골라 내는 코드구현

[구현]
```python
import sys, os 
sys.path.append(os.pardir) 
import numpy as np 
from dataset.mnist import load _mnist

(x_train, t_train), (x_test, t_test) = load _mnist(normalize = True, one _hot _label = True)

print(x_train.shape) # (60000, 784) 
print(t_train.shape) # (60000, 10)

# np.random.choice (범위, 뽑아 낼 개수) 쓰자!!!

train _size = x_train.shape[0] 
batch _size = 10
batch _mask = np.random.choice(train _size, batch _size) 
x _batch = x_train[batch _mask] 
t _batch = t_train[batch _mask]

>>> np.random.choice(60000, 10) 
array([ 8013, 14666, 58210, 23832, 52091, 10153, 8107, 19410, 27260, 21411])
```







### (배치용) 교차 엔트로피 오차 구현하기

**1) 정답 레이블이 원-핫 인코딩**
```python
def cross_entropy_error(y, t):
if y.ndim = = 1:                    # 1차원 함수
  t = t.reshape(1, t.size) 
  y = y.reshape(1, y.size)

#행의 크기
batch _size = y.shape[0]  

#배치의 크기로 나눠 정규화하고 
#이미지 1장당 평균의 교차 엔트로피 오차를 계산
return -np.sum(t * np.log(y + 1e-7)) / batch _size
```

**2) 정답 레이블이 ‘2’나 ‘7’ 등의 숫자 레이블**
```python
def cross _entropy _error(y, t):
  if y.ndim = = 1: 
    t = t.reshape(1, t.size) 
    y = y.reshape(1, y.size)
batch _size = y.shape[0] 
return -np.sum(np.log(y[np.arange(batch _size), t] + 1e-7)) / batch_size
```
```y[np.arange(batch _size), t]```      
* 정답만 교차 엔트로피를 계산하기 위해서  
* (batch_size)-> 0부터 batch_size - 1까지 배열을 생성.    
  * 즉, batch_size가 5이면 np.arange (batch_size )는 [0, 1, 2, 3, 4 ]라는 넘파이 배열을 생성     
  * t에는 레이블이 [2, 7, 0, 9, 4 ]와 같이 저장되어 있으므로 y[np.arange (batch_size ), t ]는 각 데이터의 정답 레이블에 해당하는 신경망의 출력을 추출     
  * (이 예에서는 y[np.arange (batch_size ), t ]는 [y[0,2 ], y[1,7 ], y[2,0 ], y[3,9 ], y[4,4 ] ]인 넘파이 배열을 생성).  




## 손실함수 설정 이유
‘정확도’라는 지표를 놔두고 ‘손실 함수의 값’이라는 우회적인 방법을 택하는 이유-> **미분** 떄문  

**[신경망학습 최적의 매개변수(가중치와 편향)를 탐색과정]** 
**손실 함수**의 값을 가능한 한 **작**게 하는 **매개변수** 값을 찾음  
**1>** 매개변수의 미분(정확히는 기울 기)을 계산   
**2>** 그 미분 값을 단서로 매개변수의 값을 서서히 갱신하는 과정을 반복  

**[손실 함수의 미분]**       
‘가중치 매개변수의 값을 아주 조금 변화시켰을 때, 손실 함수가 어떻게 변하나’     
* **미분값** **양수**면 **매개변수를 음의값**으로 변환(반대도 =)    
* 0일 때 멈춤   

**[정확도를 지표로 삼지 않는 이유]**    
미분 값이 대부분의 장소에서 0이 되어 매개변수를 갱신할 수 없음     
매개변수의 미소한 변화에는 거의 반응을 보이지 않고, 반응이 있더라도 그 값이 불연속적으로 갑자기 변화      
불연속적임 100개중 32개가 정확하다면 정확도는 32%(손실 함수 0.92543...)     
계단함수처럼 한순간에서만 변화를 일으킴     
 

---

# 수치 미분
아주 작은 차분으로 미분하는 것
Cf) 해석적으로 미분(analytic): 오차를 포함하지 않는 ‘진정한 미분’ 값    
수학시간에 배우건 해석적 미분 **이의 근사치**를 구하는건 수치미분    

## 미분
x의 ‘작은 변화’가 함수 f (x )를 얼마나 변화시키느냐     
![image](https://user-images.githubusercontent.com/76824611/117404683-534b9a80-af45-11eb-9d4d-b321a2f88305.png)
좌변: f (x)의 x에 대한 미분(x에 대한 f (x )의 변화량)을 나타내는 기호

## 구현시 주의
* h를 너무 작은 값으로 설정하면 반올림 오차가 일어남(0이됨)  
  * **$$10^-4$$** 정도가 적당  

* 원래 미분    
  *  x 위치의 함수의 기울기(이를 접선이라 함)에 해당   
* 여기서_ (x + h )와 x 사이의 기울기   
  * 구현값 엄밀히 일치하지 않음(h를 무한히 0으로 좁히지 못하므로)   

* 수치미분엔 오차가 포함됨  
  -> 이는 전방 차분   
![image](https://user-images.githubusercontent.com/76824611/117405154-1a5ff580-af46-11eb-8ee6-ba8011681824.png)





## 중심 차분 or 중앙 차분
위의 오차를 줄이기 위해 (x + h )와 (x -h )일 때의 함수 f의 차분을 계산하는 방법사용    
x를 중심으로 그 전후의 차분을 계산한다는 의미   

[구현]
```python
def numerical_diff(f, x): 
h = 1e-4 # 0.0001
return (f(x+h) - f(x-h)) / (2*h)
```

[계산]
```python
# 아무 함수나 정의
def function_1(x):
      return 0.01*x**2 + 0.1*x

  # 이 함수 시각화
  import numpy as np 
  import matplotlib.pylab as plt

  x = np.arange(0.0, 20.0, 0.1)
  y = function_1(x) 

  plt.xlabel("x") 
  plt.ylabel("f(x)") 
  plt.plot(x, y) 
  plt.show()
 
  # 이 함수 미분값 확인
  numerical_diff(function_1, 5)
```

## 편미분
![image](https://user-images.githubusercontent.com/76824611/117405587-d4eff800-af46-11eb-8f0e-2216352a1cd9.png)
변수가 2개인 함수 미분    
변수가 2개인 함수 식 구현   

[구현]
```python
#미분이 아닌 변수 2개인 함수 단순 식 구현
def function _2(x): 
  return x[0]**2 + x[1]**2
  # 또는 return np.sum(x**2)
```
x는 넘파이 배열이라고 가정하여 시각화
![image](https://user-images.githubusercontent.com/76824611/117405756-15e80c80-af47-11eb-8396-960c2fe66ed3.png)

    
**[미분법]**
* 따로 함수를 정의하고 각각 미분해서 더하기    
* 한번에 $$x_0=3, x_1=4$$일 때 $$(x_0, x_1)$$ 양쪽 편미분을 묶어 계산
   * 기울기: 모든 변수의 편미분을 벡터로 정리한 것   
      ![image](https://user-images.githubusercontent.com/76824611/117406144-a0307080-af47-11eb-8a75-ae770a2e28e4.png)
 
[기울기 구현]
```python         
def numerical_gradient(f, x):
  h = 1e-4 # 0.0001
  grad = np.zeros_like(x) 
  # x 와 형상이 같고 그 원소가 모두 0인 배열

  for idx in range(x.size):
    tmp_val = x[idx] 

    # f(x+h) 계산
    x[idx] = tmp_val + h 
    fxh1 = f(x)


    # f(x-h) 계산
    x[idx] = tmp _val - h 
    fxh2 = f(x)

    grad[idx] = (fxh1 - fxh2) / (2*h) 
    x[idx] = tmp_val # 값 복원

    return grad

#미분값 넣어서 프린트 해보기
numerical _gradient(function _2, np.array([3.0, 4.0])) 

>>>array([ 6., 8.])
```
![image](https://user-images.githubusercontent.com/76824611/117406474-11702380-af48-11eb-9322-09ca4af00ba7.png)

* 위의 결과값은 방향을 가진 벡터(화살표)로 그려짐   
* 그림을 보면 기울기는 함수의 ‘가장 **낮은** 장소(최솟값)’를 가리키는 것 같음     
* 화살표가 한점을 향함        
 * ‘가장 낮은 곳’에서 멀어질수록 화살표의 크기가 커짐        

그런데 꼭 ‘가장 낮은 곳’만 찾는 것도 아님 = 기울기는 **각 지점 에서 낮아지는 방향**을 가리킴    
* 기울기가 가리키는 쪽은 각 장소에서 함수의 **출력 값을 가장 크게 줄이는 방향** (출력 값을 줄이면 정확도가 올라가죠?)    

## 경사법(경사 하강법)
**[INTRO]**    
일반적인 문제에서 손실함수를 찾을 때 매개변수 공간이 광대하여 **어디가 최솟값이 되는 곳인지를 짐작할 수 없음**       
이런 상황에서 기울기를 잘 이용해 **함수의 최솟값(또는 가능한 한 작은 값)**을 찾으려는 것이 경사법입니다.    
 
**[PROBLEM]**
함수가 극솟값, 최솟값, 또 안장점 **saddle point** 이 되는 장소에서는 기울기가 0-> 반드시 최솟값아님      
**plateau, 플래토**: 복잡하고 찌그러진 모양의 함수라면 (대부분) 평평한 곳으로 파고들면서 학습이 진행되지 않는 정체기에 빠질 수 있음    
**안장점**: 안장점은 어느 방향에서 보면 극댓값이고 다른 방향에서 보면 극솟값이 되는 점
![image](https://user-images.githubusercontent.com/76824611/117407045-f05c0280-af48-11eb-84b2-7f8eb9891613.png)


**[경사법 (gradient method)]**       
1) 현 위치에서 기울어진 방향으로 일정 거리만큼 이동.   
2) 이동한 곳에서도 마찬가지로 기울기를 구하고,    
3) 또 그 기울어진 방향으로 나아가기를 반복   
-> 이렇게 해서 함수의 값을 점차 줄이는 것이 경사법      

```경사 하강법 gradient descent method```: 최솟값을 찾을 때 [more learn](https://yerimoh.github.io/DL7/)    
```경사 상승법 gradient ascent method```: 최댓값을 찾을 때     

### 수식
![image](https://user-images.githubusercontent.com/76824611/117407330-5ba5d480-af49-11eb-8726-2251a807f246.png)
* **$$η$$**(= eta, 에타): 갱신하는 양을 나타냄. 학습률 **learning rate**이라고도 함   

* 1회갱신( 이를 반복해야하여 서서히 함숫값down)   
* 변수의 수가 늘어도 같은 식(각 변수의 편미분 값)으로 갱신   

[학습률]    
한 번의 학습으로 얼마만큼 **학습**해야 할지,      
매개변수 값을 얼마나 **갱신**하느냐를 정하는 것     
* 0.01이나 0.001 등 미리 특정 값으로 정해둬야 함      
* (너무 크거나 작으면 좋은장소 찾지 못함)         
이와 같은 매개변수를 하이퍼파라미터 **hyper parameter**, 초매개변수라고 함      
* < 자동으로 설정되는 가중치와 편향 같은 신경망 매개변수와 달리 **사람이 직접 설정**해야 하는 매개변수> 

[구현]
```python   
# 인수 f: 최적화하려는 함수
# init_x: 초깃값
# lr: learning rate를 의미하는 학습률
# step_ num은 경사법에 따른 반복 횟수
def gradient_descent(f, init_x, lr = 0.01, step _num = 100):
  x = init _x

  #미분값 조작
  for i in range(step _num):
    grad = numerical_gradient(f, x) 
    x - = lr * grad return x

#그림을 그려주는 소스코드 ch04/gradient_ method.py
```

## 신경망에서의 기울기
**기울기**   
가중치 매개변수(W)에 대한 손실 함수의 기울기(L)     
아래의 두 형상(행과 열의 크기)가 같아야 함    
![image](https://user-images.githubusercontent.com/76824611/117408092-64e37100-af4a-11eb-87e8-088bb80d4d5e.png)
* 경사는 2번째처럼 나타냄   
  * EX) 1행 1번째 원소: $$w_11$$을 조금 변경했을 때 손실 함수 L이 얼마나 변화하느냐를 나타냄


[구현(기울기 구하는 소스코드)]
```python 
import sys, os 
sys.path.append(os.pardir) 
import numpy as np
from common.functions import softmax, cross _entropy _error 
from common.gradient import numerical _gradient
  
#simpleNet 클래스는 형상이 2×3인 가중치 매개변수 하나를 인스턴스 변수로 갖음
# randn-> 가우시안 표준 정규분포에서 난수 매트릭스 어래이 생성
class simpleNet:
  def __init__(self):
    self.W = np.random.randn(2,3) # 정규분포로 초기화

  # 예측을 수행, 곱해!
  def predict(self, x):
    return np.dot(x, self.W)

  # 손실 함수의 값을 구함
  # x는 입력 데이터, t는 정답 레이블
  def loss(self, x, t):
    z = self.predict(x) 
    y = softmax(z) 
    loss = cross_entropy_error(y, t)

return loss
```


---


# 학습 알고리즘 구현하기
**[절차]**   
**0) 전제**   
* 신경망에는 적응 가능한 가중치와 편향이 있고, 이 가중치와 편향을 훈련 데이터에 적응하도록 조정하는 과정을 ‘학습’이라 함    
* 신경망 학습은 다음과 같이 4단계로 수행     
 
**1) 미니배치**     
* 훈련 데이터 중 **일부**를 무작위로 가져옴     
* 이렇게 선별한 데이터를 미니배치라 하며     
* 그 미니배치의 **손실 함수 값을 줄이는 것**이 목표     

**2) 기울기 산출**  
* 미니배치의 손실 함수 값을 줄이기 위해 각 가중치 매개변수의 **기울기**를 구함    
* 기울기는 손실 함수의 값을 **가장 작게** 하는 방향을 제시     

**3) 매개변수 갱신**    
* 가중치 매개변수를 기울기 방향으로 아주 조금 갱신     
* 경사하강법 사용(stochastic gradient descent, SGD)     

**4) 반복**   
* 1~3단계 반복    


---

# 이번 장에서 배운 내용
● 기계학습에서 사용하는 데이터셋은 훈련 데이터와 시험 데이터로 나눠 사용한다.  
● 훈련 데이터로 학습한 모델의 범용 능력을 시험 데이터로 평가한다.   
● 신경망 학습은 손실 함수를 지표로, 손실 함수의 값이 작아지는 방향으로 가중치 매개변수를 갱신한다.   
● 가중치 매개변수를 갱신할 때는 가중치 매개변수의 기울기를 이용하고, 기울어진 방향으로 가중치의 값을 갱신하는 작업을 반복한다.   
● 아주 작은 값을 주었을 때의 차분으로 미분하는 것을 수치 미분이라고 한다.  
● 수치 미분을 이용해 가중치 매개변수의 기울기를 구할 수 있다.   
● 수치 미분을 이용한 계산에는 시간이 걸리지만, 그 구현은 간단하다. 한편, 다음 장에서 구현하는 (다소 복잡한) 오차역전파법은 기울기를 고속으로 구할 수 있다     

 
