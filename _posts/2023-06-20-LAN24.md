---
title: "Mat2vec: Unsupervised word embeddings capture latent knowledge from materials science literature 정리"
date:   2023-06-20
excerpt: "Unsupervised word embeddings capture latent knowledge from materials science literature"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---



**[원 논문]**     
[Unsupervised word embeddings capture latent knowledge from materials science literature](https://www.nature.com/articles/s41586-019-1335-8)https://www.nature.com/articles/s41586-019-1335-8

-----


# ABSTRACT
**[문제]**      
* scientific knowledge은 거의 text로 출판됨     
➡ 전통적인 통계 분석이나 현대적인 기계 학습 방법으로 분석하기 어려움.      
* 기존 materials research와 관련된 machine-interpretable data는 structured property databases [1](https://www.cambridge.org/core/journals/mrs-bulletin/article/materials-science-with-largescale-data-and-informatics-unlocking-new-opportunities/BE294BD45E360E45C9FE0253E702D6B5) [2](https://www.nature.com/articles/s41586-018-0337-2)에서 왔음        
➡ small fraction of the knowledge만 알 수 있음       
* 출판물에는 small fraction 값 외에도 저자가 해석한 **데이터 항목 간의 연결** 및 **관계**에 대한 귀중한 지식이 포함되어 있음.      
* 과거에는 위 문제를 개선하기 위해, large **hand-labelled datasets** for training를 사용하여 과학 문헌에서 정보를 검색하는 데 초점을 맞춤     


**[본 연구]**    
* **방법**     
   * 본 논문은 출판된 문헌에 존재하는 materials science knowledge이 **Human labelling or supervision 없이** information-dense word embeddings(EX, GLOVE)으로 효율적으로 인코딩될 수 있음을 보여줌   
* **장점**    
   * 화학 지식을 명시적으로 삽입하지 않고, 이러한 임베딩은 주기율표의 기본 구조 및 재료의 구조-특성 관계와 같은 **복잡한 재료 과학 개념을 포착** 가능          
   * 또한, 우리는 unsupervised method이 발견되기 몇 년 전 functional 응용 프로그램에 대한 materials를 추천할 수 있음을 보여줌
  ➡ 이것은 **미래의 발견에 관한 잠재적인 지식**이 **과거의 출판물에 상당 부분 내재**되어 있음을 시사함
* 본 논문의 연구 결과는 거대한 과학 문헌에서 **집단적인 방식으로 지식과 관계를 추출할 수 있는 가능성을 강조**하고 **과학 문헌 채굴에 대한 일반화된 접근 방식을 지향**합니다.     





----

# Main
**구문 및 의미 관계를 보존**하기 위해, 텍스트 말뭉치의 단어에 **high-dimensional vectors (embeddings)를 할당**하는 것은 자연어 처리(NLP)의 가장 기본적인 기술 중 하나임(EX, GloVe, Word2vec)      
* 예를 들어, 적절한 텍스트 본문에 대해 훈련할 때, 그러한 방법은 '유기' 벡터보다 '강철' 벡터에 코사인 거리만큼 더 가까운 단어 '철'을 나타내는 벡터를 생성해야 함     


**[model train]**         
* **1) text corpus 생성**      
embeddings을 trian하기 위해, 1922년부터 2018년 사이에 발표된 **약 3.3 million개의 scientific abstracts을** materials-related research가 포함될 가능성이 높은 1,000개 이상의 저널에서 수집 및 처리하여 약 **50만 개의 단어가 생성**하였다.       
* **2) skip-gram variation of Word2vec 사용**    
위에서 만든 text corpus를 skip-gram variation of Word2vec에 넣어서 훈련시킴    
이 모델은 target word의 200차원 임베딩을 학습하기 위한 수단임     

![image](https://github.com/yerimoh/yerimoh.github.io/assets/76824611/fa41fbda-af10-4eb8-81fa-d23b1b4190f5)

Target words ‘LiCoO2’
and ‘LiMn2O4’ are represented as vectors with ones at their corresponding
vocabulary indices (for example, 5 and 8 in the schematic) and zeros
everywhere else (one-hot encoding). These one-hot encoded vectors are
used as inputs for a neural network with a single linear hidden layer (for
example, 200 neurons), which is trained to predict all words mentioned
within a certain distance (context words) from the given target word.
For similar battery cathode materials such as LiCoO2 and LiMn2O4, the
context words that occur in the text are mostly the same (for example,
‘cathodes’, ‘electrochemical’, and so on), which leads to similar hidden layer
weights after the training is complete. These hidden layer weights are the
actual word embeddings. The softmax function is used at the output layer
to normalize the probabilities.


**[main idea of model]**   
* 유사한 의미를 가진 단어가 유사한 맥락에서 나타나는 경우가 많기 때문에, 해당 embeddings도 유사할 것
* 모델에 대한 자세한 내용은 방법 및 보충 정보 섹션 S1 및 S2에 포함되어 있으며, 여기서 GloVe와 같은 대체 알고리즘 옵션에 대해서도 논의합니다. 우리는 알고리듬에 화학 정보나 해석이 추가되지 않더라도, 얻은 단어 임베딩이 다양한 벡터 연산(투영, 덧셈, 뺄셈)을 사용하여 결합될 때 화학적 직관과 일관되게 동작한다는 것을 발견했습니다. 예를 들어, 우리 말뭉치의 많은 단어들은 물질의 화학적 구성을 나타내며, LiCoO2(잘 알려진 리튬 이온 양극 화합물)와 가장 유사한 다섯 가지 물질은 정규화된 단어 임베딩의 도트 생성(투영)을 통해 결정될 수 있습니다. 우리의 모델에 따르면 LiCoO2와 가장 유사한 조성은 LiMn2O4, LiNi0입니다.5Mn1.5O4, LiNi0.8Co0.2O2, LiNi0.8Co0.15Al0.05O2 및 LiNiO2—모두 리튬 이온 양극 재료입니다.    

 We then applied the skip-gram variation of Word2vec, which is trained to predict context words that appear in the proximity of the target word as a means to learn the 200-dimensional embedding of that target word, to our text corpus (Fig. 1a). The key idea is that, because words with similar meanings often appear in similar contexts, the corresponding embeddings will also be similar. More details about the model are included in the Methods and in Supplementary Information sections S1 and S2, where we also discuss alternative algorithm options such as GloVe. We find that, even though no chemical information or interpretation is added to the algorithm, the obtained word embeddings behave consistently with chemical intuition when they are combined using various vector operations (projection, addition, subtraction). For example, many words in our corpus represent chemical compositions of materials, and the five materials most similar to LiCoO2 (a well-known lithium-ion cathode compound) can be determined through a dot product (projection) of normalized word embeddings. According to our model, the compositions with the highest similarity to LiCoO2 are LiMn2O4, LiNi0.5Mn1.5O4, LiNi0.8Co0.2O2, LiNi0.8Co0.15Al0.05O2 and LiNiO2—all of which are also lithium-ion cathode materials.


Similar to the observation made in the original Word2vec paper11, these embeddings also support analogies, which in our case can be domain-specific. For instance, ‘NiFe’ is to ‘ferromagnetic’ as ‘IrMn’ is to ‘?’, where the most appropriate response is ‘antiferromagnetic’. Such analogies are expressed and solved in the Word2vec model by finding the nearest word to the result of subtraction and addition operations between the embeddings. Hence, in our model,

ferromagnetic−NiFe+IrMn≈antiferromagnetic
To better visualize such embedded relationships, we projected the embeddings of Zr, Cr and Ni, as well as their corresponding oxides and crystal structures, onto two dimensions using principal component analysis (Fig. 1b). Even in reduced dimensions, there is a consistent operation in vector space for the concepts ‘oxide of’ (Zr − ZrO2 ≈ Cr − Cr2O3 ≈ Ni − NiO) and ‘structure of’ (Zr − HCP ≈ Cr − BCC ≈ Ni − FCC). This suggests that the positions of the embeddings in space encode materials science knowledge such as the fact that zirconium has a hexagonal close packed (HCP) crystal structure under standard conditions and that its principal oxide is ZrO2. Other types of materials analogies captured by the model, such as functional applications and crystal symmetries, are listed in Extended Data Table 1. The accuracies for each category are close to 50%—similar to the baseline set in the original Word2vec study12. We stress that Word2vec treats these entities simply as strings, and no chemical interpretation is explicitly provided to the model; rather, materials knowledge is captured through the positions of the words in scientific abstracts. Notably, we also found that embeddings of chemical elements are representative of their positions in the periodic table when projected onto two dimensions (Extended Data Fig. 1a, b, Supplementary Information sections S4 and S5) and can serve as effective feature vectors in quantitative machine learning models such as formation energy prediction—outperforming several previously reported curated feature vectors (Extended Data Fig. 1c, d, Supplementary Information section S6).

The main advantage and novelty of this representation, however, is that application keywords such as ‘thermoelectric’ have the same representation as material formulae such as ‘Bi2Te3’. When the cosine similarity of a material embedding and the embedding of ‘thermoelectric’ is high, one might expect that the text corpus necessarily includes abstracts reporting on the thermoelectric behaviour of this material14,15. However, we found that a number of materials that have relatively high cosine similarities to the word ‘thermoelectric’ never appeared explicitly in the same abstract with this word, or any other words that unequivocally identify materials as thermoelectric (Fig. 2a). Rather than dismissing these instances as spurious, we investigated whether such cases could be usefully interpreted as predictions of novel thermoelectric materials.

As a first test, we compared our predicted thermoelectric compositions with available computational data. Specifically, we identified compounds mentioned in our text corpus more than three times that are also present in a dataset16 that reports the thermoelectric power factors (an important component of the overall thermoelectric figure of merit, zT) of approximately 48,000 compounds calculated using density functional theory (DFT)17,18 (see Methods). A total of 9,483 compounds overlap between the two datasets, of which 7,663 were never mentioned alongside thermoelectric keywords in our text corpus and can be considered candidates for prediction. To obtain the predictions, we ranked each of these 7,663 compounds by the dot product of their normalized output embedding with the word embedding of ‘thermoelectric’ (see Supplementary Information sections S1 and S3 regarding the use of output versus word embeddings). This ranking can be interpreted as the likelihood that that material will co-occur with the word ‘thermoelectric’ in a scientific abstract despite this never occurring explicitly in the text corpus. The distributions of DFT maximum power factor values for all 9,483 materials (separated into known thermoelectrics and candidates) are plotted in Fig. 2b, and the values of the 10 highest ranked candidates from the word embedding approach are indicated with dashed lines. We find that the top ten predictions all exhibit computed power factors significantly greater than the average of candidate materials (green), and even slightly higher than the average of known thermoelectrics (purple). The average maximum power factor of 40.8 μW K−2 cm−1 for these top ten predictions is 3.6 times larger than the average of candidate materials (11.5 μW K−2 cm−1) and 2.4 times larger than the average of known thermoelectrics (17.0 μW K−2 cm−1). Moreover, the three highest power factors from the top ten predictions are at the 99.6th, 96.5th and 95.3rd percentiles of known thermoelectrics. We note that in contrast to supervised methods, our embeddings are based only on the text corpus and are not trained or modified in any manner using the DFT data.

Next, we compared the same model directly against experimentally measured power factors and zTs19. Because our approach does not provide numerical estimations of these quantities, we compared the relative ranking of candidates through the Spearman rank correlation20 for the 83 materials that appear both in our text corpus and the experimental set. We obtained a 59% and 52% rank correlation of experimental results with the embedding-based ranking for maximum power factor and maximum zT, respectively. Unexpectedly, our model outperformed the DFT dataset of power factors used in the previous paragraph, which exhibits only a 31% rank correlation with the experimental maximum power factors.

Finally, we tested whether our model—if trained at various points in the past—would have correctly predicted thermoelectric materials reported later in the literature. Specifically, we generated 18 different ‘historical’ text corpora consisting only of abstracts published before cutoff years between 2001 and 2018. We trained separate word embeddings for each historical dataset, and used these embeddings to predict the top 50 thermoelectrics that were likely to be reported in future (test) years. For every year past the date of prediction, we tabulated the cumulative percentage of predicted thermoelectric compositions that were reported in the literature alongside a thermoelectric keyword. Figure 3a depicts the result from each such ‘historical’ dataset as a thin grey line. For example, the light grey line labelled ‘2015’ depicts the percentage of the top 50 predictions made using the model trained only on scientific abstracts published before 1 January 2015, and that were subsequently reported in the literature alongside a thermoelectric keyword after one, two, three or four years (that is, the years 2015–2018). Overall, our results indicate that materials from the top 50 word embedding-based predictions (red line) were on average eight times more likely to have been studied as thermoelectrics within the next five years as compared to a randomly chosen unstudied material from our corpus at that time (blue) and also three times more likely than a random material with a non-zero DFT bandgap (green). The use of larger corpora that incorporate data from more recent years improved the rate of successful predictions, as indicated by the steeper gradients for later years in Fig. 3a.






