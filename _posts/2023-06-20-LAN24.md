---
title: "Mat2vec: Unsupervised word embeddings capture latent knowledge from materials science literature 정리"
date:   2023-06-20
excerpt: "Unsupervised word embeddings capture latent knowledge from materials science literature"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---



**[원 논문]**     
[Unsupervised word embeddings capture latent knowledge from materials science literature](https://www.nature.com/articles/s41586-019-1335-8)https://www.nature.com/articles/s41586-019-1335-8

-----


# ABSTRACT
**[문제]**      
* scientific knowledge은 거의 text로 출판됨     
➡ 전통적인 통계 분석이나 현대적인 기계 학습 방법으로 분석하기 어려움.      
* 기존 materials research와 관련된 machine-interpretable data는 structured property databases [1](https://www.cambridge.org/core/journals/mrs-bulletin/article/materials-science-with-largescale-data-and-informatics-unlocking-new-opportunities/BE294BD45E360E45C9FE0253E702D6B5) [2](https://www.nature.com/articles/s41586-018-0337-2)에서 왔음        
➡ small fraction of the knowledge만 알 수 있음       
* 출판물에는 small fraction 값 외에도 저자가 해석한 **데이터 항목 간의 연결** 및 **관계**에 대한 귀중한 지식이 포함되어 있음.      
* 과거에는 위 문제를 개선하기 위해, large **hand-labelled datasets** for training를 사용하여 과학 문헌에서 정보를 검색하는 데 초점을 맞춤     


**[본 연구]**    
* **방법**     
   * 본 논문은 출판된 문헌에 존재하는 materials science knowledge이 **Human labelling or supervision 없이** information-dense word embeddings(EX, GLOVE)으로 효율적으로 인코딩될 수 있음을 보여줌   
* **장점**    
   * 화학 지식을 명시적으로 삽입하지 않고, 이러한 임베딩은 주기율표의 기본 구조 및 재료의 구조-특성 관계와 같은 **복잡한 재료 과학 개념을 포착** 가능          
   * 또한, 우리는 unsupervised method이 발견되기 몇 년 전 functional 응용 프로그램에 대한 materials를 추천할 수 있음을 보여줌
  ➡ 이것은 **미래의 발견에 관한 잠재적인 지식**이 **과거의 출판물에 상당 부분 내재**되어 있음을 시사함
* 본 논문의 연구 결과는 거대한 과학 문헌에서 **집단적인 방식으로 지식과 관계를 추출할 수 있는 가능성을 강조**하고 **과학 문헌 채굴에 대한 일반화된 접근 방식을 지향**합니다.     





----

# Main
**구문 및 의미 관계를 보존**하기 위해, 텍스트 말뭉치의 단어에 **high-dimensional vectors (embeddings)를 할당**하는 것은 자연어 처리(NLP)의 가장 기본적인 기술 중 하나임(EX, GloVe, Word2vec)      
* 예를 들어, 적절한 텍스트 본문에 대해 훈련할 때, 그러한 방법은 '유기' 벡터보다 '강철' 벡터에 코사인 거리만큼 더 가까운 단어 '철'을 나타내는 벡터를 생성해야 함     

---

**[model train]**         
* **1) text corpus 생성**      
embeddings을 trian하기 위해, 1922년부터 2018년 사이에 발표된 **약 3.3 million개의 scientific abstracts을** materials-related research가 포함될 가능성이 높은 1,000개 이상의 저널에서 수집 및 처리하여 약 **50만 개의 단어가 생성**하였다.       
* **2) skip-gram variation of Word2vec 사용**    
위에서 만든 text corpus를 skip-gram variation of Word2vec에 넣어서 훈련시킴    
이 모델은 target word의 200차원 임베딩을 학습하기 위한 수단임     


----

**[model 작동 예시]**      
![image](https://github.com/yerimoh/yerimoh.github.io/assets/76824611/c0e4c66c-af53-4172-a5bb-2a234bd4e092)
* **1) input word embedding**     
Target words 'LiCoO2'와 'LiMn2O4'는 one-hot encoding으로 바꿈     
즉, 해당 어휘 색인에 있는 단어(예: 그림에서 가각 5와 8번째 만 1임)는 1, 그 외에는 모두 0으로 나타냄.     
* **2) model train with input enbedding**          
이러한 one-hot encoded vectors는 single linear hidden layer (for example, 200 neurons)을 가진 신경망의 입력으로 사용됨      
주어진 Target 단어에서 특정 거리 내에서 언급된 모든 단어**(context words)를 예측하도록 훈련**됨          
ex) LiCoO2와 LiMn2O4와 같은 유사한 배터리 양극 재료의 경우, 텍스트에서 발생하는 컨텍스트 워드(예: 'cathode', 'electrochemical' 등)는 대부분 동일함
* **3) after train**     
이 output을 바탕으로 train이 완료된 후 hidden layer weights 업데이트      
이러한 hidden layer weights는 실제 단어 embedding이다.     

-----


**[main idea of model]**   
* 유사한 의미를 가진 단어가 유사한 맥락에서 나타나는 경우가 많기 때문에, 해당 embeddings도 유사할 것     
* 모델에 대한 자세한 내용은 방법 및 SUPPLEMENTARY INFORMATION S1 및 S2에 포함되어 있음     
여기서 GloVe와 같은 대체 알고리즘 옵션에 대해서도 논의합니다.
* 우리는 **algorithm에 화학 정보나 해석이 추가되지 않더라도**, 얻은 word embeddings이 다양한 벡터 연산(projection, addition, subtractio)을 사용하여 결합될 때, **화학적 직관과 일관되게 동작**한다는 것을 발견했음.
   * 예를 들어, 우리 corpus의 많은 단어들은 materials의 chemical compositions을 나타내며, LiCoO2(잘 알려진 리튬 이온 양극 화합물)와 가장 유사한 다섯 가지 물질은 normalized word embeddings의 dot product (projection)을 통해 결정될 수 있음.          
   * 우리의 모델에 따르면 $$LiCoO_2$$와 가장 유사한 compositions은 $$LiMn_2O_4$$, $$LiNi_{0.5}Mn_{1.5}O_4$$, $$LiNi_{0.8}Co_{0.2}O_2$$, $$LiNi_{0.8}Co_{0.15}Al_{0.05}O_2&&, &&LiNiO_2$$이다. 이 다섯가지는 모두 리튬 이온 양극 재료이다.              
* **domain-specific analogies 지원**       
   * 원래 Word2vec paper11에서 관찰한 것과 유사하게, 이러한 임베딩은 **도메인별일 수 있는 유사성**도 지원합니다.   
   * 예를 들어, 'NiFe'는 '강자성'이고, 'IrMn'은 '?'이고, 여기서 가장 적절한 반응은 '반강자성'이다.   
   * 이러한 유사성은 임베딩 간의 subtraction 및 addition 연산 결과에 가장 가까운 단어를 찾아 Word2vec 모델에서 표현되고 해결됨      
<center> ferromagnetic(강자성) − NiFe + IrMn ≈ antiferromagnetic(반강자성)</center>    

-----

**[임베딩 시각화]**      
* **주성분 분석(principal component analysis)을 사용한 시각화**        
  * 이러한 임베디드 관계를 더 잘 시각화하기 위해 주성분 분석(principal component analysis)(그림 1b)을 사용하여 Zr, Cr 및 Ni의 임베딩과 해당 산화물 및 결정 구조를 2차원에 투영했다.         
  * 감소된 차원에서도 '산화물(oxide of)'(Zr - ZrO2 ≈ Cr - Cr2O3 ≈ Ni - NiO)과 '구조(structure of)'(Zr - HCP ≈ Cr - BCC ≈ Ni - FCC) 개념에 대한 벡터 공간에서의 일관된 연산이 있다.       
  * 이는 zirconium이 표준 조건에서 hexagonal close packed (HCP) 결정 구조를 가지고 있고 주요 산화물이 ZrO2라는 사실이 임베딩에 잘 반영됨.       
    ![image](https://github.com/yerimoh/img/assets/76824611/39242ad7-919d-4c9e-ae51-b9c053d30302)
* **Other types of materials analogies captured by the model**     
  * 각 범주의 정확도는 50%에 육박하며, 이는 원래 Word2vec 연구에서 설정한 기준선과 유사함.       
![image](https://github.com/yerimoh/img/assets/76824611/62f885a1-4fab-423b-a57d-9899070b3aae)  
* **단어의 위치를 통한 관계성 표현**        
  * 특히, 우리는 화학 원소의 임베딩이 2차원에 투영될 때, **주기율표**에서 그들의 위치를 대표한다는 것을 발견함     
  * SUPPLEMENTARY INFORMATION 섹션 S4 및 S5에서 자세히 설명       
  * 활성화 에너지 예측(formation energy prediction—outperforming)과 같은 quantitative machine learning models에서 effective feature vectors 역할을 할 수 있음             
  ➡ 이는 이전에 보고된 몇 가지 선별된 특징 벡터를 능가함(밑의 그림 1c, d, SUPPLEMENTARY INFORMATION S6).          
    ![image](https://github.com/yerimoh/img/assets/76824611/f8950c86-6e57-4682-8cea-8e9a38fc3980)
* **본 논문의 강조점**       
  * 우리는 Word2vec가 이러한 entities를 단순히 strings로 취급함     
  * 모델에 화학적 해석이 명시적으로 제공되지 않음      
  ➡ 오히려 scientific abstracts에서 **단어의 위치를 통해** materials knowledge이 포착된다             


+ Extended Data Fig. 1 | Chemistry is captured by word embeddings
  
-----
----

## thermoelectric관련 실험



**[현 논문의 main advantage and novelty]**       
* 'thermoelectric(열전성)'과 같은 응용 키워드가 '$$Bi_2Te_3$$'와 같은 material formulae과 동일한 표현을 가지고 있다는 것임.   

* material embedding과 'thermoelectric' 임베딩의 cosine similarity이 높을 때,
text corpus는 필수적으로 이 material의 thermoelectric behaviour에 대해 보고하는 abstracts을 포함한다고 예상할 수 있다.    


**[실험 결과 & 가설 설정]**
* 그러나 본 논문의 임베딩 결과, **'thermoelectric'라는 단어와 상대적으로 코사인 유사성이 높은 다수의 물질**(아래 그림 참고)이 발견되었는데,
이 물질이 포함된 **abstract에는 본 물질이 'thermoelectric'라는 것을 명시적으로 나타내지 않는다**.         
➨ 우리는 이러한 사례들을 가짜라고 치부하기보다는, <span style="background-color:#fff5b1">그러한 사례들이 새로운 '**thermoelectric' materials의 예측으로 유용하게 해석**될 수 있는지 조사</span>했다.



<img width="93" alt="image" src="https://github.com/yerimoh/yerimoh.github.io/assets/76824611/e4dcde14-fd0e-41dd-b814-1e0754c5520f">
* thermoelectric materials의 순위는 'thermoelectric'이라는 단어가 포함된 material 임베딩의 cosine similarities을 사용하여 생성될 수 있음.      
* 'thermoelectric' 응용을 위해 아직 연구되지 않은 높은 순위의 물질이 관련선이 있다고 가정할 수 있다.   
 


----

### first test

첫 번째 테스트로, 우리는 예측된 thermoelectric compositions을 available computational data와 비교했다.         


구체적으로 본 논문은 임베딩 결과 중 **아래의 과정을 통해 선정된 material을 검증**한 것이다.       
* 1️⃣ **예측 기준이 되는 데이터 셋과 비교**      
[density functional theory (DFT)(밀도함수 이론)](https://www.nature.com/articles/sdata201785)을 사용하여 계산된 약 48,000개의 compounds의 **thermoelectric power factors**(전체 thermoelectric 가치 수치(zT)의 중요한 구성 요소)**에 대한 데이터 세트**에 **3번 이상 언급된 물질** 선정        
➨ 그 결과, 두 데이터 세트 사이에 총 **9,483개의 화합물이 중복**됨 확인      
* 2️⃣ **예측 후보 선정**      
중복 물질 중,**7,663개**는 텍스트 말뭉치에서 **thermoelectric 키워드와 함께 언급된 적이 없으며, 예측 후보로 간주**될 수 있다.
* 3️⃣ **예측 후보 순위 설정**         
예측을 얻기 위해, 우리는 이 7,663개의 화합물 각각을 'thermoelectric'이라는 단어 임베딩으로 정규화된 output 임베딩의 dot product으로 순위를 매겼다.(출력 대 단어 임베딩 사용에 관한 보충 정보 섹션 S1 및 S3 참조).





3️⃣ 예측 후보 순위 설정 방법 자세히 보기    
<img width="295" alt="image" src="https://github.com/yerimoh/yerimoh.github.io/assets/76824611/1cbb52cf-643f-4fbf-b1d8-5f8112133640">


Word embeddings corresponds to using word embeddings both
for the application keyword and the material formula, whereas output embeddings corresponds to using the
output embedding of the formula and the word embedding of the application keyword. The definitions of
the scores are the same as in fig. S2a.




The ranking (and consequently predictions) are performed by multiplying the embedding
of the application keyword (e.g. “thermoelectric”) with the embeddings of all materials
(with some count threshold, more than 3 in our case). For the application keyword we
always use the normalized word embedding. However, for the materials we attempt to
use either the word or the output embedding (fig. S1b). If we use word embeddings, the
ranking is based on similarity of the application keyword and the material word. One can
think of this as their interchangeability in text. If instead we use the normalized output
embedding of the material, the predictions are based on the likelihood of the application
keyword and the material formula being mentioned next to each other, if all materials were
mentioned equal number of times in the text‡
. This second approach generally yields better
results as shown in fig. S3 and is used throughout this work.





이 순위는 텍스트 말뭉치에서 명시적으로 발생하지 않았음에도 불구하고 해당 물질이 과학적 추상체의 '열전성'이라는 단어와 공존할 가능성으로 해석될 수 있습니다. 그림 2b에는 9,483개의 모든 재료에 대한 DFT 최대 역률 값의 분포(알려진 열전기 및 후보로 분리됨)가 표시되어 있으며, 단어 임베딩 접근법에서 가장 높은 순위를 차지한 10개의 후보의 값은 점선으로 표시되어 있습니다. 우리는 상위 10개 예측 모두 후보 물질의 평균(녹색)보다 훨씬 크고 알려진 열전기의 평균(보라색)보다 약간 더 높은 계산된 역률을 보인다는 것을 발견했습니다. 이러한 상위 10개 예측에 대한 40.8μW K-2 cm-1의 평균 최대 역률은 후보 물질의 평균(11.5μW K-2 cm-1)보다 3.6배 크고 알려진 열전기의 평균(17.0μW K-2 cm-1)보다 2.4배 큽니다. 또한 상위 10개 예측에서 가장 높은 세 가지 검정력 요인은 알려진 열전기의 99.6번째, 96.5번째 및 95.3번째 백분위수입니다. 우리는 지도된 방법과 대조적으로, 우리의 임베딩은 텍스트 말뭉치만을 기반으로 하며 DFT 데이터를 사용하여 어떤 방식으로든 훈련되거나 수정되지 않는다는 것에 주목합니다.

To obtain the predictions, we ranked each of these 7,663 compounds by the dot product of their normalized output embedding with the word embedding of ‘thermoelectric’ (see Supplementary Information sections S1 and S3 regarding the use of output versus word embeddings). This ranking can be interpreted as the likelihood that that material will co-occur with the word ‘thermoelectric’ in a scientific abstract despite this never occurring explicitly in the text corpus. The distributions of DFT maximum power factor values for all 9,483 materials (separated into known thermoelectrics and candidates) are plotted in Fig. 2b, and the values of the 10 highest ranked candidates from the word embedding approach are indicated with dashed lines. We find that the top ten predictions all exhibit computed power factors significantly greater than the average of candidate materials (green), and even slightly higher than the average of known thermoelectrics (purple). The average maximum power factor of 40.8 μW K−2 cm−1 for these top ten predictions is 3.6 times larger than the average of candidate materials (11.5 μW K−2 cm−1) and 2.4 times larger than the average of known thermoelectrics (17.0 μW K−2 cm−1). Moreover, the three highest power factors from the top ten predictions are at the 99.6th, 96.5th and 95.3rd percentiles of known thermoelectrics. We note that in contrast to supervised methods, our embeddings are based only on the text corpus and are not trained or modified in any manner using the DFT data.



Next, we compared the same model directly against experimentally measured power factors and zTs19. Because our approach does not provide numerical estimations of these quantities, we compared the relative ranking of candidates through the Spearman rank correlation20 for the 83 materials that appear both in our text corpus and the experimental set. We obtained a 59% and 52% rank correlation of experimental results with the embedding-based ranking for maximum power factor and maximum zT, respectively. Unexpectedly, our model outperformed the DFT dataset of power factors used in the previous paragraph, which exhibits only a 31% rank correlation with the experimental maximum power factors.

Finally, we tested whether our model—if trained at various points in the past—would have correctly predicted thermoelectric materials reported later in the literature. Specifically, we generated 18 different ‘historical’ text corpora consisting only of abstracts published before cutoff years between 2001 and 2018. We trained separate word embeddings for each historical dataset, and used these embeddings to predict the top 50 thermoelectrics that were likely to be reported in future (test) years. For every year past the date of prediction, we tabulated the cumulative percentage of predicted thermoelectric compositions that were reported in the literature alongside a thermoelectric keyword. Figure 3a depicts the result from each such ‘historical’ dataset as a thin grey line. For example, the light grey line labelled ‘2015’ depicts the percentage of the top 50 predictions made using the model trained only on scientific abstracts published before 1 January 2015, and that were subsequently reported in the literature alongside a thermoelectric keyword after one, two, three or four years (that is, the years 2015–2018). Overall, our results indicate that materials from the top 50 word embedding-based predictions (red line) were on average eight times more likely to have been studied as thermoelectrics within the next five years as compared to a randomly chosen unstudied material from our corpus at that time (blue) and also three times more likely than a random material with a non-zero DFT bandgap (green). The use of larger corpora that incorporate data from more recent years improved the rate of successful predictions, as indicated by the steeper gradients for later years in Fig. 3a.



To examine these results in more detail, we focus on the fate of the
top five predictions determined using only abstracts published before
the year 2009. Figure 3b plots the evolution of the prediction rank of
these top five compounds as more abstracts are added in subsequent
years. One of these compounds, CuGaTe2, represents one of the best
present-day thermoelectrics and would have been predicted as a top
five compound four years before its publication in 201221. Two of the
other predictions, ReS2 and CdIn2Te4, were suggested in the literature
to be good thermoelectrics22,23 only approximately 8–9 years after
the point at which they would have first appeared in the top five list
from our algorithm. We note that the sharp increase in the rank of
layered ReS2 in 2015 coincides with the discovery of a record zT for
SnSe24—also a layered material. The final two predictions, HgZnTe
and SmInO3, contain expensive (Sm, In) or toxic (Hg) elements and
have not been studied yet, and SmInO3 has dropped appreciably in
ranking with the addition of more data. The top 10 predictions for
each year between 2001 and 2018 are available in Supplementary
Table S3.

To illustrate how materials never mentioned next to the word
‘thermoelectric’ are identified as thermoelectrics with high expected
probability, we investigated the series of connections that can lead to
a prediction. In Fig. 2c, we present three materials from our top five
predictions (Extended Data Table 2) alongside some of the key context
words that connect these materials to ‘thermoelectric’. For instance,
CsAgGa2Se4 has high likelihood of appearing next to ‘chalcogenide’,
‘band gap’, ‘optoelectronic’ and ‘photovoltaic applications’: many
good thermoelectrics are chalcogenides, the existence of a bandgap is
crucial for the majority of thermoelectrics, and there is a large overlap
between optoelectronic, photovoltaic and thermoelectric materials (see
Supplementary Information section S8). Consequently, the correlations
between these keywords and CsAgGa2Se4 led to the prediction. This
direct interpretability is a major advantage over many other machine
learning methods for materials discovery. We also note that several predictions were found to exhibit promising properties despite not being
in any well known thermoelectric material classes (see Supplementary
Information section S10). This demonstrates that word embeddings
go beyond trivial compositional or structural similarity and have the
potential to unlock latent knowledge not directly accessible to human
scientists.




As a final step, we verified the generalizability of our approach by
performing historical validation of predictions for three additional
keywords—‘photovoltaics’, ‘topological insulator’ and ‘ferroelectric’. We
emphasize that the word embeddings used for these predictions are the
same as those for thermoelectrics predictions; we have simply modified
the dot product to be with a different target word. Notably, with almost
no change in procedure, we find trends similar to the ones in Fig. 3a
for all three functional applications, with the results summarized in
Extended Data Fig. 2 and Extended Data Table 3.


The success of our unsupervised approach can partly be attributed
to the choice of the training corpus. The main purpose of abstracts is
to communicate information in a concise and straightforward manner,
avoiding unnecessary words that may increase noise in embeddings
during training. The importance of corpus selection is demonstrated
in Extended Data Table 4, where we show that discarding abstracts
unrelated to inorganic materials science improves performance, and
models trained on the set of all Wikipedia articles (about ten times
more text than our corpus) perform substantially worse on materials
science analogies. Contrary to what might seem like the conventional
machine learning mantra, throwing more data at the problem is not
always the solution. Instead, the quality and domain-specificity of the
corpus determine the utility of the embeddings for domain-specific
tasks



We suggest that the methodology described here can also be generalized to other language models, such that the probability of an entity
(such as a material or molecule) co-occurring with words that represent a target application or property can be treated as an indicator of
performance. Such language-based inference methods can become an
entirely new field of research at the intersection between natural language processing and science, going beyond simply extracting entities
and numerical values from text and leveraging the collective associations present in the research literature. Substitution of Word2vec with
context-aware embeddings such as BERT25 or ELMo26 could lead to
improvements for functional material predictions, as these models are
able to change the embedding of the word based on its context. They
substantially outperform context-independent embeddings such as
Word2vec or GloVe across all conventional NLP tasks. Also, in addition
to co-occurrences, these models can capture more complex relationships between words in the sentence, such as negation. In the current
study, the effects of negation are somewhat mitigated because scientific
abstracts often emphasize positive relationships. However, a natural
extension of this work is to parse the full texts of articles. We expect
the full texts will contain more negative relationships and in general
more variable and complex sentences, and will therefore require more
powerful methods.
Scientific progress relies on the efficient assimilation of existing
knowledge in order to choose the most promising way forward and to
minimize re-invention. As the amount of scientific literature grows, this
is becoming increasingly difficult, if not impossible, for an individual
scientist. We hope that this work will pave the way towards making the
vast amount of information found in scientific literature accessible to
individuals in ways that enable a new paradigm of machine-assisted
scientific breakthroughs.











