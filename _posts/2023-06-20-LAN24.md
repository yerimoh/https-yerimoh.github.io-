---
title: "Mat2vec: Unsupervised word embeddings capture latent knowledge from materials science literature 정리"
date:   2023-06-20
excerpt: "Unsupervised word embeddings capture latent knowledge from materials science literature"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---



**[원 논문]**     
[Unsupervised word embeddings capture latent knowledge from materials science literature](https://www.nature.com/articles/s41586-019-1335-8)https://www.nature.com/articles/s41586-019-1335-8

-----


# ABSTRACT
**[문제]**      
* scientific knowledge은 거의 text로 출판됨     
➡ 전통적인 통계 분석이나 현대적인 기계 학습 방법으로 분석하기 어려움.      
* 기존 materials research와 관련된 machine-interpretable data는 structured property databases [1](https://www.cambridge.org/core/journals/mrs-bulletin/article/materials-science-with-largescale-data-and-informatics-unlocking-new-opportunities/BE294BD45E360E45C9FE0253E702D6B5) [2](https://www.nature.com/articles/s41586-018-0337-2)에서 왔음        
➡ small fraction of the knowledge만 알 수 있음       
* 출판물에는 small fraction 값 외에도 저자가 해석한 **데이터 항목 간의 연결** 및 **관계**에 대한 귀중한 지식이 포함되어 있음.      
* 과거에는 위 문제를 개선하기 위해, large **hand-labelled datasets** for training를 사용하여 과학 문헌에서 정보를 검색하는 데 초점을 맞춤     


**[본 연구]**    
* **방법**     
   * 본 논문은 출판된 문헌에 존재하는 materials science knowledge이 **Human labelling or supervision 없이** information-dense word embeddings(EX, GLOVE)으로 효율적으로 인코딩될 수 있음을 보여줌   
* **장점**    
   * 화학 지식을 명시적으로 삽입하지 않고, 이러한 임베딩은 주기율표의 기본 구조 및 재료의 구조-특성 관계와 같은 **복잡한 재료 과학 개념을 포착** 가능          
   * 또한, 우리는 unsupervised method이 발견되기 몇 년 전 functional 응용 프로그램에 대한 materials를 추천할 수 있음을 보여줌
  ➡ 이것은 **미래의 발견에 관한 잠재적인 지식**이 **과거의 출판물에 상당 부분 내재**되어 있음을 시사함
* 본 논문의 연구 결과는 거대한 과학 문헌에서 **집단적인 방식으로 지식과 관계를 추출할 수 있는 가능성을 강조**하고 **과학 문헌 채굴에 대한 일반화된 접근 방식을 지향**합니다.     





----

# Main
**구문 및 의미 관계를 보존**하기 위해, 텍스트 말뭉치의 단어에 **high-dimensional vectors (embeddings)를 할당**하는 것은 자연어 처리(NLP)의 가장 기본적인 기술 중 하나임(EX, GloVe, Word2vec)      
* 예를 들어, 적절한 텍스트 본문에 대해 훈련할 때, 그러한 방법은 '유기' 벡터보다 '강철' 벡터에 코사인 거리만큼 더 가까운 단어 '철'을 나타내는 벡터를 생성해야 함     


**[model train]**         
* **1) text corpus 생성**      
embeddings을 trian하기 위해, 1922년부터 2018년 사이에 발표된 **약 3.3 million개의 scientific abstracts을** materials-related research가 포함될 가능성이 높은 1,000개 이상의 저널에서 수집 및 처리하여 약 **50만 개의 단어가 생성**하였다.       
* **2) skip-gram variation of Word2vec 사용**    
위에서 만든 text corpus를 skip-gram variation of Word2vec에 넣어서 훈련시킴    
이 모델은 target word의 200차원 임베딩을 학습하기 위한 수단임     

**[model 작동 예시]**      
![image](https://github.com/yerimoh/yerimoh.github.io/assets/76824611/c0e4c66c-af53-4172-a5bb-2a234bd4e092)
* **1) input word embedding**     
Target words 'LiCoO2'와 'LiMn2O4'는 one-hot encoding으로 바꿈     
즉, 해당 어휘 색인에 있는 단어(예: 그림에서 가각 5와 8번째 만 1임)는 1, 그 외에는 모두 0으로 나타냄.     
* **2) model train with input enbedding**          
이러한 one-hot encoded vectors는 single linear hidden layer (for example, 200 neurons)을 가진 신경망의 입력으로 사용됨      
주어진 Target 단어에서 특정 거리 내에서 언급된 모든 단어**(context words)를 예측하도록 훈련**됨          
ex) LiCoO2와 LiMn2O4와 같은 유사한 배터리 양극 재료의 경우, 텍스트에서 발생하는 컨텍스트 워드(예: 'cathode', 'electrochemical' 등)는 대부분 동일함
* **3) after train**     
이 output을 바탕으로 train이 완료된 후 hidden layer weights 업데이트      
이러한 hidden layer weights는 실제 단어 embedding이다.     




**[main idea of model]**   
* 유사한 의미를 가진 단어가 유사한 맥락에서 나타나는 경우가 많기 때문에, 해당 embeddings도 유사할 것     
* 모델에 대한 자세한 내용은 방법 및 SUPPLEMENTARY INFORMATION S1 및 S2에 포함되어 있음     
여기서 GloVe와 같은 대체 알고리즘 옵션에 대해서도 논의합니다.
* 우리는 **algorithm에 화학 정보나 해석이 추가되지 않더라도**, 얻은 word embeddings이 다양한 벡터 연산(projection, addition, subtractio)을 사용하여 결합될 때, **화학적 직관과 일관되게 동작**한다는 것을 발견했음.
   * 예를 들어, 우리 corpus의 많은 단어들은 materials의 chemical compositions을 나타내며, LiCoO2(잘 알려진 리튬 이온 양극 화합물)와 가장 유사한 다섯 가지 물질은 normalized word embeddings의 dot product (projection)을 통해 결정될 수 있음.          
   * 우리의 모델에 따르면 $$LiCoO_2$$와 가장 유사한 compositions은 $$LiMn_2O_4$$, $$LiNi_{0.5}Mn_{1.5}O_4, LiNi_{0.8}Co_{0.2}O_2, LiNi_{0.8}Co_{0.15}Al_{0.05}O_2&&, &&LiNiO_2$$이다. 이 다섯가지는 모두 리튬 이온 양극 재료이다.              
* **domain-specific analogies 지원**       
   * 원래 Word2vec paper11에서 관찰한 것과 유사하게, 이러한 임베딩은 **도메인별일 수 있는 유사성**도 지원합니다.   
   * 예를 들어, 'NiFe'는 '강자성'이고, 'IrMn'은 '?'이고, 여기서 가장 적절한 반응은 '반강자성'이다.   
   * 이러한 유사성은 임베딩 간의 subtraction 및 addition 연산 결과에 가장 가까운 단어를 찾아 Word2vec 모델에서 표현되고 해결됨      
<center> ferromagnetic(강자성) − NiFe + IrMn ≈ antiferromagnetic(반강자성)</center>    


**[임베딩 시각화]**      
* **주성분 분석(principal component analysis)을 사용한 시각화**        
  * 이러한 임베디드 관계를 더 잘 시각화하기 위해 주성분 분석(principal component analysis)(그림 1b)을 사용하여 Zr, Cr 및 Ni의 임베딩과 해당 산화물 및 결정 구조를 2차원에 투영했다.         
  * 감소된 차원에서도 '산화물(oxide of)'(Zr - ZrO2 ≈ Cr - Cr2O3 ≈ Ni - NiO)과 '구조(structure of)'(Zr - HCP ≈ Cr - BCC ≈ Ni - FCC) 개념에 대한 벡터 공간에서의 일관된 연산이 있다.       
  * 이는 zirconium이 표준 조건에서 hexagonal close packed (HCP) 결정 구조를 가지고 있고 주요 산화물이 ZrO2라는 사실이 임베딩에 잘 반영됨.       
    ![image](https://github.com/yerimoh/img/assets/76824611/39242ad7-919d-4c9e-ae51-b9c053d30302)
* **Other types of materials analogies captured by the model**     
  * 각 범주의 정확도는 50%에 육박하며, 이는 원래 Word2vec 연구에서 설정한 기준선과 유사함.     
![image](https://github.com/yerimoh/img/assets/76824611/62f885a1-4fab-423b-a57d-9899070b3aae)
* 특히, 우리는 화학 원소의 임베딩이 2차원에 투영될 때 주기율표에서 그들의 위치를 대표한다는 것을 발견했습니다(확장 데이터 그림 1a, b, 보충 정보 섹션 S4 및 S5) 및 활성화 에너지 예측과 같은 정량적 기계 학습 모델에서 효과적인 특징 벡터 역할을 할 수 있습니다. 이는 이전에 보고된 몇 가지 선별된 특징 벡터를 능가합니다(확장 데이터 그림 1c, d, 보충 정보 섹션 S6).
* **본 논문의 강조점**     
  * 우리는 Word2vec가 이러한 entities를 단순히 strings로 취급함
  * 모델에 화학적 해석이 명시적으로 제공되지 않음
  ➡ 오히려 scientific abstracts에서 **단어의 위치를 통해** materials knowledge이 포착된다         

야간
 We stress that Word2vec treats these entities simply as strings, and no chemical interpretation is explicitly provided to the model; rather, materials knowledge is captured through the positions of the words in scientific abstracts. Notably, we also found that embeddings of chemical elements are representative of their positions in the periodic table when projected onto two dimensions (Extended Data Fig. 1a, b, Supplementary Information sections S4 and S5) and can serve as effective feature vectors in quantitative machine learning models such as formation energy prediction—outperforming several previously reported curated feature vectors (Extended Data Fig. 1c, d, Supplementary Information section S6).





The main advantage and novelty of this representation, however, is that application keywords such as ‘thermoelectric’ have the same representation as material formulae such as ‘Bi2Te3’. When the cosine similarity of a material embedding and the embedding of ‘thermoelectric’ is high, one might expect that the text corpus necessarily includes abstracts reporting on the thermoelectric behaviour of this material14,15. However, we found that a number of materials that have relatively high cosine similarities to the word ‘thermoelectric’ never appeared explicitly in the same abstract with this word, or any other words that unequivocally identify materials as thermoelectric (Fig. 2a). Rather than dismissing these instances as spurious, we investigated whether such cases could be usefully interpreted as predictions of novel thermoelectric materials.

As a first test, we compared our predicted thermoelectric compositions with available computational data. Specifically, we identified compounds mentioned in our text corpus more than three times that are also present in a dataset16 that reports the thermoelectric power factors (an important component of the overall thermoelectric figure of merit, zT) of approximately 48,000 compounds calculated using density functional theory (DFT)17,18 (see Methods). A total of 9,483 compounds overlap between the two datasets, of which 7,663 were never mentioned alongside thermoelectric keywords in our text corpus and can be considered candidates for prediction. To obtain the predictions, we ranked each of these 7,663 compounds by the dot product of their normalized output embedding with the word embedding of ‘thermoelectric’ (see Supplementary Information sections S1 and S3 regarding the use of output versus word embeddings). This ranking can be interpreted as the likelihood that that material will co-occur with the word ‘thermoelectric’ in a scientific abstract despite this never occurring explicitly in the text corpus. The distributions of DFT maximum power factor values for all 9,483 materials (separated into known thermoelectrics and candidates) are plotted in Fig. 2b, and the values of the 10 highest ranked candidates from the word embedding approach are indicated with dashed lines. We find that the top ten predictions all exhibit computed power factors significantly greater than the average of candidate materials (green), and even slightly higher than the average of known thermoelectrics (purple). The average maximum power factor of 40.8 μW K−2 cm−1 for these top ten predictions is 3.6 times larger than the average of candidate materials (11.5 μW K−2 cm−1) and 2.4 times larger than the average of known thermoelectrics (17.0 μW K−2 cm−1). Moreover, the three highest power factors from the top ten predictions are at the 99.6th, 96.5th and 95.3rd percentiles of known thermoelectrics. We note that in contrast to supervised methods, our embeddings are based only on the text corpus and are not trained or modified in any manner using the DFT data.

Next, we compared the same model directly against experimentally measured power factors and zTs19. Because our approach does not provide numerical estimations of these quantities, we compared the relative ranking of candidates through the Spearman rank correlation20 for the 83 materials that appear both in our text corpus and the experimental set. We obtained a 59% and 52% rank correlation of experimental results with the embedding-based ranking for maximum power factor and maximum zT, respectively. Unexpectedly, our model outperformed the DFT dataset of power factors used in the previous paragraph, which exhibits only a 31% rank correlation with the experimental maximum power factors.

Finally, we tested whether our model—if trained at various points in the past—would have correctly predicted thermoelectric materials reported later in the literature. Specifically, we generated 18 different ‘historical’ text corpora consisting only of abstracts published before cutoff years between 2001 and 2018. We trained separate word embeddings for each historical dataset, and used these embeddings to predict the top 50 thermoelectrics that were likely to be reported in future (test) years. For every year past the date of prediction, we tabulated the cumulative percentage of predicted thermoelectric compositions that were reported in the literature alongside a thermoelectric keyword. Figure 3a depicts the result from each such ‘historical’ dataset as a thin grey line. For example, the light grey line labelled ‘2015’ depicts the percentage of the top 50 predictions made using the model trained only on scientific abstracts published before 1 January 2015, and that were subsequently reported in the literature alongside a thermoelectric keyword after one, two, three or four years (that is, the years 2015–2018). Overall, our results indicate that materials from the top 50 word embedding-based predictions (red line) were on average eight times more likely to have been studied as thermoelectrics within the next five years as compared to a randomly chosen unstudied material from our corpus at that time (blue) and also three times more likely than a random material with a non-zero DFT bandgap (green). The use of larger corpora that incorporate data from more recent years improved the rate of successful predictions, as indicated by the steeper gradients for later years in Fig. 3a.






