---
title: "Self-Attention with Relative Position Representations 정리"
date:   2023-02-08
excerpt: "Self-Attention with Relative Position Representations paper review"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---



# 목차 



**[INTRO]**      
NIPS에서 Google이 소개했던 Transformer는 NLP 학계에서 정말 큰 주목을 받음  
CNN 과 RNN 이 주를 이뤘던 연구들에서 벗어나 아예 새로운 모델을 제안했기 때문    

**[핵심]**     

**[읽기 위해 필요한 지식]**    
* [attention](https://yerimoh.github.io/DL19/)       
* [transformer](https://yerimoh.github.io/Lan/)    


**[원 논문]**    
[Self-Attention with Relative Position Representations](https://arxiv.org/pdf/1803.02155.pdf)  


----




# Relative embeddings의 필요성         
self-attention는 각 단어를 병렬으로 처리하기 때문에(순서 고려못함), 순서를 부여하는 것이 일반적이다.     
즉 I love you란 문장이 있으면, 아래와 같이 **한 encoder 안에서 따로 처리**되는 것을 확인할 수 있다.    
이렇게 단어 각각이 각각 임베딩되고 **병렬적으로 학습**되므로 **이 단어들의 순서가 무시**된다는 것이다.       
즉 아래 예시에서도 어짜피 순서없이 동시에(병렬로)학습되니 왼쪽과 오른쪽의 차이가 없다.     
⚠️ 즉, <span style="background-color:#fff5b1">**병렬처리로 인해 문장 내의 순서정보가 무시**되어 **이 순서 정보를 넣어주기위해 position embeddings을 추가**</span>하는 것이다.   
![image](https://user-images.githubusercontent.com/76824611/223513725-b980210f-1360-4dc9-b623-68228836ef86.png)

-----

## 기존 embeddings과의 차이점         

원래 Transformer는 sinusoidal [position signal](https://yerimoh.github.io/Lan/#positional-encoding)를 사용하거나 position embeddings을 학습했지만, 최근에는 relative position embeddings을 사용하는 것이 더 일반화되었다.     
* **기존 position embeddings:** 각 위치에 고정된 임베딩을 사용     
* **최근 relative position embeddings:** self-attention에서 비교되는 “key” 와 “query” 사이의 오프셋에 따라 다른 학습 임베딩을 생성한다.      

Relative position embedding은 **self-attention의 key와 query 사이의 거리**에 따라 **학습된 embedding**을 생성한다.   

------




## 기존 position embeddings의 한계     
그러나 절대 위치가 아닌 상대 위치에 관한 다른 종류의 데이터를 다루는 경우에는 어떻게 될까.   
생각해보면 **I가 항상 첫번째에 나오는 절대적 위치를 갖는건 아니다.**   
어느 문장에서는 3번째 어느문장에서는 마지막에 나올 수 있기 때문이다.  
이것이 절대적 위치로 임베딩하는 position embeddings의 한계이다.     


아래 그림의 예시를 보자.      
아래 그림과 같은 경우에는 특정노드가(예를 들어 you가) 첫번째라고 말하기엔 임의성이 다분하다.    
그래서 아래와 같은 경우 각 노드(네모)에 순서를 부여하는 과정에선 많은 문제가 생길 수 있다.        
 ![image](https://user-images.githubusercontent.com/76824611/223515775-c59db6cd-1305-4002-95ad-1869c8e11d9b.png)    
 
그래서 **절대 순서를 없애고** 그래프 또는 **시퀀스의 요소 간 거리 관한 상대적 위치**로 인코딩을 하는 것이다.   

-----
-----

# Relative embeddings 만드는 법

1️⃣ **시퀀스에 있는 토큰 수 만큼 많은 위치 임베딩 만들기**    
상대 임베딩의 경우는 각 토큰에 고유한 위치 임베딩이 있는 기존 position embedding과는 다르다.   
상대적인 표현을 사용하면 각 단어 또는 토큰에 하나의 위치 임베딩만 있는 것이 아니라 **토큰 간의 관계를 설명하기 위해 시퀀스에 있는 토큰 수 만큼 많은 위치 임베딩을 만든다**.      
만드는 방법은 아래 과정과 같다.   
<center><iframe src="https://www.slideshare.net/slideshow/embed_code/key/oRhvMuYOW21nWC?hostedIn=slideshare&page=upload" width="476" height="400" frameborder="0" marginwidth="0" marginheight="0" scrolling="no"></iframe></center>

---

2️⃣ **각 토큰간의 관계 설명**    
즉 위와 같이 표현함에 따라 각 토큰이 다른 토큰에 대한 위치 관계를 인코딩하게 된다.    
즉 각 토큰($$w_n$$)은 **자기 토큰과 다른 토큰과의 위치정보를(거리 정보를)담아 두 토큰 간의 관계를 설명**한다.    
즉 $$x_4$$의 예시를 들면, 아래와 같다.    
➡ 각 position token($$w_n$$)의 n의 기준은 자신과 자신의 위치정보 $$w_0$$을 기준으로 오른쪽으로 이동하면 $$w_1,w_2$$,   
➡ 왼쪽으로 이동하면 $$w_{-1}, w_{-2}$$가 된다.   
<img width="341" alt="image" src="https://user-images.githubusercontent.com/76824611/235449580-fce56b3d-24c5-4981-ba30-148c00637a54.png">
  

이러한 벡터는 한 토큰과 다른 토큰 사이의 관계에서 비롯되므로 이를 위해 아래와 같은 표기법을 사용할 수 있다.      
<img width="341" alt="image" src="https://user-images.githubusercontent.com/76824611/235449888-64c94ba9-00b2-4ec5-aa20-da924704779e.png">

---

3️⃣ **cliping**     
논문에선 **k에서 cliping을 실험**한다.     
즉 **특정 거리 후**에 위치 임베딩이 **동일한 값**을 얻는다.    
즉 k=2인 경우 $$w_3,w_4$$는 모두 $$w_2$$의 값을 갖는다.     
![image](https://user-images.githubusercontent.com/76824611/223534198-bccc5acc-a730-4469-8cf2-e4a6777b43e1.png)

즉, 그림으로 예를 들자면, [2]에서 생성된 가중치는 아래와 같이 표현할 수 있다.   
<img width="344" alt="image" src="https://user-images.githubusercontent.com/76824611/235450598-63e7a58c-ac77-4dcd-afa0-5fb9c604b195.png">

이를 k=2인경우로 cliping하면 아래와 같이 가중치가 일반화된다.    
(이해가 되지 않는다면 변한 가중치들을 위의 cliping식에 넣어보면 된다)      
<img width="333" alt="image" src="https://user-images.githubusercontent.com/76824611/235450829-e8f7c8dd-3ee4-4099-a070-f367e4a5ff28.png">

----

4️⃣ 
이제 각 토큰에 대해 시퀀스 토큰이 있는만큼 많은 위치 임베딩이 붙어있다.
자 그런데 $$x_1$$을 표현하기위해 이 토큰($$w_1,w_2,w_3,w_4,w_5$$)들을 다 더해버리면 위치정보가 이상해진다.     

그러므로 이 위치정보를 유지하면서도 토큰을 표현하기위해 self attention을 사용한다.     

이렇게 self attention을 이용하는 식은 아래와 같다.   
![image](https://user-images.githubusercontent.com/76824611/223534400-1685ee37-d95d-48e5-a5d9-cdc6db658dda.png)


* 1) $$x_jW^v$$ 각 토큰 벡터는 먼저 선형변환으로 변환된다.     
* 2) 각 토큰에 대한 새로운 표현 $$z_i$$는 모든 토큰에 대한 가중치의 합계이다.(가중치는 일종의 중요도 점수) 
➡ 따라서 **더 중요한 토큰엔  많은 가중치가 부여**된다.     
* 3) 그래서 여기서 새 표현이 계산되는 위치에 위치 정보를 $$a_{ij}^v$$를 더해줌으로써 추가할 수 있다.(이 과정에서 차원이 더 높아진다.)     
* 4) 이렇게 서녛ㅇ변환 후, 토큰은 고차원 공간에서 더 이동된다.    
이는 각 토큰 $$x_j$$가 하위공간으로 푸쉬되어,   
<span style="background-color:#FFE6E6">"**각 토큰은 의미론적 정보**를 가지고있지만 **어떤 차원에서의 내 위치**는 **다른 모든 두번째 이웃**과 **비슷**하기 때문에 **오른쪽에 있는 두 번째 이웃**이기도 한다"</span>를 나타낼 수 있다.      

이제 새로운 표현 **z는 상대위치에 대한 정보**를 받지만, **self attention의 가중치 계수는 그렇지 않다**.   
따라서 가중치 계수는 **중요도 점수가 위치 정보에 기반한 결정**을 내리도록 **self position에 대한 푸쉬($$a_{ik}^K$$)도 받는다**.   
![image](https://user-images.githubusercontent.com/76824611/223534443-95c8213e-8bbd-45d4-b10e-3f8973cb7e30.png)



그 결과 이제 모든 토큰에는 두가지 변형의 시퀀스에 있는 토큰만큼 많은 Relative Position 임베딩을 갖는다,   
* 첫 번쨰는 위에서 설명한 것과 같이 값에 대한 상대적인 임베딩이다.  
![image](https://user-images.githubusercontent.com/76824611/223534400-1685ee37-d95d-48e5-a5d9-cdc6db658dda.png)
* 두번째도 위에서 설명한 attention 가중치를 알리는 key에 대한 임베딩이다.   
![image](https://user-images.githubusercontent.com/76824611/223534443-95c8213e-8bbd-45d4-b10e-3f8973cb7e30.png)

위으 임베딩을 바탕으로 벡터의 정확한 값이 학습되며, 이는 모델이 스스로 최상의 balance가 무엇인지 파악할 수 있게한다.     

이 논문은 텍스트로만 실험했으며 이는 분명히 시퀀스이며, 기계번역에서 약간의 성능을 얻는따.

그러나 단어 체인은 단순한 그래프일 뿐이며 이를 넘어  일반적으로 요소간의 쌍 관계가 있는 모든 그래프 표현에 이 방법을 적용할 수 있다.   

이러한 상대적 표현은  우리가 시퀀스에 있는지 아니면 더 목잡한 그래프에 있는지에 관계없이 토큰이 서로 얼마나 떨어져있는 지에만 의존하기 때문이다.     

또다른 장점은 k에 대해 클리핑할 때 이 학습된 위치 정보가 모든 시퀀스의 길이로 일반화된다는 것이다.    

k에대해서 클리핑하면 장기종속성 정보가 제거되기때문에 조금 이상하다고 느낄 수 있다.    
그래서 우리는 무언가가 가까이 있거나 멀리 있다는 것 만 저장할 수 있지만 그것이 얼마나 떨어져있는지는 알 수 없다.   

그러나 이것은 소수의 반복으로 최상의 결과가 제공되는그래프 신경망과 유사하다.
많은 반복을 통해 더 멀리있는 정보가 관심 지점으로 확산될 수있지만 다중 혹은 멀리있는 noise를 포함 할 수 있으므로 항상 좋다고볼 수 ㄴ없다.    


따라서 상대적 위치표현에서 k에서의 클리핑은 대부분 로컬 종속성에 의존하는 작업을 해치치 않는 상대적 위치 이벤트 호라이즌과 약간 비슷하게 작용한다.     





