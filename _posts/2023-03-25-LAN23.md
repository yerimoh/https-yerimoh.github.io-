---
title: "Named Entity Recognition and Normalization Applied to LargeScale Information Extraction from the Materials Science Literature 정리"
date:   2023-03-25
excerpt: "Named Entity Recognition and Normalization Applied to LargeScale Information Extraction from the Materials Science Literature paper review"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---



**[원 논문]**     
[SCIBERT: A Pretrained Language Model for Scientific Text](https://arxiv.org/pdf/1903.10676.pdf)

-----


# ABSTRACT
**[문제]**  
materials science articles의 수는 지난 수십 년 동안 여러 배로 증가했음.    
➡ <span style="background-color:#FFE6E6">**new result를** 이전에 확립된 문헌과 **연결**하는 데 있어 materials discovery pipeline의 주요 **병목 현상**이 발생</span>한다.    

**[본 논문의 해결]**    
* 이 문제에 대한 잠재적인 해결책은,  
게시된 기사의 <span style="background-color:#fff5b1">**구조화되지 않은 원시 텍스트**를 **프로그래밍 쿼리를 허용하는 구조화된 데이터베이스 항목에 매핑**하는 것</span>임.      
* 이를 위해 materials science literature에서 <span style="background-color:#fff5b1">대규모 정보 추출을 위해 **named entity recognition(NER)이 있는 text mining**을 적용</span>함    
* NER 모델은 아래의 materials science document에서 아래의 **summary-level information을 추출**하도록 훈련되었다.     
    * inorganic material mentions    
    * sample descriptors     
    * phase labels       
    * material properties and applications        
    * any synthesis and characterization methods 
* 본 논문의 classifier는 87%의 정확도(f1)를 달성하고 327만 개의 materials science abstracts에서 정보를 추출하는 데 적용된다.      
* 본 논문은 8천만 개 이상의 <span style="background-color:#fff5b1">**materials-science-related named entities를 추출**하고, 각 **abstract의 내용은 구조화된 형식의 데이터베이스 항목으로 표현**</span>된다.       
* 본 논문은 단순한 데이터베이스 쿼리를 사용하여, "meta-questions"(까다로운 질문)에 답변할 수 있음을 보여준다.     


**[구현 링크]**     
* 모든 데이터와 기능은 [Github](http://matscholar.com) 및 [웹사이트](https://github.com/materialsintelligence/matscholar)에서 무료로 풀어둠    


-----

# 1. INTRODUCTION

현재, historical materials science 지식의 대부분은 비정형 텍스트임    
또한 data도 너무 많아 materials scientists들은 평생 동안 이 정보의 일부만 액세스 가능함.     


**[NER(Named Entity Recognition)]**     
* [NER](https://wikidocs.net/30682)은 신문 기사와 같은 비정형 텍스트에서 사람의 이름과 지리적 위치와 같은 **정보를 추출**하기 위한 텍스트 마이닝 기술로 개발됨    
* 이러한 방식으로 문서는 **문서에 포함된 정보를 기반으로 구조화된 형식으로 표현**가능      



**[entity normalization]**         
* 의미: process에서 각 entity를 **고유한 데이터베이스 식별자에 매핑**하는 것         
예를 들어,         
<center> “age hardening” </center>     
<center> “precipitation hardening” </center>      
위의 두 문장은 같은 의미임. 이렇게 다른 표현들을 같은 식별자에 매핑하는 것                 
* ⚠️ 이러한 동등성을 인식하도록 기계를 훈련시키는 것은 매우 어려움        
* ⚠️ materials science domain에 사용할 수 있는 resources가 없으며 entity normalization는 아직 보고되지 않음      




![image](https://user-images.githubusercontent.com/76824611/227835972-3e121bc0-5424-4540-b98d-bbf40de0381e.png)




재료 과학 텍스트에서 요약 수준의 정보를 추출하려는 대규모 노력은 없었다.    
그러나 게시된 문서를 구조화된 데이터베이스 항목으로 표시한다면, Materials informatics researchers의 아래의 노력을 줄일 수 있다.      
* 수백 또는 수천 개의 materials에 대한 예측(관련논문 [1](https://journals.aps.org/prb/abstract/10.1103/PhysRevB.89.094104), [2](https://arxiv.org/pdf/1706.00192.pdf))               
* 출판된 문헌에 대한 large-scale questions       


**[본 논문]**      
* materials science 문헌에서 대규모 정보 추출을 위해 **entity normalization**와 함께 **NER을 적용**.      
* 327만 개 이상의  science journal articles에 정보 추출      
article의 abstract부분만 사용(가장 요약 되어있기 때문)      
* <span style="background-color:#fff5b1">**NER 모델**</span>     
    * 800개의 hand-annotated abstracts를 사용하여 훈련된 신경망임(overall f1 score of 87%)          
* <span style="background-color:#fff5b1">**Entity normalization**</span>     
    * **두 entities가 동의어인지 여부를 인식하는 방법을 학습**하는 supervised ML model사용 (f1 점수가 95%)          
    * Entity normalization는 **문서 쿼리에서 식별된 관련 항목의 수를 크게 증가**시킨다는 것을 발견했다.            
    * 각 **abstract**의 비정형 텍스트는 문서에 대한 **summary-level information**하는 **구조화된 데이터베이스 항목으로 변환**된다.            
* 위 두 방법으로 **large-scale information extraction**한 결과,       
researchers이 **이전에는 불가능했던 규모**로 출판된 문헌에 **액세스**하고 **활용**할 수 있는 방법을 보여준다.      
* 또한 다음 <span style="background-color:#fff5b1">**데이터 세트**를 공개</span>함    
    * **(i)** NER에 대한 교육 데이터로 사용할 [800 hand-annotated materials science abstracts](https://doi.org/10.6084/m9.figshare.8184428)          
    * **(ii)** named entities를 normalized 형태로 매핑하기 위한 세부 정보가 포함된 [JSON 파일](https://doi.org/10.6084/m9.figshare.8184365)             
    * **(iii)** extracted named entities, 3.2700만 개의 science articles에 대한 [corresponding digital object identifier (DOI)](https://doi.org/10.6084/m9.figshare.8184413)    
 * section 5에 요약된 것처럼 데이터와 훈련된 모델과 인터페이스하기 위한 공개 웹사이트와 API를 공개함     




---
---


# 2. METHODOLOGY
Our full information-extraction pipeline is shown in Figure 1.
Detailed information about each step in the pipeline is
presented below, along with an analysis of extracted results.
Machine learning models described below are trained using the
Scikit-learn,23 Tensorflow,24 and Keras25 python libraries



## 2.1. Data Collection and Preprocessing. 

### 2.1.1. Document Collection. This work focuses on text mining the
abstracts of materials science articles. Our aim is to collect a
majority of all English-language abstracts for materials-focused
articles published between the years 1900 and 2018. To do
this, we create a list of over 1100 relevant journals indexed by
Elsevier’s Scopus and collect articles published in these
journals that fit these criteria via the Scopus and ScienceDirect
APIs,27 the SpringerNature API,28 and web scraping for
journals published by the Royal Society of Chemistry29 and the
Electrochemical Society.30 The abstracts of these articles (and
associated metadata including title, authors, publication year,
journal, keywords, DOI, and url) are then each assigned a
unique ID and stored as individual documents in a dual
MongoDB/ElasticSearch database. Overall, our corpus contains more than 3.27 million abstracts.



### 2.1.2. Text Preprocessing. 
The first step in document
preprocessing is tokenization, which we performed using
ChemDataExtractor.12 This involves splitting the raw text into
sentences, followed by the splitting of each sentence into
individual tokens. Following tokenization, we developed
several rule-based preprocessing steps. Our preprocessing
scheme is outlined in detail in the Supporting Information
(S.1); here, we provide a brief overview. Tokens that are
identified as valid chemical formulas are normalized, such that
the order of elements and common multipliers do not matter
(e.g., NiFe is the same as Fe50Ni50); this is achieved using
regular expression and rule based techniques. Valence states of
elements are split into separate tokens (e.g., Fe(III) becomes
two separate tokens, Fe and (III)). Additionally, if a token is
neither a chemical formula nor an element symbol, and if only
the first letter is uppercase, we lowercase the word. This way
chemical formulas as well as abbreviations stay in their
common form, whereas words at the beginning of sentences as
well as proper nouns are converted to lowercase. Numbers
with units are often not tokenized correctly with ChemDataExtractor. We address this in the processing step by
splitting the common units from numbers and converting all
numbers to a special token ```<nUm>```. This reduces the
vocabulary size by several tens of thousand words.
   
   
   
### 2.1.3. Document Selection. 
   For this work, we focus on
inorganic materials science papers. However, our materials
science corpus contains some articles that fall outside the scope
of the present work; for example, we consider articles on
polymer science or biomaterials to not be relevant. In general,
we only consider an abstract to be of interest (i.e., useful for
training and testing our NER algorithm) if the abstract
mentions at least one inorganic material along with at least one
method for the synthesis or characterization of that material.
For this reason, before training an NER model, we first train a
classifier for document selection. The model is a binary
classifier capable of labeling abstracts as “relevant” or “not
relevant”. For training data, we label 1094 randomly selected
abstracts as “relevant” or “not relevant”; of these, 588 are
labeled as “relevant”, and 494 are labeled “not relevant”.
Details and specific rules used for relevance labeling are
included in the Supporting Information (S.3). The labeled
abstracts are used to train a classifier; we use a linear classifier
based on logistic regression,31 where each document is
described by a term frequency−inverse document frequency
(tf−idf) vector. The classifier achieves an accuracy (f1) score   
of 89% based on 5-fold cross-validation. Only documents
predicted to be relevant are considered as training/testing data
in the NER study; however, we perform NER over the full 3.27
million abstracts regardless of relevance. We are currently
developing text-mining tools that are optimized for a greater
scope of topics (e.g., the polymer literature).



## 2.2. Named Entity Recognition. Using NER, we are
interested in extracting specific entity types that can be used to
summarize a document. To date, there have been several
efforts to define an ontology or schema for information
representation in materials science (see ref 32 for a review of
these efforts); in the present work, for each document we wish
to know what was studied, and how it was studied. To extract
this information, we design seven entity labels: inorganic
material (MAT), symmetry/phase label (SPL), sample
descriptor (DSC), material property (PRO), material
application (APL), synthesis method (SMT), and characterization method (CMT). The selection of these labels is
somewhat motivated by the well-known materials science
tetrahedron: “processing”, “structure”, “properties”, and
“performance”. Examples for each of these tags are given in
the Supporting Information (S.4), along with a detailed
explanation regarding the rules for annotating each of these
tags
   
   
   
   
   Using the tagging scheme described above, 800 materials
science abstracts are annotated by hand; only abstracts that are
deemed to be relevant based on the relevance classifier
described earlier are annotated. We choose abstracts to
annotate by randomly sampling from relevant abstracts so as
to ensure that a diverse range of inorganic materials science
topics are covered. The annotations are performed by a single
materials scientist. We stress that there is no necessarily
“correct” way to annotate these abstracts; however, to ensure
that the labeling scheme is reasonable, a second materials
scientist annotated a subset of 25 abstracts to assess the
interannotator agreement, which was 87.4%. This was
calculated as the percentage of tokens for which the two
annotators assigned the same label.



For annotation, we use the inside−outside−beginning
(IOB) format.33 This is necessary to account for multiword
entities, such as “thin film”. In this approach, there are special
tags representing a token at the beginning (B), inside (I), or
outside (O) of an entity. For example, the text fragment “Thin
films of SrTiO3 were deposited” would be labeled as (token;
IOB-tag) pairs in the following way: (Thin; B-DSC), (films; IDSC), (of; O), (SrTiO3; B-MAT), (were; O), (deposited; O).
Before training a classifier, the 800 annotated abstracts are
split into training, development (validation), and test sets. The
development set is used for optimizing the hyperparameters of
the model, and the test set is used to assess the final accuracy
of the model on new data. We use an 80%−10%−10% split,
such that there are 640, 80, and 80 abstracts in the training,
development, and test sets, respectively
   
   
   
## 2.3. Neural Network model. The neural network
architecture for our model is based on that of Lample et
al.;34 a schematic of this architecture is shown in Figure 2a. We
explain the key features of the model below.   



The aim is to train a model in such a way that materials
science knowledge is encoded; for example, we wish to teach a
computer that the words “alumina” and “SrTiO3” represent
materials, whereas “sputtering” and “MOCVD” correspond to
synthesis methods. There are three main types of information
that can be used to teach a machine to recognize which words
correspond to a specific entity type: (i) word representation,
(ii) local (within sentence) context, and (iii) word shape.



For (i), word representation, we use word embeddings.
Word embeddings map each word onto a dense vector of real
numbers in a high-dimensional space. Words that have a
similar meaning, or are frequently used in a similar context, will
have a similar word embedding. For example, entities such as
“sputtering” and “MOCVD” will have similar vector
representations; during training, the model learns to associate
these word vectors as synthesis methods. The word
embeddings are generated using the Word2vec approach of
Mikolov et al.;26 the embeddings are 200-dimensional and are
based on the skip-gram approach. Word embeddings are
generated by training on our corpus of 3.27 million materials
science abstracts. More information about the training of word
embeddings is included in the Supporting Information (S.2).


For (ii), context, the model considers a sentence as a
sequence of words, and it takes into account the local context
of each word in the sentence. For example, in the sentence
“The band gap of ___ is 4.5 eV”, it is quite clear that the
missing word is a material, rather than a synthesis method or
some other entity, and this is obvious from the local context
alone. To include such contextual information, we use a
recurrent neural network (RNN), a type of sequence model
that is capable of sequence-to-sequence (many-to-many)
classification. As traditional RNNs suffer from problems in
dealing with long-range dependencies, we use a variant of the
RNN called long short-term memory (LSTM).35 In order to
capture both forward and backward context, we use a
bidirectional LSTM; in this way, one LSTM reads the sentence
forward and the other reads it backward, with the results being
combined.



For (iii), word shape, we include character-level information
about each word. For example, material formulas like “SrTiO3”
have a distinct shape, containing uppercase, lowercase, and
numerical characters, in a specific order; this word shape can
be used to help in entity classification. Similarly, prefixes and
suffixes provide useful information about entity type; for
example, the suffix “ium”, for example in “strontium”, is
commonly used for elemental metals, so a word that has this
suffix has a good chance of being part of a material name. In
order to encode this information into the model, we use a
character-level bidirectional LSTM over each word [Figure
2b]. The final outputs from the character-level LSTM (100
dimensional vectors) are concatenated with the word
embeddings for each word; these final vectors are used as
the word representations for the word-level LSTM [Figure 2a].



For the word-level LSTM, we use pretrained word
embeddings that have been trained on our corpus of over
3.27 million abstracts. For the character-level LSTM, the
character embeddings are not pretrained, but are learned
during the training of the model.




The output layer of the model is a conditional random fields
(CRF) classifier, rather than a typical softmax layer. Being a
sequence-level classifier, the CRF is better at capturing the
strong interdependencies of the output labels.36




The model has a number of hyperparameters, including the
word- and character-level LSTM size, the character embedding
size, the learning rate, and drop out. The NER performance
was optimized by repeatedly training the model with randomly
selected hyperparameters; the final model chosen was the one
with the highest accuracy when assessed on the development
set
   
   
## 2.4. Entity Normalization.    
After entity recognition, the
final step is entity normalization. This is necessarily required as
each entity may be written in numerous forms. For example,
“TiO2”, “titania”, “AO2 (A = Ti)”, and “titanium dioxide” all
refer to the same specific stoichiometry: TiO2. For document
querying, it is important to store these entities in a normalized
format, so that a query for documents that mention “titania”
also returns documents that mention “titanium dioxide”. In
order to normalize material mentions, we convert all material
names into a canonical normalized formula. The normalized
formula is alphabetized and divided by the highest common
factor of the stoichiometry. In this way, “TiO2”, “titania”, and
“titanium dioxide” are all normalized to “O2Ti”. In some cases,
multiple stoichiometries are extracted from a single material
mention; for example, “ZrxTi1−xO3 (x = 0, 0.5, 1)” is converted
to “O2Ti”, “O4TiZr”, and “O2Zr”. When a continuous range is
given, e.g, 0 ≥ x ≤ 1, we increment over this range in steps of
0.1. Material mentions are normalized using regular
expressions and rule-based methods, as well as by making
use of the PubChem lookup tables;37 final validity checks on
the normalized formula are performed using the pymatgen
library.38



Normalization of other entity types is also crucial for
comprehensive document querying. For example, “chemical
vapor deposition”, “chemical-vapour deposition”, and “CVD”
all refer to the same synthesis technique; i.e., they are
synonyms for this entity type. In order to determine that two
entities have the same meaning, we trained a classifier that is
capable of determining whether or not two entities are
synonyms.



The model uses the word embeddings for each entity as
features; after performing NER, each multiword entity is
concatenated into a single word, and new word embeddings
are trained such that every multiword entity has a single vector
representation. Each synonym consists of an entity pair, so the
features for the model are created by concatenating the word
embeddings of the two entities in question. In addition to the
word embeddings, which mostly capture the context in which
an entity is used, several other handcrafted features are
included (Supporting Information, S.5). To train the model,
10 000 entity pairs are labeled as being either synonyms or not
(see the Supporting Information, S.6). Using this data, a binary
random forest classifier is trained to be able to predict whether
or not two entities are synonyms of one another.



Using the synonym classifier, each extracted entity can be
normalized to a canonical form. Each entity is stored as its
most frequently occurring synonym (we exclude acronyms as a
normalized form); for example, “chemical vapor deposition”,
“chemical-vapour deposition”, and “CVD” are all stored as
“chemical vapor deposition”, as this is the most frequently
occurring synonym that is not an acronym.




