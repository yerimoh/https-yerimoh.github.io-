---
title: "Named Entity Recognition and Normalization Applied to LargeScale Information Extraction from the Materials Science Literature 정리"
date:   2023-03-25
excerpt: "Named Entity Recognition and Normalization Applied to LargeScale Information Extraction from the Materials Science Literature paper review"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---



**[원 논문]**     
[Named Entity Recognition and Normalization Applied to LargeScale Information Extraction from the Materials Science Literature](https://pubs.acs.org/doi/pdf/10.1021/acs.jcim.9b00470)



-----


# ABSTRACT
**[문제]**  
materials science articles의 수는 지난 수십 년 동안 여러 배로 증가했음.    
➡ <span style="background-color:#FFE6E6">**new result를** 이전에 확립된 문헌과 **연결**하는 데 있어 materials discovery pipeline의 주요 **병목 현상**이 발생</span>한다.    

**[본 논문의 해결]**    
* 이 문제에 대한 잠재적인 해결책은,  
게시된 기사의 <span style="background-color:#fff5b1">**구조화되지 않은 원시 텍스트**를 **프로그래밍 쿼리를 허용하는 구조화된 데이터베이스 항목에 매핑**하는 것</span>임.      
* 이를 위해 materials science literature에서 <span style="background-color:#fff5b1">대규모 정보 추출을 위해 **named entity recognition(NER)이 있는 text mining**을 적용</span>함    
* NER 모델은 아래의 materials science document에서 아래의 **summary-level information을 추출**하도록 훈련되었다.     
    * inorganic material mentions    
    * sample descriptors     
    * phase labels       
    * material properties and applications        
    * any synthesis and characterization methods 
* 본 논문의 classifier는 87%의 정확도(f1)를 달성하고 327만 개의 materials science abstracts에서 정보를 추출하는 데 적용된다.      
* 본 논문은 8천만 개 이상의 <span style="background-color:#fff5b1">**materials-science-related named entities를 추출**하고, 각 **abstract의 내용은 구조화된 형식의 데이터베이스 항목으로 표현**</span>된다.       
* 본 논문은 단순한 데이터베이스 쿼리를 사용하여, "meta-questions"(까다로운 질문)에 답변할 수 있음을 보여준다.     


**[구현 링크]**     
* 모든 데이터와 기능은 [Github](http://matscholar.com) 및 [웹사이트](https://github.com/materialsintelligence/matscholar)에서 무료로 풀어둠    


-----

# 1. INTRODUCTION

현재, historical materials science 지식의 대부분은 비정형 텍스트임    
또한 data도 너무 많아 materials scientists들은 평생 동안 이 정보의 일부만 액세스 가능함.     


**[NER(Named Entity Recognition)]**     
* [NER](https://wikidocs.net/30682)은 신문 기사와 같은 비정형 텍스트에서 사람의 이름과 지리적 위치와 같은 **정보를 추출**하기 위한 텍스트 마이닝 기술로 개발됨    
* 이러한 방식으로 문서는 **문서에 포함된 정보를 기반으로 구조화된 형식으로 표현**가능      



**[entity normalization]**         
* 의미: process에서 각 entity를 **고유한 데이터베이스 식별자에 매핑**하는 것         
예를 들어,         
<center> “age hardening” </center>     
<center> “precipitation hardening” </center>      
위의 두 문장은 같은 의미임. 이렇게 다른 표현들을 같은 식별자에 매핑하는 것                 
* ⚠️ 이러한 동등성을 인식하도록 기계를 훈련시키는 것은 매우 어려움        
* ⚠️ materials science domain에 사용할 수 있는 resources가 없으며 entity normalization는 아직 보고되지 않음      




![image](https://user-images.githubusercontent.com/76824611/227841594-3b3396fb-d7a2-470c-9d5d-5b8e1b54abc3.png)



재료 과학 텍스트에서 요약 수준의 정보를 추출하려는 대규모 노력은 없었다.    
그러나 게시된 문서를 구조화된 데이터베이스 항목으로 표시한다면, Materials informatics researchers의 아래의 노력을 줄일 수 있다.      
* 수백 또는 수천 개의 materials에 대한 예측(관련논문 [1](https://journals.aps.org/prb/abstract/10.1103/PhysRevB.89.094104), [2](https://arxiv.org/pdf/1706.00192.pdf))               
* 출판된 문헌에 대한 large-scale questions       


**[본 논문]**      
* materials science 문헌에서 대규모 정보 추출을 위해 **entity normalization**와 함께 **NER을 적용**.      
* 327만 개 이상의  science journal articles에 정보 추출      
article의 abstract부분만 사용(가장 요약 되어있기 때문)      
* <span style="background-color:#fff5b1">**NER 모델**</span>     
    * 800개의 hand-annotated abstracts를 사용하여 훈련된 신경망임(overall f1 score of 87%)          
* <span style="background-color:#fff5b1">**Entity normalization**</span>     
    * **두 entities가 동의어인지 여부를 인식하는 방법을 학습**하는 supervised ML model사용 (f1 점수가 95%)          
    * Entity normalization는 **문서 쿼리에서 식별된 관련 항목의 수를 크게 증가**시킨다는 것을 발견했다.            
    * 각 **abstract**의 비정형 텍스트는 문서에 대한 **summary-level information**하는 **구조화된 데이터베이스 항목으로 변환**된다.            
* 위 두 방법으로 **large-scale information extraction**한 결과,       
researchers이 **이전에는 불가능했던 규모**로 출판된 문헌에 **액세스**하고 **활용**할 수 있는 방법을 보여준다.      
* 또한 다음 <span style="background-color:#fff5b1">**데이터 세트**를 공개</span>함    
    * **(i)** NER에 대한 교육 데이터로 사용할 [800 hand-annotated materials science abstracts](https://doi.org/10.6084/m9.figshare.8184428)          
    * **(ii)** named entities를 normalized 형태로 매핑하기 위한 세부 정보가 포함된 [JSON 파일](https://doi.org/10.6084/m9.figshare.8184365)             
    * **(iii)** extracted named entities, 3.2700만 개의 science articles에 대한 [corresponding digital object identifier (DOI)](https://doi.org/10.6084/m9.figshare.8184413)    
 * section 5에 요약된 것처럼 데이터와 훈련된 모델과 인터페이스하기 위한 공개 웹사이트와 API를 공개함     




---
---


# 2. METHODOLOGY

아래 그림은 named entity recognition에 대한 전체 Workflow임    
![image](https://user-images.githubusercontent.com/76824611/227841719-66adeaec-e0ea-4b61-ac09-ba777ba3cfa7.png)
**[STEP]**    
* **(i)** documents가 수집되어 corpus에 추가됨      
* **(ii)** 텍스트 preprocessed (tokenized and cleaned)       
* **(iii)** training 데이터의 경우, documents의 small subset에 레이블이 지정됨     
(SPL = symmetry/phase 레이블, MAT = 재료, APL = 응용 프로그램)         
* **(iv)** labeled documents는 레이블링되지 않은 텍스트에서 생성된 word embeddings(Word2vec)와 결합된 후,    
named entity recognition을 위한 neural network를 train시킴     
* **(v)** neural network에서 entities 추출     



※ Scikit-learn, Tensorflow, Keras python 라이브러리를 사용하여 훈련됨


## 2.1. Data Collection and Preprocessing. 

**[2.2.1 Document Collection]**     
* **materials science articles**(1900년에서 2018년 사이에 출판)의 **abstracts부분**을 text mining함     
* Elsevier’s Scopus에 의해 indexe된 1100개 이상의 관련 저널 목록을 만듦, 저널은 수집 사이트는 아래와 같음      
   * Scopus and ScienceDirect APIs 27    
   * the SpringerNature API,28     
   * web scraping for journals published by the Royal Society of Chemistry29       
   * Electrochemical Society.30     
* these article의 abstracts(associated metadata including title, authors, publication year,
journal, keywords, DOI, and url)은 각 고유한 ID를 할당받고 이중 MongoDB/ElasticSearch 데이터베이스에 개별 문서로 저장됨    
* 전체적으로 our corpus는 3.27 million abstracts를 포함한다.      







**[2.2.2. Text Preprocessing]**     
문서 전처리의 단계 아래와 같다.     
* **1)** ChemDataExtractor를 사용하여 토큰화 12     
   * **1-1)** raw text를 문장으로 분할    
   * **1-2)** 각 문장을 개별 토큰으로 분할하는 것을 포함함.      
* **2)** rule-based preprocessing steps       
   * 자세한 건 [Supporting information for](https://pubs.acs.org/doi/suppl/10.1021/acs.jcim.9b00470/suppl_file/ci9b00470_si_001.pdf)에 있음       
   * valid chemical formula로 식별된 토큰은 원소와 order of elements가 중요하지 않도록 정규화됨     
   (예: NiFe는 Fe50Ni50과 동일)    
   * Valence states of elements는 별도의 토큰으로 분할된다.(예: Fe(III)는 두 개의 별도 토큰, Fe와 (III)가 된다).       
   * 토큰이 화학식이나 원소 기호가 아니며, 첫 번째 문자만 대문자인 경우에는 단어를 소문자로 만듦.      
   * 단위가 있는 숫자는 종종 ChemDataExtractor를 사용하여 토큰화되지 않는 문제 존재             
   ➡ 처리 단계에서 공통 단위를 숫자에서 분할하고 모든 숫자를 특수 토큰 ```<nUm>```으로 변환하여 이 문제를 해결한다.     
   ➡ 이렇게 하면 단어 크기가 수만 개 줄어듦     


   

**[2.1.3. Document Selection]**     
* inorganic materials science 논문에 중점을 둠.(연구범위 밖 논문도 포함하긴함_       
* abstract에 꼭 merterial에 대한 용어가 하나 이상 언급되어야지 관련 논문이라고 간주함    
➡ 이러한 분류를 위해 NER 모델을 교육하기 전에 먼저 문서 선택을 위한 classifier를 교육함.    
* 논문 분류를 위한 classifier     
    * 이 모델은 abstract을 "관련" 또는 "관련 없음"으로 분류할 수 있는 이진 classifier다.     
    * train 데이터의 경우 무작위로 선택된 1094개의 abstract에 "관련" 또는 "관련 없음"이라는 레이블을 붙d인다.   
    * 레이블이 지정된 abstract는 classifier를 훈련하는 데 사용된다.      
    * 우리는  logistic regression에 기반한 linear classifier를 사용한   
    * 여기서 각 문서는  term frequency−inverse document frequency([tf−idf](https://wikidocs.net/31698)) 벡터로 설명된다.       
    * classifier가 5-fold cross-validation에서 89%accuracy($$f_1$$) 점수를 획득다.      
* 이렇게 구분해둔 meterial과 관련없는 paper에도 훈련하였다.    
➡ 이를 통해 현재 더 넓은 범위의 주제(예: 폴리머 문헌)에 최적화된 텍스트 마이닝 도구를 개발하고 있다.   


-----

   
   

## 2.2. Named Entity Recognition. 

NER을 사용하여 **문서를 요약**하는 데 사용할 수 있는,    
specific entity types을 추출하는 데 관심이 있다.       

현재까지 재료 과학에서 정보 표현을 위한 ontology 또는 schema를 정의하기 위한 [여러 노력](https://www.semanticscholar.org/paper/A-survey-on-knowledge-representation-in-materials-Zhang-Zhao/487255347e00dcfc252e966079d6a71fba87783e)이 있었다.      

본 연구에서는 각 문서에 대해 **연구된 내용**과 **연구 방법**을 알고자 한다.     

이 정보를 추출하기 위해 아래 **7가지 entity labels을 설계**한다.       
* 무기 재료(MAT)       
* 대칭/위상 레이블(SPL)     
* 샘플 설명자(DSC)     
* 재료 특성(PRO)     
* 재료 적용(APL)     
* 합성 방법(SMT)      
* 특성화 방법(CMT)    



이러한 labels의 선택은 잘 알려진 아래의 **materials science 사면체**에 의해 어느 정도 동기 부여된다.       
* "가공(processing)"     
* "구조(structure)"    
* "특성(properties)"     
* "성능(performance)" 


각 **태그에 대한 예**는 각 **태그에 주석을 달기 위한 규칙**에 대한 자세한 설명과 함께 [Supporting information(S.4)](https://pubs.acs.org/doi/suppl/10.1021/acs.jcim.9b00470/suppl_file/ci9b00470_si_001.pdf)에 제공된다.    

<details>
<summary>📜 Supporting information(S.4)</summary>
<div markdown="1">
   
The rules for annotating each entity type are shown below.

* **1. Material (MAT):**    
Any inorganic solid or alloy, any non-gaseous element (at RT),      
e.g., “BaTiO3”, “titania”, “Fe”.       
* **2. Symmetry/phase label (SPL):**       
Names for crystal structures/phases(정방구조 이런거),       
e.g., “tetragonal”, ‘fcc”, “rutile”, “perovskite”; or, any symmetry label such as “P bnm”, or “P nma”.    
* **3. Sample descriptor (DSC):**       
Special descriptions of the type/shape of the sample.(성질을 갖는 특이한 구조)         
Examples inlcude “single crystal”, “nanotube”, “quantum dot”.       
* **4. Property (PRO)**:     
Anything measurable that can have a unit and a value,(단위와 값을 가질 수 있는 측정 가능한 모든 것)      
e.g., “conductivity”, “band gap”;(전도성, 에너지갭 이런거)    
or, any qualitative property or phenomenon exhibited by a material,(물질에 의해 나타나는 모든 질적 특성 또는 현상)    
e.g., “ferroelectric”, “metallic”.     
* **5. Application (APL):**        
Any high-level application such as “photovoltaics(태양전지)”, or any specific device such as “field-effect transistor”.    
* **6. Synthesis method (SMT):**     
Any technique for synthesising a material,      
e.g., “pulsed laser deposition”, “solid state reaction”,      
or any other step in sample production such       
as “annealing” or “etching”.
* **7. Characterization method (CMT):**     
Any method used to characterize a material, experiment or theory:     
e.g., “photoluminescence”, “XRD”, ‘tight binding”, “DFT”.     
It can also be a name for an equation or model.     
such “Bethe-Salpeter equation”.  
   
   
   
Overall, we annotated 800 materials science abstracts. This consisted of 22,306 annotated
entities (out of 111380 tokens total).  
   
   
  
</div>
</details>  




위에서 설명한 태그 지정 체계를 사용하여 800개의 materials science abstracts에 **손으로 주석**을 달았다.     

우리는 이러한 abstracts에 주석을 달 수 있는 "정확한" 방법은 없다고 강조한다.      
그러나 레이블링 체계가 합리적이라는 것을 보장하기 위해 주석을단 재료공학자들과의 일치율을 파악한 결과 이는 87.4%였다.      
이 값은 두 주석자가 동일한 레이블을 할당한 토큰의 백분율로 계산되었다.    



주석의 경우 **IOB(Inside-Outside-Beginning)** 형식을 사용한다.   

예를 들어 아래와 같은 문장은 IOB로 아래와 같이 태깅한다.   



```python
Thin films of SrTiO3 were deposited     

# IOB tagging
(Thin; B-DSC)     ## DSC란 태그의 시작     
(films; I-DSC)    ## DSC란 태그의 안      
(of; O)           ## meterial entity과 상관없어서 outside     
(SrTiO3; B-MAT)   ## MAT란 태그의 시작       
(were; O)         ## meterial entity과 상관없어서 outside       
(deposited; O)    ## meterial entity과 상관없어서 outside     
```   
   
-----
   
## 2.3. Neural Network model. The neural network
architecture for our model is based on that of Lample et al.;34 a schematic of this architecture is shown in Figure 2a. We explain the key features of the model below.

The aim is to train a model in such a way that materials science knowledge is encoded; for example, we wish to teach a computer that the words “alumina” and “SrTiO3” represent materials, whereas “sputtering” and “MOCVD” correspond to synthesis methods. There are three main types of information that can be used to teach a machine to recognize which words correspond to a specific entity type: (i) word representation, (ii) local (within sentence) context, and (iii) word shape.

For (i), word representation, we use word embeddings. Word embeddings map each word onto a dense vector of real numbers in a high-dimensional space. Words that have a similar meaning, or are frequently used in a similar context, will have a similar word embedding. For example, entities such as “sputtering” and “MOCVD” will have similar vector representations; during training, the model learns to associate these word vectors as synthesis methods. The word embeddings are generated using the Word2vec approach of Mikolov et al.;26 the embeddings are 200-dimensional and are based on the skip-gram approach. Word embeddings are generated by training on our corpus of 3.27 million materials science abstracts. More information about the training of word embeddings is included in the Supporting Information (S.2).

**[본 모델 아키텍처의 base model]**        
* [[Lample 등의 아키텍처](https://arxiv.org/abs/1603.01360)](https://arxiv.org/abs/1603.01360)     
이 아키텍처의 개략도는 아래와 같다.    
<img width="198" alt="image" src="https://user-images.githubusercontent.com/76824611/228559040-1f69d0dc-55fb-49cb-a0a5-95fb22c4bb92.png">


본 모델의 **특징**은 아래와 같다.       



**[base model의 특징]**      
* **목표**    
**materials science knowledge이 인코딩되는 방식**으로 모델을 훈련하는 것이다.        
예를 들어, "알루미나"와 "SrTiO3"라는 단어가 재료를 나타내는 반면 "퍼터링"과 "MOCVD"는 합성 방법에 해당한다는 것을 컴퓨터에 가르치고 싶습니다. (i) 단어 표현, (ii) 로컬(문장 내) 컨텍스트, (iii) 단어 모양 등 특정 엔터티 유형에 해당하는 단어를 인식하도록 기계에 가르치는 데 사용할 수 있는 세 가지 주요 정보 유형이 있습니다.

(i) 단어 표현의 경우 단어 임베딩을 사용합니다. 단어 임베딩은 각 단어를 고차원 공간에서 실수의 조밀한 벡터에 매핑합니다. 유사한 의미를 가지거나 유사한 맥락에서 자주 사용되는 단어는 유사한 단어 임베딩을 가집니다. 예를 들어, "퍼터링" 및 "MOCVD"와 같은 엔티티는 유사한 벡터 표현을 가질 것입니다. 훈련 중에 모델은 이러한 단어 벡터를 합성 방법으로 연결하는 방법을 배웁니다. 단어 임베딩은 Mikolov 등의 Word2vec 접근법을 사용하여 생성됩니다.;26 임베딩은 200차원이며 스킵-그램 접근법을 기반으로 합니다. 단어 임베딩은 327만 개의 재료 과학 추상체에 대한 교육을 통해 생성됩니다. 단어 임베딩 훈련에 대한 자세한 내용은 지원 정보(S.2)에 포함되어 있습니다.
puts/outputs from
different components of the model. The model in (a) represents the
word-level bidirectional LSTM, which takes as input a sequence of
words and returns a sequence of entity tags in IOB format. The wordlevel features for this model are the word embeddings for each word,
which are concatenated with the output of the character-level LSTM
run over the same word

나와 있습니다. 우리는 아래 모델의 주요 특징을 설명합니다.


For (ii), context, the model considers a sentence as a sequence of words, and it takes into account the local context of each word in the sentence. For example, in the sentence “The band gap of ___ is 4.5 eV”, it is quite clear that the missing word is a material, rather than a synthesis method or some other entity, and this is obvious from the local context alone. To include such contextual information, we use a recurrent neural network (RNN), a type of sequence model that is capable of sequence-to-sequence (many-to-many) classification. As traditional RNNs suffer from problems in dealing with long-range dependencies, we use a variant of the RNN called long short-term memory (LSTM).35 In order to capture both forward and backward context, we use a bidirectional LSTM; in this way, one LSTM reads the sentence forward and the other reads it backward, with the results being combined.

For (iii), word shape, we include character-level information about each word. For example, material formulas like “SrTiO3” have a distinct shape, containing uppercase, lowercase, and numerical characters, in a specific order; this word shape can be used to help in entity classification. Similarly, prefixes and suffixes provide useful information about entity type; for example, the suffix “ium”, for example in “strontium”, is commonly used for elemental metals, so a word that has this suffix has a good chance of being part of a material name. In order to encode this information into the model, we use a character-level bidirectional LSTM over each word [Figure 2b]. The final outputs from the character-level LSTM (100 dimensional vectors) are concatenated with the word embeddings for each word; these final vectors are used as the word representations for the word-level LSTM [Figure 2a].

For the word-level LSTM, we use pretrained word embeddings that have been trained on our corpus of over 3.27 million abstracts. For the character-level LSTM, the character embeddings are not pretrained, but are learned during the training of the model.

The output layer of the model is a conditional random fields (CRF) classifier, rather than a typical softmax layer. Being a sequence-level classifier, the CRF is better at capturing the strong interdependencies of the output labels.36

The model has a number of hyperparameters, including the word- and character-level LSTM size, the character embedding size, the learning rate, and drop out. The NER performance was optimized by repeatedly training the model with randomly selected hyperparameters; the final model chosen was the one with the highest accuracy when assessed on the development set
   
   
## 2.4. Entity Normalization.    
After entity recognition, the
final step is entity normalization. This is necessarily required as
each entity may be written in numerous forms. For example,
“TiO2”, “titania”, “AO2 (A = Ti)”, and “titanium dioxide” all
refer to the same specific stoichiometry: TiO2. For document
querying, it is important to store these entities in a normalized
format, so that a query for documents that mention “titania”
also returns documents that mention “titanium dioxide”. In
order to normalize material mentions, we convert all material
names into a canonical normalized formula. The normalized
formula is alphabetized and divided by the highest common
factor of the stoichiometry. In this way, “TiO2”, “titania”, and
“titanium dioxide” are all normalized to “O2Ti”. In some cases,
multiple stoichiometries are extracted from a single material
mention; for example, “ZrxTi1−xO3 (x = 0, 0.5, 1)” is converted
to “O2Ti”, “O4TiZr”, and “O2Zr”. When a continuous range is
given, e.g, 0 ≥ x ≤ 1, we increment over this range in steps of
0.1. Material mentions are normalized using regular
expressions and rule-based methods, as well as by making
use of the PubChem lookup tables;37 final validity checks on
the normalized formula are performed using the pymatgen
library.38



Normalization of other entity types is also crucial for
comprehensive document querying. For example, “chemical
vapor deposition”, “chemical-vapour deposition”, and “CVD”
all refer to the same synthesis technique; i.e., they are
synonyms for this entity type. In order to determine that two
entities have the same meaning, we trained a classifier that is
capable of determining whether or not two entities are
synonyms.



The model uses the word embeddings for each entity as
features; after performing NER, each multiword entity is
concatenated into a single word, and new word embeddings
are trained such that every multiword entity has a single vector
representation. Each synonym consists of an entity pair, so the
features for the model are created by concatenating the word
embeddings of the two entities in question. In addition to the
word embeddings, which mostly capture the context in which
an entity is used, several other handcrafted features are
included (Supporting Information, S.5). To train the model,
10 000 entity pairs are labeled as being either synonyms or not
(see the Supporting Information, S.6). Using this data, a binary
random forest classifier is trained to be able to predict whether
or not two entities are synonyms of one another.



Using the synonym classifier, each extracted entity can be
normalized to a canonical form. Each entity is stored as its
most frequently occurring synonym (we exclude acronyms as a
normalized form); for example, “chemical vapor deposition”,
“chemical-vapour deposition”, and “CVD” are all stored as
“chemical vapor deposition”, as this is the most frequently
occurring synonym that is not an acronym.




