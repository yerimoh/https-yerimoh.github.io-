---
title: "[05] Deep learning 1 (밑바닥 부터 시작하는 딥러닝 1) "
date:   2020-03-8
excerpt: "오차역전법(backpropagation)을 위한 계산그래프 computational graph,활성화 함수 계층 구현[ReLU, Sigmoid 계층],Affine/Softmax 구현, 오차역전파법 구현"
category: [Deep Learning]
layout: post
tag:
- Deep Learning
order: 0

comments: true
---

학습 관련 기술들

매개변수 갱신

> 최적화(optimization)
: 곧매개변수의 최적값을 찾는 문제/신경망 학습의 
목적은 손실 함수의 값을 가능한 한 낮추는 매개변수를 찾는 것

 - 확률적 경사 하강법 (SGD)
: 배운 방법_ 매개변수의 기울기(미분)이용
W: 갱신할 가중치 매개변수
aL/aW: W에 대한 손실 함수의 기울기
ƞ: 학습률(실제론 0.01이나 0.001로 미리 정함)
   -> 기울어진 방향으로 일정 거리만 가겠다는 단순한 방법

   구현

     class SGD:
def _ _init _ _(self, lr = 0.01): 
self.lr = lr

def update(self, params, grads):
for key in params.keys():
params[key] -= self.lr * grads[key]

    update (params, grads) 메서드: SGD 과정에서 반복해서 불림
인수인 params와 grads: (지금까지의 신경망 구현과 마찬가지로) 딕셔너
리 변수

   구현

    network = TwoLayerNet(...)
optimizer = SGD()

for i in range(10000):
...
x _batch, t _batch = get _mini _batch(...) # 미니배치
grads = network.gradient(x _batch, t _batch) 
params = network.params
optimizer.update(params, grads)
...

    -> optimizer: ‘최적화를 행하는 자’라는 뜻의 단어입니다. 
이 코드에서는 SGD가 그 역할 함
매개변수 갱신은 optimizer가 책임지고 수행하니 우리는 
optimizer에 매개변수와 기울기 정보만 넘겨주면 됨
    -> 이처럼 최적화를 담당하는 클래스를 분리해 구현하면 기능을 모듈화
하기 좋음
 
   - SGD의 단점 
        
         -> 위 식의 최솟 값은 (0,0) 인데 대부분의 화살표 방향이 그렇지X
         -> 최적화 갱신 경로 : 최솟값인 (0, 0)까지 지그재그로 이동하니 
비효율적
         => 비등방성 anisotropy 함수: (방향에 따라 성질, 즉 여기에서는 
기울기가 달라 지는 함수)에서는 탐색 경로가 비효율적이라는 것
원인_  기울어진 방향이 본래의 최솟값과 다른 방향을 가리켜서
         -> 모멘텀, AdaGrad, Adam로 단점 개선
     => Y축방향으로의 갱신 강도(위 아래)를 줄이자!!!!

- 모멘텀(Momentum)
:‘운동량’을 뜻하는 단어
W는 갱신할 가중치 매개변수,
aL/aW: W에 대한 손실 함수의 기울기
ƞ는 학습률
v(velocity): 물리에서 말하는 속도

     + αv: 물체가 아무런 힘을 받지 않을 때 서서히 하강시키는 역할
(α는 0.9 등의 값으로 설정). 
물리에서의 지면 마찰이나 공기 저항에 해당
       
구현

class Momentum:
# 인스턴스 변수 v가 물체의 속도: v는 초기화 때는 아무 값도 
담지X
def _ _init _ _(self, lr = 0.01, momentum = 0.9):
self.lr = lr 
self.momentum = momentum 
self.v = None

# W 업데이트 식
def update(self, params, grads):
#update ( ) 가 처음 호출될 때 매개변수와 같은 구
조의 데이터를 딕셔너리 변수로 저장
if self.v is None:
self.v = {} 
for key, val in params.items():
self.v[key] = np.zeros _like(val)

# 첫번 째 식 momentum= a/해당 속도와 기울기 조정
for key in params.keys():
self.v[key] = self.momentum*self.v[key] – 
self.lr*grads[key] 
params[key] + = self.v[key]

   
-> 모멘텀의 갱신 경로는 공이 그릇 바닥을 구르듯 움직임

-> SGD와 비교하면 ‘지그재그 정도’가 덜함





=> x축의 힘은 아주 작지만 방향은 변하지 않아서한 방향으로 일정하
게 가속
반면, y축의 힘은 크지만 위아래로 번갈아 받아서 상충하여 y축 방향
의 속도는 안정적X
=> 전체적으로는 SGD보다 x축 방향으로 빠르게 다가가 지그재그 움직
임이 줄어듦

- AdaGrad
-학습률(ƞ): 신경망 학습에서 중요
이 값이 너무 작으면 학습 시간이 너무 길어지고,
반대로 너무 크면 발산하여 학습이 제대로 이뤄지지 않음
- 학습률 감소(learning rate decay): 이 학습률을 정하는 효과적 기술
이는 학습을 진행하면서 학습률을 점차 줄여가는 방법
처음에는 크게 학습하다가 조금씩 작게 학습 
실제 신경망 학습에 자주 쓰임
      
+ 학습률을 서서히 낮추는 가장 간단한 방법: 매개변수 ‘전체’의 학
습률 값을 일괄적으로 낮추는 것. 
-> 이를 더욱 발전시킨 것이 AdaGrad

- AdaGrad: ‘각각의’ 매개변수에 ‘맞춤형’ 값을 만들어 줌
개별 매개변수에 적응적으로 adaptive 학습률을 조정하면서 
학습 진행.

>AdaGrad의 갱신 방법 수식
              
W: 갱신할 가중치 매개변수
aL/aW: W에 대한 손실 함수의 기울기
ƞ: 학습률을 뜻합니다. 
H: (기존 기울기 값)^2하여 계속+
+ ☉기호: 행렬의 원소별 곱셈
  1/h^(1/2):매개변수를 갱신할 때 이를 곱해 학습률을 조정
: 매개변수의 원소 중에서 많이 움직인(크게 갱신된) 원소는 학습률
이 낮아진다는 뜻-> 학습률 감소가 매개변수의 원소마다 다르게 적용

       + 작동방식
 AdaGrad는 과거의 기울기를 제곱하여 계속 더해감
   -> 학습을 진행할수록 갱신 강도가 약해짐.
-> [P]무한히 계속 학습한다면 갱신량이 0이 되어 더 이상 갱신 X

- RMSProp
: [P]이 문제를 개선한 기법. 
과거의 모든 기울기를 균일하게 더해가는 것X 
: 지수이동평균(Exponential Moving Average, EMA); 먼 과거의 기울기
는 서서히 잊고 새로운 기울기 정보를 크게 -> 과거 기울기의 반영 규
모를 기하급수적으로 감소시킴


AdaGrad의 구현
  
    class AdaGrad:
def _ _init _ _(self, lr = 0.01):
self.lr = lr 
self.h = None

def update(self, params, grads):
if self.h is None:
self.h = {}
    		     for key, val in params.items():
self.h[key] = np.zeros_like(val)

for key in params.keys():
self.h[key] + = grads[key] * grads[key] 
params[key] - = self.lr * grads[key] / 
(np.sqrt(self.h[key]) + 1e-7)

-> 1e-7 이라는 작은 값 더함(이 작은 값은 self.h[key]=0일 떄 대비)
대부분의 딥러닝 프레임 워크에서는 이 값도 인수로 설정 가능.

 AdaGrad를 사용해서 최적화 
 

-> y축 방향은 기울 기가 커서 처음에는 크게 움직이지만, 그 큰 움직임
에 비례해 갱신 정도도 큰 폭으로 작아지도록 조정 
그래서 y축 방향으로 갱신 강도가 빠르게 약해지고, 지그재그 움직임이 
줄어듦

 - Adam

: 모멘텀_ 공이 그릇 바닥을 구르는 듯한 움직임을 보임
: AdaGrad_ 매개변수의 원소마다 적응적으로 갱신 정도를 조정.
=> 그럼 혹시 이 두 기법을 융합한 것

- Adam : 하이퍼파라미터의 ‘편향 보정’진행
   
-> 그릇 바닥을 구르듯 움직임(모멘텀) 
모멘텀 때보다 공의 좌우 흔들림이 적음(이는 학습의 갱신 강도
를 적응적으로 조정해서)
- Adam은 하이퍼파라미터를 3개 설정. 
학습률, 일차 모멘텀용 계수 β 1 , 이차 모멘텀용 계수 β 2 
-> 논문에 따르면 기본 설정값은 β 1 은 0.9, β 2 는 0.999이며, 이 
값이면 많은 경우에 좋은 결과를 얻을 수 있음.

> 어느 갱신 방법을 이용할 것인가?

- 위 4개 결과 비교
  
-> 각자의 장단이 있어 잘 푸는 문제와 서툰 문제가 있죠.
지금도 많은 연구에서 SGD를 사용. 
모멘텀과 AdaGrad도 시도해볼 만한 가치가 충분.
요즘에는 많은 분이 Adam에 만족해함. 
일반적 으로 SGD보다 다른 세 기법이 빠르게 학습하고, 때로는 최종 
정확도도 높게 나타납니다.

> 가중치의 초깃값
신경망 학습에서 특히 중요한 것이 가중치의 초깃값

 - 초깃값을 0으로
- 가중치 감소 weight decay 기법
: 오버피팅을 억제해 범용성능을 높이는 테크닉 
-> 가중치 매개변수의 값이 작아지도록 학습하는 방법 
가중치 값을 작게 하여 오버피팅이 일어나지 않게 하는 것
: 사실 지금까지 가중치의 초깃값은 0.01 * np.random.randn (10, 
100 )처럼 정규분포에서 생성되는 값을 0.01배 한 작은 값(표준편차가 
0.01인 정규분포)을 사용.

- 결론
: 가중치 초깃값을 0으로 하면 학습이 올바로 이뤄지지 않습니다.
-> 정확히는 가중치를 균일한 값으로 설정 해서는 안 됨
-> 이유_ 바로 오차역전파법(이렇게 오차를 점점 거슬러 올라가면서 
비율에 맞게 다시 전파하는 것을 의미)에서 모든 가중치의 값이 똑같
이 갱신되기 때문 
(ex_ 2층 신경망에서 첫 번째와 두 번째 층의 가중치가 0
-> 순전파_ 입력층의 가중치가 0이기 때문에 두 번째 층의 뉴런에 모
두 같은 값이 전달됩니다. 
이는 역전파 때 두 번째 층의 가중치가 모두 똑같이 갱신되
는 것<‘곱셈 노드의 역전파’를 떠올려보세요>
        + 역전파 방법은 결과 값을 통해서 다시 역으로 input 방향으로 오차를 다시 보내며 가중치를 재업데이트 하는 것이다. 물론 결과(오차율)에 영향을 많이 미친 노드(뉴런)에 더 많은 오차를 돌려줄 것이다.
	 


. : 이 ‘가중치가 고르게 되어버리는 상황’을 막으려면 (정확히는 
가중치의 대칭적인 구조를 무너뜨리려면) 초깃값을 무작위로 설정해야 
함.

- 은닉층의 활성화값 분포
: 은닉층의 활성화값(다른 문헌에서는 계층 사이를 흐르는 데이터)(활성
화 함수의 출력 데이터)의 분포를 관찰하면 중요한 정보를 얻을 수있음. 
-> 이번 절에서는 가중치의 초깃값에 따라 은닉층 활성화값들이 어떻게 
변화하는지 확인
    
    1) 기울기 소실 gradient vanishing
     : 활성화값들이 0과 1에 치우쳐 분포
       -> 역전파의 기울기 값이 점점 작아지다가 사라짐
     
    2) 표현력 제한
     : 0.5 부근에 집중 -> 표현력 관점에서는 큰 문제
      다수의 뉴런이 거의 같은 값을 출력하고 있으니 뉴런을 여러 개 둔 의
미가 없어짐
     

+ 각 층의 활성화값은 적당히 고루 분포되어야 함.
  층과 층 사이에 적당하게 다양한 데이터가 흐르게 해야 신경망 학습이 
효율적으로 이뤄지기 때문 

- sigmoid나 tanh 등의 S자 모양 곡선
+ Xavier 초깃값(1/√n): 권장하는 가중치 초깃값
n_ 앞 계층의 노드가 n개라면 표준편차가 이것인 
표준분초 사용(앞 층의 노드 수)
일반적인 딥러닝 프레 임워크들이 표준적으로 이용

Xavier 초깃값을 사용하면 앞 층에 노드가 많을수록 대상 노드의 초깃값으로 설정하는 가중치가 좁게 퍼집니다. 
-> 많을 수록 좁게 분포
 
-> 이 결과를 보면 층이 깊어지면서 형태가 다소 일그러지지만, 
앞에서 본 방식보다는 넓게 분포됨

- ReLU를 사용할 때의 가중치 초깃값
Xavier 초깃값은 활성화 함수가 선형인 것을 전제로 이끈 결과. 
-> sigmoid 함수와 tanh 함수는 좌우 대칭이라 중앙 부근이 선형인 함수

> He 초깃값(√(2/n)  ): ReLU에 특화된 초깃값을 이용 권장 이 특화된 초깃값
ReLU는 음의 영역이 0이라서 더 넓게 분포시키기 위해 
2배의 계수가 필요
    
- std = 0.01
: 신경망에 아주 작은 데이터가 흘러 기울기 작음
학습이 거의 X


- Xavier 초깃값
: 기울기 소실(층이 깊어지면 활성화값들의 치우침)








> 배치 정규화

: 앞 절에서는 각 층의 활성화값 분포를 직접 관찰 후, 가중치의 초깃값을 적절히 설정
: 그렇다면 각 층이 활성화를 적당히 퍼뜨리도록 ‘강제’해보면?
-> 배치 정규화 Batch Normalization

- 배치 정규화 알고리즘.
 > 장점
   학습을 빨리 진행할 수 있다(학습 속도 개선).
   초깃값에 크게 의존하지 않는다(골치 아픈 초깃값 선택 장애여 안녕!).
   오버피팅을 억제한다(드롭아웃 등의 필요성 감소).

 >‘배치 정규화 Batch Norm 계층’을 신경망에 삽입
   

- 배치 정규화 과정
: 학습 시 미니배치를 단위로 정규화
  -> 데이터 분포가 평균이 0, 분산이 1이 되도록 정규화
+  미니배치 B = {x 1 , x 2 , ..., x m }이라는 m개의 입력 데이터집합
1) 평균 μ B 와분산 σ B2 구함
2) 입력 데이터를 평균이 0, 분산이 1이 되게(적절한 분포가 되게) 정
규화.
+ ɛ(epsilon, 엡실론) 는 작은 값(예컨대 10e-7 등)으로, 0으로 나
누는 사태를 예방하는 역할
     

-> 단순히 미니배치 입력 데이터
 {x 1 , x 2 , ..., x m }을 
평균 0, 분산 1인 
데이터 {x ˆ 1 , x ˆ 2 , ..., x ˆ m }으로 변환하는 일을 함
{4,5,6} -> {-0.01,0,0.01}


=> 이 처리를 활성화 함수의 앞(혹은 뒤)에 삽입함으로써 데이터 분
포가 덜 치우치게 할 수 있습니다.
   3) 배치 정규화 계층마다 이 정규화된 데이터에 고유한 확대 scale 와 
이동 shift 변환을 수행합니다.
      γ: 확대
 β: 이동 

처음에는 γ = 1(1배 확대), β = 0(이동 안함)부터 시작하고,
학습하면서 적합한 값으로 조정해감
-> 이 알고리즘이 신경망에서 순전파 때 적용
  

> 바른 학습을 위해

.- 오버피팅
> 오버피팅이 일어나는 경우
- 매개변수가 많고 표현력이 높은 모델
- 훈련 데이터가 적음
    
 - 가중치 감소(weight decay)
: 오버피팅 억제기제
: 학습 과정에서 큰 가중치에 대해 큰 페널티를 부과하여 오버피팅 억제
( 오버피팅은 가중치 매개변수의 값이 커서 발생하는 경우가 많기 때문 )

=> 신경망 학습의 목적은 손실 함수의 값↓ 
   가중치의 제곱 노름 norm (L2 노름)(||W||2)(  1/2 λw^2)을 손실 함수에 (+)
  -> 손실함수 ↑
->.가중치(W)가 커지는 것을 억제 가능(손실함수 줄이려고)
  
+ L2노름에 따른 가중치 감소= 1/2 λw^2 
(λ 람다 는 정규화의 세기를 조절하는 하이퍼파라미터)
+ λ를 크게 설정할수록 큰 가중치에 대한 페널티가 커짐
+ 1/2 미분 결과인 λw^2를 조절하는 상수

+ L2 노름은 각 원소의 제곱들을 더한 것
가중치 W = (w 1 , w 2 , … ,w n ) ->√(W_I^2+⋯+W_n^2 )

- 드롭아웃
: 신경망 모델이 복잡해지면 가중치 감소만으로는 대응하기 어려워짐

- 드롭아웃: 뉴런을 임의로 삭제하면서 학습하는 방법.
           훈련 때 은닉층의 뉴런을 무작위로 골라 삭제.
-> 삭제된 뉴런은 신호 전달 X
            훈련 때는 데이터를 흘릴 때마다 삭제할 뉴런을 무작위로 선택
시험 때는 모든 뉴런에 신호를 전달
-> 단, 시험 때는 각 뉴런의 출력에 훈련 때 삭제 안 한 비율을 
곱하여 출력
 

- 구현

class Dropout:
# self.mask는 x와 형상이 같은 배열을 무작위로 생성하고, 그 값이 
# dropout_ ratio보다 큰 원소만 True로 설정
def _ _init _ _(self, dropout _ratio = 0.5):
self.dropout _ratio = dropout _ratio 
self.mask = Non

def forward(self, x, train _flg = True): 
if train _flg:
self.mask = np.random.rand(*x.shape) > 
self.dropout _ratio 
return x * self.mask 
else:
return x * (1.0 - self.dropout _ratio)

# 역전파 때의 동작은 ReLU와 같음
# 즉, 순전파때 신호를 통과시키는 뉴런은 역전파 때도 신호를 그대로 
통과시키고
# 순전파 때 통과시키지 않은 뉴런은 역전파 때도 신호를 차단
def backward(self, dout):
return dout * self.mask

  + forward(순전파) 메서드에서는 훈련 때(train_flg = True일 때)만 잘 계산해두면 시험 때. 삭제 안 한 비율은 곱하지 않아도 OK
실제 딥러닝 프레임워크들도 비율을 곱하지 않습니다.
-> 핵심은 훈련 시에는 순전파 때마다 self.mask에 삭제할 뉴런을 False
로 표시
 
     -> 훈련 데이터와 시험 데이터에 대한 정확도 차이↓ 
훈련 데이터에 대한 정확도가 100%에 도달X
-> 표현력↑ 오버피팅을 억제가능

+ 앙상블 학습(ensemble learning)
: 개별적으로 학습시킨 여러 모델의 출력을 평균 내어 추론하는 방식입
-> 신경망의 정확도가 몇% 정도 개선
-> 드롭아웃과 밀접_ 학습 때 드롭아웃이 뉴런을 무작위로 삭제하는 행
위를 매번 다른 모델을 학습시키는 것으로 해석
        추론 때는 뉴런의 출력에 삭제한 비율(이를테면 
0.5 등)을 곱함으로써 앙상블 학습에서 여러 모델
의 평균을 내는 것과 같은 효과를 
 즉, 드롭아웃은 앙상블 학습과 같은 효과를 (대략) 하나의 네트워크
로 구현했다고 생각 가능

> 적절한 하이퍼파라미터 값 찾기

하이퍼파라미터: 예를 들어각 층의 뉴런 수, 배치 크기, 매개변수 갱신 시의 
학습률과 가중치 감소 등입니다. 
모델의 성능에 큰 영향
그 값을 결정하기까지는 많은 시행착오를 겪음.


 - 정의: 파라미터가 매게변수이므로 이는 초 매개변수
 - 적용
   1) dropout_ 하이퍼파라미터는 뉴런들을 out시킬 비율
               -> 확률이 낮으면 효과 얻지X/ 값이 크면 underfitting
   https://ikkison.tistory.com/92

- 검증 데이터
: 하이퍼파라미터의 성능을 평가 시_ 시험데이터(test용 not학습)를 사용X
이유 시험 데이터를 사용하여 조정 시 하이퍼파라미터 값이 시험 데이터
에 오버피팅되기 때문
-> 하이퍼파라미터 값의 ‘좋음’을 시험 데이터로 확인
-> 하이퍼파라미터의 값이 시험 데이터에만 적합하도록 조정
-> 범용 성능이 떨어짐

결론 검증 데이터 (validation data) 필요
_ 하이퍼파라미터를 조정시 하이퍼파라미터 전용 확인 데이터

=> 훈련 데이터_ 매개변수(가중치와 편향)의 학습에 이용
   검증 데이터_ 하이퍼파라미터의 성능을 평가하는 데 이용
   시험 데이터_ 신경망의 범용 성능 평가 

- 하이퍼파라미터 최적화
: 하이퍼파라미터의 ‘최적 값’이 존재하는 범위를 조금씩 줄여가는 것 
범위를 조금씩 줄이려면 
1) 대략적인 범위를 설정하고 
2) 그 범위에서 무작위로 하이퍼파라미터 값을 골라낸(샘플링)
3) 2단계에서 샘플링한 하이퍼파라미터 값을 사용하여 학습하고, 
검증 데이터로 정확도를 평가합니다(단, 에폭은 작게 설정합니다).
4) 2,3단계를 특정 횟수(100회 등) 반복하며,
그 정확도의 결과를 보고 하이퍼파라미터의 범위를 좁힘

+ 신경망의 하이퍼파라미터 최적화에서는
그리드 서치(grid search) 같은 규칙적인 탐색보다는 
무작위로 샘플링해 탐색하는 편이 좋은 결과
-> 최종 정확도에 미치는 영향력이 하이퍼파라미터마다 다르기 때문

: 하이퍼파라미터의 범위_ ‘대략적으로’ 지정하는 것이 효과적
+ <실제> 로그 스케일(log scale) 로 지정
_ 0.001에서 1,000 사이(10 -3 ~10 3 )와 같이 ‘10의 거듭제곱’ 단
위로 범위를 지정 

: 하이퍼파라미터를 최적화할 때는 딥러닝 학습에는 오랜 시간(예컨대 며
칠이나 몇 주 이상)이 걸림 -> 따라서 나쁠 듯한 값은 일찍 포기해
=> 학습을 위한 에폭을 작게 하여, 1회 평가에 걸리는 시간을 단축하는 것
이 효과적

정리
● 매개변수 갱신 방법에는 확률적 경사 하강법(SGD ) 외에도 모멘텀, 
AdaGrad, Adam 등이 있다.
● 가중치 초깃값을 정하는 방법은 올바른 학습을 하는 데 매우 중요.
● 가중치의 초깃값으로는 ‘Xavier 초깃값’과 ‘He 초깃값’이 효과적
● 배치 정규화를 이용하면 학습을 빠르게 진행할 수 있으며, 초깃값에 영향
을 덜 받게 된다.
● 오버피팅을 억제하는 정규화 기술로는 가중치 감소와 드롭아웃이 있다.
● 하이퍼파라미터 값 탐색은 최적 값이 존재할 법한 범위를 점차 좁히면서 
하는 것이 효과적이다.
