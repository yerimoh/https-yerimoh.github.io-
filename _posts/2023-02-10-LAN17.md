---
title: "IS REINFORCEMENT LEARNING (NOT) FOR NATURAL LANGUAGE PROCESSING?: BENCHMARKS, BASELINES, AND BUILDING BLOCKS FOR NATURAL LANGUAGE POLICY OPTIMIZATION 정리"
date:   2023-02-10
excerpt: "IS REINFORCEMENT LEARNING (NOT) FOR NATURAL LANGUAGE PROCESSING?: BENCHMARKS, BASELINES, AND BUILDING BLOCKS FOR NATURAL LANGUAGE POLICY OPTIMIZATION paper review"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---

# 목차


# ABSTRACT
**[LM with human preferences에 RL 적용]**     
* 본 논문은 인간 선호도에 맞추는 pre-trained large language models (LMs) 문제를 해결한다.        
* 텍스트 생성을 순차적 decision-making problem로 본다면, **강화 학습(RL)은 적절한 conceptual 프레임워크**가 될 수 있다.         

**[단점]**    
* 그러나 LM 기반 생성을 위해 RL을 사용하는 것은 combinatorial action space으로 인한 **training 불안전성** 존재     
* LM alignment을 위해 맞춤화된 **오픈 소스 라이브러리와 벤치마크의 부족**을 포함한 경험적 challenges에 직면한다.     
➡ 따라서, <span style="background-color:#fff5b1"> RL이 NLP를 위한 실용적인 패러다임인가? </span>라는 질문이  제기된다.


**[단점 해결]**    
이에 대한 답변을 돕기 위해 아래 3가지 방법을 제시한다,     
* **RL4LMs를 소개**   
  * **RL로 언어 생성기를 optimizing**하기 위한 오픈 소스 모듈식 라이브러리     
  * 라이브러리는 **임의의 reward 함수를 사용**하여 HuggingFace 라이브러리(Wolf et al., 2020)에서 encoder 또는 encoder-decoder **LM을 훈련하는 데 사용할 수 있는 on-policy RL algorithms으로 구성**된다.     
* **GRUE (General Reinforced-language Understanding Evaluation) benchmark 제시**     
  * target 문자열이 아니라 **인간 선호도의 자동화된 측정**을 capture하는 reward 기능에 의해 supervised되는 6개 언어 생성 작업 세트      
  * GRUE는 NLP 작업에 대한 RL algorithms의 첫 번째 leaderboard-style evaluation이다.      
* **NLPO(Natural Language Policy Optimization) 소개**    
   * 언어 생성에서 **combinatorial action space을 효과적으로 줄이는 방법**을 배우는 사용하기 쉬운 성능의 RL algorithm       

**[결과]**    
**1)** **RL 기술**이 일반적으로 인간 선호도에 LM을 맞추는 데 **supervised 방법보다 낫다**는 것을 보여준다.     
**2)** **NLPO가** 자동 및 인간 평가를 모두 기반으로 previous policy gradient methods **(PPO)보다 더 큰 안정성과 성능**을 나타낸다       




-----

# 1 INTRODUCTION
The ultimate aim of language technology is to interact with humans. However, most language models
are trained without direct signals of human preference, with supervised target strings serving as (a
sometimes crude) proxy. One option to incorporate user feedback is via human-in-the-loop, i.e., a
user would be expected to provide feedback for each sample online as the model trains, but this
degree of dense supervision is often prohibitive and inefficient. Automated metrics offer a promising
compromise: models of human preference like pairwise learned preference models (Ouyang et al.,
2022), BERTScore (Zhang et al., 2019), BLEURT (Sellam et al., 2020) have significantly improved
correlation with human judgment compared to earlier metrics (BLEU, METEOR, etc.), and are cheap
to evaluate. But — these functions are usually not per-token differentiable: like humans, metrics
can only offer quality estimates for full generations. Reinforcement Learning (RL) offers a natural
path forward for optimizing non-differentiable, scalar objectives for LM-based generation when it
is cast as a sequential decision-making problem. However, Goodhart’s Law3
looms: particularly
in the case of imperfect metrics that use neural networks, it is easy to find nonsense samples that
achieve high-quality estimates. Recent works have shown promising results in aligning LMs to human
preferences via RL by constraining preference-based rewards to incorporate notions of fluency (Wu
et al., 2021; Ouyang et al., 2022) but progress in this line of work is heavily hindered by a lack
of open-source benchmarks and algorithmic implementations—resulting in perception that RL is a
challenging paradigm for NLP (Choshen et al., 2020; Kreutzer et al., 2021).


To facilitate research in building RL algorithms to better align LMs, we release a library, a benchmark,
and an algorithm. First, we release the RL4LMs library, which enables generative HuggingFace
models (e.g., GPT-2 or T5) to be trained using a variety of existing RL methods like PPO/A2C/etc.
Next, we apply models trained using RL4LMs to the new GRUE (General Reinforced-language
Understanding Evaluation) benchmark: GRUE is a collection of 7 contemporary NLP tasks (see
Table1 for details); in contrast to other benchmarks, instead of supervised training, we pair each
task with reward function(s). GRUE challenges models to optimize these reward functions while
remaining fluent language generators. We train language models via RL—both with and without
task specific supervised pre-training—to optimize rewards. Finally, beyond existing RL methods, we
introduce a novel on-policy RL algorithm called NLPO (Natural Language Policy Optimization),
that dynamically learns task-specific constraints over the distribution of language at a token level.


Experiments on GRUE and human evaluations show that NLPO better balances learning preference
rewards while maintaining language fluency compared to alternatives, including PPO (Figure 1). We
find that using RL to learn from scalar reward feedback can be more: (1) data efficient than using
additional expert demonstrations via supervised learning (though a combination of both is best)—a
learned reward function enables greater performance when used as a signal for an RL method than
a supervised method trained with 5 times more data, and (2) parameter efficient—enabling a 220
million parameter model trained with a combination of supervision and NLPO to outperform a 3
billion supervised model. We hope that the benchmarks, baselines, and building blocks we release
serve to drive forward research in aligning LMs to human preferences.

