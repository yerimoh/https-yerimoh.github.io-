---
title: "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer 정리"
date:   2023-02-10
excerpt: "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer paper review"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---


# Abstract
Transfer learning, where a model is first pre-trained on a data-rich task before being fine�tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new “Colossal Clean Crawled Corpus”, we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.1

**[논문 배경]**
모델이 Downstream Task에서 fine-tuning되기 전에 data-rich task에서 pre-train을 하는 것은  자연어 처리(NLP)의 강력한 기술로 부상했다.     
[transfer learning](https://yerimoh.github.io/DL12/)의 효과는 다양한 접근법, 방법론 및 실천을 낳았다. 본 논문에서는 모든 텍스트 기반 언어 문제를 텍스트 대 텍스트 형식으로 변환하는 통합 프레임워크를 도입하여 NLP에 대한 전이 학습 기술의 풍경을 탐구한다. 우리의 체계적인 연구는 수십 가지 언어 이해 과제에 대한 사전 훈련 목표, 아키텍처, 레이블이 지정되지 않은 데이터 세트, 전송 접근법 및 기타 요소를 비교한다. 우리의 탐사에서 얻은 통찰력과 규모 및 새로운 "Colossal Clean Crawled Corpus"를 결합하여 요약, 질문 답변, 텍스트 분류 등을 포함하는 많은 벤치마크에서 최첨단 결과를 달성한다. NLP에 대한 이전 학습에 대한 향후 작업을 촉진하기 위해 데이터 세트, 사전 훈련된 모델 및 코드를 릴리스한다.1


<details>
<summary>📜 Downstream Task 의미 보기</summary>
<div markdown="1">
  

구체적으로 풀고 싶은 문제들을 말한다.

NLP에서는 언어모델을 pre-train방식을 이용해 학습을 진행하고,    
그 후에 원하고자 하는 task를 fine-tuning하는 방식을 통해 모델을 업데이트 하는 방식을 사용하는데 이때, task를 Downstream Task라 한다.

예를들어, BERT의 언어모델을 질의응답 Task라인 squad를 학습한다고 할때, 이때 질의응답 Task를 다운스트림 Task로 볼 수 있을것이다.  
  
  
</div>
</details>  

