---
title: "(T5) Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer 정리"
date:   2023-02-10
excerpt: "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer paper review"
category: #[Paper]
layout: post
tag:
#- Paper
order: 0

comments: true
---


# Abstract
<span style="background-color:#F5F5F5">**[논문 배경]**</span>    
모델이 Downstream Task에서 fine-tuning되기 전에 data-rich task에서 pre-train을 하는 것은  자연어 처리(NLP)의 강력한 기술로 부상했다.     
즉 이러한 [transfer learning](https://yerimoh.github.io/DL12/)은 NLP에 큰 발전을 가져왔다.    

<span style="background-color:#F5F5F5">**[논문의 목적]**</span>    
* 모든 텍스트 기반 언어 문제를 text-to-text 형식으로 변환하는 통합 프레임워크를 도입하여 <span style="background-color:#fff5b1">NLP에 대한 [transfer learning](https://yerimoh.github.io/DL12/)를 전반적으로 탐구</span>한다.     
* 위 탐구를 통해 얻은 통찰력과 규모 및 본 논문에서 제안할 <span style="background-color:#fff5b1">"Colossal Clean Crawled Corpus"를 결합</span>하여 많은 NLP evaluation에서 SOTA 성능을 보였다.        
➡ 공개한 [data set, pre-trained models, and code](https://github.com/google-research/text-to-text-transfer-transformer) 링크를 첨부하였다.        


<details>
<summary>📜 Downstream Task 의미 보기</summary>
<div markdown="1">
  

구체적으로 풀고 싶은 문제들을 말한다.

NLP에서는 언어모델을 pre-train방식을 이용해 학습을 진행하고,    
그 후에 원하고자 하는 task를 fine-tuning하는 방식을 통해 모델을 업데이트 하는 방식을 사용하는데 이때, task를 Downstream Task라 한다.

예를들어, BERT의 언어모델을 질의응답 Task라인 squad를 학습한다고 할때, 이때 질의응답 Task를 다운스트림 Task로 볼 수 있을것이다.  
  
  
</div>
</details>  

----
----



# Introduction

<span style="background-color:#F5F5F5">**[최근 연구 동향]**</span>    
자연어 처리(NLP)작을 을 위한 train이 가능하여면 모델이 **downstream learning에 적합한 방식으로 텍스트를 처리**할 수 있어야 한다.              
그런데 이는 **모델이 텍스트를 "이해"할 수 있게 학습한다고 보기엔 힘들다**.               
➡ 현대 기계 학습 관행에서 이러한 train이 명시적으로 수행되는 경우가 거의 없어, 대신 **Auxiliary Task의 일부**로 학습되는 경우가 많다.        


<details>
<summary>📜 Auxiliary Task 의미 보기</summary>
<div markdown="1">

본 task는 아니지만, 본 task에서의 성능이 더 잘 나올 수 있도록 도와주는 보조 task  

</div>
</details> 
  
최근에는 **데이터가 풍부한 작업**에 대해 **전체 모델을 pre-train하는 것**이 점점 더 **일반화**되고 있다.     
이상적으로, 이 pre-train은 모델이 범용 능력과 지식을 개발하게 하고, 이를 다운스트림 작업으로 이전할 수 있도록 한다.     
➡ 더 큰 데이터 세트에서 더 큰 모델을 훈련시키는 것만으로 더 나은 성능을 달성할 수 있다.    




<span style="background-color:#F5F5F5">**[현 경향으로 인한 한계]**</span>    
이러한 경향으로 인해 NLP에 대한 **transfer learning 방법론을 개발**하는 연구들이 활발하게 이루어졌다.      
이렇게 이 분야가 급성장하여 연구가 활발해지니 아래와 같은 작업들이 어려워졌다.       
* 여러 algorithms을 비교    
* 새로운 contributions의 효과 파악    
* transfer learning을 위한 기존 방법의 space 이해        


✔ 그래서 본 논문은 이 분야의 원활한 이해를 위해, **다양한 접근법을 체계적으로 연구**하고 **필드의 현재 한계를 밀어낼 수 있는 transfer learning에 대한 통합된 접근법을 활용**한다. 
 


<span style="background-color:#F5F5F5">**[본 논문의 해결책]**</span>      
본 논문 work의 기본 아이디어는 <span style="background-color:#fff5b1">모든 텍스트 처리 문제를 **“text-to-text” 문제로 처리**</span>하는 것이다.             
즉, **텍스트를 입력으로 받아들이고 새로운 텍스트를 출력으로 생성**하는 것이다.          
* 이러한 텍스트 간 프레임워크는 우리가 고려하는 모든 작업에 **동일한 모델, 목표, 훈련 절차 및 decoding 프로세스를 직접 적용**할 수 있게 한다.          
* 본 논문은 질문 답변, 문서 요약 및 감정 분류를 포함한 다양한 영어 기반 NLP 문제에 대한 성능을 평가하여 이러한 **유연성을 활용**한다.        
* 이 통합 접근법을 통해, 우리는 다양한 transfer learning의 target, 레이블이 지정되지 않은 데이터 세트 및 기타 요인의 효과를 비교하는 동시에 이전에 고려되었던 것 이상으로 **모델과 데이터 세트를 확장**하여 NLP에 대한 **transfer learning의 한계를 탐색**할 수 있다.    




본 논문은 본 논문의 목표가 새로운 방법을 제안하는 것이 아니라, **그 분야가 어디에 서 있는지에 대한 포괄적인 관점을 제공하는 것**임을 강조한다.      
즉, 우리의 작업은 본 목표를 달성하기 위해 아래와 같은 task로 구성된다.         
➡ 주로 기존 기술의 조사, 탐구 및 경험적 비교       


또한 본 논문은 다음과 같은 방식으로 **현재 접근 방식의 한계를 탐구**한다.        
본 논문이 고려하는 많은 task에서 SOTA 결과를 얻기 위해 체계적인 연구(train 모델을 최대 110억 개 parameters까지 확장)수행       
* 이 규모의 실험을 수행하기 위해 웹에서 긁어낸 수백 기가바이트의 clean 영어 텍스트로 구성된 데이터 세트인 "Colossal Clean Crawled Corpus"**(C4)** 를 소개한다.        
* **transfer learning의 핵심 기능**은, **데이터가 부족한 환경에서  pre-trained models을 활용할 수 있는 가능성**이라는 것을 인식하여,      
[코드, 데이터 세트 및 사전 훈련된 모델](https://github.com/google-research/text-to-text-transfer-transformer
)을 릴리스했다.    




<span style="background-color:#F5F5F5">**[논문 구성]**</span>            
* base model과 그 구현    
* 모든 text processing 문제를 text-to-text 작업으로 공식화하는 절차 및 고려하는 작업 모음에 대한 논의    
* 섹션 3에서, NLP에 대한 transfer learning 분야를 탐구하는 대규모 실험 세트를 제시          
* 섹션(섹션 3.7)의 끝에서, 우리는 체계적인 연구의 통찰력을 결합하여 광범위한 벤치마크에 대한 최첨단 결과를 얻음    
* 섹션 4에서, 결과에 대한 요약을 제공 한 뒤, 미래에 대한 전망으로 마무리             




---
---


# 2. Setup    



본 논문은 large-scale 경험적 연구의 결과를 제시하기 전에 아래와 같은 것들을 먼저 제시한다,   
* [Transformer 모델 아키텍처](https://yerimoh.github.io/Lan/)와 평가하는 downstream tasks을 포함하여 결과를 이해하는 데 필요한 **배경 주제를 검토**한다.        
* **모든 문제를 text-to-text task으로 처리하기 위한 접근 방식**을 소개    
* **"Colossal Clean Crawled Corpus"(C4) 설명:** 레이블이 지정되지 않은 텍스트 데이터의 소스로 생성한 공통 크롤 기반 데이터 세트임     




우리는 우리의 모델과 프레임워크를  <span style="background-color:#fff5b1">“Text-to-Text Transfer Transformer” (T5)</span> 라고 부른다.      





---


## 2.1 Model

### base architecture: “Transformer”
NLP에 대한 전이 학습에 대한 초기 결과는 반복 신경망을 활용했지만,  
최근에는 "Transformer" 아키텍처를 기반으로 한 모델을 사용하는 것이 더 일반화되었다.     


Transformer는 transfer learning에 효과적이므로, 이후 다양한 NLP 설정에 사용되었다.          
본 논문에서도 **모든 모델의 base를 Transformer 아키텍처**로 하였다.         

아래에 언급된 세부사항과 섹션 3.2에서 탐구한 변형을 제외하고,     
본 논문은 Transformer 아키텍처에서 크게 벗어나지 않는다.      


<details>
<summary>📜 Transformer를 자세히 알고 싶다면? (참고자료) </summary>
<div markdown="1">

이 논문(아키텍처)에 대해 더 자세히 알고싶다면 아래 자료들을 참고하자,    
* [원본 논문](https://arxiv.org/abs/1706.03762)    
* 후속 튜토리얼 3,4(뒤에 나옵니다.)    
* [본 포스트를 정리한 필자가 정리한 포스트](https://yerimoh.github.io/Lan/)         

</div>
</details> 
  85.5


그리고 본 논문에서 Transformer에대해 간단간단하게 설명해줬는데 원한다면 아래 자세한 설명 보기를 눌러 한 번 읽어보길 바란다.(그렇지만 원 논문을 읽어봐야 아래 내용들도 이해가 갈 것이다.)   

<details>
<summary>📜 Transformer를 자세히 알고 싶다면?(본 논문의 설명) </summary>
<div markdown="1">




 
**Transformer의 중요한 요소는,** [self-attention](https://yerimoh.github.io/Lan/#sub-layer1-self-attention)이다.         
self-attention은 각 요소를 나머지 sequence의 가중 평균으로 대체하여 시퀀스를 처리하는 attention의 변형이다.    

**Transformer의 구조는,** [encoder-decoder 아키텍처](https://yerimoh.github.io/Lan/#transformer-%EB%AA%A8%EB%8D%B8-%EA%B0%9C%EC%9A%94)로 구성되었으며 sequence-to-sequence 작업을 위해 고안되었다.        

전반적으로, 본 논문 모델의 encoder-decoder Transformer의 구현은 원래 Transformer의 형태를 밀접하게 따른다.               

**[Transformer의 동작 과정]**       
* **1)** 토큰의 input sequence를 embeddings sequence를에 매핑한 다음 encoder로 전달한다.      
* **2)** encoder는  “blocks”의 스택으로 구성되며, 각 스택마다 self-attention layer 계층과 small feed-forward network를 갖고있다.     
* **3)** 각 스택 안의 2가지 요소들의 입력에는 Layer normalization가 적용된다.     
* **4)** 이후, activations이 재조정되고 추가 bias이 적용되지 않는 단순화된 버전의 Layer normalization를 사용한다.      
* **5)** layer normalization 후, residual skip connection은 각 스택 안의 2가지 요소의 입력을 출력에 추가한다.      
* **6)** 드롭아웃은 피드포워드 네트워크 내에서 스킵 연결, 주의 가중치 및 전체 스택의 입력 및 출력에 적용된다.   
* **7)** decoder는 encoder의 출력에 참여하는 각 self-attention layer 다음에 standard attention 메커니즘을 포함한다는 점을 제외하고는 인코더와 구조가 유사하다.       
decoder의 self-attention 메커니즘은 모델이 과거 출력에만 주의를 기울일 수 있도록 한다.     
* **8)** 최종 decoder 블록의 출력은 소프트맥스 출력을 가진  dense 레이어로 들어가며, 그 가중치는 입력 임베딩 매트릭스와 공유된다.      
* ✨ 여기서, 트랜스포머의 모든 attention 메커니즘은 추가로 처리되기 전에 출력이 연결되는 독립적인 “heads”로 나뉘어있다.          
![image](https://user-images.githubusercontent.com/76824611/132572962-94a60e8b-2182-466a-8d1d-47a86ee83a14.gif)
</div>
</details> 
  


### relative position embeddings
Transformer의 self-attention는 병렬처리를하여 순서 정보를 갖지 못하므로, 임베딩에 순서정보를 넣어준다.    
이 논문에서는 기존 Transformer와 다른 위치 임베딩을 사용하였다.     
* **기존 모델**: sinusoidal position signal or learned position embeddings (단어의 절대적 위치 정보 표현)           
* **T5**: [relative position embeddings](https://yerimoh.github.io/LAN18/) (단어의 상대적 위치 표현)     


      



<details>
<summary>📜 relative position embeddings를 자세히 알고 싶다면?(본 논문의 설명) </summary>
<div markdown="1">

각 위치에 대해 고정 임베딩을 사용하는 대신, relative position embeddings은 self-attention에서 비교되는 "key"와 "query" 사이의 오프셋에 따라 다른 학습된 임베딩을 생성한다.    

또한 효율성을 위해 **모델의 모든 레이어**에 걸쳐 **position embeddings parameters를 공유**하지만,     
**주어진 layer 내**에서 **각 attention head**는 **서로 다른 학습된 position embedding을 사용**한다.      

일반적으로, 각각 가능한 "key"와 "query" 오프셋 범위에 해당하는 **고정된 수의 embeddings**이 학습된다.      

이 연구에서, 우리는 로그적으로 **최대 128의 오프셋까지 크기가 증가**하는 범위를 가진 **모든 모델**에    
**32개의 embedding을 사용**하여 모든 relative position를 동일한 임베딩에 할당한다.      

특정 계층은 128개 토큰을 초과하는 relative position에 민감하지 않지만,     
subsequent 계층은 이전 계층의 로컬 정보를 결합하여 더 큰 오프셋에 민감하게 반응할 수 있다.

</div>
</details> 
  
**[summary: T5와 기존 Transformer의 차이점]**       
아래를 제외하고 기존 Transformer와 동일       
* T5는 Layer Norm bias를 제거함    
* Layer normalization를 residual path 외부에 배치     
* 다른 position embedding 방식 사용(relative position embeddings)    

이러한 아키텍처 변화는 transfer learning에 대한 경험적 조사에서 고려하는 실험 요소와 직교하기 때문에,  
우리는 향후 작업을 위해 영향의 절제를 남겨둠둠.  
  
### 실험  
* T5의 확장성(scalability)실험    
➡ 즉 더 많은 parameters나 layers을 가질수록 성능이 어떻게 변하는지 실험한다.      
* 결과적으로, 우리는 모델과 데이터 병렬화를 결합하여 “slices” of Cloud TPU Pods"에서 모델을 훈련시킴.       



----

##  2.2 The Colossal Clean Crawled Corpus
데이터는  **large unlabeled** data sets for **unsupervised learning**를 사용한다.    

텍스트 데이터 소스는 **Common Crawl**을 사용한다


<details>
<summary>📜 Common Crawl을 자세히 알고 싶다면? </summary>
<div markdown="1">
  
Common Crawl이란    
* 일반적으로 사용 가능한 웹 아카이브임     
* 스크랩된 HTML 파일에서 마크업 및 기타 비텍스트 콘텐츠를 제거하여 "웹 추출 텍스트"를 제공      
* 이 프로세스는 매달 약 20TB의 스크랩된 텍스트 데이터를 생성.     


preprocessing     
* Common Crawl의 문제     
   * 불행하게도, Common Crawl의 결과 텍스트의 대부분은 자연어가 아니다.     
   * 오류 메시지 또는 중복 텍스트와 같은 횡설수설하거나 boiler-plate 텍스트로 구성됨    
* 해결: 다음 휴리스틱을 사용     
   * 마침표, 느낌표, 물음표 또는 끝 따옴표로 끝나는 행만 추출     
   * 5개 미만의 문장이 있는 페이지는 폐기하고 최소 3개의 단어가 포함된 행만 추출      
   * "더티, 장난, 외설 또는 기타 나쁜 단어 목록"에 있는 모든 단어가 포함된 페이지를 제거          
   * 대부분의 스크랩 페이지에는 Javascript를 활성화해야 한다는 경고가 포함되어 있어 Javascript라는 단어가 있는 줄은 모두 제거          
   * 일부 페이지에는 플레이스홀더 "lorem ipsum" 텍스트가 포함되어 있으며, "lorem ipsum"이라는 문구가 나타나는 페이지는 모두 제거      
   *  "{"가 포함된 모든 페이지를 제거     
   *  데이터 세트의 중복을 제거하기 위해 데이터 세트에서 두 번 이상 발생하는 세 문장 범위 중 하나를 제외하고 모두 삭제     


</div>
</details> 

-----

## 2.3 Downstream Tasks
* Sentence acceptability judgment (CoLA (Warstadt et al., 2018))     
* Sentiment analysis (SST-2 (Socher et al., 2013))    
* Paraphrasing/sentence similarity (MRPC (Dolan and Brockett, 2005), STS-B (Ceret al., 2017), QQP (Iyer et al., 2017))    
* Natural language inference (MNLI (Williams et al., 2017), QNLI (Rajpurkar et al.,2016), RTE (Dagan et al., 2005), CB (De Marneff et al., 2019))      
* Coreference resolution (WNLI and WSC (Levesque et al., 2012))   
* Sentence completion (COPA (Roemmele et al., 2011))   
* Word sense disambiguation (WIC (Pilehvar and Camacho-Collados, 2018))   
* Question answering (MultiRC (Khashabi et al., 2018), ReCoRD (Zhang et al., 2018), BoolQ (Clark et al., 2019))      


------

## 2.4 Input and Output Format

In order to train a single model on the diverse set of tasks described above, we cast all of
the tasks we consider into a “text-to-text” format—that is, a task where the model is fed
some text for context or conditioning and is then asked to produce some output text. This
framework provides a consistent training objective both for pre-training and fine-tuning.
Specifically, the model is trained with a maximum likelihood objective (using “teacher forcing”
(Williams and Zipser, 1989)) regardless of the task. To specify which task the model should
perform, we add a task-specific (text) prefix to the original input sequence before feeding it
to the model.



As an example, to ask the model to translate the sentence “That is good.” from English
to German, the model would be fed the sequence “translate English to German: That is
good.” and would be trained to output “Das ist gut.” For text classification tasks, the
model simply predicts a single word corresponding to the target label. For example, on the
MNLI benchmark (Williams et al., 2017) the goal is to predict whether a premise implies
(“entailment”), contradicts (“contradiction”), or neither (“neutral”) a hypothesis. With
our preprocessing, the input sequence becomes “mnli premise: I hate pigeons. hypothesis:
My feelings towards pigeons are filled with animosity.” with the corresponding target word
“entailment”. Note that an issue arises if our model outputs text on a text classification
task that does not correspond to any of the possible labels (for example if the model
outputs “hamburger” when the only possible labels for a task were “entailment”, “neutral”,
or “contradiction”). In this case, we always count the model’s output as wrong, though we
never observed this behavior in any of our trained models. Note that the choice of text prefix
used for a given task is essentially a hyperparameter; we found that changing the exact
wording of the prefix had limited impact and so did not perform extensive experiments into
different prefix choices. A diagram of our text-to-text framework with a few input/output
examples is shown in Figure 1. We provide full examples of preprocessed inputs for every
task we studied in Appendix D



Our text-to-text framework follows previous work that casts multiple NLP tasks into
a common format: McCann et al. (2018) propose the “Natural Language Decathlon”, a
benchmark that uses a consistent question-answering format for a suite of ten NLP tasks.
The Natural Language Decathlon also stipulates that all models must be multi-task, i.e.
are able to simultaneously tackle all of the tasks at once. We instead allow for separately
fine-tuning the model on each individual task and use short task prefixes instead of an explicit
question-answer format. Radford et al. (2019) evaluate the zero-shot learning capabilities of
language models by feeding some input to the model as a prefix and then autoregressively
sampling an output. For example, automatic summarization is done by feeding in a document
followed by the text “TL;DR:” (short for “too long, didn’t read”, a common abbreviation)
and then the summary is predicted via autoregressive decoding. We mainly consider models
that explicitly process an input with an encoder before generating an output with a separate
decoder and we focus on transfer learning rather than zero-shot learning. Finally, Keskar
et al. (2019b) unify many NLP tasks as “span extraction”, where text corresponding to
possible output choices are appended to the input and the model is trained to extract the
input span corresponding to the correct choice. In contrast, our framework also allows for
generative tasks like machine translation and abstractive summarization where it is not
possible to enumerate all possible output choices.


We were able to straightforwardly cast all of the tasks we considered into a text-to-text
format with the exception of STS-B, which is a regression task where the goal is to predict
a similarity score between 1 and 5. We found that most of these scores were annotated
in increments of 0.2, so we simply rounded any score to the nearest increment of 0.2 and
converted the result to a literal string representation of the number (e.g. the floating-point
value 2.57 would be mapped to the string “2.6”). At test time, if the model outputs a
string corresponding to a number between 1 and 5, we convert it to a floating-point value;
otherwise, we treat the model’s prediction as incorrect. This effectively recasts the STS-B
regression problem as a 21-class classification problem.


Separately, we also convert the Winograd tasks (WNLI from GLUE, WSC from SuperGLUE, and the DPR data set we add to SuperGLUE) into a simpler format that is more
amenable to the text-to-text framework. Examples from the Winograd tasks consist of a
text passage containing an ambiguous pronoun that could refer to more than one of the noun
phrases in the passage. For example, the passage might be “The city councilmen refused
the demonstrators a permit because they feared violence.”, which contains the ambiguous
pronoun “they” that could refer to “city councilmen” or “demonstrators”. We cast the WNLI,
WSC, and DPR tasks as text-to-text problems by highlighting the ambiguous pronoun in
the text passage and asking the model to predict the noun that it refers to. The example
mentioned above would be transformed to the input “The city councilmen refused the
demonstrators a permit because *they* feared violence.” and the model would be trained to
predict the target text “The city councilmen


For WSC, examples contain the passage, the ambiguous pronoun, a candidate noun,
and a True/False label reflecting whether the candidate matches the pronoun (ignoring any
articles). We only train on examples with a “True” label since we do not know the correct
noun targets for examples with a “False” label. For evaluation, we assign a “True” label if
the words in the model’s output are a subset of the words in the candidate noun phrase
(or vice versa) and assign a “False” label otherwise. This removes roughly half of the WSC
training set, but the DPR data set adds about 1,000 pronoun resolution examples. Examples
from DPR are annotated with the correct referent noun, making it easy to use this data set
in the format listed above.


For WSC, examples contain the passage, the ambiguous pronoun, a candidate noun,
and a True/False label reflecting whether the candidate matches the pronoun (ignoring any
articles). We only train on examples with a “True” label since we do not know the correct
noun targets for examples with a “False” label. For evaluation, we assign a “True” label if
the words in the model’s output are a subset of the words in the candidate noun phrase
(or vice versa) and assign a “False” label otherwise. This removes roughly half of the WSC
training set, but the DPR data set adds about 1,000 pronoun resolution examples. Examples
from DPR are annotated with the correct referent noun, making it easy to use this data set
in the format listed above. practice (Devlin et al., 2018) due to the fact that it is “adversarial” with respect to the
training set, i.e. validation examples are all slightly-perturbed versions of training examples
with the opposite label. As such, we do not include WNLI in the average GLUE score
whenever we report on the validation set (all sections except Section 3.7 where results
are presented on the test sets). Converting examples from WNLI to the “referent noun
prediction” variant described above is a little more involved; we describe this process in
Appendix B.






















