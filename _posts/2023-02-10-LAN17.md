---
title: "IS REINFORCEMENT LEARNING (NOT) FOR NATURAL LANGUAGE PROCESSING?: BENCHMARKS, BASELINES, AND BUILDING BLOCKS FOR NATURAL LANGUAGE POLICY OPTIMIZATION 정리"
date:   2023-02-10
excerpt: "IS REINFORCEMENT LEARNING (NOT) FOR NATURAL LANGUAGE PROCESSING?: BENCHMARKS, BASELINES, AND BUILDING BLOCKS FOR NATURAL LANGUAGE POLICY OPTIMIZATION paper review"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---

# 목차


# ABSTRACT
**[LM with human preferences에 RL 적용]**     
* 본 논문은 인간 선호도에 맞추는 pre-trained large language models (LMs) 문제를 해결한다.        
* 텍스트 생성을 순차적 decision-making problem로 본다면, **강화 학습(RL)은 적절한 conceptual 프레임워크**가 될 수 있다.         

**[단점]**    
* 그러나 LM 기반 생성을 위해 RL을 사용하는 것은 combinatorial action space으로 인한 **training 불안전성** 존재     
* LM alignment을 위해 맞춤화된 **오픈 소스 라이브러리와 벤치마크의 부족**을 포함한 경험적 challenges에 직면한다.     
➡ 따라서, <span style="background-color:#fff5b1"> RL이 NLP를 위한 실용적인 패러다임인가? </span>라는 질문이  제기된다.


**[단점 해결]**    
이에 대한 답변을 돕기 위해 아래 3가지 방법을 제시한다,     
* **RL4LMs를 소개**   
  * **RL로 언어 생성기를 optimizing**하기 위한 오픈 소스 모듈식 라이브러리     
  * 라이브러리는 **임의의 reward 함수를 사용**하여 HuggingFace 라이브러리(Wolf et al., 2020)에서 encoder 또는 encoder-decoder **LM을 훈련하는 데 사용할 수 있는 on-policy RL algorithms으로 구성**된다.     
* **GRUE (General Reinforced-language Understanding Evaluation) benchmark 제시**     
  * target 문자열이 아니라 **인간 선호도의 자동화된 측정**을 capture하는 reward 기능에 의해 supervised되는 6개 언어 생성 작업 세트      
  * GRUE는 NLP 작업에 대한 RL algorithms의 첫 번째 leaderboard-style evaluation이다.      
* **NLPO(Natural Language Policy Optimization) 소개**    
   * 언어 생성에서 **combinatorial action space을 효과적으로 줄이는 방법**을 배우는 사용하기 쉬운 성능의 RL algorithm       

**[결과]**    
**1)** **RL 기술**이 일반적으로 인간 선호도에 LM을 맞추는 데 **supervised 방법보다 낫다**는 것을 보여준다.     
**2)** **NLPO가** 자동 및 인간 평가를 모두 기반으로 previous policy gradient methods **(PPO)보다 더 큰 안정성과 성능**을 나타낸다       




-----

# 1 INTRODUCTION
언어 기술의 궁극적인 목표는 **인간과 상호작용**하는 것이다.     
**[단점]**    
* 그러나 대부분의 언어 모델은 **인간 선호도의 직접적인 신호 없이 훈련**되며, supervised target 문자열은 (때로는 조잡한) proxy 역할을 한다.     
* 사용자 피드백을 통합하는 한 가지 옵션은 human-in-the-loop를 통한 것이다.   
* 즉, 모델이 훈련할 때 사용자는 각 샘플에 대한 피드백을 온라인으로 제공해야 하지만, 이 정도의 **밀도 높은 supervision**은 종종 **금지**되고 **비효율**적이다.     


**[해결: Automated metrics]**    
* 위의 문제에 대해 <span style="background-color:#DCFFE4">Automated metrics</span>은 괜찮은 절충안을 제공한다.    
   * pairwise learned preference models(Ouyang et al., 2022), BERTScore(Zhang et al., 2019), BLEURT(Sellam et al., 2020)는 이전 메트릭(BLEU, METERE et al.)에 비해 **인간 판단과의 상관관계가 크게 개선** 되었으며 **평가 비용이 저렴**하다.     

**[Automated metrics의 문제]**     
* 그러나 이러한 기능은 일반적으로 **per-token별로 차별화 불과**.       
* 사람과 마찬가지로 메트릭은 **전체 generations**에 대한 **품질 추정치만 제공** 가능      


**[해결: RL]**      
* <span style="background-color:#DCFFE4">강화 학습(RL)</span>은 **순차적 의사 결정 문제로 캐스팅**될 때 LM 기반 생성에 대한 **미분 불가능한 스칼라 목표를 최적화**하기 위한 자연스러운 경로를 제공한다.     
* 최근 연구는 **preference-based rewards을 제한**하여 **fluency 개념을 통합**함으로써 RL을 통해 LM을 인간 선호도에 맞추는 유망한 결과를 보여주었지만,     
⚠️ 이러한 방식의 진전은 **오픈 소스 벤치마크와 알고리즘 구현의 부족**으로 인해 방해받고 있다.     



**[본 논문]**     
LM을 더 잘 정렬하기 위해 RL 알고리듬을 구축하는 연구를 용이하게 하기 위해 라이브러리, 벤치마크 및 알고리듬을 출시한다. 먼저 PPO/A2C 등과 같은 다양한 기존 RL 방법을 사용하여 생성적 HuggingFace 모델(예: GPT-2 또는 T5)을 훈련할 수 있는 RL4LMs 라이브러리를 릴리스한다. 다음으로, 우리는 RL4를 사용하여 훈련된 모델을 적용한다새로운 GRUE(일반 강화 언어 이해 평가) 벤치마크에 대한 LM: GRUE는 7개의 현대 NLP 작업의 모음이다(자세한 내용은 표 1 참조). 다른 벤치마크와 달리, 우리는 감독된 훈련 대신 각 작업을 보상 기능과 쌍으로 구성한다. GRUE는 유창한 언어 생성기를 유지하면서 이러한 보상 기능을 최적화하기 위해 모델에 도전한다. 우리는 보상을 최적화하기 위해 작업별 감독 사전 훈련 유무에 관계없이 RL을 통해 언어 모델을 훈련한다. 마지막으로, 기존 RL 방법을 넘어 토큰 수준에서 언어 분포에 대한 작업별 제약 조건을 동적으로 학습하는 NLPO(Natural Language Policy Optimization)라는 새로운 정책 RL 알고리듬을 소개한다.
To facilitate research in building RL algorithms to better align LMs, we release a library, a benchmark, and an algorithm. First, we release the RL4LMs library, which enables generative HuggingFace models (e.g., GPT-2 or T5) to be trained using a variety of existing RL methods like PPO/A2C/etc. Next, we apply models trained using RL4LMs to the new GRUE (General Reinforced-language Understanding Evaluation) benchmark: GRUE is a collection of 7 contemporary NLP tasks (see Table1 for details); in contrast to other benchmarks, instead of supervised training, we pair each task with reward function(s). GRUE challenges models to optimize these reward functions while remaining fluent language generators. We train language models via RL—both with and without task specific supervised pre-training—to optimize rewards. Finally, beyond existing RL methods, we introduce a novel on-policy RL algorithm called NLPO (Natural Language Policy Optimization), that dynamically learns task-specific constraints over the distribution of language at a token level.


GRUE와 인간 평가에 대한 실험은 NLPO가 PPO를 포함한 대안에 비해 언어 유창성을 유지하면서 학습 선호 보상의 균형을 더 잘 유지한다는 것을 보여준다(그림 1). 우리는 스칼라 보상 피드백에서 학습하기 위해 RL을 사용하는 것이 더 나을 수 있음을 발견했다. (1) 지도 학습을 통한 추가 전문가 시연을 사용하는 것보다 데이터 효율성이 더 높을 수 있다. 학습된 보상 함수는 5배 더 많은 데이터로 훈련된 지도 방법보다 RL 방법에 대한 신호로 사용될 때 더 큰 성능을 가능하게 한다nd (2) 매개 변수 효율성—감독과 NLPO의 조합으로 훈련된 2억 2천만 개의 매개 변수 모델이 30억 개의 감독 모델을 능가할 수 있도록 한다. 우리는 우리가 공개한 벤치마크, 기준선 및 빌딩 블록이 LM을 인간 선호도에 맞추는 연구를 추진하는 역할을 하기를 바란다.
Experiments on GRUE and human evaluations show that NLPO better balances learning preference rewards while maintaining language fluency compared to alternatives, including PPO (Figure 1). We find that using RL to learn from scalar reward feedback can be more: (1) data efficient than using additional expert demonstrations via supervised learning (though a combination of both is best)—a learned reward function enables greater performance when used as a signal for an RL method than a supervised method trained with 5 times more data, and (2) parameter efficient—enabling a 220 million parameter model trained with a combination of supervision and NLPO to outperform a 3 billion supervised model. We hope that the benchmarks, baselines, and building blocks we release serve to drive forward research in aligning LMs to human preferences.


----
----

# 3 RL4LMS: A LIBRARY FOR TRAINING LMS WITH RL
We introduce RL4LMs, an open-source library with building blocks for fine-tuning and evaluating
RL algorithms on LM-based generation. The library is built on HuggingFace (Wolf et al., 2020)
and stable-baselines-3 (Raffin et al., 2021), combining important components from their interfaces.
RL4LMs can be used to train any decoder only or encoder-decoder transformer models from HuggingFace with any on-policy RL algorithm from stable-baselines-3. Furthermore, we provide reliable
implementations of popular on-policy RL algorithms that are tailored for LM fine-tuning such as
PPO (Schulman et al., 2017), TRPO (Schulman et al., 2015a), A2C (Mnih et al., 2016), and our own
NLPO (§4). The library is modular, which enables users to plug-in customized environments, reward
functions, metrics, and algorithms. In the initial release, we provide support for 6 different NLP tasks,
16 evaluation metrics and rewards, and 4 RL algorithms.


# 3.1 ENVIRONMENTS: GENERATION AS A TOKEN-LEVEL MDP
Each environment is an NLP task: we are given a supervised dataset D = {(x
i
, y
i
)}
N
i=1 of N
examples, where x ∈ X is an language input and y ∈ Y is the target string. Generation can be
viewed as a Markov Decision Process (MDP) hS, A, R, P, γ, Ti using a finite vocabulary V. Each
episode in the MDP begins by sampling a datapoint (x, y) from our dataset and ends when the
current time step t exceeds the horizon T or an end of sentence (EOS) token is generated. The
input x = (x0, · · · , xm) is a task-specific prompt that is used as our initial state s0 = (x0, · · · , xm),
where s0 ∈ S and S is the state space with xm ∈ V. An action in the environment at ∈ A consists of
a token from our vocabulary V. The transition function P : S × A → S deterministically appends an
action at to the end of the state st−1 = (x0, · · · , xm, a0, · · · , at−1). This continues until the end of
the horizon t ≤ T and we obtain a state sT = (x0, · · · , xm, a0, · · · , aT ). At the end of an episode a
reward R : S × A × Y → R
1
that depends on the (sT , y) (e.g., an automated metric like PARENT
Dhingra et al. (2019)) is emitted. RL4LMs provides an OpenAI gym (Brockman et al., 2016) style
API for an RL environment that simulates this LM-Based MDP formulation. Abstracting the details
of the MDP environment structure allows for new tasks to be added quickly with compatibility across
all implemented algorithms.



## 3.2 REWARD FUNCTIONS AND EVALUATION METRICS
Because RL4LMs provides a generic interface for per-token or per-sequence generation rewards, it
is possible to quickly apply a wide array of RL algorithms to a similarly diverse range of textual
metrics-as-rewards. Specifically, we provide interfaces to 1) n-gram overlap metrics metrics such
as ROUGE (Lin, 2004), BLEU (Papineni et al., 2002), SacreBLEU (Post, 2018), METEOR (Banerjee
& Lavie, 2005); (2) model-based semantic metrics such as BertScore (Zhang et al., 2019) and
BLEURT (Sellam et al., 2020) which generally provide higher correlation with human judgment;
3) task-specific metrics such as CIDER (Vedantam et al., 2015), SPICE (Anderson et al., 2016)
(for captioning/commonsense generation), PARENT (Dhingra et al., 2019) (for data-to-text) and
SummaCZS (Laban et al., 2022) (for factuality of summarization); 4) diversity/fluency/naturalness
metrics such as perplexity, Mean Segmented Type Token Ratio (MSSTR) (Johnson, 1944), Shannon
entropy over unigrams and bigrams (Shannon, 1948), the ratio of distinct n-grams over the total
number of n-grams (Distinct-1, Distinct-2) and count of n-grams that appear only once in the entire
generated text (Li et al., 2015); 5) task-specific, model-based human preference metrics such as
classifiers trained on human preference data collected in the methodology of Ouyang et al. (2022).


## 3.3 ON-POLICY ACTOR-CRITIC ALGORITHMS
RL4LMs supports fine-tuning and training LMs from scratch via on-policy actor-critic algorithms
on language environments. Formally, this class of algorithms allows us to train a parameterized
control policy defined as πθ : S → A, a function that attempts to select an action in a given
state so as to maximize long term discounted rewards over a trajectory Eπ[
PT
t=0 γ
tR(st, at)]. Our
benchmark experiments focus on fine-tuning a pre-trained LM denoted as π0 from which we initial
our agent’s policy πθ = π0. Similarly, the value network Vφ used to estimate the value function is
also initialized from π0 except for the final layer which is randomly initialized to output a single scalar
value. As with other deep RL actor-critic algorithms, we define our value and Q-value functions as
V
π
t = Eat∼π[
PT
τ=t
γR(sτ , aτ , y)], Qπ
t
(st, at) = R(st, at, y) + γEst+1∼P [V
π
t+1(st+1)] leading to
a definition of our advantage function as Aπ
t
(s, a) = Qπ
t
(s, a) − V
π
t
. To increase training stability,
advantage is appoximated using Generalized Advantage Estimation (Schulman et al., 2015b).
Given an input-output pair (x, y) and generation predictions from our agent; because the environment
rewards are sequence-level and sparse, following Wu et al. (2021) we regularize the reward function
using a token-level KL penalty for all on-policy algorithms, to prevent the model from deviating too
far from the initialized LM π0. Formally, the regularized reward function is:
Rˆ(st, at, y) = R(st, at, y) − βKL (πθ(at|st)||π0(at|st)) (1)
where Rˆ is the regularized KL reward, y is gold-truth predictions, KL (πθ(at|st)||π0(at|st)) =
(log π0(at|st) − log πθ(at|st)) and the KL coefficient β is dynamically adapted (Ziegler et al., 2019).
Further details on actor-critic methods can be found in Appendix A.


