---
title: "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer 정리"
date:   2023-02-10
excerpt: "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer paper review"
category: #[Paper]
layout: post
tag:
#- Paper
order: 0

comments: true
---


# Abstract
<span style="background-color:#F5F5F5">**[논문 배경]**</span>    
모델이 Downstream Task에서 fine-tuning되기 전에 data-rich task에서 pre-train을 하는 것은  자연어 처리(NLP)의 강력한 기술로 부상했다.     
즉 이러한 [transfer learning](https://yerimoh.github.io/DL12/)은 NLP에 큰 발전을 가져왔다.    

<span style="background-color:#F5F5F5">**[논문의 목적]**</span>    
* 모든 텍스트 기반 언어 문제를 text-to-text 형식으로 변환하는 통합 프레임워크를 도입하여 <span style="background-color:#fff5b1">NLP에 대한 [transfer learning](https://yerimoh.github.io/DL12/)를 전반적으로 탐구</span>한다.     
* 위 탐구를 통해 얻은 통찰력과 규모 및 본 논문에서 제안할 <span style="background-color:#fff5b1">"Colossal Clean Crawled Corpus"를 결합</span>하여 많은 NLP evaluation에서 SOTA 성능을 보였다.        
➡ 공개한 [data set, pre-trained models, and code](https://github.com/google-research/text-to-text-transfer-transformer) 링크를 첨부하였다.        


<details>
<summary>📜 Downstream Task 의미 보기</summary>
<div markdown="1">
  

구체적으로 풀고 싶은 문제들을 말한다.

NLP에서는 언어모델을 pre-train방식을 이용해 학습을 진행하고,    
그 후에 원하고자 하는 task를 fine-tuning하는 방식을 통해 모델을 업데이트 하는 방식을 사용하는데 이때, task를 Downstream Task라 한다.

예를들어, BERT의 언어모델을 질의응답 Task라인 squad를 학습한다고 할때, 이때 질의응답 Task를 다운스트림 Task로 볼 수 있을것이다.  
  
  
</div>
</details>  

----
----



# Introduction

<span style="background-color:#F5F5F5">**[최근 연구 동향]**</span>    
* 자연어 처리(NLP)작을 을 위한 train이 가능하여면 모델이 **downstream learning에 적합한 방식으로 텍스트를 처리**할 수 있어야 한다.              
그런데 이는 **모델이 텍스트를 "이해"할 수 있게 학습한다고 보기엔 힘들다**.               
➡ 현대 기계 학습 관행에서 이러한 train이 명시적으로 수행되는 경우가 거의 없어, 대신 **Auxiliary Task**(본 task는 아니지만, 본 task에서의 성능이 더 잘 나올 수 있도록 도와주는 보조 task)**의 일부**로 학습되는 경우가 많다.          
( 최근에는 **데이터가 풍부한 작업**에 대해 **전체 모델을 pre-train하는 것**이 점점 더 **일반화**되고 있다.     
이상적으로, 이 pre-train은 모델이 범용 능력과 지식을 개발하게 하고, 이를 다운스트림 작업으로 이전할 수 있도록 한다.     
➡ 더 큰 데이터 세트에서 더 큰 모델을 훈련시키는 것만으로 더 나은 성능을 달성할 수 있다.    




<span style="background-color:#F5F5F5">**[현 경향으로 인한 한계]**</span>    
* 이러한 경향으로 인해 NLP에 대한 **transfer learning 방법론을 개발**하는 연구들이 활발하게 이루어졌다.      
이렇게 이 분야가 급성장하여 연구가 활발해지니 아래와 같은 작업들이 어려워졌다.       
  * 여러 algorithms을 비교    
  * 새로운 contributions의 효과 파악    
  * transfer learning을 위한 기존 방법의 space 이해        


➡ <span style="background-color:#fff5b1">그래서 본 논문은 이 분야의 원활한 이해를 위해, **다양한 접근법을 체계적으로 연구**하고 **필드의 현재 한계를 밀어낼 수 있는 transfer learning에 대한 통합된 접근법을 활용**한다.</span>    
 


<span style="background-color:#F5F5F5">**[본 논문의 해결책]**</span>      
* 본 논문 work의 기본 아이디어는 <span style="background-color:#fff5b1">모든 텍스트 처리 문제를 **“text-to-text” 문제로 처리**</span>하는 것이다.             
즉, **텍스트를 입력으로 받아들이고 새로운 텍스트를 출력으로 생성**하는 것이다.          
    * 이러한 텍스트 간 프레임워크는 우리가 고려하는 모든 작업에 **동일한 모델, 목표, 훈련 절차 및 decoding 프로세스를 직접 적용**할 수 있게 한다.          
    * 본 논문은 질문 답변, 문서 요약 및 감정 분류를 포함한 다양한 영어 기반 NLP 문제에 대한 성능을 평가하여 이러한 **유연성을 활용**한다.        
    * 이 통합 접근법을 통해, 우리는 다양한 transfer learning의 target, 레이블이 지정되지 않은 데이터 세트 및 기타 요인의 효과를 비교하는 동시에 이전에 고려되었던 것 이상으로 **모델과 데이터 세트를 확장**하여 NLP에 대한 **transfer learning의 한계를 탐색**할 수 있다.    




We emphasize that our goal is not to propose new methods but instead to provide a
comprehensive perspective on where the field stands. As such, our work primarily comprises
a survey, exploration, and empirical comparison of existing techniques. We also explore the
limits of current approaches by scaling up the insights from our systematic study (training
models up to 11 billion parameters) to obtain state-of-the-art results in many of the tasks
we consider. In order to perform experiments at this scale, we introduce the “Colossal Clean
Crawled Corpus” (C4), a data set consisting of hundreds of gigabytes of clean English text
scraped from the web. Recognizing that the main utility of transfer learning is the possibility
of leveraging pre-trained models in data-scarce settings, we release our code, data sets, and
pre-trained models.1




The remainder of the paper is structured as follows: In the following section, we discuss
our base model and its implementation, our procedure for formulating every text processing
problem as a text-to-text task, and the suite of tasks we consider. In Section 3, we present a
large set of experiments that explore the field of transfer learning for NLP. At the end of the
section (Section 3.7), we combine insights from our systematic study to obtain state-of-the-art
results on a wide variety of benchmarks. Finally, we provide a summary of our results and
wrap up with a look towards the future in Section 4.
















