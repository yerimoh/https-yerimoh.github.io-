---
title: "IS REINFORCEMENT LEARNING (NOT) FOR NATURAL LANGUAGE PROCESSING?: BENCHMARKS, BASELINES, AND BUILDING BLOCKS FOR NATURAL LANGUAGE POLICY OPTIMIZATION 정리"
date:   2023-02-10
excerpt: "IS REINFORCEMENT LEARNING (NOT) FOR NATURAL LANGUAGE PROCESSING?: BENCHMARKS, BASELINES, AND BUILDING BLOCKS FOR NATURAL LANGUAGE POLICY OPTIMIZATION paper review"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---

# 목차


# ABSTRACT
**[LM with human preferences에 RL 적용]**     
* 본 논문은 인간 선호도에 맞추는 pre-trained large language models (LMs) 문제를 해결한다.        
* 텍스트 생성을 순차적 decision-making problem로 본다면, **강화 학습(RL)은 적절한 conceptual 프레임워크**가 될 수 있다.         

**[단점]**    
* 그러나 LM 기반 생성을 위해 RL을 사용하는 것은 combinatorial action space으로 인한 **training 불안전성** 존재     
* LM alignment을 위해 맞춤화된 **오픈 소스 라이브러리와 벤치마크의 부족**을 포함한 경험적 challenges에 직면한다.     
➡ 따라서, <span style="background-color:#fff5b1"> RL이 NLP를 위한 실용적인 패러다임인가? </span>라는 질문이  제기된다.


**[단점 해결]**    
이에 대한 답변을 돕기 위해 아래 3가지 방법을 제시한다,     
* **RL4LMs를 소개**   
  * **RL로 언어 생성기를 optimizing**하기 위한 오픈 소스 모듈식 라이브러리     
  * 라이브러리는 **임의의 reward 함수를 사용**하여 HuggingFace 라이브러리(Wolf et al., 2020)에서 encoder 또는 encoder-decoder **LM을 훈련하는 데 사용할 수 있는 on-policy RL algorithms으로 구성**된다.     
* **GRUE (General Reinforced-language Understanding Evaluation) benchmark 제시**     
  * target 문자열이 아니라 **인간 선호도의 자동화된 측정**을 capture하는 reward 기능에 의해 supervised되는 6개 언어 생성 작업 세트      
  * GRUE는 NLP 작업에 대한 RL algorithms의 첫 번째 leaderboard-style evaluation이다.      
* **NLPO(Natural Language Policy Optimization) 소개**    
   * 언어 생성에서 **combinatorial action space을 효과적으로 줄이는 방법**을 배우는 사용하기 쉬운 성능의 RL algorithm       

**[결과]**    
**1)** **RL 기술**이 일반적으로 인간 선호도에 LM을 맞추는 데 **supervised 방법보다 낫다**는 것을 보여준다.     
**2)** **NLPO가** 자동 및 인간 평가를 모두 기반으로 previous policy gradient methods **(PPO)보다 더 큰 안정성과 성능**을 나타낸다       

