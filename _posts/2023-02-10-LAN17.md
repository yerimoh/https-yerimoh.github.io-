---
title: "(T5) Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer 정리"
date:   2023-02-10
excerpt: "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer paper review"
category: #[Paper]
layout: post
tag:
#- Paper
order: 0

comments: true
---


# Abstract
<span style="background-color:#F5F5F5">**[논문 배경]**</span>    
모델이 Downstream Task에서 fine-tuning되기 전에 data-rich task에서 pre-train을 하는 것은  자연어 처리(NLP)의 강력한 기술로 부상했다.     
즉 이러한 [transfer learning](https://yerimoh.github.io/DL12/)은 NLP에 큰 발전을 가져왔다.    

<span style="background-color:#F5F5F5">**[논문의 목적]**</span>    
* 모든 텍스트 기반 언어 문제를 text-to-text 형식으로 변환하는 통합 프레임워크를 도입하여 <span style="background-color:#fff5b1">NLP에 대한 [transfer learning](https://yerimoh.github.io/DL12/)를 전반적으로 탐구</span>한다.     
* 위 탐구를 통해 얻은 통찰력과 규모 및 본 논문에서 제안할 <span style="background-color:#fff5b1">"Colossal Clean Crawled Corpus"를 결합</span>하여 많은 NLP evaluation에서 SOTA 성능을 보였다.        
➡ 공개한 [data set, pre-trained models, and code](https://github.com/google-research/text-to-text-transfer-transformer) 링크를 첨부하였다.        


<details>
<summary>📜 Downstream Task 의미 보기</summary>
<div markdown="1">
  

구체적으로 풀고 싶은 문제들을 말한다.

NLP에서는 언어모델을 pre-train방식을 이용해 학습을 진행하고,    
그 후에 원하고자 하는 task를 fine-tuning하는 방식을 통해 모델을 업데이트 하는 방식을 사용하는데 이때, task를 Downstream Task라 한다.

예를들어, BERT의 언어모델을 질의응답 Task라인 squad를 학습한다고 할때, 이때 질의응답 Task를 다운스트림 Task로 볼 수 있을것이다.  
  
  
</div>
</details>  

----
----



# Introduction

<span style="background-color:#F5F5F5">**[최근 연구 동향]**</span>    
자연어 처리(NLP)작을 을 위한 train이 가능하여면 모델이 **downstream learning에 적합한 방식으로 텍스트를 처리**할 수 있어야 한다.              
그런데 이는 **모델이 텍스트를 "이해"할 수 있게 학습한다고 보기엔 힘들다**.               
➡ 현대 기계 학습 관행에서 이러한 train이 명시적으로 수행되는 경우가 거의 없어, 대신 **Auxiliary Task의 일부**로 학습되는 경우가 많다.        


<details>
<summary>📜 Auxiliary Task 의미 보기</summary>
<div markdown="1">

본 task는 아니지만, 본 task에서의 성능이 더 잘 나올 수 있도록 도와주는 보조 task  

</div>
</details> 
  
최근에는 **데이터가 풍부한 작업**에 대해 **전체 모델을 pre-train하는 것**이 점점 더 **일반화**되고 있다.     
이상적으로, 이 pre-train은 모델이 범용 능력과 지식을 개발하게 하고, 이를 다운스트림 작업으로 이전할 수 있도록 한다.     
➡ 더 큰 데이터 세트에서 더 큰 모델을 훈련시키는 것만으로 더 나은 성능을 달성할 수 있다.    




<span style="background-color:#F5F5F5">**[현 경향으로 인한 한계]**</span>    
이러한 경향으로 인해 NLP에 대한 **transfer learning 방법론을 개발**하는 연구들이 활발하게 이루어졌다.      
이렇게 이 분야가 급성장하여 연구가 활발해지니 아래와 같은 작업들이 어려워졌다.       
* 여러 algorithms을 비교    
* 새로운 contributions의 효과 파악    
* transfer learning을 위한 기존 방법의 space 이해        


✔ 그래서 본 논문은 이 분야의 원활한 이해를 위해, **다양한 접근법을 체계적으로 연구**하고 **필드의 현재 한계를 밀어낼 수 있는 transfer learning에 대한 통합된 접근법을 활용**한다. 
 


<span style="background-color:#F5F5F5">**[본 논문의 해결책]**</span>      
본 논문 work의 기본 아이디어는 <span style="background-color:#fff5b1">모든 텍스트 처리 문제를 **“text-to-text” 문제로 처리**</span>하는 것이다.             
즉, **텍스트를 입력으로 받아들이고 새로운 텍스트를 출력으로 생성**하는 것이다.          
* 이러한 텍스트 간 프레임워크는 우리가 고려하는 모든 작업에 **동일한 모델, 목표, 훈련 절차 및 decoding 프로세스를 직접 적용**할 수 있게 한다.          
* 본 논문은 질문 답변, 문서 요약 및 감정 분류를 포함한 다양한 영어 기반 NLP 문제에 대한 성능을 평가하여 이러한 **유연성을 활용**한다.        
* 이 통합 접근법을 통해, 우리는 다양한 transfer learning의 target, 레이블이 지정되지 않은 데이터 세트 및 기타 요인의 효과를 비교하는 동시에 이전에 고려되었던 것 이상으로 **모델과 데이터 세트를 확장**하여 NLP에 대한 **transfer learning의 한계를 탐색**할 수 있다.    




본 논문은 본 논문의 목표가 새로운 방법을 제안하는 것이 아니라, **그 분야가 어디에 서 있는지에 대한 포괄적인 관점을 제공하는 것**임을 강조한다.      
즉, 우리의 작업은 본 목표를 달성하기 위해 아래와 같은 task로 구성된다.         
➡ 주로 기존 기술의 조사, 탐구 및 경험적 비교       


또한 본 논문은 다음과 같은 방식으로 **현재 접근 방식의 한계를 탐구**한다.        
본 논문이 고려하는 많은 task에서 SOTA 결과를 얻기 위해 체계적인 연구(train 모델을 최대 110억 개 parameters까지 확장)수행       
* 이 규모의 실험을 수행하기 위해 웹에서 긁어낸 수백 기가바이트의 clean 영어 텍스트로 구성된 데이터 세트인 "Colossal Clean Crawled Corpus"**(C4)** 를 소개한다.        
* **transfer learning의 핵심 기능**은, **데이터가 부족한 환경에서  pre-trained models을 활용할 수 있는 가능성**이라는 것을 인식하여,      
[코드, 데이터 세트 및 사전 훈련된 모델](https://github.com/google-research/text-to-text-transfer-transformer
)을 릴리스했다.    




<span style="background-color:#F5F5F5">**[논문 구성]**</span>            
* base model과 그 구현    
* 모든 text processing 문제를 text-to-text 작업으로 공식화하는 절차 및 고려하는 작업 모음에 대한 논의    
* 섹션 3에서, NLP에 대한 transfer learning 분야를 탐구하는 대규모 실험 세트를 제시          
* 섹션(섹션 3.7)의 끝에서, 우리는 체계적인 연구의 통찰력을 결합하여 광범위한 벤치마크에 대한 최첨단 결과를 얻음    
* 섹션 4에서, 결과에 대한 요약을 제공 한 뒤, 미래에 대한 전망으로 마무리             




---
---


# 2. Setup    



본 논문은 large-scale 경험적 연구의 결과를 제시하기 전에 아래와 같은 것들을 먼저 제시한다,   
* [Transformer 모델 아키텍처](https://yerimoh.github.io/Lan/)와 평가하는 downstream tasks을 포함하여 결과를 이해하는 데 필요한 **배경 주제를 검토**한다.        
* **모든 문제를 text-to-text task으로 처리하기 위한 접근 방식**을 소개    
* **"Colossal Clean Crawled Corpus"(C4) 설명:** 레이블이 지정되지 않은 텍스트 데이터의 소스로 생성한 공통 크롤 기반 데이터 세트임     




우리는 우리의 모델과 프레임워크를  <span style="background-color:#fff5b1">“Text-to-Text Transfer Transformer” (T5)</span> 라고 부른다.      





---


## 2.1 Model

### base architecture: “Transformer”
NLP에 대한 전이 학습에 대한 초기 결과는 반복 신경망을 활용했지만,  
최근에는 "Transformer" 아키텍처를 기반으로 한 모델을 사용하는 것이 더 일반화되었다.     


Transformer는 transfer learning에 효과적이므로, 이후 다양한 NLP 설정에 사용되었다.          
본 논문에서도 **모든 모델의 base를 Transformer 아키텍처**로 하였다.         

아래에 언급된 세부사항과 섹션 3.2에서 탐구한 변형을 제외하고,     
본 논문은 Transformer 아키텍처에서 크게 벗어나지 않는다.      


<details>
<summary>📜 Transformer를 자세히 알고 싶다면? (참고자료) </summary>
<div markdown="1">

이 논문(아키텍처)에 대해 더 자세히 알고싶다면 아래 자료들을 참고하자,    
* [원본 논문](https://arxiv.org/abs/1706.03762)    
* 후속 튜토리얼 3,4(뒤에 나옵니다.)    
* [본 포스트를 정리한 필자가 정리한 포스트](https://yerimoh.github.io/Lan/)         

</div>
</details> 
  85.5


그리고 본 논문에서 Transformer에대해 간단간단하게 설명해줬는데 원한다면 아래 자세한 설명 보기를 눌러 한 번 읽어보길 바란다.(그렇지만 원 논문을 읽어봐야 아래 내용들도 이해가 갈 것이다.)   

<details>
<summary>📜 Transformer를 자세히 알고 싶다면?(본 논문의 설명) </summary>
<div markdown="1">




 
**Transformer의 중요한 요소는,** [self-attention](https://yerimoh.github.io/Lan/#sub-layer1-self-attention)이다.         
self-attention은 각 요소를 나머지 sequence의 가중 평균으로 대체하여 시퀀스를 처리하는 attention의 변형이다.    

**Transformer의 구조는,** [encoder-decoder 아키텍처](https://yerimoh.github.io/Lan/#transformer-%EB%AA%A8%EB%8D%B8-%EA%B0%9C%EC%9A%94)로 구성되었으며 sequence-to-sequence 작업을 위해 고안되었다.        

전반적으로, 본 논문 모델의 encoder-decoder Transformer의 구현은 원래 Transformer의 형태를 밀접하게 따른다.               

**[Transformer의 동작 과정]**       
* **1)** 토큰의 input sequence를 embeddings sequence를에 매핑한 다음 encoder로 전달한다.      
* **2)** encoder는  “blocks”의 스택으로 구성되며, 각 스택마다 self-attention layer 계층과 small feed-forward network를 갖고있다.     
* **3)** 각 스택 안의 2가지 요소들의 입력에는 Layer normalization가 적용된다.     
* **4)** 이후, activations이 재조정되고 추가 bias이 적용되지 않는 단순화된 버전의 Layer normalization를 사용한다.      
* **5)** layer normalization 후, residual skip connection은 각 스택 안의 2가지 요소의 입력을 출력에 추가한다.      
* **6)** 드롭아웃은 피드포워드 네트워크 내에서 스킵 연결, 주의 가중치 및 전체 스택의 입력 및 출력에 적용된다.   
* **7)** decoder는 encoder의 출력에 참여하는 각 self-attention layer 다음에 standard attention 메커니즘을 포함한다는 점을 제외하고는 인코더와 구조가 유사하다.       
decoder의 self-attention 메커니즘은 모델이 과거 출력에만 주의를 기울일 수 있도록 한다.     
* **8)** 최종 decoder 블록의 출력은 소프트맥스 출력을 가진  dense 레이어로 들어가며, 그 가중치는 입력 임베딩 매트릭스와 공유된다.      
* ✨ 여기서, 트랜스포머의 모든 attention 메커니즘은 추가로 처리되기 전에 출력이 연결되는 독립적인 “heads”로 나뉘어있다.          
![image](https://user-images.githubusercontent.com/76824611/132572962-94a60e8b-2182-466a-8d1d-47a86ee83a14.gif)
</div>
</details> 
  


### relative position embeddings
Transformer의 self-attention는 병렬처리를하여 순서 정보를 갖지 못하므로, 임베딩에 순서정보를 넣어준다.    
이 논문에서는 기존 Transformer와 다른 위치 임베딩을 사용하였다.     
* **기존 모델**: sinusoidal position signal or learned position embeddings (단어의 절대적 위치 정보 표현)           
* **T5**: [relative position embeddings](https://yerimoh.github.io/LAN18/) (단어의 상대적 위치 표현)     


      



<details>
<summary>📜 relative position embeddings를 자세히 알고 싶다면?(본 논문의 설명) </summary>
<div markdown="1">

각 위치에 대해 고정 임베딩을 사용하는 대신, relative position embeddings은 self-attention에서 비교되는 "key"와 "query" 사이의 오프셋에 따라 다른 학습된 임베딩을 생성한다.    

또한 효율성을 위해 **모델의 모든 레이어**에 걸쳐 **position embeddings parameters를 공유**하지만,     
**주어진 layer 내**에서 **각 attention head**는 **서로 다른 학습된 position embedding을 사용**한다.      

일반적으로, 각각 가능한 "key"와 "query" 오프셋 범위에 해당하는 **고정된 수의 embeddings**이 학습된다.      

이 연구에서, 우리는 로그적으로 **최대 128의 오프셋까지 크기가 증가**하는 범위를 가진 **모든 모델**에    
**32개의 embedding을 사용**하여 모든 relative position를 동일한 임베딩에 할당한다.      

특정 계층은 128개 토큰을 초과하는 relative position에 민감하지 않지만,     
subsequent 계층은 이전 계층의 로컬 정보를 결합하여 더 큰 오프셋에 민감하게 반응할 수 있다.

</div>
</details> 
  
**[summary: T5와 기존 Transformer의 차이점]**       
아래를 제외하고 기존 Transformer와 동일       
* T5는 Layer Norm bias를 제거함    
* Layer normalization를 residual path 외부에 배치     
* 다른 position embedding 방식 사용(relative position embeddings)    

이러한 아키텍처 변화는 transfer learning에 대한 경험적 조사에서 고려하는 실험 요소와 직교하기 때문에,  
우리는 향후 작업을 위해 영향의 절제를 남겨둠둠.  
  
### 실험  
* T5의 확장성(scalability)실험    
➡ 즉 더 많은 parameters나 layers을 가질수록 성능이 어떻게 변하는지 실험한다.      
* 결과적으로, 우리는 모델과 데이터 병렬화를 결합하여 “slices” of Cloud TPU Pods"에서 모델을 훈련시킴.       



----

##  2.2 The Colossal Clean Crawled Corpus
데이터는  **large unlabeled** data sets for **unsupervised learning**를 사용한다.    

텍스트 데이터 소스는 **Common Crawl**을 사용한다


<details>
<summary>📜 Common Crawl을 자세히 알고 싶다면? </summary>
<div markdown="1">
  
**Common Crawl이란**    
* 일반적으로 사용 가능한 웹 아카이브임     
* 스크랩된 HTML 파일에서 마크업 및 기타 비텍스트 콘텐츠를 제거하여 "웹 추출 텍스트"를 제공      
* 이 프로세스는 매달 약 20TB의 스크랩된 텍스트 데이터를 생성.     


**preprocessing**     
* Common Crawl의 문제     
   * 불행하게도, Common Crawl의 결과 텍스트의 대부분은 자연어가 아니다.     
   * 오류 메시지 또는 중복 텍스트와 같은 횡설수설하거나 boiler-plate 텍스트로 구성됨    
* 해결: 다음 휴리스틱을 사용     
   * 마침표, 느낌표, 물음표 또는 끝 따옴표로 끝나는 행만 추출     
   * 5개 미만의 문장이 있는 페이지는 폐기하고 최소 3개의 단어가 포함된 행만 추출      
   * "더티, 장난, 외설 또는 기타 나쁜 단어 목록"에 있는 모든 단어가 포함된 페이지를 제거          
   * 대부분의 스크랩 페이지에는 Javascript를 활성화해야 한다는 경고가 포함되어 있어 Javascript라는 단어가 있는 줄은 모두 제거          
   * 일부 페이지에는 플레이스홀더 "lorem ipsum" 텍스트가 포함되어 있으며, "lorem ipsum"이라는 문구가 나타나는 페이지는 모두 제거      
   *  "{"가 포함된 모든 페이지를 제거     
   *  데이터 세트의 중복을 제거하기 위해 데이터 세트에서 두 번 이상 발생하는 세 문장 범위 중 하나를 제외하고 모두 삭제     


</div>
</details> 

-----

## 2.3 Downstream Tasks
* Sentence acceptability judgment (CoLA (Warstadt et al., 2018))     
* Sentiment analysis (SST-2 (Socher et al., 2013))    
* Paraphrasing/sentence similarity (MRPC (Dolan and Brockett, 2005), STS-B (Ceret al., 2017), QQP (Iyer et al., 2017))    
* Natural language inference (MNLI (Williams et al., 2017), QNLI (Rajpurkar et al.,2016), RTE (Dagan et al., 2005), CB (De Marneff et al., 2019))      
* Coreference resolution (WNLI and WSC (Levesque et al., 2012))   
* Sentence completion (COPA (Roemmele et al., 2011))   
* Word sense disambiguation (WIC (Pilehvar and Camacho-Collados, 2018))   
* Question answering (MultiRC (Khashabi et al., 2018), ReCoRD (Zhang et al., 2018), BoolQ (Clark et al., 2019))      


------

## 2.4 Input and Output Format


**“text-to-text”**      
* context 또는 conditioning을 위해, 모델에 some text를 제공한 다음 some output text를 생성하도록 요청하는 작업     
* 즉 **input, output이 모두 text**      



**add a task-specific (text) prefix**      
* 모델이 **수행해야 하는 작업을 지정**하기 위해 입력 시퀀스에 추가      
* ex       
  * **translate task**       
  ex) 영어에서 독일어로 문장 "That is good"를 번역하는 task는,    
  모델의 fed the sequence가 ```“translate English to German: That is good."```이고,         
  모델은 ```“Das ist gut.”```를 출력하도록 훈련될 것이다.       
  * **text classification tasks**
  모델은 target label에 해당하는 single word만 예측           
  ex) MNLI benchmark의 목표는 전제가 가설을 암시하는지(“entailment”), 모순되는지(“contradiction”), 또는 둘 다(“neutral”)인지 여부를 예측하는 것임.       
  입력 시퀀스는 ```“mnli premise: I hate pigeons. hypothesis: My feelings towards pigeons are filled with animosity.”```이면,     
  모델은 target 단어 "entailment"을 예측해야한다.     
* task-specific (text) prefix의 선택은 **하이퍼파라미터**       
* 아래 "더 알아보기"에서 연구한 모든 작업에 대해 prefix 입력의 모든 예를 제공    




<img width="340" alt="image" src="https://github.com/yerimoh/yerimoh.github.io/assets/76824611/dd4e89c3-64a0-4245-a206-527cc0a22a34">


    

<details>
<summary>📜 더 알아보기: prefix 입력의 모든 예 </summary>
<div markdown="1">
  
**Text-to-text**     
Our text-to-text framework provides a simple way to train a single model
on a wide variety of text tasks using the same loss function and decoding procedure.
We showed how this approach can be successfully applied to generative tasks like
abstractive summarization, classification tasks like natural language inference, and
even regression tasks like STS-B. In spite of its simplicity, we found the text-totext framework obtained comparable performance to task-specific architectures and
ultimately produced state-of-the-art results when combined with scale.


**Architectures**        
While some work on transfer learning for NLP has considered architectural
variants of the Transformer, we found the original encoder-decoder form worked
best in our text-to-text framework. Though an encoder-decoder model uses twice as
many parameters as “encoder-only” (e.g. BERT) or “decoder-only” (language model)
architectures, it has a similar computational cost. We also showed that sharing the
parameters in the encoder and decoder did not result in a substantial performance
drop while halving the total parameter count.



**Unsupervised objectives**      
Overall, we found that most “denoising” objectives, which train
the model to reconstruct randomly corrupted text, performed similarly in the text-totext setup. As a result, we suggest using objectives that produce short target sequences
so that unsupervised pre-training is more computationally efficient.


**Data sets**         
We introduced the “Colossal Clean Crawled Corpus” (C4), which comprises
heuristically-cleaned text from the Common Crawl web dump. When comparing C4 to
data sets that use additional filtering, we found that training on in-domain unlabeled
data could boost performance in a few downstream tasks. However, constraining to
a single domain typically results in a smaller data set. We separately showed that
performance can degrade when an unlabeled data set is small enough that it is repeated
many times over the course of pre-training. This motivates the use of a large and
diverse data set like C4 for generic language understanding tasks.


**Training strategies**      
We found that the basic approach of updating all of a pre-trained
model’s parameters during fine-tuning outperformed methods that are designed to
update fewer parameters, although updating all parameters is most expensive. We also
experimented with various approaches for training the model on multiple tasks at once,
which in our text-to-text setting simply corresponds to mixing examples from different
data sets when constructing batches. The primary concern in multi-task learning is
setting the proportion of each task to train on. We ultimately did not find a strategy
for setting mixing proportions that matched the performance of the basic approach of
unsupervised pre-training followed by supervised fine-tuning. However, we found that
fine-tuning after pre-training on a mixture of tasks produced comparable performance
to unsupervised pre-training.


**Scaling**       
We compared various strategies for taking advantage of additional compute, including training the model on more data, training a larger model, and using an ensemble
of models. We found each approach conferred a significant boost in performance,
though training a smaller model on more data was often outperformed by training
a larger model for fewer steps. We also showed an ensemble of models can provide
substantially better results than a single model, which provides an orthogonal means
of leveraging additional computation. Ensembling models that were fine-tuned from
the same base pre-trained model performed worse than pre-training and fine-tuning
all models completely separately, though fine-tune-only ensembling still substantially
outperformed a single model.


**Pushing the limits**     
We combined our above insights and trained substantially larger
models (up to 11 billion parameters) to achieve state-of-the-art results across many of
the benchmarks we considered. For unsupervised training, we extracted text from our
C4 data set and applied a denoising objective that corrupts contiguous spans of tokens.
We pre-trained on a multi-task mixture before fine-tuning on individual tasks. Overall,
our models were trained on over 1 trillion tokens. In the interest of facilitating the
replication, extension, and application of our results, we release our code, the C4 data
set, and pre-trained model weights for each T5 variant.1

</div>
</details> 




---
---


# 3. Experiments 


## 3.1 Baseline

baseline에 대한 목표는 **typical modern 관행을 반영**하는 것이다.        
simple denoising objective를 사용하여 Transformer를 pre-train한 다음 각 downstream tasks을 별도로 fine-tune한다.       

### 3.1.1 Model


* model: standard encoder-decoder Transformer를 사용      
* baseline model: 인코더와 디코더가 각각 "$$BERT_{BASE}$$" 스택과 크기와 구성이 유사하도록 설계됨.      


<details>
<summary>📜 baseline model을 자세히 알고 싶다면? </summary>
<div markdown="1">
  

**baseline model**    
* 특히, 인코더와 디코더는 모두 12개의 블록(각 블록은 자기 주의, 선택적 인코더-디코더 주의 및 피드 포워드 네트워크로 구성됨)으로 구성      
* encoder and decoder consist of 12 blocks     
(each block comprising self-attention, optional encoder-decoder attention, and a feed-forward network)     
* The feed-forward networks in each block consist of a dense layer (output dimensionality of dff = 3072)    
* followed by a ReLU nonlinearity and another dense layer.        
* The “key” and “value” matrices of all attention mechanisms have an inner dimensionality of dkv = 64         
* all attention mechanisms have 12 heads        
* All other sub-layers and embeddings have a dimensionality of dmodel = 768        
* In total, this results in a model with about 220 million parameters.        
* 기준 모델에 하나의 레이어 스택이 아닌 두 개의 레이어 스택이 포함되어 있기 때문에 이는 BERTBASE 매개 변수의 약 두 배     
* 정규화를 위해 모델에 드롭아웃이 적용되는 모든 곳에서 드롭아웃 확률 0.1을 사용합니다.
  

</div>
</details> 





### 3.1.2 Training
Section 2.4에서 설명한대로 all tasks는 **text-to-text tasks**다.     
* 이를 통해 우리는 **standard maximum likelihood**를 사용하여 trainning 가능      
  * i.e. teacher forcing (Williams and Zipser, 1989)            
  * cross-entropy loss        
* **optimization**를 위해 **AdaFactor**(Shazeer and Stern, 2018)를 사용    
* **test time**에 우리는 **greedy decoding**사용    
  * 즉, 매 timestep에서 highest-probability logit 선택)을 사용       
* $$2^35$$ ≈ 34B 토큰에 대해 pre-train (**BERT와 RoBERTa에 비해 상당히 작은 수치**)        
* **pre-training** 동안          
  * **“inverse square root” learning rate schedule** ( $$1/ \sqrt{max(n, k)} $$)   사용          
      * n: current training iteration          
      * k: number of warm-up steps (set to $$10^4$$ in all of our experiments)      
      * 사용하는 이유: 여기서 이것은 처음 $$10^4$$단계에 대해 0.01의 일정한 학습률을 설정한 다음 pre-train이 끝날 때까지 **학습률을 기하급수적으로 감소**시킴    
  * **triangular learning rate** (Howard and Ruder, 2018)을 사용하여 실험       
      * 이는 약간 더 나은 결과를 내지만 사전에 총 training steps 수를 알아야 함     
  * 우리는 일부 실험에서 훈련 단계의 수를 변경할 것이기 때문에, 우리는 더 일반적인 **“inverse square root”을 선택**함       
  


<details>
<summary>📜 pre-train each model을 더 자세히 알고싶다면? </summary>
<div markdown="1">
  




* pre-train each model for $$2^19$$ = 524,288 steps on C4 before fine-tuning.        
* We use a maximum sequence length of 512     
* batch size of 128 sequences.     
* 가능할 때마다, 배치의 각 항목에 여러 시퀀스를 "pack"하여 배치에 약 $$2^16 = 65,536$$개의 토큰이 포함되도록 함      
* 이 배치 크기와 단계 수는 $$2^35$$ ≈ 34B 토큰에 대한 pre-train에 해당함      
* 이는 약 137B 토큰을 사용한 BERT(Devlin et al., 2018)나 약 2.2T개의 토큰을 사용한 RoBERTa(Lu et al., 2019c)에 비해 상당히 적은 수치         
* 235개의 토큰은 전체 C4 데이터 세트의 일부만 포함하므로 pre-train육 중에는 데이터를 **반복하지 않음**             

</div>
</details> 

<details>
<summary>📜 pre-train each model을 더 자세히 알고싶다면? </summary>
<div markdown="1">
  
fine-tuning에 대해 더 자세히 알고싶다면?

* 모든 작업에서 $$2^18 = 262,196$$ 단계에 대해 미세 조정된다.        
* 128개의 128 length-512 sequence(즉, 배치당 216개의 토큰)를 가진 배치를 계속 사용.     
* fine-tuning 시 0.001의 일정한 학습률을 사용함    
* 우리는 5,000 단계마다 체크포인트를 저장하고 가장 높은 validation 성능에 해당하는 모델 체크포인트에 결과를 보고        
* 섹션 3.7의 실험을 제외한 모든 실험에 대해, 우리는 시험 세트에서 모델 선택을 수행하지 않기 위해 validation set 세트에 결과를 보고합니다       


  
  
</div>
</details> 






### 3.1.3 Vocabulary
* **SentencePiece**(Kudo and Richardson, 2018)를 사용하여 텍스트를 WordPiece 토큰으로 인코딩합니다.       
* 모든 실험에서 우리는 32,000개의 단어를 사용       



### 3.1.4 Unsupervised Objective
* **unlabeled data를 활용**하여 모델을 pretrain downstream 작업에 유용한 모델 **일반화 가능한 지식**을 가르쳐 줍니다.       
* <span style="background-color:#fff5b1">**denoising objective**사용</span>: 모델은 입력에서 **누락되거나 손상된 토큰을 예측하도록 훈련**된다.      
  * BERT의 **"masked language modeling"** 목표와 **“word dropout”**정규화 기법에서 영감을 받아 입력 시퀀스에서 토큰의 15%를 무작위로 샘플링한 후 드롭하는 목표를 설계.     
  * 모든 연속된 삭제된 토큰 범위는 단일 **sentinel token으로 대체**됨.     
     * 각 Sentinel 토큰에는 시퀀스에 고유한 토큰 ID가 할당됨     
     *  Sentinel ID는 어휘에 추가되는 특수 토큰이며 워드피스와 일치하지 않음.       
  *  그러면 **target**은 **입력 시퀀스에 사용된 것과 동일한 Sentinel 토큰**과 **targert 시퀀스의 끝**을 표시하는 **최종 동일한 토큰**으로 구분된 모든 토큰 범위에 해당함       
* 즉 아래의 그림을 예시로 보자.    
<img width="241" alt="image" src="https://github.com/yerimoh/yerimoh.github.io/assets/76824611/285fe79c-1d4e-4e37-98d1-880d1ceafe60">
   * **1)** words “for”, “inviting” and “last” (marked with an ×) are **randomly chosen** for **corruption(손상)**    
   * **2)** 손상(corruption)된 토큰의 각 연속 범위는 예제에서 고유한 sentinel 토큰(```<X>``` 및 ```<Y>```로 표시됨)으로 대체됨     
   * **3)** "for"와 "inviting"이 연속적으로 발생하기 때문에 이들은 하나의 sentinel ```<X>```로 대체됨.       
   * **4)** 그런 다음 출력 시퀀스는  dropped-out로 구성됨 (즉 보면 예측해야될 것(마스팅 한 단어들)만 traget값으로 나와있다.)     
    입력에서 이를 대체하는 데 사용되는 sentinel 토큰과 최종 sentinel 토큰 ```<Z>```로 구분됨       
    
  

### 3.1.5 Baseline Performance
* 이 섹션에서는 위에서 설명한 baseline 실험 절차를 사용하여 **downstream tasks에서 어떤 성능을 기대하는지 파악**함      
* baseline configuration이 나타나는 곳마다 ★로 표시함          
* 또한 주어진 실험에서 최대(최상)의 두 표준 편차 내에 있는 점수에 대해서도 굵은 글씨로 표시함     
<img width="311" alt="image" src="https://github.com/yerimoh/yerimoh.github.io/assets/76824611/3a99b93b-8341-47e2-a9f3-6d8fa97c8e68">
  * <span style="background-color:#fff5b1">**pre train이** 거의 모든 벤치마크에서 **상당한 이점**을 제공한다는 것을 발견</span>함           
  * 가장 성능이 좋은 체크포인트를 선택하여 early stopping를 수행하         
  ➡ our baseline과 "no pre-training" 사이의 큰 차이: **pre-training이 제한된 데이터가 있는 작업**에서 **성능을 얼마나 향상**시키는지를 **강조**         
  
-----


## 3.2 Architectures
트랜스포머는 원래 encoder-decoder architecture와 함께 도입되었지만,       
NLP에 대한 transfer learning에 대한 많은 현대 연구는 대체 architecture를 사용한다.              

이 섹션에서는 이러한 architecture의 변형을 검토하고 비교            


### 3.2.1 Model Structures
* 다양한 architecture의 주요 차별화 요소는 모델의 다양한 attention mechanisms 의해 사용되는 "mask"다.        
* Transformer의 self-attention operation     
   * **input:** sequence      
   * **output**:동일한 길이의 새로운 sequence를 출력합니다. (출력 sequence의 각 항목은 입력 sequence의 가중치 평균을 계산하여 생성됨)       

<img width="269" alt="image" src="https://github.com/yerimoh/yerimoh.github.io/assets/76824611/6f2ae2d2-c17b-476d-9aac-34feab52bcef">
* 구성     
  * **$$y_i$$**: output sequence의 i번째 요소      
  * **$$x_j$$**: input sequence의 j번째 항목             
* **$$y_i$$의 계산:** $$\displaystyle\sum_{j}{w_{i,j}}x_{j}$$,       
  * **$$w_{i,j}$$:** $$x_i$$와 $$x_j$$의 함수로서 self-attention mechanism에 의해 생성된 scalar weight임       
  * 그런 다음 attention mask를 사용하여 특정 가중치를 0으로 설정하여,   
   특정 output timestep에서 input의 어떤 항목에 attention 기울일 수 있는지를 제한(constrain)함.        
  * eg) **the causal mask (Figure 3, middle) sets**:는 j > i일 경우 임의의 $$w_{i,j}$$를 0으로 설정함      


* <span style="background-color:#F5F5F5">**distinguishing factor for different architecture: encoder-decoder Transformer**</span>     
  * **두 개의 레이어 스택으로 구성됨:** encoder(input sequence가 공급됨) 및 decoder(새 output sequence를 생성)     
  * Figure 4의 첫번째      
  * **encoder:** **“fully-visible” attention mask**를 사용 (Figure 3)      
        * 각 출력 항목을 생성할 때 self-attention mechanism이 input 항목에 attention할 수 있다.     
        * 이 이러한 마스킹 형식은 "prefix", 즉 나중에 예측할 때 사용되는 모델에 제공되는 일부 context정보를 제공할 때 적합.       * **decoder:**  **“causal” masking patter**을 사용 (Figure 3)                    
        * 출력 시퀀스의 i번째 항목을 생성할 때 “causal” masking은 모델이 j > i에 대한 입력 시퀀스의 j번째 항목에 참여하는 것을 방지.          
        * 이것은 모델이 출력을 생성할 때 **"미래를 내다볼 수" 없도록 훈련 중에 사용**됩니다.      


인코더-디코더 변압기의 디코더는 출력 시퀀스를 자동으로 생성하는 데 사용됩니다. 즉, 각 출력 시간 단계에서 토큰이 모델의 예측 분포에서 샘플링되고 샘플이 모델로 피드백되어 다음 출력 시간 단계에 대한 예측을 생성하는 등의 작업을 수행합니다. 이와 같이, 트랜스포머 디코더(인코더 없음)는 언어 모델(LM), 즉 다음 단계 예측만을 위해 훈련된 모델로 사용될 수 있습니다(Liu et al., 2018; Radford et al., 2018; Al-Rfou et al., 2019). 이것은 우리가 고려하는 두 번째 모델 구조를 구성합니다. 이 아키텍처의 개략도는 그림 4의 가운데에 나와 있습니다. 사실, NLP에 대한 전이 학습에 대한 초기 연구는 사전 훈련 방법으로 언어 모델링 목표와 함께 이 아키텍처를 사용했습니다(Radford et al., 2018).



The decoder in an encoder-decoder Transformer is used to autoregressively produce an
output sequence. That is, at each output timestep, a token is sampled from the model’s
predicted distribution and the sample is fed back into the model to produce a prediction for
the next output timestep, and so on. As such, a Transformer decoder (without an encoder)
can be used as a language model (LM), i.e. a model trained solely for next-step prediction
(Liu et al., 2018; Radford et al., 2018; Al-Rfou et al., 2019). This constitutes the second
model structure we consider. A schematic of this architecture is shown in Figure 4, middle.
In fact, early work on transfer learning for NLP used this architecture with a language
modeling objective as a pre-training method (Radford et al., 2018).



Language models are typically used for compression or sequence generation (Graves,
2013). However, they can also be used in the text-to-text framework simply by concatenating
the inputs and targets. As an example, consider the case of English to German translation:
If we have a training datapoint with input sentence “That is good.” and target “Das ist
gut.”, we would simply train the model on next-step prediction over the concatenated input
sequence “translate English to German: That is good. target: Das ist gut.” If we wanted to
obtain the model’s prediction for this example, the model would be fed the prefix “translate
English to German: That is good. target:” and would be asked to generate the remainder
of the sequence autoregressively. In this way, the model can predict an output sequence
given an input, which satisfies the needs of text-to-text tasks. This approach was recently
used to show that language models can learn to perform some text-to-text tasks without
supervision (Radford et al., 2019).


A fundamental and frequently cited drawback of using a language model in the textto-text setting is that causal masking forces the model’s representation of the ith entry of
the input sequence to only depend on the entries up until i. To see why this is potentially
disadvantageous, consider the text-to-text framework where the model is provided with a
prefix/context before being asked to make predictions (e.g., the prefix is an English sentence
and the model is asked to predict the German translation). With fully causal masking, the
model’s representation of a prefix state can only depend on prior entries of the prefix. So,
when predicting an entry of the output, the model will attend to a representation of the
prefix that is unnecessarily limited. Similar arguments have been made against using a
unidirectional recurrent neural network encoder in sequence-to-sequence models (Bahdanau
et al., 2015).


This issue can be avoided in a Transformer-based language model simply by changing
the masking pattern. Instead of using a causal mask, we use fully-visible masking during
the prefix portion of the sequence. This masking pattern and a schematic of the resulting
“prefix LM” (the third model structure we consider) are illustrated in the rightmost panels of
Figures 3 and 4, respectively. In the English to German translation example mentioned above,
fully-visible masking would be applied to the prefix “translate English to German: That is
good. target:” and causal masking would be used during training for predicting the target
“Das ist gut.” Using a prefix LM in the text-to-text framework was originally proposed by
Liu et al. (2018). More recently, Dong et al. (2019) showed that this architecture is effective
on a wide variety of text-to-text tasks. This architecture is similar to an encoder-decoder
model with parameters shared across the encoder and decoder and with the encoder-decoder
attention replaced with full attention across the input and target sequence.


We note that when following our text-to-text framework, the prefix LM architecture
closely resembles BERT (Devlin et al., 2018) for classification tasks. To see why, consider an
example from the MNLI benchmark where the premise is “I hate pigeons.”, the hypothesis is
“My feelings towards pigeons are filled with animosity.” and the correct label is “entailment”.
To feed this example into a language model, we would transform it into the sequence “mnli
premise: I hate pigeons. hypothesis: My feelings towards pigeons are filled with animosity.
target: entailment”. In this case, the fully-visible prefix would correspond to the entire input
sequence up to the word “target:”, which can be seen as being analogous to the “classification”
token used in BERT. So, our model would have full visibility over the entire input, and then
would be tasked with making a classification by outputting the word “entailment”. It is easy
for the model to learn to output one of the valid class labels given the task prefix (“mnli” in
this case). As such, the main difference between a prefix LM and the BERT architecture is
that the classifier is simply integrated into the output layer of the Transformer decoder in
the prefix LM












