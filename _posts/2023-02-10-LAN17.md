---
title: "IS REINFORCEMENT LEARNING (NOT) FOR NATURAL LANGUAGE PROCESSING?: BENCHMARKS, BASELINES, AND BUILDING BLOCKS FOR NATURAL LANGUAGE POLICY OPTIMIZATION 정리"
date:   2023-02-10
excerpt: "IS REINFORCEMENT LEARNING (NOT) FOR NATURAL LANGUAGE PROCESSING?: BENCHMARKS, BASELINES, AND BUILDING BLOCKS FOR NATURAL LANGUAGE POLICY OPTIMIZATION paper review"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---

# 목차


# ABSTRACT
우리는 사전 훈련된 대형 언어 모델(LM)을 인간 선호도에 맞추는 문제를 해결한다. 텍스트 생성을 순차적 의사 결정 문제로 본다면, 강화 학습(RL)은 자연스러운 개념적 프레임워크로 보인다. 그러나 LM 기반 생성을 위해 RL을 사용하는 것은 조합 작업 공간으로 인한 훈련 불안정성뿐만 아니라 LM 정렬을 위해 맞춤화된 오픈 소스 라이브러리와 벤치마크의 부족을 포함한 경험적 과제에 직면한다. 따라서, RL이 NLP를 위한 실용적인 패러다임인가?라는 질문이 연구 커뮤니티에서 제기된다?
We tackle the problem of aligning pre-trained large language models (LMs) with human preferences. If we view text generation as a sequential decision-making problem, reinforcement learning (RL) appears to be a natural conceptual framework. However, using RL for LM-based generation faces empirical challenges, including training instability due to the combinatorial action space, as well as a lack of opensource libraries and benchmarks customized for LM alignment. Thus, a question rises in the research community: is RL a practical paradigm for NLP?


이에 대한 답변을 돕기 위해 먼저 RL로 언어 생성기를 최적화하기 위한 오픈 소스 모듈식 라이브러리인 RL4LMs1, 2를 소개한다. 라이브러리는 임의의 보상 함수를 사용하여 HuggingFace 라이브러리(Wolf et al., 2020)에서 인코더 또는 인코더-디코더 LM을 훈련하는 데 사용할 수 있는 온 폴리시 RL 알고리듬으로 구성된다. 다음으로, 우리는 대상 문자열이 아니라 인간 선호도의 자동화된 측정을 캡처하는 보상 기능에 의해 감독되는 6개 언어 생성 작업 세트인 GRUE(General Enforced-Language Understanding Evaluation) 벤치마크를 제시한다. GRUE는 NLP 작업에 대한 RL 알고리듬의 첫 번째 리더보드 스타일 평가이다. 마지막으로, 우리는 언어 생성에서 조합 작업 공간을 효과적으로 줄이는 방법을 배우는 사용하기 쉬운 성능의 RL 알고리듬인 NLPO(Natural Language Policy Optimization)를 소개한다. 우리는 1) RL 기술이 일반적으로 인간 선호도에 LM을 맞추는 데 감독 방법보다 낫다는 것을 보여준다. 2) NLPO가 자동 및 인간 평가를 모두 기반으로 이전 정책 그레이디언트 방법(예: PPO(Schulman et al., 2017)보다 더 큰 안정성과 성능을 나타낸다
To help answer this, we first introduce an open-source modular library, RL4LMs1 , 2 for optimizing language generators with RL. The library consists of on-policy RL algorithms that can be used to train any encoder or encoder-decoder LM in the HuggingFace library (Wolf et al., 2020) with an arbitrary reward function. Next, we present the GRUE (General Reinforced-language Understanding Evaluation) benchmark, a set of 6 language generation tasks which are supervised not by target strings, but by reward functions which capture automated measures of human preference. GRUE is the first leaderboard-style evaluation of RL algorithms for NLP tasks. Finally, we introduce an easy-to-use, performant RL algorithm, NLPO (Natural Language Policy Optimization) that learns to effectively reduce the combinatorial action space in language generation. We show 1) that RL techniques are generally better than supervised methods at aligning LMs to human preferences; and 2) that NLPO exhibits greater stability and performance than previous policy gradient methods (e.g., PPO (Schulman et al., 2017)), based on both automatic and human evaluation
