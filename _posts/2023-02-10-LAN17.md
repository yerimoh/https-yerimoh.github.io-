---
title: "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer 정리"
date:   2023-02-10
excerpt: "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer paper review"
category: #[Paper]
layout: post
tag:
#- Paper
order: 0

comments: true
---


# Abstract
<span style="background-color:#F5F5F5">**[논문 배경]**</span>    
모델이 Downstream Task에서 fine-tuning되기 전에 data-rich task에서 pre-train을 하는 것은  자연어 처리(NLP)의 강력한 기술로 부상했다.     
즉 이러한 [transfer learning](https://yerimoh.github.io/DL12/)은 NLP에 큰 발전을 가져왔다.    

<span style="background-color:#F5F5F5">**[논문의 목적]**</span>    
* 모든 텍스트 기반 언어 문제를 text-to-text 형식으로 변환하는 통합 프레임워크를 도입하여 <span style="background-color:#fff5b1">NLP에 대한 [transfer learning](https://yerimoh.github.io/DL12/)를 전반적으로 탐구</span>한다.     
* 위 탐구를 통해 얻은 통찰력과 규모 및 본 논문에서 제안할 <span style="background-color:#fff5b1">"Colossal Clean Crawled Corpus"를 결합</span>하여 많은 NLP evaluation에서 SOTA 성능을 보였다.        
➡ 공개한 [data set, pre-trained models, and code](https://github.com/google-research/text-to-text-transfer-transformer) 링크를 첨부하였다.        


<details>
<summary>📜 Downstream Task 의미 보기</summary>
<div markdown="1">
  

구체적으로 풀고 싶은 문제들을 말한다.

NLP에서는 언어모델을 pre-train방식을 이용해 학습을 진행하고,    
그 후에 원하고자 하는 task를 fine-tuning하는 방식을 통해 모델을 업데이트 하는 방식을 사용하는데 이때, task를 Downstream Task라 한다.

예를들어, BERT의 언어모델을 질의응답 Task라인 squad를 학습한다고 할때, 이때 질의응답 Task를 다운스트림 Task로 볼 수 있을것이다.  
  
  
</div>
</details>  

----
----



# Introduction

<span style="background-color:#F5F5F5">**[최근 연구 동향]**</span>    
자연어 처리(NLP)작을 을 위한 train이 가능하여면 모델이 **downstream learning에 적합한 방식으로 텍스트를 처리**할 수 있어야 한다.              
그런데 이는 **모델이 텍스트를 "이해"할 수 있게 학습한다고 보기엔 힘들다**.               
➡ 현대 기계 학습 관행에서 이러한 train이 명시적으로 수행되는 경우가 거의 없어, 대신 **Auxiliary Task의 일부**로 학습되는 경우가 많다.        


<details>
<summary>📜 Auxiliary Task 의미 보기</summary>
<div markdown="1">

본 task는 아니지만, 본 task에서의 성능이 더 잘 나올 수 있도록 도와주는 보조 task  

</div>
</details> 
  
최근에는 **데이터가 풍부한 작업**에 대해 **전체 모델을 pre-train하는 것**이 점점 더 **일반화**되고 있다.     
이상적으로, 이 pre-train은 모델이 범용 능력과 지식을 개발하게 하고, 이를 다운스트림 작업으로 이전할 수 있도록 한다.     
➡ 더 큰 데이터 세트에서 더 큰 모델을 훈련시키는 것만으로 더 나은 성능을 달성할 수 있다.    




<span style="background-color:#F5F5F5">**[현 경향으로 인한 한계]**</span>    
이러한 경향으로 인해 NLP에 대한 **transfer learning 방법론을 개발**하는 연구들이 활발하게 이루어졌다.      
이렇게 이 분야가 급성장하여 연구가 활발해지니 아래와 같은 작업들이 어려워졌다.       
* 여러 algorithms을 비교    
* 새로운 contributions의 효과 파악    
* transfer learning을 위한 기존 방법의 space 이해        


✔ 그래서 본 논문은 이 분야의 원활한 이해를 위해, **다양한 접근법을 체계적으로 연구**하고 **필드의 현재 한계를 밀어낼 수 있는 transfer learning에 대한 통합된 접근법을 활용**한다. 
 


<span style="background-color:#F5F5F5">**[본 논문의 해결책]**</span>      
본 논문 work의 기본 아이디어는 <span style="background-color:#fff5b1">모든 텍스트 처리 문제를 **“text-to-text” 문제로 처리**</span>하는 것이다.             
즉, **텍스트를 입력으로 받아들이고 새로운 텍스트를 출력으로 생성**하는 것이다.          
* 이러한 텍스트 간 프레임워크는 우리가 고려하는 모든 작업에 **동일한 모델, 목표, 훈련 절차 및 decoding 프로세스를 직접 적용**할 수 있게 한다.          
* 본 논문은 질문 답변, 문서 요약 및 감정 분류를 포함한 다양한 영어 기반 NLP 문제에 대한 성능을 평가하여 이러한 **유연성을 활용**한다.        
* 이 통합 접근법을 통해, 우리는 다양한 transfer learning의 target, 레이블이 지정되지 않은 데이터 세트 및 기타 요인의 효과를 비교하는 동시에 이전에 고려되었던 것 이상으로 **모델과 데이터 세트를 확장**하여 NLP에 대한 **transfer learning의 한계를 탐색**할 수 있다.    




본 논문은 본 논문의 목표가 새로운 방법을 제안하는 것이 아니라, **그 분야가 어디에 서 있는지에 대한 포괄적인 관점을 제공하는 것**임을 강조한다.      
즉, 우리의 작업은 본 목표를 달성하기 위해 아래와 같은 task로 구성된다.         
➡ 주로 기존 기술의 조사, 탐구 및 경험적 비교       


또한 본 논문은 다음과 같은 방식으로 **현재 접근 방식의 한계를 탐구**한다.        
본 논문이 고려하는 많은 task에서 SOTA 결과를 얻기 위해 체계적인 연구(train 모델을 최대 110억 개 parameters까지 확장)수행       
* 이 규모의 실험을 수행하기 위해 웹에서 긁어낸 수백 기가바이트의 clean 영어 텍스트로 구성된 데이터 세트인 "Colossal Clean Crawled Corpus"**(C4)** 를 소개한다.        
* **transfer learning의 핵심 기능**은, **데이터가 부족한 환경에서  pre-trained models을 활용할 수 있는 가능성**이라는 것을 인식하여,      
[코드, 데이터 세트 및 사전 훈련된 모델](https://github.com/google-research/text-to-text-transfer-transformer
)을 릴리스했다.    



그 외의 본 논문 내용의 구성은 다음과 같다.        
* base model과 그 구현    
* 모든 text processing 문제를 text-to-text 작업으로 공식화하는 절차 및 고려하는 작업 모음에 대한 논의    
* 섹션 3에서, NLP에 대한 transfer learning 분야를 탐구하는 대규모 실험 세트를 제시          
* 섹션(섹션 3.7)의 끝에서, 우리는 체계적인 연구의 통찰력을 결합하여 광범위한 벤치마크에 대한 최첨단 결과를 얻음    
* 섹션 4에서, 결과에 대한 요약을 제공 한 뒤, 미래에 대한 전망으로 마무리             




---
---


# 2. Setup    

Before presenting the results from our large-scale empirical study, we review the necessary background topics required to understand our results, including the Transformer model architecture and the downstream tasks we evaluate on. We also introduce our approach for treating every problem as a text-to-text task and describe our “Colossal Clean Crawled Corpus” (C4), the Common Crawl-based data set we created as a source of unlabeled text data. We refer to our model and framework as the “Text-to-Text Transfer Transformer” (T5).



본 논문은 large-scale 경험적 연구의 결과를 제시하기 전에 [Transformer 모델 아키텍처]()와 평가하는 다운스트림 작업을 포함하여 결과를 이해하는 데 필요한 배경 주제를 검토한다. 또한 모든 문제를 텍스트 대 텍스트 작업으로 처리하기 위한 접근 방식을 소개하고 레이블이 지정되지 않은 텍스트 데이터의 소스로 생성한 공통 크롤 기반 데이터 세트인 "Colossal Clean Crawled Corpus"(C4)를 설명한다. 우리는 우리의 모델과 프레임워크를 "텍스트 간 전송 변환기"(T5)라고 부른다.











