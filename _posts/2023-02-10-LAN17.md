---
title: "(T5) Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer 정리"
date:   2023-02-10
excerpt: "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer paper review"
category: #[Paper]
layout: post
tag:
#- Paper
order: 0

comments: true
---


# Abstract
<span style="background-color:#F5F5F5">**[논문 배경]**</span>    
모델이 Downstream Task에서 fine-tuning되기 전에 data-rich task에서 pre-train을 하는 것은  자연어 처리(NLP)의 강력한 기술로 부상했다.     
즉 이러한 [transfer learning](https://yerimoh.github.io/DL12/)은 NLP에 큰 발전을 가져왔다.    

<span style="background-color:#F5F5F5">**[논문의 목적]**</span>    
* 모든 텍스트 기반 언어 문제를 text-to-text 형식으로 변환하는 통합 프레임워크를 도입하여 <span style="background-color:#fff5b1">NLP에 대한 [transfer learning](https://yerimoh.github.io/DL12/)를 전반적으로 탐구</span>한다.     
* 위 탐구를 통해 얻은 통찰력과 규모 및 본 논문에서 제안할 <span style="background-color:#fff5b1">"Colossal Clean Crawled Corpus"를 결합</span>하여 많은 NLP evaluation에서 SOTA 성능을 보였다.        
➡ 공개한 [data set, pre-trained models, and code](https://github.com/google-research/text-to-text-transfer-transformer) 링크를 첨부하였다.        


<details>
<summary>📜 Downstream Task 의미 보기</summary>
<div markdown="1">
  

구체적으로 풀고 싶은 문제들을 말한다.

NLP에서는 언어모델을 pre-train방식을 이용해 학습을 진행하고,    
그 후에 원하고자 하는 task를 fine-tuning하는 방식을 통해 모델을 업데이트 하는 방식을 사용하는데 이때, task를 Downstream Task라 한다.

예를들어, BERT의 언어모델을 질의응답 Task라인 squad를 학습한다고 할때, 이때 질의응답 Task를 다운스트림 Task로 볼 수 있을것이다.  
  
  
</div>
</details>  

----
----



# Introduction

<span style="background-color:#F5F5F5">**[최근 연구 동향]**</span>    
자연어 처리(NLP)작을 을 위한 train이 가능하여면 모델이 **downstream learning에 적합한 방식으로 텍스트를 처리**할 수 있어야 한다.              
그런데 이는 **모델이 텍스트를 "이해"할 수 있게 학습한다고 보기엔 힘들다**.               
➡ 현대 기계 학습 관행에서 이러한 train이 명시적으로 수행되는 경우가 거의 없어, 대신 **Auxiliary Task의 일부**로 학습되는 경우가 많다.        


<details>
<summary>📜 Auxiliary Task 의미 보기</summary>
<div markdown="1">

본 task는 아니지만, 본 task에서의 성능이 더 잘 나올 수 있도록 도와주는 보조 task  

</div>
</details> 
  
최근에는 **데이터가 풍부한 작업**에 대해 **전체 모델을 pre-train하는 것**이 점점 더 **일반화**되고 있다.     
이상적으로, 이 pre-train은 모델이 범용 능력과 지식을 개발하게 하고, 이를 다운스트림 작업으로 이전할 수 있도록 한다.     
➡ 더 큰 데이터 세트에서 더 큰 모델을 훈련시키는 것만으로 더 나은 성능을 달성할 수 있다.    




<span style="background-color:#F5F5F5">**[현 경향으로 인한 한계]**</span>    
이러한 경향으로 인해 NLP에 대한 **transfer learning 방법론을 개발**하는 연구들이 활발하게 이루어졌다.      
이렇게 이 분야가 급성장하여 연구가 활발해지니 아래와 같은 작업들이 어려워졌다.       
* 여러 algorithms을 비교    
* 새로운 contributions의 효과 파악    
* transfer learning을 위한 기존 방법의 space 이해        


✔ 그래서 본 논문은 이 분야의 원활한 이해를 위해, **다양한 접근법을 체계적으로 연구**하고 **필드의 현재 한계를 밀어낼 수 있는 transfer learning에 대한 통합된 접근법을 활용**한다. 
 


<span style="background-color:#F5F5F5">**[본 논문의 해결책]**</span>      
본 논문 work의 기본 아이디어는 <span style="background-color:#fff5b1">모든 텍스트 처리 문제를 **“text-to-text” 문제로 처리**</span>하는 것이다.             
즉, **텍스트를 입력으로 받아들이고 새로운 텍스트를 출력으로 생성**하는 것이다.          
* 이러한 텍스트 간 프레임워크는 우리가 고려하는 모든 작업에 **동일한 모델, 목표, 훈련 절차 및 decoding 프로세스를 직접 적용**할 수 있게 한다.          
* 본 논문은 질문 답변, 문서 요약 및 감정 분류를 포함한 다양한 영어 기반 NLP 문제에 대한 성능을 평가하여 이러한 **유연성을 활용**한다.        
* 이 통합 접근법을 통해, 우리는 다양한 transfer learning의 target, 레이블이 지정되지 않은 데이터 세트 및 기타 요인의 효과를 비교하는 동시에 이전에 고려되었던 것 이상으로 **모델과 데이터 세트를 확장**하여 NLP에 대한 **transfer learning의 한계를 탐색**할 수 있다.    




본 논문은 본 논문의 목표가 새로운 방법을 제안하는 것이 아니라, **그 분야가 어디에 서 있는지에 대한 포괄적인 관점을 제공하는 것**임을 강조한다.      
즉, 우리의 작업은 본 목표를 달성하기 위해 아래와 같은 task로 구성된다.         
➡ 주로 기존 기술의 조사, 탐구 및 경험적 비교       


또한 본 논문은 다음과 같은 방식으로 **현재 접근 방식의 한계를 탐구**한다.        
본 논문이 고려하는 많은 task에서 SOTA 결과를 얻기 위해 체계적인 연구(train 모델을 최대 110억 개 parameters까지 확장)수행       
* 이 규모의 실험을 수행하기 위해 웹에서 긁어낸 수백 기가바이트의 clean 영어 텍스트로 구성된 데이터 세트인 "Colossal Clean Crawled Corpus"**(C4)** 를 소개한다.        
* **transfer learning의 핵심 기능**은, **데이터가 부족한 환경에서  pre-trained models을 활용할 수 있는 가능성**이라는 것을 인식하여,      
[코드, 데이터 세트 및 사전 훈련된 모델](https://github.com/google-research/text-to-text-transfer-transformer
)을 릴리스했다.    




<span style="background-color:#F5F5F5">**[논문 구성]**</span>            
* base model과 그 구현    
* 모든 text processing 문제를 text-to-text 작업으로 공식화하는 절차 및 고려하는 작업 모음에 대한 논의    
* 섹션 3에서, NLP에 대한 transfer learning 분야를 탐구하는 대규모 실험 세트를 제시          
* 섹션(섹션 3.7)의 끝에서, 우리는 체계적인 연구의 통찰력을 결합하여 광범위한 벤치마크에 대한 최첨단 결과를 얻음    
* 섹션 4에서, 결과에 대한 요약을 제공 한 뒤, 미래에 대한 전망으로 마무리             




---
---


# 2. Setup    



본 논문은 large-scale 경험적 연구의 결과를 제시하기 전에 아래와 같은 것들을 먼저 제시한다,   
* [Transformer 모델 아키텍처](https://yerimoh.github.io/Lan/)와 평가하는 downstream tasks을 포함하여 결과를 이해하는 데 필요한 **배경 주제를 검토**한다.        
* **모든 문제를 text-to-text task으로 처리하기 위한 접근 방식**을 소개    
* **"Colossal Clean Crawled Corpus"(C4) 설명:** 레이블이 지정되지 않은 텍스트 데이터의 소스로 생성한 공통 크롤 기반 데이터 세트임     




우리는 우리의 모델과 프레임워크를  <span style="background-color:#fff5b1">“Text-to-Text Transfer Transformer” (T5)</span> 라고 부른다.      





---


## 2.1 Model

### base architecture: “Transformer”
NLP에 대한 전이 학습에 대한 초기 결과는 반복 신경망을 활용했지만,  
최근에는 "Transformer" 아키텍처를 기반으로 한 모델을 사용하는 것이 더 일반화되었다.     


Transformer는 transfer learning에 효과적이므로, 이후 다양한 NLP 설정에 사용되었다.          
본 논문에서도 **모든 모델의 base를 Transformer 아키텍처**로 하였다.         

아래에 언급된 세부사항과 섹션 3.2에서 탐구한 변형을 제외하고,     
본 논문은 Transformer 아키텍처에서 크게 벗어나지 않는다.      


<details>
<summary>📜 Transformer를 자세히 알고 싶다면? (참고자료) </summary>
<div markdown="1">

이 논문(아키텍처)에 대해 더 자세히 알고싶다면 아래 자료들을 참고하자,    
* [원본 논문](https://arxiv.org/abs/1706.03762)    
* 후속 튜토리얼 3,4(뒤에 나옵니다.)    
* [본 포스트를 정리한 필자가 정리한 포스트](https://yerimoh.github.io/Lan/)         

</div>
</details> 
  85.5


그리고 본 논문에서 Transformer에대해 간단간단하게 설명해줬는데 원한다면 아래 자세한 설명 보기를 눌러 한 번 읽어보길 바란다.(그렇지만 원 논문을 읽어봐야 아래 내용들도 이해가 갈 것이다.)   

<details>
<summary>📜 Transformer를 자세히 알고 싶다면?(본 논문의 설명) </summary>
<div markdown="1">




 
**Transformer의 중요한 요소는,** [self-attention](https://yerimoh.github.io/Lan/#sub-layer1-self-attention)이다.         
self-attention은 각 요소를 나머지 sequence의 가중 평균으로 대체하여 시퀀스를 처리하는 attention의 변형이다.    

**Transformer의 구조는,** [encoder-decoder 아키텍처](https://yerimoh.github.io/Lan/#transformer-%EB%AA%A8%EB%8D%B8-%EA%B0%9C%EC%9A%94)로 구성되었으며 sequence-to-sequence 작업을 위해 고안되었다.        

전반적으로, 본 논문 모델의 encoder-decoder Transformer의 구현은 원래 Transformer의 형태를 밀접하게 따른다.               

**[Transformer의 동작 과정]**       
* **1)** 토큰의 input sequence를 embeddings sequence를에 매핑한 다음 encoder로 전달한다.      
* **2)** encoder는  “blocks”의 스택으로 구성되며, 각 스택마다 self-attention layer 계층과 small feed-forward network를 갖고있다.     
* **3)** 각 스택 안의 2가지 요소들의 입력에는 Layer normalization가 적용된다.     
* **4)** 이후, activations이 재조정되고 추가 bias이 적용되지 않는 단순화된 버전의 Layer normalization를 사용한다.      
* **5)** layer normalization 후, residual skip connection은 각 스택 안의 2가지 요소의 입력을 출력에 추가한다.      
* **6)** 드롭아웃은 피드포워드 네트워크 내에서 스킵 연결, 주의 가중치 및 전체 스택의 입력 및 출력에 적용된다.   
* **7)** decoder는 encoder의 출력에 참여하는 각 self-attention layer 다음에 standard attention 메커니즘을 포함한다는 점을 제외하고는 인코더와 구조가 유사하다.       
decoder의 self-attention 메커니즘은 모델이 과거 출력에만 주의를 기울일 수 있도록 한다.     
* **8)** 최종 decoder 블록의 출력은 소프트맥스 출력을 가진  dense 레이어로 들어가며, 그 가중치는 입력 임베딩 매트릭스와 공유된다.      
* ✨ 여기서, 트랜스포머의 모든 attention 메커니즘은 추가로 처리되기 전에 출력이 연결되는 독립적인 “heads”로 나뉘어있다.          
![image](https://user-images.githubusercontent.com/76824611/132572962-94a60e8b-2182-466a-8d1d-47a86ee83a14.gif)
</div>
</details> 
  


### relative position embeddings
Transformer의 self-attention는 병렬처리를하여 순서 정보를 갖지 못하므로, 임베딩에 순서정보를 넣어준다.    
이 논문에서는 기존 Transformer와 다른 위치 임베딩을 사용하였다.     
* **기존 모델**: sinusoidal position signal or learned position embeddings (단어의 절대적 위치 정보 표현)           
* **T5**: [relative position embeddings](https://yerimoh.github.io/LAN18/) (단어의 상대적 위치 표현)     


      



<details>
<summary>📜 relative position embeddings를 자세히 알고 싶다면?(본 논문의 설명) </summary>
<div markdown="1">

각 위치에 대해 고정 임베딩을 사용하는 대신, relative position embeddings은 self-attention에서 비교되는 "key"와 "query" 사이의 오프셋에 따라 다른 학습된 임베딩을 생성한다.    

또한 효율성을 위해 **모델의 모든 레이어**에 걸쳐 **position embeddings parameters를 공유**하지만,     
**주어진 layer 내**에서 **각 attention head**는 **서로 다른 학습된 position embedding을 사용**한다.      

일반적으로, 각각 가능한 "key"와 "query" 오프셋 범위에 해당하는 **고정된 수의 embeddings**이 학습된다.      

이 연구에서, 우리는 로그적으로 **최대 128의 오프셋까지 크기가 증가**하는 범위를 가진 **모든 모델**에    
**32개의 embedding을 사용**하여 모든 relative position를 동일한 임베딩에 할당한다.      

특정 계층은 128개 토큰을 초과하는 relative position에 민감하지 않지만,     
subsequent 계층은 이전 계층의 로컬 정보를 결합하여 더 큰 오프셋에 민감하게 반응할 수 있다.

</div>
</details> 
  
**[summary: T5와 기존 Transformer의 차이점]**       
아래를 제외하고 기존 Transformer와 동일       
* T5는 Layer Norm bias를 제거함    
* Layer normalization를 residual path 외부에 배치     
* 다른 position embedding 방식 사용(relative position embeddings)    

이러한 아키텍처 변화는 transfer learning에 대한 경험적 조사에서 고려하는 실험 요소와 직교하기 때문에,  
우리는 향후 작업을 위해 영향의 절제를 남겨둠둠.  
  
### 실험  
* T5의 확장성(scalability)실험    
➡ 즉 더 많은 parameters나 layers을 가질수록 성능이 어떻게 변하는지 실험한다.      
* 결과적으로, 우리는 모델과 데이터 병렬화를 결합하여 “slices” of Cloud TPU Pods"에서 모델을 훈련시킴.       



----

##  2.2 The Colossal Clean Crawled Corpus
데이터는  **large unlabeled** data sets for **unsupervised learning**를 사용한다.    

텍스트 데이터 소스는 **Common Crawl**을 사용한다


<details>
<summary>📜 Common Crawl을 자세히 알고 싶다면? </summary>
<div markdown="1">
  
Common Crawl이란    
* 일반적으로 사용 가능한 웹 아카이브임     
* 스크랩된 HTML 파일에서 마크업 및 기타 비텍스트 콘텐츠를 제거하여 "웹 추출 텍스트"를 제공      
* 이 프로세스는 매달 약 20TB의 스크랩된 텍스트 데이터를 생성.     


preprocessing     
* Common Crawl의 문제     
   * 불행하게도, Common Crawl의 결과 텍스트의 대부분은 자연어가 아니다.     
   * 오류 메시지 또는 중복 텍스트와 같은 횡설수설하거나 boiler-plate 텍스트로 구성됨    
* 해결: 다음 휴리스틱을 사용     
   * 마침표, 느낌표, 물음표 또는 끝 따옴표로 끝나는 행만 추출     
   * 5개 미만의 문장이 있는 페이지는 폐기하고 최소 3개의 단어가 포함된 행만 추출      
   * "더티, 장난, 외설 또는 기타 나쁜 단어 목록"에 있는 모든 단어가 포함된 페이지를 제거          
   * 대부분의 스크랩 페이지에는 Javascript를 활성화해야 한다는 경고가 포함되어 있어 Javascript라는 단어가 있는 줄은 모두 제거          
   * 일부 페이지에는 플레이스홀더 "lorem ipsum" 텍스트가 포함되어 있으며, "lorem ipsum"이라는 문구가 나타나는 페이지는 모두 제거      
   *  "{"가 포함된 모든 페이지를 제거     
   *  데이터 세트의 중복을 제거하기 위해 데이터 세트에서 두 번 이상 발생하는 세 문장 범위 중 하나를 제외하고 모두 삭제     


</div>
</details> 

-----

## 2.3 Downstream Tasks
* Sentence acceptability judgment (CoLA (Warstadt et al., 2018))     
* Sentiment analysis (SST-2 (Socher et al., 2013))    
* Paraphrasing/sentence similarity (MRPC (Dolan and Brockett, 2005), STS-B (Ceret al., 2017), QQP (Iyer et al., 2017))    
* Natural language inference (MNLI (Williams et al., 2017), QNLI (Rajpurkar et al.,2016), RTE (Dagan et al., 2005), CB (De Marneff et al., 2019))      
* Coreference resolution (WNLI and WSC (Levesque et al., 2012))   
* Sentence completion (COPA (Roemmele et al., 2011))   
* Word sense disambiguation (WIC (Pilehvar and Camacho-Collados, 2018))   
* Question answering (MultiRC (Khashabi et al., 2018), ReCoRD (Zhang et al., 2018), BoolQ (Clark et al., 2019))      


------

## 2.4 Input and Output Format


**“text-to-text”**      
* context 또는 conditioning을 위해, 모델에 some text를 제공한 다음 some output text를 생성하도록 요청하는 작업     
* 즉 **input, output이 모두 text**      



**add a task-specific (text) prefix**      
* 모델이 **수행해야 하는 작업을 지정**하기 위해 입력 시퀀스에 추가      
* ex       
  * **translate task**       
  ex) 영어에서 독일어로 문장 "That is good"를 번역하는 task는,    
  모델의 fed the sequence가 ```“translate English to German: That is good."```이고,         
  모델은 ```“Das ist gut.”```를 출력하도록 훈련될 것이다.       
  * **text classification tasks**
  모델은 target label에 해당하는 single word만 예측           
  ex) MNLI benchmark의 목표는 전제가 가설을 암시하는지(“entailment”), 모순되는지(“contradiction”), 또는 둘 다(“neutral”)인지 여부를 예측하는 것임.       
  입력 시퀀스는 ```“mnli premise: I hate pigeons. hypothesis: My feelings towards pigeons are filled with animosity.”```이면,     
  모델은 target 단어 "entailment"을 예측해야한다.     
* task-specific (text) prefix의 선택은 **하이퍼파라미터**       
* 아래 "더 알아보기"에서 연구한 모든 작업에 대해 prefix 입력의 모든 예를 제공    




<img width="340" alt="image" src="https://github.com/yerimoh/yerimoh.github.io/assets/76824611/dd4e89c3-64a0-4245-a206-527cc0a22a34">


    

<details>
<summary>📜 더 알아보기: prefix 입력의 모든 예 </summary>
<div markdown="1">
  
**Text-to-text**     
Our text-to-text framework provides a simple way to train a single model
on a wide variety of text tasks using the same loss function and decoding procedure.
We showed how this approach can be successfully applied to generative tasks like
abstractive summarization, classification tasks like natural language inference, and
even regression tasks like STS-B. In spite of its simplicity, we found the text-totext framework obtained comparable performance to task-specific architectures and
ultimately produced state-of-the-art results when combined with scale.


**Architectures**        
While some work on transfer learning for NLP has considered architectural
variants of the Transformer, we found the original encoder-decoder form worked
best in our text-to-text framework. Though an encoder-decoder model uses twice as
many parameters as “encoder-only” (e.g. BERT) or “decoder-only” (language model)
architectures, it has a similar computational cost. We also showed that sharing the
parameters in the encoder and decoder did not result in a substantial performance
drop while halving the total parameter count.



**Unsupervised objectives**      
Overall, we found that most “denoising” objectives, which train
the model to reconstruct randomly corrupted text, performed similarly in the text-totext setup. As a result, we suggest using objectives that produce short target sequences
so that unsupervised pre-training is more computationally efficient.


**Data sets**         
We introduced the “Colossal Clean Crawled Corpus” (C4), which comprises
heuristically-cleaned text from the Common Crawl web dump. When comparing C4 to
data sets that use additional filtering, we found that training on in-domain unlabeled
data could boost performance in a few downstream tasks. However, constraining to
a single domain typically results in a smaller data set. We separately showed that
performance can degrade when an unlabeled data set is small enough that it is repeated
many times over the course of pre-training. This motivates the use of a large and
diverse data set like C4 for generic language understanding tasks.


**Training strategies**      
We found that the basic approach of updating all of a pre-trained
model’s parameters during fine-tuning outperformed methods that are designed to
update fewer parameters, although updating all parameters is most expensive. We also
experimented with various approaches for training the model on multiple tasks at once,
which in our text-to-text setting simply corresponds to mixing examples from different
data sets when constructing batches. The primary concern in multi-task learning is
setting the proportion of each task to train on. We ultimately did not find a strategy
for setting mixing proportions that matched the performance of the basic approach of
unsupervised pre-training followed by supervised fine-tuning. However, we found that
fine-tuning after pre-training on a mixture of tasks produced comparable performance
to unsupervised pre-training.


**Scaling**       
We compared various strategies for taking advantage of additional compute, including training the model on more data, training a larger model, and using an ensemble
of models. We found each approach conferred a significant boost in performance,
though training a smaller model on more data was often outperformed by training
a larger model for fewer steps. We also showed an ensemble of models can provide
substantially better results than a single model, which provides an orthogonal means
of leveraging additional computation. Ensembling models that were fine-tuned from
the same base pre-trained model performed worse than pre-training and fine-tuning
all models completely separately, though fine-tune-only ensembling still substantially
outperformed a single model.


**Pushing the limits**     
We combined our above insights and trained substantially larger
models (up to 11 billion parameters) to achieve state-of-the-art results across many of
the benchmarks we considered. For unsupervised training, we extracted text from our
C4 data set and applied a denoising objective that corrupts contiguous spans of tokens.
We pre-trained on a multi-task mixture before fine-tuning on individual tasks. Overall,
our models were trained on over 1 trillion tokens. In the interest of facilitating the
replication, extension, and application of our results, we release our code, the C4 data
set, and pre-trained model weights for each T5 variant.1

</div>
</details> 




---
---


# 3. Experiments 


## 3.1 Baseline

baseline에 대한 목표는 **typical modern 관행을 반영**하는 것이다.        
simple denoising objective를 사용하여 Transformer를 pre-train한 다음 각 downstream tasks을 별도로 fine-tune한다.       

### 3.1.1 Model


model: standard encoder-decoder Transformer를 사용      


우리의 기본 모델은 인코더와 디코더가 각각 "BERTBASE"(데블린 외, 2018) 스택과 크기와 구성이 유사하도록 설계되었습니다. 특히, 인코더와 디코더는 모두 12개의 블록(각 블록은 자기 주의, 선택적 인코더-디코더 주의 및 피드 포워드 네트워크로 구성됨)으로 구성됩니다. 각 블록의 피드포워드 네트워크는 출력 차원이 dff = 3072인 고밀도 레이어와 ReLU 비선형성 및 또 다른 고밀도 레이어로 구성됩니다. 모든 주의 메커니즘의 "키" 및 "값" 행렬은 dkv = 64의 내부 차원을 가지며 모든 주의 메커니즘에는 12개의 헤드가 있습니다. 다른 모든 하위 모델 및 임베딩의 치수는 d 모델 = 768입니다. 전체적으로, 이것은 약 2억 2천만 개의 매개 변수를 가진 모델이 됩니다. 기준 모델에 하나의 레이어 스택이 아닌 두 개의 레이어 스택이 포함되어 있기 때문에 이는 BERTBASE 매개 변수의 약 두 배입니다. 정규화를 위해 모델에 드롭아웃이 적용되는 모든 곳에서 드롭아웃 확률 0.1을 사용합니다.

Our baseline model is designed so that the encoder and decoder are each similar in
size and configuration to a “BERTBASE” (Devlin et al., 2018) stack. Specifically, both the
encoder and decoder consist of 12 blocks (each block comprising self-attention, optional
encoder-decoder attention, and a feed-forward network). The feed-forward networks in each
block consist of a dense layer with an output dimensionality of dff = 3072 followed by a
ReLU nonlinearity and another dense layer. The “key” and “value” matrices of all attention
mechanisms have an inner dimensionality of dkv = 64 and all attention mechanisms have 12
heads. All other sub-layers and embeddings have a dimensionality of dmodel = 768. In total,
this results in a model with about 220 million parameters. This is roughly twice the number
of parameters of BERTBASE since our baseline model contains two layer stacks instead of
one. For regularization, we use a dropout probability of 0.1 everywhere dropout is applied
in the model.


### 3.1.2 Training
As described in Section 2.4, all tasks are formulated as text-to-text tasks. This allows us to
always train using standard maximum likelihood, i.e. using teacher forcing (Williams and
Zipser, 1989) and a cross-entropy loss. For optimization, we use AdaFactor (Shazeer and
Stern, 2018). At test time, we use greedy decoding (i.e. choosing the highest-probability
logit at every timestep).

We pre-train each model for 2
19 = 524,288 steps on C4 before fine-tuning. We use a
maximum sequence length of 512 and a batch size of 128 sequences. Whenever possible,
we “pack” multiple sequences into each entry of the batch10 so that our batches contain
roughly 2
16 = 65,536 tokens. In total, this batch size and number of steps corresponds
to pre-training on 2
35 ≈ 34B tokens. This is considerably less than BERT (Devlin et al.,
2018), which used roughly 137B tokens, or RoBERTa (Liu et al., 2019c), which used roughly
2.2T tokens. Using only 2
35 tokens results in a reasonable computational budget while still
providing a sufficient amount of pre-training for acceptable performance. We consider the
effect of pre-training for more steps in Sections 3.6 and 3.7. Note that 2
35 tokens only covers
a fraction of the entire C4 data set, so we never repeat any data during pre-training





During pre-training, we use an “inverse square root” learning rate schedule: 1
p
max(n, k)
where n is the current training iteration and k is the number of warm-up steps (set to 104
in all of our experiments). This sets a constant learning rate of 0.01 for the first 104
steps,
then exponentially decays the learning rate until pre-training is over. We also experimented
with using a triangular learning rate (Howard and Ruder, 2018), which produced slightly
better results but requires knowing the total number of training steps ahead of time. Since
we will be varying the number of training steps in some of our experiments, we opt for the
more generic inverse square root schedule.


Our models are fine-tuned for 2
18 = 262,144 steps on all tasks. This value was chosen
as a trade-off between the high-resource tasks (i.e. those with large data sets), which
benefit from additional fine-tuning, and low-resource tasks (smaller data sets), which overfit
quickly. During fine-tuning, we continue using batches with 128 length-512 sequences (i.e.
2
16 tokens per batch). We use a constant learning rate of 0.001 when fine-tuning. We save
a checkpoint every 5,000 steps and report results on the model checkpoint corresponding
to the highest validation performance. For models fine-tuned on multiple tasks, we choose
the best checkpoint for each task independently. For all of the experiments except those in
Section 3.7, we report results in the validation set to avoid performing model selection on
the test set















