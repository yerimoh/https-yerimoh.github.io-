---
title: "[25] CS231N: Lecture 5 Convolutional Neural Networks"
date:   2020-02-8
excerpt: "Lecture 5 | Convolutional Neural Networks 요약"  
category: [Deep Learning]
layout: post
tag:
- Deep Learning
order: 0

comments: true
---



# 목차


------

👀 코드 보기 , 🤷‍♀️     
이 두개의 아이콘을 누르시면 코드, 개념 부가 설명을 보실 수 있습니다:)

------


[CS231N: Lecture 4](https://www.youtube.com/watch?v=bNb2fEVKeEo&list=PLC1qU-LWwrF64f4QKQT-Vg5Wr4qEE1Zxk&index=5)

------


오늘 배울것
CNN: NN과 같은 부류이긴 하지만 이번에는 Convolutional Layer에 대해 배울 것     
이 레이어는 기본적으로 "공간적 구조"를 유지       

CNN의 역사도


---
----

# **History of CNN**
전체 개요를 정리해보자면 아래와 같다.     



## 신경망의 

**[1957년: Mark I Perceptron machine 개발]**        
* **개발자**: Frank Rosenblatt            
* **의의**     
     * 이 기계는 ["perceptron"](https://yerimoh.github.io/DL1/)을 구현한 최초의 기계    
     * "Perceptron"은 우리가 배운 Wx + b 와 유사한 함수를 사용          
         * 다른점: 여기에서는 출력 값이 1 또는 0     
         * 같은점: 가중치 W를 Update 하는 Update Rule이 존재합니다.    
         * 이 Update Rule은 Backprop과 유사     
         * 하지만 당시에는 backprob이라는 개념이 없어서, 단지 W를 이리저리 조절하면서 맞추는 식이었음     


**[1960년: Adaline and Madaline을 개발]**      
* **개발자**: Widrow와 Hoff     
* **의의**    
    * 최초의 Multilayer Perceptron Network       
    * 이 시점에서야 비로소 Neural network와 비슷한 모양을 하기 시작하긴 했지만 아직 Backprob같은 학습 알고리즘은 없음        
    
    
**[1986: 최초의 Backporp]**           
* **개발자**: Rumelhart       
* **의의**    
   * 우리에게 익숙한 Chain rule과 Update rule        
   * 이때 최초로 network를 학습시키는 것에 관한 개념이 정립되기 시작함         
   * 하지만 그 이후로 NN을 더 크게 만들지는 못함.        


**[암흑기]**      
* 그리고 한동안은 새로운 이론이 나오지 못했고 널리 쓰이지도 못함      
* 2000년대가 되서야 다시 활기를 찾기 시작함       




**[2006: DNN의 학습가능성]**      
* **개발자**: Geoff Hinton 과 Ruslan Salakhutdinov      
* **의의**      
    * DNN의 학습가능성을 선보임      
    * 그것이 실제로 아주 효과적이라는 것을 보여줌      
    * 하지만 그 때 까지도 아직 모던한 NN는 아니었음     
    * backprop이 가능하려면 아주 세심한 초기화가 필요하기 때문          
    * 그래서 여기에서는 **전처리 과정이 필요**했고, 초기화를 위해 RBM을 이용해서 각 히든레이어 가중치를 학습시켜야 했음        
    * 이렇게 초기화된 히든 레이어를 이용해서 전체 신경망을 backprop하거나 fine tune하는 것이었음         



**[2012: NN의 광풍]**       
* **개발자**: Hintin lab            
* **의의**      
   * NN이 음성 인식에서 아주 좋은 성능을 보임     
   * acoustic modeling과 speech recognition에 관한 것



**[2012: AlexNet]**       
* **개발자**: Hintin lab            
* **의의**      
   * 영상인식에 관한 landmark paper      
   * 이 논문에서는 ImageNet Classification에서 최초로 NN을 사용했고, 결과는 정말 놀라웠음     
   * AlexNet은 ImageNet benchmark의 Error를 극적으로 감소시킴      
   * 그 이후로  ConNets은 아주 널리 쓰이고 있음             



## CNN의 역사    

다시 돌아가서 구체적으로 "CNN이 어떻게 유명해졌는지" 에 대해 한 번 알아보도록 하겠다.


**[1950:  일차시각피질의 뉴런에 관한 연구]**       
* **개발자**: Hubel과 Wiesel                 
* **의의**      
    * 고양이의 뇌에 전극을 꽂아 고양이에게 다양한 자극을 주며 실험       
    * 이 실험에서 뉴런이 oriented edges와 shapes같은 것에 반응한다는 것을 알아냄         
* **결론 **          
    * 피질 내부에 지형적인 매핑(topographical mapping)이 있다    
        * 피질 내 서로 인접해 있는 세포들은 visual field내에 어떤 지역성을 띄고 있음.       
        * 밑의 그림은 보면 해당하는 spatial mapping을 볼 수 있음     
        * 중심에서 더 벗어난 파란색 지역도 볼 수 있습니다.
    * 뉴런들이 계층구조를 지닌다는 것도 발견      
        * 다양한 종류의 시각자극을 관찰하면서 시각 신호가 가장 먼저 도달하는 곳이 바로 Retinal ganglion 이라는 것을 발견함        
        * Retinal ganglion cell은 원형으로 생긴 지역임     
        * 가장 상위에는 Simple cells이 있는데, 이 세포들은 다양한 edges의 방향과 빛의 방향에 반응함         
        * 그리고 더 나아가, 그런 Simple Cells 이 Complex cells과 연결되어 있다는 것을 발견함.       
        * Complex cells는 빛의 방향 뿐만 아니라 움직임에서 반응함       
        * 복잡도가 증가함게 따라, 가령  hypercomplex cells은 끝 점(end point) 과 같은것에 반응하게 되는 것입니다.
     * 이런 결과로부터  "corner" 나 "blob"에 대한 아이디어를 얻기 시작한 것임        



**[1980: neocognitron]**       
* **개발자**: Hubel과 Wiesel                 
* **의의**      
     * simple/complex cells의 아이디어를 사용한 최초의 NN     
     * Fukishima는 simple/complex cells을 교차시킴 (SCSCSC..)     
     * Simple cells은 학습가능한 parameters를 가지고 있고 Complex cells은 pooling과 같은 것으로 구현했는데
작은 변화에 Simple cells보다 좀 더 강인함.       





**[1998: Backprob과 gradient-based learning을 적용]**       
* **개발자**: Yann LeCun                 
* **의의**    
      * 1998년 Yann LeCun이 최초로 NN을 학습시키기 위해 Backprob과 gradient-based learning을 적용함.     
      * 실제로 그 방법은 문서인식에 아주 잘 동작함     
      * 그리고 우편번호의 숫자를 인식하는데도 아주 잘 동작함      
      * 그리고 실제 우편 서비스에서 우편번호 인식에 널리 쓰임         
      * 하지만 아직 이 Network를 더 크게만들 수는 없었습니다.     
      * 그리고 숫자 라는 데이터는 단순했습니다.       







**[2012: CNN의 현대화 바람]**       
* **개발자**: Alex Krizhevsky                      
* **의의**    
     * 이 Network는 AlexNet이라고도 불림      
     * Yann LeCun의 CNN과 크게 달라보이진 않지만 더 크고 깊어짐.         
     * 여기서 가장 중요한 점은 지금은 ImageNet dataset과 같이 대규모의 데이터를 활용할 수 있다는 것임.    
     * 또한 GPU의 힘도 있었음        



**[현재]**         
* ConvNets은 모든 곳에 쓰임     
* AlexNet의 ImageNet 데이터 분류 결과를 살펴보자면 이미지 검색에 정말 좋은 성능을 보이고 있음       
* 가령 꽃을 검색하는 것을 보면 학습된 특징이 유사한 것을 매칭시키는데 아주 강력하다는 것을 볼 수 있음         
* Detection에서도 ConvNet을 사용 ➡ 영상 내에 객체가 어디에 있는지를 아주 잘 찾아냄          
* segmentation 가능: 단지 네모박스만 치는 것이 아니라 나무나 사람 등을 구별하는데 픽셀 하나 하나에 모두 레이블링하는 것임 ➡ 이런 알고리즘은 자율주행 자동차에 사용 가능      
* 대부분의 작업은 GPU가 수행할 수 있으며, 병렬처리를 통해 ConvNet을 아주 효과적으로 훈련하고 실행시키기 가능        
* 이와 같이 많은 분야에서 사용되고 있음      








----
-----




# Convolutional Neural NetworK

Convolutional Neural Network의 작동 원리에 대해 알아 볼 것이다.    

이번에 CNN의 첫 시간이니, CNN이 어떻게 동작하는지에 대해서만 간단하게 이야기해 볼 것이다.      


    



---


## Fully Connected Layer(FC Layer)     
본격적인 CNN을 들어가기 전, 그 보다 하위 모델인 FC Layer를 면저 보고 가겠다.     

**[역할]**     
어떤 벡터를 가지고 연산 함


**[과정]**   
* **1)** 우선 입력으로 32 x 32 x 3 의 이미지가 가 있다고 가정하자      
* **2)** 이 이미지를 길게 펴서 3072차원의 벡터로 만든다.         
* **3)** 가중치 W와 벡터를 곱한다. (Wx)   
     * 이 예시에서는 W가 10x3072 행렬이다         
* **4)** 그리고 activation 을 얻는다    
     * 이 Layer의 출력이다.     
     * 출력: 10개의 행으로 되어있는데 3072 차원의 입력와 내적을 한 결과라고 할 수 있음
     * 내적 결과, 어떤 숫자 하나를 얻게 되는데 이는 그 Neuron의 한 값이라고 할 수 있음
     * 이 예시의 경우 10개의 출력이 있게 됨       

![image](https://user-images.githubusercontent.com/76824611/172739470-79d128a9-4fdf-4244-bfc8-6ec55544879b.png)


**[단점]**            
**2)** 의 과정에서 이미지를 이미지 자체로 못보고 길게펴서 보기 떄문에 **이미지 자체의 픽셀의 위치 정보가 무시**되게 됨   
그런데 Convolution Layer은 기존의 FC레이어와 달리 Convolution Layer는 기존의 구조를 보존시켜 위의 단점을 보완함.            
➡ 기존의 FC Layer가 입력 이미지를 길게 쭉 펴서 이미지 위치 정보를 잃었다면,    
➡ CNN은 기존의 이미지 구조를 그대로 유지하여 입력값으로 받아 위치 정보를 유지하게됨.          


---
----


# CNN
이 강의를 보기 전에 [이 포스트](https://yerimoh.github.io/DL8/)를 보고오면 훨씬 이해가 잘 될 것이다.     
* **input image**          
   * FC Layer와 달리 input 이미지가 한줄로 펴진것이 아닌 이미지가 보존 된 상태로 들어옴     
   ![image](https://user-images.githubusercontent.com/76824611/172740204-dc4924ee-4f77-493c-ad8f-69284e4abe0b.png)

* **filter**
   * 가중치다.     
   * input image에 맞춰 가중치도 이와 같은 필터이다.          
   * 이 예시에서는 5x5x3 필터가, 이 필터를 가지고 이미지를 슬라이딩하면서 공간적으로 내적을 수행하게 된다.         
   ![image](https://user-images.githubusercontent.com/76824611/172740216-ff9a024c-f791-4b71-b253-4a1638b96ccb.png)
  


--


## 필터의 연산
이제 이것을 어떻게 수행하는지 자세하게 알아보자


**[필터 연산]**    
* **1) 필터의 깉이 확장**       
    * 우선 필터는 입력의 깊이(Depth)만큼 확장된다.           
    * CNN의 하나의 필터는 아주 작은 부분만 취할 수 있다 ➡ 전체 32x32 이미지의 5x5 만 취하는 것            
    * 하지만 **깊이**는 전체 깊이를 전부 취함 ➡ 여기에서는 5 x 5 x **3** 가 됨(image: 32 x 32 x **3**)     
    * 이 깊이는 주로 이미지의 색([RGB](https://yerimoh.github.io//C1/#rgb-%EC%83%89%EA%B3%B5%EA%B0%84))를 의미한다.      
    ![image](https://user-images.githubusercontent.com/76824611/172740886-cf0d136c-fb91-4ba9-b032-4699d8857469.png)
* **2) 내적**    
    * 이 필터를 가지고 전체 이미지에 내적을 시킬 것이다.    
    * 이 필터를 이미지의 어떤 공간에 겹쳐놓고 내적을 수행한다.     
    * 그리고 필터의 각 w와, 이에 해당하는 이미지의 픽셀을 곱해준다    
    * 여기에 필터가 5 x 5 x 3 라는건 그만큼 곱셈연산을 한다는 것이다.       
    * bias term인 b도 보인다(얜 곱셈이 아니다!)     
    * 여기에서는 기본적으로 $$W^{t}x + b$$를 수행한다.     
    ![image](https://user-images.githubusercontent.com/76824611/172741567-c45fc2d6-15cb-4488-94c6-07e3548b4d14.png)
* **3) 슬라이딩**    
    * 그럼 필터 전체의 연산을 보겠다.     
    * 실제는 슬라이딩을 하며 각 슬라이딩 구역에 앞에서 살펴본 내적을 하는 것이다.      
        * Convolution은 이미지의 좌상단부터 시작한다      
        * 그리고 필터의 중앙을 값들을 모은다.       
        * 필터의 모든 요소를 가지고 내적을 수행하게 되면 하나의 값을 얻게된다     
        * 그리고 슬라이딩한다.        
    * Conv연산을 수행하는 값들을 다시 Output activation map의 해당하는 위치에 저장한다.        
    ![image](https://user-images.githubusercontent.com/76824611/172745021-1b5f1959-e848-44e7-9e66-3df8824db1ca.png)


**[출력크기가 입력크기와 다른 이유]**     
* 여기 보면 입력 이미지와 출력 activation map의 차원이 다르다는 것을 알 수 있다.   
    * ➡ 입력은 32 x 32 이고 출력은 28 x  28 이다    
    * 그래서 우리는 나중에 수학에 대해 설명 할 것이다.     
    * 수학적 설명 후엔 이 방법이 차원 적으로 어떻게 작동하는지 정확히 알 수 있다.     
    ![image](https://user-images.githubusercontent.com/76824611/172747478-22ac7d23-7987-4baa-b32d-39f6985bd65a.png)
* 기본적으로는 어떻게 슬라이딩을 할 것인지를 선택할 수 있다.     
    * 출력 형을 위와 같이 맞추기 위해선 슬라이딩여부와 관계없이 입력 값을 두 개씩 뽑아서 연산을 수행할 수도 있을 것이다.(하지만 이렇게 하지는 않을 것이다.)       
* 출력 행렬의 크기는 슬라이드를 어떻게 하느냐에 따라 다르다.    
    * 하지만 보편적으로 사용하는 슬라이딩 방법이 있다      


**[여러개의 필터 사용]**       
하나의 하나의 필터를 가지고 전체 이미지에 Convolution 연산을 수행한다.      
➡ 그러면 activation map이라는 출력값을 얻게 되는 것입니다.


보통 Convolution Layer에서는 여러개의 필터를 사용합니다.
왜냐하면 필터마다 다른 특징을 추출하고 싶기 때문입니다.

185
00:22:16,250 --> 00:22:26,359
따라서 우리는 보통 여러개의 필터를 사용합니다. 가령 여기에
두 번째 필터가 있습니다. 초록색의 5 x 5 x 3 필터입니다.

186
00:22:26,359 --> 00:22:37,425
이 녹색 필터를 연산하고 나면, 앞서 계산했던 activate map과
같은 크기의 새로운 map이 만들어집니다.

187
00:22:40,081 --> 00:22:43,553
우리는 한 Layer에서 원하는 만큼 여러개의 필터를
사용할 수 있습니다.

188
00:22:43,553 --> 00:22:51,698
가령 5 x 5 필터가 6개가 있다면
총 6개의 activation map을 얻게 될 것입니다.

189
00:22:51,698 --> 00:22:58,368
출력 map의 크기는 28 x 28이 되겠죠.

190
00:23:01,607 --> 00:23:11,152
이 CNN을 어떻게 활용할 지를 조금 말씀드려보면
이런식으로  Cov Layer들의 연속된 형태가 될 것입니다.

191
00:23:11,152 --> 00:23:16,676
그리고 각각을 쌓아 올리게 되면 보시는 것과 같이
간단한 Linear Layer로 된 Neural Network가 됩니다.

192
00:23:16,676 --> 00:23:23,057
이제는 그 사이 사이에 activation function을 넣을 것입니다.
가령 ReLU 같은 것들을 사용할 수 있겠죠.

193
00:23:24,503 --> 00:23:31,257
그렇게 되면 Conv-ReLU 가 반복되겠죠
그리고 가끔은 pooling layer도 들어갑니다.

194
00:23:31,257 --> 00:23:40,465
그리고 각 Layer의 출력은 다음 Layer의 입력이 됩니다.

195
00:23:43,638 --> 00:23:52,957
그리고 말씀드렸듯 각 Layer는 여러개의 필터를 가지고 있습니다.
그리고 각 필터마다 각각의 출력 map을 만듭니다.

196
00:23:52,957 --> 00:24:01,175
그러므로 여러개의 Layer들을 쌓고나서 보면 결국
각 필터들이 계층적으로 학습을 하는것을 보게됩니다.

197
00:24:01,175 --> 00:24:09,257
앞쪽에 있는 필터들은 low-level feature를 학습하게 됩니다.
Edge와 같은것들이 보입니다.

198
00:24:09,257 --> 00:24:19,113
Mid-level을 보면 좀더 복잡한 특징을 가지게 됩니다.
코너나 blobs 등과 같이 보입니다.

199
00:24:19,113 --> 00:24:25,852
그리고 high-level features를 보면 좀 더 객체와 닮은 것들이
출력으로 나오는 것을 볼 수 있습니다.

200
00:24:25,852 --> 00:24:35,561
나중 수업에서 객 특징을 어떻게 시각화하는지, 그리고 특징들이
어떻게 학습되는지를 살펴보겠습니다.

201
00:24:35,561 --> 00:24:46,967
여기에서 이해하고 넘어가야 하는 것은 특징이 어떻게 생겼고,
Layer의 계층에 따라 단순/복잡한 특징을이 존재한다는 것을 아는 것입니다.

202
00:24:48,305 --> 00:24:49,138
[학생이 질문]

203
00:24:49,138 --> 00:25:03,113
질문은 바로 필터의 Depth를 늘리는데 어떤 직관을 가져야 하는지 입니다.

204
00:25:03,113 --> 00:25:08,814
이 예시에서 처음에는 3개의 필터가 그 다음에는
6개의 필터가 있었습니다.

205
00:25:08,814 --> 00:25:17,255
이는 어떻게 모델을 디자인해야 되는지의 문제인데
실제로는 어떤것이 더 좋은지를 찾아내야 합니다.

206
00:25:17,255 --> 00:25:28,344
나중 수업에서 다양한 CNN 아키텍쳐를 살펴보면서 왜 어떤
모델이 더 좋은지를 살펴볼 것입니다.

207
00:25:28,344 --> 00:25:33,238
하지만 기본적으로 여러분은 아주 다양하 방법으로
CNN 모델을 디자인할 수 있습니다.

208
00:25:33,238 --> 00:25:39,611
필터 사이즈라던가 Stride, 그리고 얼마나 많은 필터를
사용할지 등을 말이죠. 다음에 다시 이야기하겠습니다.

209
00:25:39,611 --> 00:25:41,246
질문있나요?

210
00:25:41,246 --> 00:25:43,996
[학생이 질문]

211
00:25:50,300 --> 00:26:00,177
질문은 바로 우리가 전체 이미지를 슬라이딩 하면서 필터링을 하는데
이미지의 가장자리는 필터가 덜 적용되지 않냐는 것입니다.

212
00:26:00,177 --> 00:26:07,900
아주 좋은 질문 입니다. 그리고 어떻게 그것을 다시 보완해야
하는지가 앞으로 할 내용입니다. (ex zero-padding)

213
00:26:12,009 --> 00:26:26,228
지금까지는 Conv Layer를 계층적으로 쌓아서 단순한 특징을을 뽑고
그것을 또 조합해서 더 복잡한 특징으로 활용했습니다.

214
00:26:26,228 --> 00:26:32,549
그리고 이는 Hubel과 Wiesel의 이론과도 잘 맞습니다.

215
00:26:32,549 --> 00:26:39,532
네트워크에 앞쪽에서는 단순한 것들일 처리하고
뒤로 갈수록 점점 더 복잡해 지는 식이죠

216
00:26:39,532 --> 00:26:55,041
우리는 그것을 강제로 학습시킨 것이 아니라 계층적 구조를 설계하고
역전파로 학습시킨 것 뿐이지만 필터는 이렇게 학습되는 것입니다.

217
00:26:55,041 --> 00:26:57,791
[학생의 질문]

218
00:27:05,555 --> 00:27:10,979
질문은 바로 여기에서 시각화 한 것이
무엇인지 입니다.

219
00:27:10,979 --> 00:27:20,975
가령 Cov1을 보면 각 그리드의 요소가
하나의 뉴런(필터) 라고 보시면 됩니다.

220
00:27:20,975 --> 00:27:29,956
그리고 시각화 시킨 이 뉴런의 모습은 바로 이미지가 어떻게 생겨야 해당
뉴런의 활성을 최대화시킬 수 있는지는 나타내는 것입니다.

221
00:27:29,956 --> 00:27:36,594
이미지가 뉴런과 비슷하게 생겼으면 출력 값을 큰 값을 가지게 됩니다.

222
00:27:36,594 --> 00:27:56,280
시각화는 Backpopagation을 통해 해볼 수 있는데.
이를 시각화하는 방법은 나중에 더 자세하게 배울 것입니다.

223
00:27:56,280 --> 00:28:06,775
그러나 기본적으로 여기 그리드의 각 요소는 각 뉴런의 활성을
최대화시키는 입력의 모양을 나타내게 됩니다.

224
00:28:06,775 --> 00:28:10,608
그런 의미에서 뉴런이 어떻게 생겼는지를 의미합니다.

225
00:28:13,537 --> 00:28:19,835
Activation map의 예제를 한 번 보겠습니다.
각 필터가 만든 출력값이죠

226
00:28:19,835 --> 00:28:30,407
위에 5 x 5 필터들을 시각화 한 것을 볼 수 있습니다.
실제 ConvNet을 학습시킨 결과입니다.

227
00:28:30,407 --> 00:28:38,511
5 x 5 필터가 어떻게 생겼는지를 보여 줍니다. 그리고
이미지와 필터간 Conv의 activation 결과를 볼 수 있습니다.

228
00:28:38,511 --> 00:28:44,346
이 경우 입력 이미지는 자동차의 한 부분인것 같습니다.

229
00:28:44,346 --> 00:28:51,330
우선 여기 빨간색 네모박스를 친 필터를 한 번 봅시다.

230
00:28:51,330 --> 00:28:56,432
이 필터는 edge를 찾고 있는 것입니다.

231
00:28:56,432 --> 00:29:06,601
그리고 이 필터를 슬라이딩 시키면 이 필터와 비슷한 값들은 값이
더 커지게 됩니다.

232
00:29:06,601 --> 00:29:12,358
따라서 각 actavation은 이미지가 필터를 통과한
결과가 되며

233
00:29:12,358 --> 00:29:20,747
이미지 중 어느 위치에서 이 필터가 크게 반응하지는지를
보여줍니다.

234
00:29:20,747 --> 00:29:29,153
우리가 이걸 Convolution이러고 칭하는 이유는 바로 위에 언급
한 것이 바로 두 신호 사이에 cov를 하는것과 유사하기 때문입니다.

235
00:29:29,153 --> 00:29:38,927
여기 Conv 식이 있습니다. 예전에 신호처리에서 Conv를 본 적이
있는 학생이라면 이 식이 correlation 같아 보일 것입니다.

236
00:29:38,927 --> 00:29:50,149
우리는 사실 Conv의 뒤집힌 버전을 쓰고 있는 것입니다.
하지만 이 수업에서는 이를 더 자세히 다루지는 않겠습니다.

237
00:29:50,149 --> 00:29:58,385
하지만 손으로 직접 계산해보면 conv에 정확한 정의와
크게 다르지 않을 것입니다.

238
00:29:58,385 --> 00:30:06,432
하지만 기본적으로는 필터를 가지고 이미지에 슬라이딩하면서
모든 위치에서 내적을 수행하게 되는 것입니다.

239
00:30:09,088 --> 00:30:17,278
이전에도 언급했듯이 CNN이 어떻게 수행되는지를 살펴보면

240
00:30:17,278 --> 00:30:28,236
입력 이미지는 여러 레이어를 통과하게 됩니다. 가령 첫 번째
Conv Layer 후에는  non-linear layer를 통과합니다.

241
00:30:28,236 --> 00:30:33,608
나중에 배우겠지만, ReLU를 가장 많이 사용합니다.

242
00:30:33,608 --> 00:30:41,244
Conv, ReLU, Conv, ReLU를 하고나면 pooling layer를
거치게 됩니다. 다시 배우겠지만-

243
00:30:41,244 --> 00:30:45,411
pooling 은 activation maps의 사이즈를
줄이는 역할을 합니다.

244
00:30:47,300 --> 00:30:56,872
그리고 CNN의 끝단에는 FC-Layer가 있습니다.
FC-Layer는 지난시간까지 배운 레이어입니다.

245
00:30:56,872 --> 00:31:07,178
마지막 Conv 출력 모두와 연결되어 있으며 최종 스코어를 계산하기
위해 사용합니다.

246
00:31:08,445 --> 00:31:14,181
그럼 이제 "Spatial dimension"에 대해 알아보겠습니다.

247
00:31:18,363 --> 00:31:28,025
자 여기 32 x 32 x 3 이미지가 있습니다. 그리고 이 이미지를
5 x 5 x 3 필터를 가지고 연산을 수행합니다.

248
00:31:28,025 --> 00:31:34,337
이제 어떻게 이 둘을 가지고 28 x 28 activation map이
생기는지를 알아보겠습니다.

249
00:31:34,337 --> 00:31:41,505
간단한 예시로 7 x 7  입력에 3 x 3 필터가 있다고 해봅시다.

250
00:31:41,505 --> 00:31:47,418
이제 이 필터를 이미지의 왼상당부터 씌웁니다.

251
00:31:47,418 --> 00:31:53,169
그리고 이제 해당 값들의 내적을 수행할 것입니다.

252
00:31:53,169 --> 00:31:56,764
이 값들은 activation map의 좌 상단에 위치하게 되겠죠

253
00:31:56,764 --> 00:32:02,389
그리고 다음 단계로 필터를 오른쪽으로 한칸 움직입니다.

254
00:32:02,389 --> 00:32:05,535
그러면 값 하나를 또 얻을 수 있을 것입니다.

255
00:32:05,535 --> 00:32:14,528
이렇게 계속 반복하게 되면  결국 5 x 5의 출력을 얻게 됩니다.

256
00:32:14,528 --> 00:32:25,381
이 슬라이드 필터는 좌우 방향으로는 5번만 수행가능하고
상하 방향으로도 5번만 수행가능합니다.

257
00:32:27,834 --> 00:32:31,906
그리고 여기에는 다양한 방법을 이용해 볼 수 있습니다.

258
00:32:31,906 --> 00:32:40,326
지금까지는 슬라이딩을 한칸씩만 진행했었죠.
이때 움직이는 칸을  바로 "stride" 라고 합니다.

259
00:32:40,326 --> 00:32:46,700
지금까지는 stide = 1을 사용했습니다.
그럼 stride = 2 일때는 어떨까요?

260
00:32:46,700 --> 00:32:58,944
다시 왼쪽 위부터 시작해서 움직입니다. 다만 여기에서는
1칸은 건너뛰고 그 다음칸으로 이동해서 계산을 합니다.

261
00:33:00,773 --> 00:33:11,257
이렇게 tride가 2 이면 3칸이면 못움직이고
결국 출력은 3 x 3 이 됩니다.

262
00:33:13,035 --> 00:33:18,653
그렇다면 stride = 3 이면 출력의 사이즈는 몇일까요?

263
00:33:18,653 --> 00:33:27,905
stride가 3인 경우에는 이미지를 슬라이딩해도 필터가
모든 이미지를 커버할 수 없습니다.

264
00:33:27,905 --> 00:33:32,363
이 예시에서 stride 가 3이면 이미지에 잘 맞아떨어지지 않습니다.

265
00:33:32,363 --> 00:33:41,903
실제로 이렇게 되면 잘 동작하지 않습니다. 이렇게 하면 안됩니다.
이로 인해 불균형한 결과를 볼 수도 있기 때문입니다.

266
00:33:46,095 --> 00:33:54,690
그래서 상황에 따라 출력의 사이즈가 어떻게 될 것인지를
계산할 수 있는 아주 유용한 수식이 있습니다.

267
00:33:54,690 --> 00:34:05,597
입력의 차원이 N이고 필터 사이즈가 F이고
스트라이드가 몇이다 라고 주어지게 되면

268
00:34:06,992 --> 00:34:12,850
출력의 크기는 (N - F) / stride + 1 이 됩니다.

269
00:34:12,850 --> 00:34:18,619
이를 이용해서 어떤 필터 크기를 사용해야 하는지를 알 수 있습니다.

270
00:34:18,620 --> 00:34:27,326
그리고 어떤 stride를 사용했을때 이미지에 꼭 맞는지,
그리고 몇 개의 출력값을 낼 수 있는지도 알 수 있습니다.

271
00:34:29,257 --> 00:34:32,546
지금까지 유용한 수식을 한 번 알아봤습니다.

272
00:34:32,547 --> 00:34:38,637
앞서 보신바와 같이 N = 7 이고 F = 3 일때,
그리고 stride가 1이면

273
00:34:38,637 --> 00:34:43,498
이걸 그대로 수식에 적용해보면 5 x 5 출력이
나올 것이라는 것을 알 수 있습니다.

274
00:34:43,498 --> 00:34:47,665
stride가 3인 경우를 보면, 잘 동작하지 않겠죠. (2.33)

275
00:34:50,288 --> 00:34:59,552
그리고 가장 흔히 쓰는 기법은 zero-pad 입니다.
출력의 사이즈 의도대로 만들어 주기 위해서죠

276
00:34:59,552 --> 00:35:04,140
이는 이전 질문이었던 "코너는 어떻게 처리하나요"
와도 연결되는 문제입니다.

277
00:35:04,140 --> 00:35:09,222
이를 위해 할 일은, 이미지의 가장자리에 0을 채워 넣는 것입니다.

278
00:35:09,222 --> 00:35:19,134
이렇게 되면 좌 상단의 자리에서도 필터 연산을
수행할 수 있게 됩니다.

279
00:35:19,134 --> 00:35:33,654
자 이제 다시 질문입니다. 7 x 7 입력에 3 x 3 필터 연산을 수행할 때
zero-padding을 하면 출력이 어떻게 될까요?

280
00:35:33,654 --> 00:35:36,285
[학생들이 대답]

281
00:35:36,285 --> 00:35:44,847
몇 명이 6이라고 하는 것 같은데요
저 수식을 한 번 잘 생각해 보세요

282
00:35:44,847 --> 00:35:52,594
N에 7을, F에 3을 대입하고 stride를 1으로 하면

283
00:35:52,594 --> 00:35:57,264
그래서 우리가 실제로 얻는 것, 실제로
이것은 우리에게주는 것입니다.

284
00:35:57,264 --> 00:36:06,707
(7 - 3) /1 + 1 = 5 입니다. 이건 zero-padding이 없고
우리는 이 수식을 살짝 고쳐야합니다.

285
00:36:06,707 --> 00:36:12,161
이 수식은 zero-padding을 하지 않은 수식이죠

286
00:36:12,161 --> 00:36:24,173
하지만 zero-padding을 하면 출력이 7이 됩니다.
그래서 결국 7 x 7 출력을 얻게 되는 것입니다.

287
00:36:24,173 --> 00:36:30,178
원래 공식으로 돌아가보자면 N은 7이 아니라 9가 되겠죠

288
00:36:30,178 --> 00:36:42,253
N = 9 가 되고 필터 사이즈인 3을 빼면 6이 됩니다. 이를 stride
1나누면 다시 6이고 여기에 1을 더하면 7이 됩니다.

289
00:36:42,253 --> 00:36:47,974
padding을 하려면 수식을 이런식으로 적용하면 되겠습니다.

290
00:36:49,739 --> 00:36:51,646
질문 있나요?

291
00:36:51,646 --> 00:36:54,396
[학생이 질문]

292
00:37:00,717 --> 00:37:08,962
질문은 "실제 출력 값의 사이즈는 몇입니까?
7 x 7 입니까 7 x 7 x 3 입니까?" 입니다.

293
00:37:08,962 --> 00:37:14,495
출력은 7 x 7 x "필터의 갯수" 가 됩니다.

294
00:37:14,495 --> 00:37:21,320
명심해야할 점은 각 필터가 입력의 모든 depth에
내적을 수행한다는 것입니다.

295
00:37:21,320 --> 00:37:23,801
하나의 값만 나오겠죠

296
00:37:23,801 --> 00:37:37,124
다시 이전 슬라이드로 돌아가보겠습니다. 이전의 예제에서는
각 필터는 7 x 7 x 1이 되겠죠

297
00:37:37,124 --> 00:37:40,493
activation map의 depth는 우리

























