---
title: "[25] CS231N: Lecture 5 Convolutional Neural Networks"
date:   2020-02-8
excerpt: "Lecture 5 | Convolutional Neural Networks 요약"  
category: [Deep Learning]
layout: post
tag:
- Deep Learning
order: 0

comments: true
---



# 목차


------

👀 코드 보기 , 🤷‍♀️     
이 두개의 아이콘을 누르시면 코드, 개념 부가 설명을 보실 수 있습니다:)

------


[CS231N: Lecture 4](https://www.youtube.com/watch?v=bNb2fEVKeEo&list=PLC1qU-LWwrF64f4QKQT-Vg5Wr4qEE1Zxk&index=5)

------


오늘 배울것
CNN: NN과 같은 부류이긴 하지만 이번에는 Convolutional Layer에 대해 배울 것     
이 레이어는 기본적으로 "공간적 구조"를 유지       

CNN의 역사도


---
----

# **History of CNN**
전체 개요를 정리해보자면 아래와 같다.     



## 신경망의 

**[1957년: Mark I Perceptron machine 개발]**        
* **개발자**: Frank Rosenblatt            
* **의의**     
     * 이 기계는 ["perceptron"](https://yerimoh.github.io/DL1/)을 구현한 최초의 기계    
     * "Perceptron"은 우리가 배운 Wx + b 와 유사한 함수를 사용          
         * 다른점: 여기에서는 출력 값이 1 또는 0     
         * 같은점: 가중치 W를 Update 하는 Update Rule이 존재합니다.    
         * 이 Update Rule은 Backprop과 유사     
         * 하지만 당시에는 backprob이라는 개념이 없어서, 단지 W를 이리저리 조절하면서 맞추는 식이었음     


**[1960년: Adaline and Madaline을 개발]**      
* **개발자**: Widrow와 Hoff     
* **의의**    
    * 최초의 Multilayer Perceptron Network       
    * 이 시점에서야 비로소 Neural network와 비슷한 모양을 하기 시작하긴 했지만 아직 Backprob같은 학습 알고리즘은 없음        
    
    
**[1986: 최초의 Backporp]**           
* **개발자**: Rumelhart       
* **의의**    
   * 우리에게 익숙한 Chain rule과 Update rule        
   * 이때 최초로 network를 학습시키는 것에 관한 개념이 정립되기 시작함         
   * 하지만 그 이후로 NN을 더 크게 만들지는 못함.        


**[암흑기]**      
* 그리고 한동안은 새로운 이론이 나오지 못했고 널리 쓰이지도 못함      
* 2000년대가 되서야 다시 활기를 찾기 시작함       




**[2006: DNN의 학습가능성]**      
* **개발자**: Geoff Hinton 과 Ruslan Salakhutdinov      
* **의의**      
    * DNN의 학습가능성을 선보임      
    * 그것이 실제로 아주 효과적이라는 것을 보여줌      
    * 하지만 그 때 까지도 아직 모던한 NN는 아니었음     
    * backprop이 가능하려면 아주 세심한 초기화가 필요하기 때문          
    * 그래서 여기에서는 **전처리 과정이 필요**했고, 초기화를 위해 RBM을 이용해서 각 히든레이어 가중치를 학습시켜야 했음        
    * 이렇게 초기화된 히든 레이어를 이용해서 전체 신경망을 backprop하거나 fine tune하는 것이었음         



**[2012: NN의 광풍]**       
* **개발자**: Hintin lab            
* **의의**      
   * NN이 음성 인식에서 아주 좋은 성능을 보임     
   * acoustic modeling과 speech recognition에 관한 것



**[2012: AlexNet]**       
* **개발자**: Hintin lab            
* **의의**      
   * 영상인식에 관한 landmark paper      
   * 이 논문에서는 ImageNet Classification에서 최초로 NN을 사용했고, 결과는 정말 놀라웠음     
   * AlexNet은 ImageNet benchmark의 Error를 극적으로 감소시킴      
   * 그 이후로  ConNets은 아주 널리 쓰이고 있음             



## CNN의 역사    

다시 돌아가서 구체적으로 "CNN이 어떻게 유명해졌는지" 에 대해 한 번 알아보도록 하겠다.


**[1950:  일차시각피질의 뉴런에 관한 연구]**       
* **개발자**: Hubel과 Wiesel                 
* **의의**      
    * 고양이의 뇌에 전극을 꽂아 고양이에게 다양한 자극을 주며 실험       
    * 이 실험에서 뉴런이 oriented edges와 shapes같은 것에 반응한다는 것을 알아냄         
* **결론 **          
    * 피질 내부에 지형적인 매핑(topographical mapping)이 있다    
        * 피질 내 서로 인접해 있는 세포들은 visual field내에 어떤 지역성을 띄고 있음.       
        * 밑의 그림은 보면 해당하는 spatial mapping을 볼 수 있음     
        * 중심에서 더 벗어난 파란색 지역도 볼 수 있습니다.
    * 뉴런들이 계층구조를 지닌다는 것도 발견      
        * 다양한 종류의 시각자극을 관찰하면서 시각 신호가 가장 먼저 도달하는 곳이 바로 Retinal ganglion 이라는 것을 발견함        
        * Retinal ganglion cell은 원형으로 생긴 지역임     
        * 가장 상위에는 Simple cells이 있는데, 이 세포들은 다양한 edges의 방향과 빛의 방향에 반응함         
        * 그리고 더 나아가, 그런 Simple Cells 이 Complex cells과 연결되어 있다는 것을 발견함.       
        * Complex cells는 빛의 방향 뿐만 아니라 움직임에서 반응함       
        * 복잡도가 증가함게 따라, 가령  hypercomplex cells은 끝 점(end point) 과 같은것에 반응하게 되는 것입니다.
     * 이런 결과로부터  "corner" 나 "blob"에 대한 아이디어를 얻기 시작한 것임        



**[1980: neocognitron]**       
* **개발자**: Hubel과 Wiesel                 
* **의의**      
     * simple/complex cells의 아이디어를 사용한 최초의 NN     
     * Fukishima는 simple/complex cells을 교차시킴 (SCSCSC..)     
     * Simple cells은 학습가능한 parameters를 가지고 있고 Complex cells은 pooling과 같은 것으로 구현했는데
작은 변화에 Simple cells보다 좀 더 강인함.       





**[1998: Backprob과 gradient-based learning을 적용]**       
* **개발자**: Yann LeCun                 
* **의의**    
      * 1998년 Yann LeCun이 최초로 NN을 학습시키기 위해 Backprob과 gradient-based learning을 적용함.     
      * 실제로 그 방법은 문서인식에 아주 잘 동작함     
      * 그리고 우편번호의 숫자를 인식하는데도 아주 잘 동작함      
      * 그리고 실제 우편 서비스에서 우편번호 인식에 널리 쓰임         
      * 하지만 아직 이 Network를 더 크게만들 수는 없었습니다.     
      * 그리고 숫자 라는 데이터는 단순했습니다.       







**[2012: CNN의 현대화 바람]**       
* **개발자**: Alex Krizhevsky                      
* **의의**    
     * 이 Network는 AlexNet이라고도 불림      
     * Yann LeCun의 CNN과 크게 달라보이진 않지만 더 크고 깊어짐.         
     * 여기서 가장 중요한 점은 지금은 ImageNet dataset과 같이 대규모의 데이터를 활용할 수 있다는 것임.    
     * 또한 GPU의 힘도 있었음        



**[현재]**         
* ConvNets은 모든 곳에 쓰임     
* AlexNet의 ImageNet 데이터 분류 결과를 살펴보자면 이미지 검색에 정말 좋은 성능을 보이고 있음       
* 가령 꽃을 검색하는 것을 보면 학습된 특징이 유사한 것을 매칭시키는데 아주 강력하다는 것을 볼 수 있음         
* Detection에서도 ConvNet을 사용 ➡ 영상 내에 객체가 어디에 있는지를 아주 잘 찾아냄          
* segmentation 가능: 단지 네모박스만 치는 것이 아니라 나무나 사람 등을 구별하는데 픽셀 하나 하나에 모두 레이블링하는 것임 ➡ 이런 알고리즘은 자율주행 자동차에 사용 가능      
* 대부분의 작업은 GPU가 수행할 수 있으며, 병렬처리를 통해 ConvNet을 아주 효과적으로 훈련하고 실행시키기 가능        
* 이와 같이 많은 분야에서 사용되고 있음      








----
-----




# Convolutional Neural NetworK

Convolutional Neural Network의 작동 원리에 대해 알아 볼 것이다.    

이번에 CNN의 첫 시간이니, CNN이 어떻게 동작하는지에 대해서만 간단하게 이야기해 볼 것이다.      


    



---


## Fully Connected Layer(FC Layer)     
본격적인 CNN을 들어가기 전, 그 보다 하위 모델인 FC Layer를 면저 보고 가겠다.     

**[역할]**     
어떤 벡터를 가지고 연산 함


**[과정]**   
* **1)** 우선 입력으로 32 x 32 x 3 의 이미지가 가 있다고 가정하자      
* **2)** 이 이미지를 길게 펴서 3072차원의 벡터로 만든다.         
* **3)** 가중치 W와 벡터를 곱한다. (Wx)   
     * 이 예시에서는 W가 10x3072 행렬이다         
* **4)** 그리고 activation 을 얻는다    
     * 이 Layer의 출력이다.     
     * 출력: 10개의 행으로 되어있는데 3072 차원의 입력와 내적을 한 결과라고 할 수 있음
     * 내적 결과, 어떤 숫자 하나를 얻게 되는데 이는 그 Neuron의 한 값이라고 할 수 있음
     * 이 예시의 경우 10개의 출력이 있게 됨       

![image](https://user-images.githubusercontent.com/76824611/172739470-79d128a9-4fdf-4244-bfc8-6ec55544879b.png)


**[단점]**            
**2)** 의 과정에서 이미지를 이미지 자체로 못보고 길게펴서 보기 떄문에 **이미지 자체의 픽셀의 위치 정보가 무시**되게 됨   
그런데 Convolution Layer은 기존의 FC레이어와 달리 Convolution Layer는 기존의 구조를 보존시켜 위의 단점을 보완함.            
➡ 기존의 FC Layer가 입력 이미지를 길게 쭉 펴서 이미지 위치 정보를 잃었다면,    
➡ CNN은 기존의 이미지 구조를 그대로 유지하여 입력값으로 받아 위치 정보를 유지하게됨.          


---
----


# CNN
이 강의를 보기 전에 [이 포스트](https://yerimoh.github.io/DL8/)를 보고오면 훨씬 이해가 잘 될 것이다.     
* **input image**          
   * FC Layer와 달리 input 이미지가 한줄로 펴진것이 아닌 이미지가 보존 된 상태로 들어옴     
   ![image](https://user-images.githubusercontent.com/76824611/172740204-dc4924ee-4f77-493c-ad8f-69284e4abe0b.png)

* **filter**
   * 가중치다.     
   * input image에 맞춰 가중치도 이와 같은 필터이다.          
   * 이 예시에서는 5x5x3 필터가, 이 필터를 가지고 이미지를 슬라이딩하면서 공간적으로 내적을 수행하게 된다.         
   ![image](https://user-images.githubusercontent.com/76824611/172740216-ff9a024c-f791-4b71-b253-4a1638b96ccb.png)
  


--


## 필터의 연산
이제 이것을 어떻게 수행하는지 자세하게 알아보자


**[필터 연산]**    
* **1) 필터의 깉이 확장**       
    * 우선 필터는 입력의 깊이(Depth)만큼 확장된다.           
    * CNN의 하나의 필터는 아주 작은 부분만 취할 수 있다 ➡ 전체 32x32 이미지의 5x5 만 취하는 것            
    * 하지만 **깊이**는 전체 깊이를 전부 취함 ➡ 여기에서는 5 x 5 x **3** 가 됨(image: 32 x 32 x **3**)     
    * 이 깊이는 주로 이미지의 색([RGB](https://yerimoh.github.io//C1/#rgb-%EC%83%89%EA%B3%B5%EA%B0%84))를 의미한다.      
    ![image](https://user-images.githubusercontent.com/76824611/172740886-cf0d136c-fb91-4ba9-b032-4699d8857469.png)
* **2) 내적**    
    * 이 필터를 가지고 전체 이미지에 내적을 시킬 것이다.    
    * 이 필터를 이미지의 어떤 공간에 겹쳐놓고 내적을 수행한다.     
    * 그리고 필터의 각 w와, 이에 해당하는 이미지의 픽셀을 곱해준다    
    * 여기에 필터가 5 x 5 x 3 라는건 그만큼 곱셈연산을 한다는 것이다.       
    * bias term인 b도 보인다(얜 곱셈이 아니다!)     
    * 여기에서는 기본적으로 $$W^{t}x + b$$를 수행한다.     
    ![image](https://user-images.githubusercontent.com/76824611/172741567-c45fc2d6-15cb-4488-94c6-07e3548b4d14.png)
* **3) 슬라이딩**    
    * 그럼 필터 전체의 연산을 보겠다.     
    * 실제는 슬라이딩을 하며 각 슬라이딩 구역에 앞에서 살펴본 내적을 하는 것이다.      
        * Convolution은 이미지의 좌상단부터 시작한다      
        * 그리고 필터의 중앙을 값들을 모은다.       
        * 필터의 모든 요소를 가지고 내적을 수행하게 되면 하나의 값을 얻게된다     
        * 그리고 슬라이딩한다.        
    * Conv연산을 수행하는 값들을 다시 Output activation map의 해당하는 위치에 저장한다.        
    ![image](https://user-images.githubusercontent.com/76824611/172745021-1b5f1959-e848-44e7-9e66-3df8824db1ca.png)


**[출력크기가 입력크기와 다른 이유]**     
* 여기 보면 입력 이미지와 출력 activation map의 차원이 다르다는 것을 알 수 있다.   
    * ➡ 입력은 32 x 32 이고 출력은 28 x  28 이다    
    * 그래서 우리는 나중에 수학에 대해 설명 할 것이다.     
    * 수학적 설명 후엔 이 방법이 차원 적으로 어떻게 작동하는지 정확히 알 수 있다.     
    ![image](https://user-images.githubusercontent.com/76824611/172747478-22ac7d23-7987-4baa-b32d-39f6985bd65a.png)
* 기본적으로는 어떻게 슬라이딩을 할 것인지를 선택할 수 있다.     
    * 출력 형을 위와 같이 맞추기 위해선 슬라이딩여부와 관계없이 입력 값을 두 개씩 뽑아서 연산을 수행할 수도 있을 것이다.(하지만 이렇게 하지는 않을 것이다.)       
* 출력 행렬의 크기는 슬라이드를 어떻게 하느냐에 따라 다르다.    
    * 하지만 보편적으로 사용하는 슬라이딩 방법이 있다      


**[여러개의 필터 사용]**       
* 하나의 필터를 가지고 전체 이미지에 Convolution 연산을 수행한다.      
     * 그러면 하나의 activation map이라는 출력값을 얻게 된다.   
     * 즉 이는 여러개의 필터를 사용하면 여러개의 activation map이라는 출력값을 갖을 수 있다는 것을 의미한다.        
* 보통 Convolution Layer에서는 **여러개의 필터를 사용**한다.     
     * 왜냐하면 **필터마다 다른 특징**을 **추출**하고 싶기 때문이다.      
     * ex) 초록색의 5 x 5 x 3 형태의 두번째 필터     
     * 이 녹색 필터를 연산하고 나면, 앞서 계산했던 activate map과 같은 크기의 새로운 map이 만들어집니다.   
![image](https://user-images.githubusercontent.com/76824611/172751714-21c5c2ac-9543-4d10-9e94-7f8ebad9c2fd.png)
* 우리는 한 Layer에서 원하는 만큼 여러개의 필터를 사용할 수 있다.    
     * 가령 5 x 5 필터가 6개가 있다면 총 6개의 activation map을 얻게 될 것이다.     
     * 이 경우, 출력 map의 크기는 28 x 28이 되겠다.     
     ![image](https://user-images.githubusercontent.com/76824611/172824592-44a4356c-3841-4d7f-8f81-afa957d11349.png)
* **여러개의 필터 활용**       
     * 위의 필터들을 담은 CNN을 다시 그려보면 아래 그림과 같이  Cov Layer들의 연속된 형태가 될 것이다.    
     * 그리고 각각을 쌓아 올리게 되면 아래 그림과 같이 간단한 Linear Layer로 된 Neural Network가 된다.    
     * 이제는 그 사이 사이에 activation function을 넣을 것이다([ReLU](https://yerimoh.github.io/DL2/#relu-%ED%95%A8%EC%88%98) 같은 것들을 사용 가능)      
     ![image](https://user-images.githubusercontent.com/76824611/172833994-979f096b-be6f-4b8b-ac0c-4ed21e2b8c0e.png)
     * 그렇게 되면 Conv-ReLU 가 반복되겠죠 그리고 가끔은 pooling layer도 들어갑니다.     
     * 그리고 각 Layer의 출력은 다음 Layer의 입력이 됩니다.
     * 그리고 말씀드렸듯 각 Layer는 여러개의 필터를 가지고 있다.     
     * 그리고 각 필터마다 각각의 출력 map을 만든다.
     ➡ **그러므로 여러개의 Layer들을 쌓고나서 보면 결국 각 필터들이 계층적으로 학습을 하는것을 보게된다.**      
     ![image](https://user-images.githubusercontent.com/76824611/172834065-6a6aad44-d1d0-4043-8330-79bf5ba193d4.png)
    



**[필터의 계층적 학습]**      
* 1) low-level feature        
     * 앞쪽에 있는 필터들은 low-level feature를 학습하게 된다.        
     * 여기선 Edge와 같은것들이 보입니다.
* 2) Mid-level
     * 여기선 좀 더 복잡한 특징을 가지게 된다.       
     * 코너나 blobs 등과 같이 보입니다.
* 3) high-level features       
     * 그리고 high-level features를 보면 좀 더 객체와 닮은 것들이 출력으로 나오는 것을 볼 수 있다.    
* +) 나중 수업에서 객 특징을 어떻게 시각화하는지, 그리고 특징들이 어떻게 학습되는지를 살펴볼 것이다.     
![image](https://user-images.githubusercontent.com/76824611/172836809-81b9e385-c275-4c31-a8ce-cdca24325572.png)

➡ 위에서  이해하고 넘어가야 하는 것은 **특징이 어떻게 생겼**고, Layer의 **계층에 따라 단순/복잡한 특징**을이 **존재**한다는 것을 아는 것이다.     


<details>
<summary>📜 학생 질문: 우리가 전체 이미지를 슬라이딩 하면서 필터링을 하는데 이미지의 가장자리는 필터가 덜 적용되지 않냐</summary>
<div markdown="1">
  
A: 아주 좋은 질문 입니다.    
그리고 어떻게 그것을 다시 보완해야 하는지가 앞으로 할 내용이다. (ex zero-padding)


지금까지는 Conv Layer를 계층적으로 쌓아서 단순한 특징을을 뽑고 그것을 또 조합해서 더 복잡한 특징으로 활용했습니다.

그리고 이는 Hubel과 Wiesel의 이론과도 잘 맞습니다.

네트워크에 앞쪽에서는 단순한 것들일 처리하고 뒤로 갈수록 점점 더 복잡해 지는 식이죠


우리는 그것을 강제로 학습시킨 것이 아니라 계층적 구조를 설계하고 역전파로 학습시킨 것 뿐이지만 필터는 이렇게 학습되는 것입니다.
    
    
    
</div>
</details>



<details>
<summary>📜 학생 질문: 여기에서 시각화 한 것이 무엇인지</summary>
<div markdown="1">
  
A:  
가령 Cov1을 보면 각 그리드의 요소가 하나의 뉴런(필터) 라고 보시면 됩니다.


그리고 시각화 시킨 이 뉴런의 모습은 바로 이미지가 어떻게 생겨야 해당 뉴런의 활성을 최대화시킬 수 있는지는 나타내는 것입니다.   

이미지가 뉴런과 비슷하게 생겼으면 출력 값을 큰 값을 가지게 됩니다.


시각화는 Backpopagation을 통해 해볼 수 있는데. 이를 시각화하는 방법은 나중에 더 자세하게 배울 것입니다.


그러나 기본적으로 여기 그리드의 각 요소는 각 뉴런의 활성을최대화시키는 입력의 모양을 나타내게 됩니다.


그런 의미에서 뉴런이 어떻게 생겼는지를 의미합니다.


       
    
</div>
</details>





----


## Activation map의 예제: Convolution
아래는 각 필터가 만든 출력값이다.     

**[필터 Conv 연산]**        
* 슬라이드 위쪽엔 5 x 5 필터들을 시각화 한 것을 볼 수 있다.       
* 이는 실제 ConvNet을 학습시킨 결과이다.      
* 이제 이 각각의 필터를 아래 input image(자동차의 한 부분(모서리와 조명쪽))에 곱해줄 것이다.     
➡ 그럼 이미지와 필터간 Conv 결과, 한 필터당 하나의 activation map이 나올 것이다.         
![image](https://user-images.githubusercontent.com/76824611/172844939-5971e93c-1ce9-466d-9079-aed421f0433a.png)


**[activation map]**      
* 위의 연산 결과가  activation map으로 한 필터당 하나의 activation map이 있다는 것을 확인 할 수 있다.   
* 그럼 결과를 해석해보자.    
 * 이 필터는 edge를 찾고 있는 것이다    
 * 그리고 이 필터를 input image에 슬라이딩 시키면 이 필터와 비슷한 값들은 값이 더 커지게 된다     
 * 따라서 각 actavation은 이미지가 필터를 통과한 결과가 되며 이미지 중 어느 위치에서 이 필터가 크게 반응하지는지를 보여준다    
 (즉 무슨 필터와 더 잘 맞는지를 보여주는 것이다)     
 (인풋과 필터가 비슷하면 슬리이딩 시 각 반응이 커지니까)       
 ![image](https://user-images.githubusercontent.com/76824611/172846500-cd699217-a520-4ef9-9cbe-7c04e9498b45.png)




**[Convolution]**     
* 위와 같이 필터에서 activation map을 도출하는 연산을 Convolution이라고 칭한다.     
* 우리가 이걸 Convolution이러고 칭하는 이유는 바로 위에 언급한 것이 바로 두 신호 사이에 cov를 하는것과 유사하기 때문이다.     
* Conv 식      
![image](https://user-images.githubusercontent.com/76824611/172847519-14609947-fa5e-4581-8ba9-16b4b4881b88.png)
   * 예전에 신호처리에서 Conv를 본 적이 있는 학생이라면 이 식이 correlation 같아 보일 것이다.     
   * 우리는 사실 Conv의 뒤집힌 버전을 쓰고 있는 것이다. 하지만 이 수업에서는 이를 더 자세히 다루지는 않겠다.       
   * 하지만 손으로 직접 계산해보면 conv에 정확한 정의와 크게 다르지 않을 것이다.         
   * 하지만 기본적으로는 필터를 가지고 이미지에 슬라이딩하면서 모든 위치에서 내적을 수행하게 되는 것이다.     


----
----


# **CNN의 수행 개요**
이전에도 언급했듯이 CNN이 어떻게 수행되는지를 살펴보면, 입력 이미지는 여러 레이어를 통과하게 됩니다.    
한번 간단한 예를 들어보겠다.    

**1)** 첫 번째 Conv Layer 후에는  non-linear layer를 통과합니다.
: 나중에 배우겠지만, non-linear layer로는 ReLU를 가장 많이 사용한다             

**2)** Conv, ReLU, Conv, ReLU를 하고나면 pooling layer를 거치게 된다.     
: 다시 배우겠지만 pooling 은 activation maps의 사이즈를 줄이는 역할을 한다.         

**3)** 그리고 CNN의 끝단에는 FC-Layer가 있다.        
: FC-Layer는 우에서 배운 완전 연결 레이어이다.          
FC-Layer는 마지막 Conv 출력 모두와 연결되어 있으며 **최종 스코어를 계산**하기위해 사용한다.      


![image](https://user-images.githubusercontent.com/76824611/172849713-09205b04-2734-4f47-a34b-ed1f67ed5cf8.png)




----

## Spatial dimension
그럼 이제 "Spatial dimension"에 대해 알아보겠다.


자 여기 32 x 32 x 3 이미지가 있다.     
그리고 이 이미지를 5 x 5 x 3 필터를 가지고 연산을 수행할 것이다.      


이제 어떻게 이 둘을 가지고 28 x 28 activation map이 생기는지를 알아보겠다.



![image](https://user-images.githubusercontent.com/76824611/172851470-15b4863b-d684-4ce7-877c-c7512ce49e74.png)




간단한 예시로 7 x 7  입력에 3 x 3 필터가 있다고 해보자

이제 이 필터를 이미지의 왼상단부터 씌운다    

그리고 이제 해당 값들의 내적을 수행할 것이다.

252
00:31:53,169 --> 00:31:56,764
이 값들은 activation map의 좌 상단에 위치하게 되겠죠

253
00:31:56,764 --> 00:32:02,389
그리고 다음 단계로 필터를 오른쪽으로 한칸 움직입니다.

254
00:32:02,389 --> 00:32:05,535
그러면 값 하나를 또 얻을 수 있을 것입니다.

255
00:32:05,535 --> 00:32:14,528
이렇게 계속 반복하게 되면  결국 5 x 5의 출력을 얻게 됩니다.

256
00:32:14,528 --> 00:32:25,381
이 슬라이드 필터는 좌우 방향으로는 5번만 수행가능하고
상하 방향으로도 5번만 수행가능합니다.

257
00:32:27,834 --> 00:32:31,906
그리고 여기에는 다양한 방법을 이용해 볼 수 있습니다.

258
00:32:31,906 --> 00:32:40,326
지금까지는 슬라이딩을 한칸씩만 진행했었죠.
이때 움직이는 칸을  바로 "stride" 라고 합니다.

259
00:32:40,326 --> 00:32:46,700
지금까지는 stide = 1을 사용했습니다.
그럼 stride = 2 일때는 어떨까요?

260
00:32:46,700 --> 00:32:58,944
다시 왼쪽 위부터 시작해서 움직입니다. 다만 여기에서는
1칸은 건너뛰고 그 다음칸으로 이동해서 계산을 합니다.

261
00:33:00,773 --> 00:33:11,257
이렇게 tride가 2 이면 3칸이면 못움직이고
결국 출력은 3 x 3 이 됩니다.

262
00:33:13,035 --> 00:33:18,653
그렇다면 stride = 3 이면 출력의 사이즈는 몇일까요?

263
00:33:18,653 --> 00:33:27,905
stride가 3인 경우에는 이미지를 슬라이딩해도 필터가
모든 이미지를 커버할 수 없습니다.

264
00:33:27,905 --> 00:33:32,363
이 예시에서 stride 가 3이면 이미지에 잘 맞아떨어지지 않습니다.

265
00:33:32,363 --> 00:33:41,903
실제로 이렇게 되면 잘 동작하지 않습니다. 이렇게 하면 안됩니다.
이로 인해 불균형한 결과를 볼 수도 있기 때문입니다.

266
00:33:46,095 --> 00:33:54,690
그래서 상황에 따라 출력의 사이즈가 어떻게 될 것인지를
계산할 수 있는 아주 유용한 수식이 있습니다.

267
00:33:54,690 --> 00:34:05,597
입력의 차원이 N이고 필터 사이즈가 F이고
스트라이드가 몇이다 라고 주어지게 되면

268
00:34:06,992 --> 00:34:12,850
출력의 크기는 (N - F) / stride + 1 이 됩니다.

269
00:34:12,850 --> 00:34:18,619
이를 이용해서 어떤 필터 크기를 사용해야 하는지를 알 수 있습니다.

270
00:34:18,620 --> 00:34:27,326
그리고 어떤 stride를 사용했을때 이미지에 꼭 맞는지,
그리고 몇 개의 출력값을 낼 수 있는지도 알 수 있습니다.

271
00:34:29,257 --> 00:34:32,546
지금까지 유용한 수식을 한 번 알아봤습니다.

272
00:34:32,547 --> 00:34:38,637
앞서 보신바와 같이 N = 7 이고 F = 3 일때,
그리고 stride가 1이면

273
00:34:38,637 --> 00:34:43,498
이걸 그대로 수식에 적용해보면 5 x 5 출력이
나올 것이라는 것을 알 수 있습니다.

274
00:34:43,498 --> 00:34:47,665
stride가 3인 경우를 보면, 잘 동작하지 않겠죠. (2.33)

275
00:34:50,288 --> 00:34:59,552
그리고 가장 흔히 쓰는 기법은 zero-pad 입니다.
출력의 사이즈 의도대로 만들어 주기 위해서죠

276
00:34:59,552 --> 00:35:04,140
이는 이전 질문이었던 "코너는 어떻게 처리하나요"
와도 연결되는 문제입니다.

277
00:35:04,140 --> 00:35:09,222
이를 위해 할 일은, 이미지의 가장자리에 0을 채워 넣는 것입니다.

278
00:35:09,222 --> 00:35:19,134
이렇게 되면 좌 상단의 자리에서도 필터 연산을
수행할 수 있게 됩니다.

279
00:35:19,134 --> 00:35:33,654
자 이제 다시 질문입니다. 7 x 7 입력에 3 x 3 필터 연산을 수행할 때
zero-padding을 하면 출력이 어떻게 될까요?

280
00:35:33,654 --> 00:35:36,285
[학생들이 대답]

281
00:35:36,285 --> 00:35:44,847
몇 명이 6이라고 하는 것 같은데요
저 수식을 한 번 잘 생각해 보세요

282
00:35:44,847 --> 00:35:52,594
N에 7을, F에 3을 대입하고 stride를 1으로 하면

283
00:35:52,594 --> 00:35:57,264
그래서 우리가 실제로 얻는 것, 실제로
이것은 우리에게주는 것입니다.

284
00:35:57,264 --> 00:36:06,707
(7 - 3) /1 + 1 = 5 입니다. 이건 zero-padding이 없고
우리는 이 수식을 살짝 고쳐야합니다.

285
00:36:06,707 --> 00:36:12,161
이 수식은 zero-padding을 하지 않은 수식이죠

286
00:36:12,161 --> 00:36:24,173
하지만 zero-padding을 하면 출력이 7이 됩니다.
그래서 결국 7 x 7 출력을 얻게 되는 것입니다.

287
00:36:24,173 --> 00:36:30,178
원래 공식으로 돌아가보자면 N은 7이 아니라 9가 되겠죠

288
00:36:30,178 --> 00:36:42,253
N = 9 가 되고 필터 사이즈인 3을 빼면 6이 됩니다. 이를 stride
1나누면 다시 6이고 여기에 1을 더하면 7이 됩니다.

289
00:36:42,253 --> 00:36:47,974
padding을 하려면 수식을 이런식으로 적용하면 되겠습니다.

290
00:36:49,739 --> 00:36:51,646
질문 있나요?

291
00:36:51,646 --> 00:36:54,396
[학생이 질문]

292
00:37:00,717 --> 00:37:08,962
질문은 "실제 출력 값의 사이즈는 몇입니까?
7 x 7 입니까 7 x 7 x 3 입니까?" 입니다.

293
00:37:08,962 --> 00:37:14,495
출력은 7 x 7 x "필터의 갯수" 가 됩니다.

294
00:37:14,495 --> 00:37:21,320
명심해야할 점은 각 필터가 입력의 모든 depth에
내적을 수행한다는 것입니다.

295
00:37:21,320 --> 00:37:23,801
하나의 값만 나오겠죠

296
00:37:23,801 --> 00:37:37,124
다시 이전 슬라이드로 돌아가보겠습니다. 이전의 예제에서는
각 필터는 7 x 7 x 1이 되겠죠

297
00:37:37,124 --> 00:37:40,493
activation map의 depth는 우리 depth는 우리가 가진 필터의 갯수입니다.

298
00:37:40,493 --> 00:37:43,243
[학생이 질문]

299
00:37:50,161 --> 00:37:57,350
잘 못들었습니다?

300
00:37:57,350 --> 00:38:00,267
[다시 질문]

301
00:38:12,936 --> 00:38:19,735
질문은 바로, 이전 예제에서는 32 x 32 x 3 의 입력이었는데,
방금 설명한 것을 어떻게 적용할 수 있는지 입니다.

302
00:38:19,735 --> 00:38:24,721
이전 예제에서는 depth가 있었는데
지금 예제에서는 depth가 없었죠

303
00:38:24,721 --> 00:38:27,226
여기에서는 단순하게 예제를 만들었지만

304
00:38:27,226 --> 00:38:34,188
실제로는 이전에 했던것 처럼 depth만큼 곱해주면 됩니다.

305
00:38:34,188 --> 00:38:39,850
그러니 이 예제에서 3 x 3 x "depth" 가 되겠습니다.

306
00:38:39,850 --> 00:38:46,854
그러니 이전 예제의 경우 3 x 3 x 3 이 되겠고
다른 것을 다 똑같습니다.

307
00:38:46,854 --> 00:38:48,390
질문 있나요?

308
00:38:48,390 --> 00:38:51,307
[학생이 질문]

309
00:38:53,529 --> 00:38:58,664
질문은 바로 0 을 padding 하게되면 모서리에
필요없는 특징을 추가하게 되는게 아닌지 입니다.

310
00:38:58,664 --> 00:39:06,289
우리가 지금 하고싶은 일은 영상 내에 어떤 모서리 부분에서
값을 얻고싶은 것이고

311
00:39:06,289 --> 00:39:10,343
zero-padding은 이를 할 수 있는 하나의 방법일 뿐입니다.

312
00:39:10,343 --> 00:39:16,097
우리는 지금  필터가 닿지 않는 모서리 부분에서도 값을
뽑을 수 있게 됩니다.

313
00:39:16,097 --> 00:39:18,323
물론 zero-padding 말고 다른 방법을 사용할 수도 있습니다.

314
00:39:18,323 --> 00:39:23,615
zero가 아닌 mirror나 extend하는 방법도 있습니다.
꼭 zero-padding을 써야 하는 것은 아닙니다.

315
00:39:23,615 --> 00:39:26,530
하지만 zero-padding도 제법 잘 동작하는 방법 중 하나입니다.

316
00:39:26,530 --> 00:39:36,486
물론 모서리 부분에 약간의 artifact이 생긴 순 있습니다. 당연히
고려해야 하 부분이죠. 하지만 대부분의 경우 제법 잘 동작합니다.

317
00:39:36,486 --> 00:39:41,283
다른 질문도 있었던 것 같은데요

318
00:39:41,283 --> 00:39:44,033
[학생이 질문]

319
00:39:48,015 --> 00:39:54,330
이미지가 정사각 행렬이 아니라면 수평, 수직방향의
stride를 다르게 적용해야 하는지 입니다.

320
00:39:54,330 --> 00:40:02,841
물론 당연히 가능합니다. 하지만 보통은 정사각 행렬을
사용하고 같은 stride를 적용합니다.

321
00:40:02,841 --> 00:40:08,238
보통은 모든 영역에 같은 Stride를 적용합니다.

322
00:40:08,238 --> 00:40:10,218
어떤 의미에서 그것은 조금 비슷합니다.

323
00:40:10,218 --> 00:40:20,693
이는 이미지 해상도 문제와도 관련이 있습니다. 아마 여러분은
입력 이미지의 종횡비를 유지하고 싶을 것이지만

324
00:40:20,693 --> 00:40:28,373
대부분의 사람들은 정사각으로 사용합니다.
(잘 대답한건지 모르겠음)

325
00:40:28,373 --> 00:40:31,453
[학생이 질문]

326
00:40:31,453 --> 00:40:33,710
질문을 바로 왜 zero-padding을 하냐는 것입니다.

327
00:40:33,710 --> 00:40:39,376
 zero-padding을 하는 이유는 레이어를 거치면서도
입력의 사이즈를 유지하기 위해서입니다.

328
00:40:39,376 --> 00:40:49,019
가령 7 x 7 필터를 연산한다고 하면 출력의 사이즈는
입력보다 작아집니다.

329
00:40:49,019 --> 00:40:53,186
하지만 우리는 지금 그 사이즈를 유지하고 싶은 것입니다.

330
00:40:56,276 --> 00:41:10,753
지금까지 Padding을 하게되면 출력 사이즈를 유지시켜주고 필터의
중앙이 닿지 않는 곳도 연산할 수 있다는 것을 배웠습니다.

331
00:41:10,753 --> 00:41:18,998
어떤 stride와 filter를 쓸건지를 일반적으로 정하는 방법이 있습니다.

332
00:41:18,998 --> 00:41:25,427
보통 filter는 3x3, 5x5, 7x7 을 씁니다.

333
00:41:25,427 --> 00:41:33,567
보통 3x3 filter에는 stride를 1로 줍니다.

334
00:41:33,567 --> 00:41:39,422
그리고 5x5 filter에는 보통 stride를 2로 주죠

335
00:41:39,422 --> 00:41:43,505
7x7 일때는 3을 줍니다.

336
00:41:44,722 --> 00:41:52,184
다시 한 번 말하지만 zero-padding을 하는 목적은
입력 사이즈와 출력 사이즈를 같도록 하기 위해서 입니다.

337
00:41:52,184 --> 00:42:10,566
이전에 말했듯이 레이어가 여러겹 쌓인다고 생각해 보면
그리고 그때 zero-padding을 하지 않는다면

338
00:42:10,566 --> 00:42:16,616
출력 사이즈는 아주 빠르게 줄어들게됩니다.
우리가 원하는 결과는 아니죠

339
00:42:16,616 --> 00:42:25,907
만일 엄청 깊은 네트워크가 있다고 생각해보면
Activation Map은 점점 줄어들어 엄청 작아지게 될 것입니다.

340
00:42:25,907 --> 00:42:36,578
그렇게 되면 일부 정보를 잃게 되는 것이고
원본 이미지를 표현하기에 너무 작은 값을 사용하게 될 것입니다.

341
00:42:36,578 --> 00:42:53,590
그리고 그렇게 줄어드는 이유는 매번 각 코너에 있는 값들을
계산하지 못하기 때문입니다.

342
00:42:55,203 --> 00:43:00,060
자 그럼 몇가지 예를 더 살펴보겠습니다.

343
00:43:00,991 --> 00:43:09,244
여기에 입력 이미지 32 x 32 x 3 이 있습니다.
그리고 10개의 5 x 5 필터도 있습니다.

344
00:43:09,244 --> 00:43:16,717
여기 stride를 1로하고 padding을 2로 했을때
 출력사이즈는 몇이 될까요?

345
00:43:18,188 --> 00:43:21,728
이전 공식을 한 번 생각해 보세요.

346
00:43:21,728 --> 00:43:23,263
[학생이 대답]

347
00:43:23,263 --> 00:43:26,180
32 x 32 x 10 네 맞습니다.

348
00:43:27,572 --> 00:43:33,707
입력 사이즈 F는 32이죠

349
00:43:33,707 --> 00:43:41,209
padding으로 이 값을 2씩 증가시킵니다.
각 차원에 값을 2 증가시킵니다.

350
00:43:41,209 --> 00:43:47,181
그렇게 각 양쪽에 2씩 추가시키면
32 + 4 가 되겠죠

351
00:43:47,181 --> 00:43:49,992
그리고 필터사이즈 5를 뺴줍니다.

352
00:43:49,992 --> 00:43:51,716
그리고 다시 1을 더하면 32를 얻게 됩니다.

353
00:43:51,716 --> 00:43:55,883
그러면 각 필터는 32 x 32가 됩니다.

354
00:43:57,213 --> 00:44:02,193
전체 필터의 갯수는 10개이니
10개의 activation map이 나오겠죠

355
00:44:02,193 --> 00:44:06,360
그럼 전체 출력의 크기는
32 x 32 x 10 이 됩니다.

356
00:44:08,244 --> 00:44:14,478
자 그럼 다음 질문입니다.
이 레이어의 파라미터는 총 몇개일까요?

357
00:44:14,478 --> 00:44:18,145
우리에게 10개의 5 x 5 필터가 있다는 것이 힌트입니다.

358
00:44:19,769 --> 00:44:22,698
[학생이 질문]

359
00:44:22,698 --> 00:44:36,226
잘 못들었습니다?
250이라고 답했습니다.

360
00:44:37,829 --> 00:44:42,149
명심해야 할 점은 필터가 입력의 Depth만큼도 통과한다는 것입니다.

361
00:44:42,149 --> 00:44:50,300
이 슬라이드에는 그 내용이 명확히 쓰여있지는 않습니다.
하지만 묵시적으로 여기에 Depth도 존재합니다.

362
00:44:50,300 --> 00:44:55,876
필터가 입력의 전체 depth를 통과하게 되겠죠.
750이라는 정답을 들은것 같습니다.

363
00:44:55,876 --> 00:45:03,374
거의 다 근접했습니다. 우리는 여기 bias term도 있다는
것을 명심해야 합니다.

364
00:45:03,374 --> 00:45:08,084
이 각각의 5 x 5 x 3 가중치에는
하나의 bias term이 들어있습니다.

365
00:45:08,084 --> 00:45:15,609
그러니 필터당 76개의 파라미터가 있는 것이고 이런 필터가
총 10개 있으니 전체 760개의 파라미터가 있는 것입니다.

366
00:45:18,412 --> 00:45:25,890
자 여기에 Conv layer를 한 번 요약정리해 보았습니다.
나중에 주의깊게 읽어보시기 바랍니다.

367
00:45:25,890 --> 00:45:31,137
어떤 n차원의 입력이 있습니다.
그리고 어떤 필터를 쓸지 선택해야 합니다.

368
00:45:31,137 --> 00:45:38,682
몇개의 필터를 쓸건지, 필터 크기는 몇인지, stride는 몇으로 할지
zero-padding은 몇 으로 할지를 다 정해줘야합니다.

369
00:45:38,682 --> 00:45:43,866
그리고 앞서 말한 수식을 이용해서 출력의 사이즈가 어떻게
될 것인지를 계산해 봅니다.

370
00:45:43,866 --> 00:45:48,033
그리고 전체 파라미터가 몇개가 될 것인지도
확인해 봐야 합니다.

371
00:45:49,282 --> 00:45:58,555
여기에는 일반적으로 사용하는 값들이 있습니다.
필터 사이즈는 3x3, 5x5 를 씁니다.

372
00:45:58,555 --> 00:46:01,739
Stride는 보통 1이나 2가 가장 흔합니다.

373
00:46:01,739 --> 00:46:10,401
그리고 padding은 그 설정에 따라 조금씩 다르겠죠

374
00:46:10,401 --> 00:46:19,009
그리고 보통 필터의 갯수는 2의 제곱수로 합니다.
32, 64, 128, 512 와 같이 말이죠

375
00:46:19,903 --> 00:46:24,505
위의 숫자들은 아주 자주 보게될 것입니다.

376
00:46:24,505 --> 00:46:29,488
우선 다음으로 넘어가서,
1 x 1 Convolution에 대해 말해봅시다.

377
00:46:29,488 --> 00:46:35,852
1 x 1 Convolution도 의미가 있습니다. 똑같이 슬라이딩 하면서
값을 구하게 되겠죠

378
00:46:35,852 --> 00:46:46,680
하지만 여기에서는 5x5 처럼 공간적인 정보를 이용하지 않습니다.
하지만 이 필터는 여전히 Depth만큼 연산을 수행합니다.

379
00:46:46,680 --> 00:46:52,053
그러니 1 x 1 Conv는 입력의 전체 Depth에 대한
내적을 수행하는 것과 같습니다.

380
00:46:52,053 --> 00:47:07,062
입력은 56x56x64 입니다. 그리고 여기에 32개의 1x1
Conv 수행하면 56x56x32의 출력값이 나오게 됩니다.

381
00:47:10,076 --> 00:47:16,210
자 여기 TORCH로 구현된 Conv Layer 예제가 있습니다.

382
00:47:16,210 --> 00:47:25,017
지난 시간에 Torch에 대해 알아봤었는데요
Torch에서는 다양한 레이어가 정의되어 있습니다.

383
00:47:25,017 --> 00:47:28,667
그리고 그 레이어의 forward/backward pass가
구현되어 있습니다.

384
00:47:28,667 --> 00:47:35,360
여기 spatial convolution이 바로 그 중 하나입니다.

385
00:47:35,360 --> 00:47:42,781
여기에 arguments를 가지고 여러 디자인을 선택할 수 있습니다.
입/출력 사이즈를 정할수도 있고

386
00:47:42,781 --> 00:47:50,161
커널 사이즈, Padding 와 같은 것들을 전부 정해줄 수 있습니다.

387
00:47:50,161 --> 00:47:54,737
또한 다른 프레임워크인 Caffe를 살펴봅시다.
뭐 다들 비슷비슷 합니다.

388
00:47:54,737 --> 00:48:07,160
여기 보면 네트워크가 정의되어있는데 Caffe에서는 proto
text라는 파일을 이용합니다. 여기에 디자인을 명시합니다.

389
00:48:07,160 --> 00:48:18,244
여기 conv layer의 예시가 있는데 출력의 사이즈나 필터의 갯수,
커널 사이즈 stride 등등을 여기에 명시하는 것입니다.

390
00:48:21,144 --> 00:48:29,512
진행하기에 앞서, 혹시 Conv 연산에 대한 질문이 있습니까?

391
00:48:30,868 --> 00:48:32,161
네 질문해주세요

392
00:48:32,161 --> 00:48:34,911
[학생이 질문]

393
00:48:51,604 --> 00:48:55,902
질문은 바로 stride를 선택하는데 있어 가질 수 있는
직관은 무엇인지 입니다.

394
00:48:55,902 --> 00:49:09,258
우선 한가지는 stride를 크게 가져갈수록 출력은 점점 작아집니다.

395
00:49:09,258 --> 00:49:23,025
이미지를 다운샘플링하는 것은 Pooling을 하는것과 비슷합니다. 하지만
그 둘은 확실이 다르고, pooling보다 더 좋은 성능을 보이기도 합니다.

396
00:49:23,025 --> 00:49:27,192
Pooling 처럼 다운 샘플링하는 동일한 효과를 얻으면서도
더 좋은 성능을 낼 수도 있습니다.

397
00:49:28,183 --> 00:49:40,825
그리고 activation map의 사이즈를 줄이는 것은 추후
모델의 전체 파라미터의 갯수에도 영향을 미칩니다.

398
00:49:40,825 --> 00:49:48,611
가령 Conv Layer의 마지막에는 FC Layer가 있을 수 있겠죠.

399
00:49:48,611 --> 00:49:56,099
FC Layer는 Conv의 출력 모두와 연결되어 있습니다.

400
00:49:56,099 --> 00:50:02,596
Conv Layer의 출력이 작을수록
FC Layer에서 필요한 파라미터의 수가 더 작겠죠

401
00:50:02,596 --> 00:50:10,076
파라미터의 수, 모델의 사이즈 그리고 Overfitting 과 같은 것들에는
다양한 trade-off 가 있습니다.

402
00:50:10,076 --> 00:50:15,538
이 trade-off는 stride를 몇 으로 할지를
결정할때 고려해야 하는 문제입니다.

403
00:50:18,496 --> 00:50:25,356
자 그럼 Conv Layer을 Brain Neuron의 관점에서 살펴보겠습니다.

404
00:50:25,356 --> 00:50:31,599
뉴런에 대해서는 지난 시간에 조금 배웠었죠

405
00:50:31,599 --> 00:50:39,216
Conv Layer를 보면, 전체 이미지의 특정 위치에 필터를 가지고
내적을 수행했습니다.

406
00:50:39,216 --> 00:50:42,077
그러면 하나의 값을 얻게 됩니다.

407
00:50:42,077 --> 00:50:46,042
이는 여기 오른쪽 그림의 내적과 같은 아이디어입니다.

408
00:50:46,042 --> 00:50:55,227
입력이 들어오면 Ws와 곱하고, Ws는 필터 값이죠
그리고 하나의 값을 출려합니다.

409
00:50:55,227 --> 00:50:59,517
하지만 가장 큰 차이점은 우리의 뉴런은
Local connectivity 를 가지고 있다는 것입니다.

410
00:50:59,517 --> 00:51:06,536
Conv Layer처럼 슬라이딩을 하는게 아니라
특정 부분에만 연결되어 있습니다.

411
00:51:06,536 --> 00:51:17,500
하나의 뉴런은 한 부분만 처리하고, 그런 뉴런들이
모여서 전체 이미지를 처리하는 것입니다.

412
00:51:17,500 --> 00:51:26,652
이런 식으로 spatial structure를 유지한 채로
Layer의 출력인 activation map을 만드는 것입니다.

413
00:51:30,048 --> 00:51:36,931
중요한 용어들을 한 번 정리해 보겠습니다.
가령 5 x 5 필터가 있다고 해봅시다.

414
00:51:36,931 --> 00:51:41,726
한 뉴런의  "Receptive field" 가 5 x 5 다
라고 할 수 있습니다.

415
00:51:41,726 --> 00:51:48,518
"Receptive field" 란 한 뉴런이 한 번에 수용할 수 있는
영역을 의미합니다.

416
00:51:48,518 --> 00:51:53,315
자주 등장하는 용어이니 익숙해 지시기 바랍니다.

417
00:51:53,315 --> 00:51:55,743
그런 다음 5 개의 필터로 5 개를 다시 기억하십시오.

418
00:51:55,743 --> 00:52:03,089
그 필터가 슬라이딩하면서 계산을 하는데 중요한 것은
필터 값이 항상 같다는 것입니다.

419
00:52:05,440 --> 00:52:11,200
그럼 이제 출력 값에 대해 알아보면 출력 값은
여기 보이는 파란색 Volume 처럼 생길 것입니다.

420
00:52:11,200 --> 00:52:16,373
크기는 28 x 28 x (필터의 갯수) 가 될텐데요.

421
00:52:16,373 --> 00:52:23,381
필터가 총 5종류가 있는 경우라면
출력은 28 x 28 x 5 인 3D Grid가 됩니다.

422
00:52:23,381 --> 00:52:36,003
그러면 이제 어떤 한 점을 찍어서 depth방향으로 바라보면
(파란색 Map 안에 5개 점)

423
00:52:36,003 --> 00:52:40,590
이 5개의 점은 정확하게 같은 지역에서 추출된
서로다른 특징이라고 할 수 있습니다.

424
00:52:40,590 --> 00:52:42,344
하지만 각 필터는 서로 다른 특징을 추출합니다.

425
00:52:42,344 --> 00:52:48,120
그러므로 각 필터는 이미지에서 같은 지역을 돌더라도
서로 다른 특징을 뽑아낸다고 볼 수 있습니다.

426
00:52:49,152 --> 00:52:55,443
복습겸 다시 FC Layer를 한 번 살펴보겠습니다.

427
00:52:55,443 --> 00:53:06,637
FC-Layer 의 경우면 32 x 32 x 3 을 다 편 다음에
전체를 연결하여 계산할 것입니다.

428
00:53:06,637 --> 00:53:12,805
Conv Layer가 전체가 아직 지역정보만 이용하다는 점과
비교해 보시면 될 것 같습니다.

429
00:53:12,805 --> 00:53:14,255
질문 있나요?

430
00:53:14,255 --> 00:53:17,088
[학생이 질문]

431
00:53:22,648 --> 00:53:28,137
Layer 내에서 filter가 하는 일이  Symmetric하냐는 것입니다.

432
00:53:30,158 --> 00:53:34,325
어떤 점에서 Symmetric하다는 것을 질문하는것인가요?

433
00:53:42,200 --> 00:53:50,556
필터들이 같은 차원을 가지고 같은 계산을 한다는 것을
의미하는 것  같은데요.

434
00:53:52,784 --> 00:53:59,624
필터 연산을 할때 다른 특수한 경우가 있냐는 질문입니다.
답은 없습니다.

435
00:53:59,624 --> 00:54:04,973
가령 우리가 5 x 5 필터가 있다면

436
00:54:04,973 --> 00:54:11,502
슬라이딩 할때마다 같은 연산을 수행하고,
그렇게 나온 값이 activation map입니다.

437
00:54:14,596 --> 00:54:20,592
좋습니다 지금까지 Conv Layer 꽤 심도깊게 살펴보았습니다.

438
00:54:20,592 --> 00:54:28,802
지금부터는 CNN에 들어가는 다른 Layer를 살펴보겠습니다.

439
00:54:28,802 --> 00:54:36,653
CNN에는 Conv Layer와 Pooling Layer가 있었죠.
그리고 다른 비선형 연산들도 있습니다.

440
00:54:36,653 --> 00:54:42,716
Pooling Layer는 Representation들을
더 작고 관리하게 쉽게 해줍니다.

441
00:54:42,716 --> 00:54:51,562
누가 이전에 왜 Representation을 작게 만드냐고 했엇죠

442
00:54:51,562 --> 00:54:58,343
다시 한 번 말하지만, 작아지면 파라미터의 수가 줄게 됩니다.

443
00:54:58,343 --> 00:55:04,425
그리고 일종의 공간적인 불변성(invaiance)
을 얻을 수도 있습니다.

444
00:55:04,425 --> 00:55:09,460
Pooling Layer가 하는 일은 아주 간단합니다.
Downsample 하는 거죠

445
00:55:09,460 --> 00:55:20,861
가령 224 x 224 x 64 인 입력이 있다면, 이를
112 x 112 x 64 로 "공간적"으로 줄여줍니다.

446
00:55:20,861 --> 00:55:26,588
중요한 점은 "Depth"에는 아무 짓도 하지 않는다는 것입니다.

447
00:55:26,588 --> 00:55:36,948
따라서 Depth에는 영향을 주지 않습니다.
그리고 Max Pooling이 일반적으로 쓰입니다.

448
00:55:36,948 --> 00:55:46,825
Pooling에도 필터 크기를 정할 수 있습니다.
얼마만큼의 영역을 한 번에 묶을지를 정하는 것입니다.

449
00:55:46,825 --> 00:55:53,572
여기 2 x 2 필터가 있습니다.
그리고 여기에서 Stride는 2 입니다.

450
00:55:53,572 --> 00:56:01,672
그리고  Conv Layer가 했던 것 처럼 슬라이딩하면서
연산을 수행합니다.

451
00:56:01,672 --> 00:56:08,338
대신 내적을 하는 것이 아니라, 필터 안에 가장
큰 값 중에 하나를 고르는 것입니다.

452
00:56:08,338 --> 00:56:15,655
여기 빨간색 영역을 보면 6이 제일 큽니다.
녹색을 보면 8이 제일 크죠

453
00:56:15,655 --> 00:56:18,655
나머지도 마찬가지로 3과 4의 값이 되겠습니다.

454
00:56:23,433 --> 00:56:24,931
질문있나요?

455
00:56:24,931 --> 00:56:27,848
[학생이 질문]

456
00:56:29,010 --> 00:56:34,406
네 질문은 Pooling 할 때 겹치지 않는 것이 일반적입니까?
입니다

457
00:56:34,406 --> 00:56:41,256
네 맞습니다. 보통은 겹치지 않는 것이 일반적입니다.

458
00:56:41,256 --> 00:56:50,560
기본적으로 Downsample을 하고싶은 것이기 때문에

459
00:56:50,560 --> 00:56:55,874
한 지역을 선택하고 값 하나 뽑고, 또 다른 지역 선택하고
값 하나 뽑고 이런식으로 진행합니다.

460
00:56:55,874 --> 00:56:57,379
질문있나요?

461
00:56:57,379 --> 00:57:00,129
[학생이 질문]

462
00:57:02,415 --> 00:57:07,636
Max pooling이 average pooling보다 더 좋은 이유는
무엇인지? 입니다.

463
00:57:07,636 --> 00:57:12,017
좋은 질문입니다. Average Pooling도
물론 사용할 수 있습니다.

464
00:57:12,017 --> 00:57:17,979
왜 max pooling을 주로 사용하지는지에
대한 직관을 설명드리자면

465
00:57:17,979 --> 00:57:26,972
우리가 다루는 값들은, 얼마나 이 뉴런이
활성되었는 지를 나타내는 값 들입니다.

466
00:57:26,972 --> 00:57:29,253
즉, 이 필터가 각 위치에서 얼마나 활성되었는지 입니다.

467
00:57:29,253 --> 00:57:39,133
Max pooling은 그 지역이 어디든,  어떤 신호에 대해 "얼마나"
그 필터가 활성화 되었는지를 알려준다고 알 수 있습니다.

468
00:57:39,133 --> 00:57:46,535
우리가 "인식" 에 대해 생각해보면
어느정도의 직관을 얻을 수 있는데,

469
00:57:46,535 --> 00:57:57,073
그 값이 어디에 있었다는 것 보다는
그 값이 얼마나 큰지가 중요한 것입니다.

470
00:57:57,940 --> 00:57:59,129
질문 있나요?

471
00:57:59,129 --> 00:58:02,046
[학생이 질문]

472
00:58:06,200 --> 00:58:14,223
질문은 바로 "Pooling" 이나 Conv Layer의 "Stride"나
매한가지 아니냐는 것입니다.

473
00:58:14,223 --> 00:58:27,704
네 맞습니다. 요즘들어 사람들이 Downsample할때 pooling을
하기보단 stride를 많이 사용하고 있는 추세입니다.

474
00:58:27,704 --> 00:58:32,801
제 생각에는 Pooling 도 일종의
Stride 기법이라고 볼 수 있다고 봅니다.

475
00:58:32,801 --> 00:58:41,892
실제로 요즘은 stride가 더 좋은 성능을 보이는 것 같습니다.

476
00:58:41,892 --> 00:58:47,292
그러니 당연히 Conv Layer의 stride를 사용해도 무방합니다.
그리고 사람들도 그렇게 하고 있습니다.

477
00:58:49,672 --> 00:59:00,358
다시 Pooling Layer로 돌아가 봅시다.
Pooling Layer에는 몇가지 Design Choice가 있습니다.

478
00:59:00,358 --> 00:59:10,107
입력이 W(width), H(Height), D(Depth) 면
이를 통해 Filter Size를 정해줄 수 있습니다.

479
00:59:10,107 --> 00:59:21,325
여기에 Stride까지 정해주면, 앞서 Conv Layer에서 사용했던
수식을 그대로 이용해서 Design Choice를 할 수 있습니다.

480
00:59:21,325 --> 00:59:27,780
이렇게 (W - Filter Size) / Stride + 1
로 구하면 됩니다.

481
00:59:30,880 --> 00:59:41,262
한가지 특징이 있다면 pooling layer에서는 보통 padding을
하지 않습니다. 왜냐면 우리는  downsampling하고 싶고,

482
00:59:41,262 --> 00:59:47,045
또한 Conv 때처럼 코너의 값을 계산하지 못하는 경우도
없기 때문입니다.

483
00:59:47,045 --> 00:59:52,939
그러므로 Pooling 할때는 padding을 고려하지 않아도 되고
그냥 downsample만 하면 됩니다.

484
00:59:52,939 --> 01:00:06,269
가장 널리 쓰이는 필터사이즈는 2 x 2, 3 x 3 이고
보통 stride는 2로 합니다.

485
01:00:06,269 --> 01:00:14,956
필터가 3 x 3 일때도 보통 stride는 2 x 2 로 합니다.
앞서 언급했듯, 2 x 2 가 좀더 자주 쓰입니다.

486
01:00:17,958 --> 01:00:21,527
우리는 지금 Conv Layer를 배우고 있습니다.

487
01:00:21,527 --> 01:00:31,492
ReLU Layer는 지난 강의에 했던것과 같습니다.

488
01:00:31,492 --> 01:00:37,865
그리고 downsampling을 하고싶을때는 Pooling을 섞어줍니다.

489
01:00:37,865 --> 01:00:43,766
그리고 여기 마지막에는 FC Layer가 있습니다.

490
01:00:43,766 --> 01:00:48,790
우리가 이전에 살펴본 FC와 똑같습니다.

491
01:00:48,790 --> 01:01:00,421
여기 마지막 Conv Layer의 출력은 3차원 volume으로 이루어집니다.

492
01:01:00,421 --> 01:01:16,275
이 값들을 전부 펴서(stretch) 1차원 벡터로 만듭니다.
그리고 이를 가지고 FC Layer의 입력으로 사용합니다.

493
01:01:16,275 --> 01:01:21,715
그렇게 되면 Conv Net의 모든 출력을 서로 연결하게
되는 것입니다.

494
01:01:22,676 --> 01:01:30,897
이 마지막 Layer부터는 공간적 구조(spatial structure)를
신경쓰지 않게 됩니다.

495
01:01:30,897 --> 01:01:43,185
전부 다 하나로 통합시키고는 최종적인 추론을 하게 됩니다.
그렇게 되면 Score가 출력으로 나오게 되는 것입니다.

496
01:01:45,744 --> 01:01:47,232
자 그럼..

497
01:01:47,232 --> 01:02:00,357
[학생이 질문]

498
01:02:00,357 --> 01:02:04,238
잘 못들었습니다??

499
01:02:04,238 --> 01:02:14,472
[학생이 질문]

500
01:02:15,772 --> 01:02:21,866
질문은 바로, 여기 열(col)들을 각각 어떻게 해석해야 하는지 입니다.
각 열들의 위에는 POOL같은 것들이 써있죠

501
01:02:21,866 --> 01:02:29,676
우선 여기 보이는 것들은 출력 Activation map입니다.

502
01:02:29,676 --> 01:02:31,187
각 출력들은 하나의 Layer의 결과 값입니다.

503
01:02:31,187 --> 01:02:41,837
처음부터 한 번 살펴보겠습니다. 처음에 Conv Layer를 거치면
activation map이 나옵니다.

504
01:02:41,837 --> 01:02:59,570
그리고 ReLU를 거칩니다. 그리고 Pooling Layer는 그
ReLU의 출력을 입력으로 받아서 Downsampling 하는 것입니다.

505
01:02:59,570 --> 01:03:05,810
Pooling 을 통해 Downsample합니다.

506
01:03:05,810 --> 01:03:13,004
Pool Layer의 출력을 한 번 보면
RELU의 출력과 비슷하게 생겨 보입니다.

507
01:03:13,004 --> 01:03:23,834
하지만 ReLU의 출력을 Downsample한 것이죠.
눈으로 보기에는 비슷하게 생겼을 수 있습니다.

508
01:03:23,834 --> 01:03:26,751
[학생이 질문]

509
01:03:31,823 --> 01:03:38,291
질문은 저 결과만 보면 정보 자체는 엄청 적을 것 같은데
저 정보만 가지고 어떻게 분류를 할 수 있는지 입니다.

510
01:03:38,291 --> 01:03:51,996
우선 저기 오른쪽 맨 끝에있는 Pool Layer의 출력 값은
전체 네트워크를 통과한 집약체라고 할 수 있습니다.

511
01:03:51,996 --> 01:03:58,902
그러므로 우리가 만든 계층구조의 최상위라고 할 수 있습니다.
실제로 Higher level concept을 표현하고 있는 것들입니다.

512
01:03:58,902 --> 01:04:04,871
예전에 Hubel and Wiesel의 예를 보았고, 이를 기반으로
우리는 필터를 계층적으로 쌓아 올렸습니다.

513
01:04:04,871 --> 01:04:11,557
최하위 계층에서는 edges 와 같은 단순한 구조를 찾아냅니다.

514
01:04:11,557 --> 01:04:24,000
그러니 여기 첫번째 열(col)의 map들이 의미하는 것은
각 자리에서 edges같은 것들이 얼마나 존재하는지를 의미합니다.

515
01:04:24,000 --> 01:04:28,215
그리고 지나면 지날수록 더 복잡한 것들을 찾아냅니다.

516
01:04:28,215 --> 01:04:37,380
두 번째 레이어에서는 (edge보다 복잡한) corner
같은 것이 얼마나 있는지를 보여주는 것입니다.

517
01:04:37,380 --> 01:04:43,927
각 Conv의 입력이 원본 이미지(자동차)가 아닙니다.
이전 레이어에서 나온 edge maps과 같은 것들이죠

518
01:04:43,927 --> 01:04:50,352
이런 edge map을 가지고 더 복잡한 것들을 추론을 하는 것입니다.

519
01:04:50,352 --> 01:04:58,679
마지막 pooling layer를 거치기 전까지의 각 값들은
각 필터가 가진 templete이 얼마나 활성되었는지를 표현합니다.

520
01:05:00,065 --> 01:05:11,811
그리고 이런 정보를 가지고 FC Layer를 거치게 되면,
그 정보들을 한데 모아 클래스 스코어를 계산하는 것입니다.

521
01:05:11,811 --> 01:05:18,351
요약하자면, 각 값들이 의미하는 것은 필터의 Templete이
얼마나 활성화 되었는가를 의미하는 것입니다.

522
01:05:20,343 --> 01:05:21,760
질문 있나요?

523
01:05:21,760 --> 01:05:24,539
[학생이 질문]

524
01:05:24,539 --> 01:05:28,596
질문은 바로 Classification 문제를 푸는데
pooling을 얼마나 해야하는지를 어떻게 아는지 입니다.

525
01:05:28,596 --> 01:05:32,022
제 대답은, 그저 여러분이 시도해 봐야 한다는 것입니다.

526
01:05:32,022 --> 01:05:38,730
실제로 여러분이 어떤 Design choices를 할때
조금의 직관은 가질 수 있습니다.

527
01:05:38,730 --> 01:05:47,260
Pool을 너무 많이하면 값이 너무 작아질 것이고
전체 이미지를 잘 표현하지 못하겠죠

528
01:05:47,260 --> 01:05:53,590
때로는 조금의 직관이 있을수도 있겠지만,
일반적으로는 다양한 옵션을 시도해 봅니다.

529
01:05:53,590 --> 01:06:00,792
다양한 pooling 사이즈, 필터 크기, 레이어 수 등을 시도해 보는
등의 Crossvalidation을 해야 할 것입니다

530
01:06:00,792 --> 01:06:11,640
그리고 가장 좋은 것을 찾는 것이지요. 어떤 하이퍼파라미터가 좋은지는
어떤 문제이냐에 따라 달라질 수 있습니다.

531
01:06:14,688 --> 01:06:27,724
그리고 마지막으로 이 demo를 보여드리겠습니다. CS231N강의를 만든
Andre Kapathy의 작품입니다.

532
01:06:27,724 --> 01:06:37,174
이 Demo는 CIFAR-10을 학습시킵니다. CIFAR-10은 이전에
살펴봤습니다. 10개의 클래스를 가지고 있죠.

533
01:06:37,174 --> 01:06:45,560
이 Demo가 좋은점은 실제로 Filter가 어떻게 생겼고
Activation Map이 어떻게 생겼는지를 볼 수 있다는 점입니다.

534
01:06:45,560 --> 01:06:49,135
앞서 슬라이드에서 소개했던 이미지 중 일부는
이 Demo에서 가져온 것들입니다.

535
01:06:49,135 --> 01:06:56,568
각자 demo에 접속해서 activation map이 어떻게
생겼는지도 한 번씩 보면 좋을 것 같습니다.

536
01:06:56,568 --> 01:07:03,009
그리고 대게는 가장 처음에 있는 Layer의 activation map은
여러분도 해석할 수 있을 것입니다.

537
01:07:03,009 --> 01:07:06,832
첫 번째 Layer의 입력은 원본 이미지 이기 때문에
직관적으로 이 template이 의미하는것이 무엇인지를 알 수 있습니다.

538
01:07:06,832 --> 01:07:12,463
반면 더 높은 Layer로 가면 갈수록 해당 Layer가 무슨
일을 하는지 해석하기가 점점 힘들어 집니다.

539
01:07:12,463 --> 01:07:18,835
그런 레이어들을 해석하지 못한다고 너무 상심하지 마세요

540
01:07:18,835 --> 01:07:23,571
Demo를 보며 흐름이 어떻게되고 출력이 어떻게 되는지를
전체적으로 살펴보는 것 만으로도 좋습니다.

541
01:07:25,285 --> 01:07:31,246
자 마지막으로 요약해보자면 오늘 우리는
CNN이 어떻게 동작하는지를 배웠습니다.

542
01:07:31,246 --> 01:07:39,591
기본적으로는 Conv 와 Pool 를 쌓아 올리다가
마지막에 FC Layer로 끝나게 됩니다.

543
01:07:39,591 --> 01:07:48,664
그리고 네트워크의 필터는 점점 더 작아지고, 아키텍쳐는 점점
깊어지는 경향을 배웠습니다. 나중에 예시를 더 다루겠습니다.

544
01:07:48,664 --> 01:07:53,515
그리고 Pooling 이나 FC Layer를 점점
더 없애는 추세이기도 합니다.

545
01:07:53,515 --> 01:08:02,358
그냥 Conv Layer만 깊게 쌓는 것입니다.
나중에 더 이야기 할 시간이 있을 것입니다.

546
01:08:03,198 --> 01:08:14,497
전형적인 CNN 아키텍쳐는 Conv와 ReLU를 몇 번(n번) 반복합니다.
그리고 그 중간에 pooling도 몇 번 들어갑니다.

547
01:08:14,497 --> 01:08:21,587
그리고 FC Layer가 이어집니다.
한 번에서 두 번 그 이상일수도 있습니다.

548
01:08:21,587 --> 01:08:25,360
그리고 Class score를 구하기 위해 softmax를 사용합니다.

549
01:08:25,360 --> 01:08:30,483
그리고 일반적으로 n은 5 정도가 될 수 있습니다.

550
01:08:31,708 --> 01:08:41,000
그렇게 되면 엄청 깊은 Conv, ReLU, Pool 시퀀스를 구성하게
될 것이고, 그다음 한두 번의 FC Layer가 이어지는 것입니다.

551
01:08:41,001 --> 01:08:51,055
그리고 다음 시간에는 더 최신의 다양한 아키텍쳐를 배우겠습니다. 
ResNet이나 GoogLeNet 같은 것들이죠

552
01:08:51,056 --> 01:08:54,056
감사합니다. 수업 끝

























