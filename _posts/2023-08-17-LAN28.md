---
title: "Quantifying the advantage of domain-specific pre-training on named entity recognition tasks in materials science
 정리" 
date:   2023-08-17
excerpt: "Quantifying the advantage of domain-specific pre-training on named entity recognition tasks in materials science"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---

   **[원 논문]**     
[Quantifying the advantage of domain-specific pre-training on named entity recognition tasks in materials science](https://www.sciencedirect.com/science/article/pii/S2666389922000733)


-----


# THE BIGGER PICTURE 
A bottleneck in efficiently connecting new materials discoveries to established literature has arisen due to a massive increase in publications. Four different language models are trained to
automatically collect important information from materials science articles. We compare a simple model
(BiLSTM) with materials science knowledge to three variants of a more complex model: one with general
knowledge (BERT), one with general scientific knowledge (SciBERT), and one with materials science knowledge (MatBERT). We find that MatBERT performs the best overall. This implies that language models with
greater extents of materials science knowledge will perform better on materials science-related tasks. The
simpler model even consistently outperforms BERT. Furthermore, the performance gaps grow when the
models are given fewer examples of information extraction to learn from. MatBERT’s higher-quality results
should accelerate the collection of information from materials science literature.

# SUMMARY
A bottleneck in efficiently connecting new materials discoveries to established literature has arisen due to an
increase in publications. This problem may be addressed by using named entity recognition (NER) to extract
structured summary-level data from unstructured materials science text. We compare the performance of
four NER models on three materials science datasets. The four models include a bidirectional long shortterm memory (BiLSTM) and three transformer models (BERT, SciBERT, and MatBERT) with increasing degrees of domain-specific materials science pre-training. MatBERT improves over the other two BERTBASEbased models by 1%12%, implying that domain-specific pre-training provides measurable advantages.
Despite relative architectural simplicity, the BiLSTM model consistently outperforms BERT, perhaps due
to its domain-specific pre-trained word embeddings. Furthermore, MatBERT and SciBERT models outperform the original BERT model to a greater extent in the small data limit. MatBERT’s higher-quality predictions
should accelerate the extraction of structured data from materials science literature.



# **ABSTRACT**
