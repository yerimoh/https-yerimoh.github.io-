---
title: "Quantifying the advantage of domain-specific pre-training on named entity recognition tasks in materials science
 정리" 
date:   2023-08-17
excerpt: "Quantifying the advantage of domain-specific pre-training on named entity recognition tasks in materials science"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---

   **[원 논문]**     
[Quantifying the advantage of domain-specific pre-training on named entity recognition tasks in materials science](https://www.sciencedirect.com/science/article/pii/S2666389922000733)


-----


# THE BIGGER PICTURE 
새로운 materials 발견을 기존 문헌에 효율적으로 연결하는 데 병목 현상이 발생했음.     
그래서 이를 해결하기위해 materials science articles에서 **중요한 정보를 자동으로 수집**하도록 네 가지 언어 모델을 교육한다.      
그리고 materials 과학 지식을 갖춘 간단한 모델(BiLSTM)을 3가지 science model과 비교한다.       
**1)** 일반 지식을 갖춘 모델 (BERT)    
**2)** 일반 과학 지식을 갖춘 모델 (SciBERT)    
**3)** materials 과학 지식을 갖춘 모델(MatBERT)    

그 결과,    
* **MatBERT가 전반적으로 가장 성능이 좋음**을 발견함.          
이것은 materials 과학 지식을 담은 언어 모델이 materials 과학 관련 작업에서 더 잘 수행할 것임을 암시한다.     
* 더 간단한 모델은 심지어 지속적으로 BERT를 능가한다.           
* 모델이 **배울 정보 추출의 예가 더 적을 때 성능 격차가 증가**     


MatBERT의 더 높은 품질의 결과는 materials 과학 문헌에서 정보 수집을 가속화해야할 것이다.     


----

# SUMMARY
많은 established literature로 인해 **새로운 재료 발견을 기존 문헌에 효율적으로 연결하는 데 병목 현상이 발생**함      
이 문제는 **NER(Named Entity Recognition)** 를 사용하여 <span style="background-color:#fff5b1">**비정형 재료 과학 텍스트**에서 **구조화된 요약 수준 데이터**를 **추출**함으로써 해결</span> 가능.    

**[실험]**
우리는 3개의  materials science datasets에서 4개의 NER 모델의 성능을 비교함.      
* **4개의 모델:**
a bidirectional long shortterm memory (BiLSTM)      
domain-specific materials science pre-training 정도가 증가하는 3개의 transformer models(BERT, SciBERT 및 MatBERT)이 포함됨

**[결과]**       
* **MatBERT**는 다른 2개의 BERTBASE 기반 모델보다 1%~12% 향상되 **domain-specific pre-training이 측정 가능한 이점을 제공**한다는 것을 의미함       
* **BiLSTM** 모델은  **domain-specific pre-trained word embeddings** 때문에 BERT를 지속적으로 능가
* 게다가, **MatBERT와 SciBERT** 모델은 **작은 데이터 한계에도 original BERT 모델보다 더 우수**      

 
 MatBERT의 materials science literature에서 **구조화된 데이터 추출을 가속화**해야한다.        

----


# **INTRODUCTION**
**[PROBLEM]**        
* 최근 재료 과학 분야의 출판물 수가 기하급수적으로 증가하고 있다.       
그 결과, 연구자들은 상대적으로 <span style="background-color:#fff5b1">제한된 하위 영역 내에서도 **연구 성과를 따라가기가 점점 어려워지고 있다**</span>       
* <span style="background-color:#fff5b1">재료과학 문헌이 매우 많아 졌다는 것은 이전에 **특정 응용을 위해 연구된 재료 후보들이 어떤 것**인지와 같은 **비교적 간단한 질문**들조차도 포괄적으로 **대답하기가 어렵거나 불가능**할 수 있다</span>는 것을 의미한다.       
* <span style="background-color:#FFE6E6">이것은 **관련 문헌과 관련된 정보를 추출**하기 위한 **새롭고 효율적인 방법에 대한 필요성**을 창출함</span>           


**[Motivation]**      
* **Natural language processing (NLP)로 접근**해 위 문제 해결 시도.        
NLP는 많은 재료 과학 응용 분야에 성공적으로 적용되었고 재료 정보학 분야에서 최근 몇 가지 연구의 주제이기 때문        
* BERT와 같은 **transformer ML 아키텍처**의 등장은 NLP를 벤치마크화했다          
Transformer models 은 downstream tasks을 위해 pre train과 fine tuning의 개념을 도입합.    
이 방식은 task별 모델이 비교적 적은 hand-annotated examples를 사용하여 교육될 수 있도록 한다.      
* single pre-trained model이 여러 NLP 작업(예: QA, NER, 다음 문장 예측)을 처리할 수 있지만,
domain-specific pre-training 모델(예: BioBERT, CaseHOLD)의 성공이 아래의 질문을 제기한다.
<span style="background-color:#FFE6E6"> **can transformer models be further improved with even more domain-specific pre-training?**</span>
즉, domain-specific pre-training을 transformer에 model에 적용해해도 성능이 좋을까라는 의문이다.


**[Main Idea]**       
* domain-specific pre-training로 보여졌던 측정 가능한 이점(예: SciBERT)이 **재료 과학과 같은 더 좁은 과학 분야에 특화된 모델**로 **다시 확장될 수 있다고 가정**
✨ 여기서, domain-specific 모델 성능이 향상되었다는 것은 (NLP 모델의 관점에서) 가장 복잡하고 성가신 과학 영역에서도 **자동화된 지식 추출 능력이 향상**되었다는 것을 의미함



In this work, we apply transformer models to the task of named
entity recognition (NER)22 to extract and label important scientific entities relevant to materials chemistry from unstructured
text. A well-trained NER model will be capable of automatically
mapping the unstructured text of materials science publications
to a queryable database of key terms. Historically, NER has been
used to extract information such as names and locations from
various articles, though recently it has been employed in the
chemical, medical, and materials sciences as well.1–4,23–39 For
material science, this may include terms that refer to materials
and their geometries, properties, syntheses, methods of characterization, and downstream applications. Strongly related work
in text mining and language modeling has also been employed
in the same fields.5,40–60 BERT has additionally found use in
biology, medicine, and materials science.18,61,62
Specific to the field of materials science, there have been significant efforts to apply NER to the extraction of materials synthesis recipes, including using BERT.2,28,29,57,62 In the past,
these have employed a combination of the aforementioned
work in the chemical sciences to extract inorganic material entities with syntax trees and lookup tables to extract properties
and processing conditions. The recently developed transformer-based models have been shown to offer significant performance improvements on NLP tasks.9 This provides an excellent opportunity to evaluate the performance of these new
models on NER tasks specific to materials science.



In this work, we apply four different NER models to three
different materials science datasets and analyze their performance. The simplest model considered is a bidirectional long
short-term memory (BiLSTM) recurrent neural network. The
other three models, variants of the popular transformer-based
BERTBASE neural network structure,9 have identical model structures but use pre-training corpora of varying domain specificity.
The considered datasets consist of one general-purpose materials science dataset (referred to as the solid-state dataset) and
two topic-specific datasets that respectively focus on doping
and gold nanoparticle synthesis. We use the results of NER on
these materials science datasets to relate the domain specificity
of the pre-training corpus to measurable performance differences in extracting named entities.
