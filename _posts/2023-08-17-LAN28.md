---
title: "Quantifying the advantage of domain-specific pre-training on named entity recognition tasks in materials science
 정리" 
date:   2023-08-17
excerpt: "Quantifying the advantage of domain-specific pre-training on named entity recognition tasks in materials science"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---

   **[원 논문]**     
[Quantifying the advantage of domain-specific pre-training on named entity recognition tasks in materials science](https://www.sciencedirect.com/science/article/pii/S2666389922000733)


-----


# THE BIGGER PICTURE 
새로운 materials 발견을 기존 문헌에 효율적으로 연결하는 데 병목 현상이 발생했음.     
그래서 이를 해결하기위해 materials science articles에서 **중요한 정보를 자동으로 수집**하도록 네 가지 언어 모델을 교육한다.      
그리고 materials 과학 지식을 갖춘 간단한 모델(BiLSTM)을 3가지 science model과 비교한다.       
**1)** 일반 지식을 갖춘 모델 (BERT)    
**2)** 일반 과학 지식을 갖춘 모델 (SciBERT)    
**3)** materials 과학 지식을 갖춘 모델(MatBERT)    

그 결과,    
* **MatBERT가 전반적으로 가장 성능이 좋음**을 발견함.          
이것은 materials 과학 지식을 담은 언어 모델이 materials 과학 관련 작업에서 더 잘 수행할 것임을 암시한다.     
* 더 간단한 모델은 심지어 지속적으로 BERT를 능가한다.           
* 모델이 **배울 정보 추출의 예가 더 적을 때 성능 격차가 증가**     


MatBERT의 더 높은 품질의 결과는 materials 과학 문헌에서 정보 수집을 가속화해야할 것이다.     


----

# SUMMARY
많은 established literature로 인해 **새로운 재료 발견을 기존 문헌에 효율적으로 연결하는 데 병목 현상이 발생**함      
이 문제는 **NER(Named Entity Recognition)** 를 사용하여 <span style="background-color:#fff5b1">**비정형 재료 과학 텍스트**에서 **구조화된 요약 수준 데이터**를 **추출**함으로써 해결</span> 가능.    

**[실험]**
우리는 3개의  materials science datasets에서 4개의 NER 모델의 성능을 비교함.      
* **4개의 모델:**
a bidirectional long shortterm memory (BiLSTM)
domain-specific materials science pre-training 정도가 증가하는 3개의 transformer models(BERT, SciBERT 및 MatBERT)이 포함됨

**[결과]**       
* **MatBERT**는 다른 2개의 BERTBASE 기반 모델보다 1%~12% 향상되 **domain-specific pre-training이 측정 가능한 이점을 제공**한다는 것을 의미함       
* **BiLSTM** 모델은  **domain-specific pre-trained word embeddings** 때문에 BERT를 지속적으로 능가
* 게다가, **MatBERT와 SciBERT** 모델은 **작은 데이터 한계에도 original BERT 모델보다 더 우수**      

 
 MatBERT의 materials science literature에서 **구조화된 데이터 추출을 가속화**해야한다.        

----


# **INTRODUCTION**
Recently, the number of publications in the field of materials science has grown exponentially.1 As a result, it has become
increasingly difficult for researchers to follow research progress
as it emerges, even within relatively restricted sub-domains. The
size of the materials science literature means that even relatively
simple questions, such as which material candidates have
previously been studied for a particular application, can be difficult or impossible to comprehensively answer. This has created
a need for new, more efficient ways to engage with the literature
and extract the relevant information therein.


Natural language processing (NLP), the analysis of unstructured text using computers, provides a natural candidate for
such an alternative approach. NLP has successfully been
applied to a number of materials science applications and is
the topic of several recent investigations in materials informatics.2–5 Additionally, work has been done to develop metalearning strategies for NER.6–8 Recently, the advent of transformer ML architectures such as BERT9 have revolutionized
NLP; leading benchmarks such as GLUE10 are now dominated
by models utilizing attention-based encoder-decoder architectures called transformers11 and perform comparably to humans
on some tasks. Transformer models have ushered in a new NLP
paradigm where large and general NLP models are ‘‘pre-trained’’
on semi-supervised tasks before being fine-tuned for downstream tasks.9,12–17 The pre-training approach allows for taskspecific models to be trained using relatively few hand-annotated examples; this is a useful feature for practical applications
of NLP bottlenecked by annotation such as scientific tasks that
contain technical text and esoteric vocabulary




Although a single pre-trained model may address multiple NLP
tasks (e.g., question answering, named entity recognition, next
sentence prediction), the success of models with domain-specific pre-training such as BioBERT,18 CaseHOLD,19 and
FinBERT20 begs the question: can transformer models be further
improved with even more domain-specific pre-training? We hypothesize that the measurable advantages previously shown
with domain-specific pre-training—for example, of SciBERT
over BERT21—can again be extended to models specific to narrower scientific disciplines such as materials science. Improved
domain-specific model performance implies improved ability for
automated knowledge extraction from even the most complex
and vexing (from the perspective of NLP models) scientific domains. Exploring this problem in-depth presents an opportunity
for the collation and synthesis of massive numbers of highly
complex scientific publications into otherwise inaccessible
structured databases and models for knowledge generation



In this work, we apply transformer models to the task of named
entity recognition (NER)22 to extract and label important scientific entities relevant to materials chemistry from unstructured
text. A well-trained NER model will be capable of automatically
mapping the unstructured text of materials science publications
to a queryable database of key terms. Historically, NER has been
used to extract information such as names and locations from
various articles, though recently it has been employed in the
chemical, medical, and materials sciences as well.1–4,23–39 For
material science, this may include terms that refer to materials
and their geometries, properties, syntheses, methods of characterization, and downstream applications. Strongly related work
in text mining and language modeling has also been employed
in the same fields.5,40–60 BERT has additionally found use in
biology, medicine, and materials science.18,61,62
Specific to the field of materials science, there have been significant efforts to apply NER to the extraction of materials synthesis recipes, including using BERT.2,28,29,57,62 In the past,
these have employed a combination of the aforementioned
work in the chemical sciences to extract inorganic material entities with syntax trees and lookup tables to extract properties
and processing conditions. The recently developed transformer-based models have been shown to offer significant performance improvements on NLP tasks.9 This provides an excellent opportunity to evaluate the performance of these new
models on NER tasks specific to materials science.



In this work, we apply four different NER models to three
different materials science datasets and analyze their performance. The simplest model considered is a bidirectional long
short-term memory (BiLSTM) recurrent neural network. The
other three models, variants of the popular transformer-based
BERTBASE neural network structure,9 have identical model structures but use pre-training corpora of varying domain specificity.
The considered datasets consist of one general-purpose materials science dataset (referred to as the solid-state dataset) and
two topic-specific datasets that respectively focus on doping
and gold nanoparticle synthesis. We use the results of NER on
these materials science datasets to relate the domain specificity
of the pre-training corpus to measurable performance differences in extracting named entities.
