---
title: "[23] CS231N: Lecture 3 Regularization and Optimization Regularization (2/2)"
date:   2020-02-10
excerpt: "Lecture 3 | Stochastic Gradient Descent, Momentum, AdaGrad, Adam, Learning rate schedules 요약"
category: [Deep Learning]
layout: post
tag:
- Deep Learning
order: 0

comments: true
---





---

## 경사 따라가기
더 나은 전략은 주변의 기하학을 이용하는 것이다.      

더 나은 이해를 위해서 비유를 해보자.      
여러분이 아래 풍경에서 계곡바닥으로 내려가는 길을 찾는 상황이라고 가정해보자      
이때,     
* 여러분: 최솟값을 찾고자 하는 사람                
* 풍경의 모든 지점: W 파라미터의 어떤 세팅          
* 이 점들의 높이: 손실, W에 의해 발생        
* 계곡바닥: 찾고자 하는 W 값      
![image](https://user-images.githubusercontent.com/76824611/169579436-bfb5621e-6175-49f8-a8d5-bcd99437dcc3.png)      


여러분이 이 풍경을 돌아다니는 이 작은 사람이라면,     
아마도 여러분은 계곡 바닥으로 내려 가는 길을 바로 알 수는 없다.       

그러나 여러분이 할 수 있는 것은 발로 느끼면 그 근처의 기하학적 구조 어떻게 생겼는지 알아내는 것이다.       
➡ 여러분이 여기 서 있다면, 어떤 방향이 나를 좀 더 아래쪽으로 데려다 줄 지를 발로 느껴보는 것이다.      
➡ 땅의 경사가 어디 있는지, 나를 이 방향으로 내려가게 해 줄 지를 말이다.      
➡ 그 방향으로 한발 움직여서 조금 내려가고, 어느쪽이 아래인지를 발로 다시 느끼는 걸 계속 반복해서, 결국 계곡의 바닥에 도착하길 바라는 방법이다.         


**[경사(gradient)의 의미]**       
적어도 1차원에서는, 경사는 이 함수의 미분이다.        
![image](https://user-images.githubusercontent.com/76824611/169581184-f2f6e2c2-0af4-4e9c-8f3e-edc3bec3f715.png)             
만약 1차원 함수 f가 있다면, 그건 스칼라 x를 받아서, 어떤 **커브의 높이**를 돌려준다.     
➡ 우리는 이 덕분에 어떤 지점에서든지 경사를 혹은 미분값을 계산 가능하다.      
➡ 어느 방향으로든지 **작은 걸음 h를 받아서** **그 걸음에 대한 함수값의 차이를 비교**하는 것이다.       
➡ 그리고 **걸음 크기를 0**으로 하면, 그 지점에서 그 함수의 경사가 나오는 것이다.      


**[경사(gradient) 일반화]**      
실제에서, x(스칼라)가 **전체 벡터**로 바뀜              
➡ x가 벡터이기 때문에 위의 개념을 **다변수**로 확장시켜야 한다.              
* **의미**           
   * 다변수인 상황에서의 **미분으로 일반화**시켜보면 **gradient**이고,     
   * gradient는 **벡터 x의 각 요소를 편도함수들의 집합**이다.        
* **형(shape)**           
    * gradient의 모양(shape)은 x와 같다. (입력의 shape이 (3,)이면 gradient의 shape도 (3,))           
    ➡ 각 x값마다의 기울기를 구한 것이기 때문이다.       
* **방향의 의미**        
    * 함수에서 "가장 많이 올라가는 방향" 이 된다.(ㅇ리의 목표와 반대)     
    * 그러므로 우리의 목표인 "가장 적게 올라가는 방향"을 얻기 위해선 gradient의 반대방향으로 가면 된다.           



* x: 전체 벡터인데, 이 우리가 미분값을 사용한다는 개념이 다변수까지 일반화되죠. 다변수 세팅에서 경사는 부분 미분의 벡터입니다. 그리고 경사는 x와 같은 모양을 가질 겁니다. 경사의 각 항목은 만약 우리가 좌표에서 그 방향으로 움직일 때, 함수 f의 경사값이 뭔지를 얘기해 주죠. 경사는 매우 좋은 특성을 갖는 걸로 알려졌는데, 경사는 부분 미분한 벡터이기 때문에 함수의 가장 크게 증가하는 방향을 가리킵니다. 따라서, 반대 경사 방향을 보면, 함수의 가장 크게 감소하는 방향을 알려주죠. 더 일반적으로, 그 풍경에서 모든 방향으로의 경사를 알고 싶다면, 그건 경사와  그 방향을 가리키는 단위 벡터 (unit vector)와의 내적과 같죠. 경사가 중요한 건, 현재지점에서 여러분의 함수로의 이 선형 1차 근사를 제공합니다. 실제론, 딥 러닝의 많은 부분은 여러분의 함수의 경사를 계산하는 것에 관한 겁니다. 그 경사를 이용해서 여러분의 파라미터나 벡터를 반복적으로 업데이트하죠. 컴퓨터에서 이 경사를 평가하는 방법으로 생각해 볼 수 있는 나이브 (naive)한 방법은, 유한 차분 (finite difference)의 방법을 이용하는 거죠. 

