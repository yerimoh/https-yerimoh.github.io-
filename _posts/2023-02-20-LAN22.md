---
title: "SCIBERT: A Pretrained Language Model for Scientific Text 정리"
date:   2023-02-20
excerpt: "SCIBERT: A Pretrained Language Model for Scientific Text"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---


# Abstract
Obtaining large-scale annotated data for NLP
tasks in the scientific domain is challenging and expensive. We release SCIBERT,
a pretrained language model based on BERT (Devlin et al., 2019) to address the lack
of high-quality, large-scale labeled scientific
data. SCI
BERT leverages unsupervised
pretraining on a large multi-domain corpus
of scientific publications to improve performance on downstream scientific NLP tasks.
We evaluate on a suite of tasks including
sequence tagging, sentence classification and
dependency parsing, with datasets from a
variety of scientific domains. We demonstrate statistically significant improvements
over BERT and achieve new state-of-theart results on several of these tasks. The
code and pretrained models are available at
https://github.com/allenai/scibert/


# 1 Introduction
The exponential increase in the volume of scientific publications in the past decades has made
NLP an essential tool for large-scale knowledge
extraction and machine reading of these documents. Recent progress in NLP has been driven
by the adoption of deep neural models, but training such models often requires large amounts of
labeled data. In general domains, large-scale training data is often possible to obtain through crowdsourcing, but in scientific domains, annotated data
is difficult and expensive to collect due to the expertise required for quality annotation.
As shown through ELMo (Peters et al.
,
2018), GPT (Radford et al.
, 2018) and
BERT
(Devlin et al.
, 2019), unsupervised pretraining of language models on large corpora
significantly improves performance on many
NLP tasks. These models return contextualized
embeddings for each token which can be passed
into minimal task-specific neural architectures.
Leveraging the success of unsupervised pretraining has become especially important especially
when task-specific annotations are difficult to
obtain, like in scientific NLP. Yet while both BERT and ELMo have released pretrained models,
they are still trained on general domain corpora
such as news articles and Wikipedia

In this work, we make the following contributions:

(i) We release SCIBERT, a new resource demonstrated to improve performance on a range of NLP
tasks in the scientific domain. SCI
BERT is a pretrained language model based on BERT but trained
on a large corpus of scientific text

(ii) We perform extensive experimentation to
investigate the performance of finetuning versus task-specific architectures atop frozen embeddings, and the effect of having an in-domain vocabulary

(iii) We evaluate SCIBERT on a suite of tasks
in the scientific domain, and achieve new state-ofthe-art (SOTA) results on many of these tasks.


---
----


# 2 Methods
## Background 
The BERT model architecture (Devlin et al., 2019) is based on a multilayer bidirectional Transformer (Vaswani et al., 2017). Instead of the traditional left-to-right language modeling objective, BERT is trained on two tasks: predicting randomly masked tokens and predicting
whether two sentences follow each other. SCIBERT follows the same architecture as BERT but is
instead pretrained on scientific text.


## Vocabulary
BERT uses WordPiece (Wu et al.
,
2016) for unsupervised tokenization of the input
text. The vocabulary is built such that it contains
the most frequently used words or subword units.
We refer to the original vocabulary released with BERT as BASEVOCAB.'

We construct SCIVOCAB, a new WordPiece vocabulary on our scientific corpus using the SentencePiece1
library. We produce both cased and
uncased vocabularies and set the vocabulary size
to 30K to match the size of BASEVOCAB. The resulting token overlap between BASEVOCAB and
SCIVOCAB is 42%, illustrating a substantial difference in frequently used words between scientific and general domain texts

## Corpus 
We train SCIBERT on a random
sample of 1.14M papers from Semantic
Scholar (Ammar et al., 2018). This corpus
consists of 18% papers from the computer science
domain and 82% from the broad biomedical
domain. We use the full text of the papers, not
just the abstracts. The average paper length is
154 sentences (2,769 tokens) resulting in a corpus
size of 3.17B tokens, similar to the 3.3B tokens
on which BERT was trained. We split sentences
using ScispaCy (Neumann et al., 2019),2 which is
optimized for scientific text.









