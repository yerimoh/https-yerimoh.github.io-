---
title: "SCIBERT: A Pretrained Language Model for Scientific Text 정리"
date:   2023-02-20
excerpt: "SCIBERT: A Pretrained Language Model for Scientific Text"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---


# Abstract

<span style="background-color:#F5F5F5">**[문제]**</span>         
과학 영역에서 NLP 작업에 대한 large-scale annotated data를 얻는 것은 어렵고 비용이 많이 든다.     

<span style="background-color:#F5F5F5">**[해결: SCIBERT]**</span>         
* 우리는 high-quality의  large-scale labeled scientific data의 부족을 해결하기 위해,     
 [BERT](https://yerimoh.github.io/Lan2/)를 기반으로 pre-train된 SCIBERT를 만들었    
 * <span style="background-color:#FFE6E6">SCIBERT는 **scientific publications**의  **large multi-domain corpus에서 unsupervised pretraining 을 활용**하여 downstream scientific NLP tasks의 성능을 향상</span>시킨다.     
 * 이는 BERT에 비해 더 좋은 성능을 낸다.    
 * 코드 및 사전 교육된 모델은 [링크](https://github.com/allenai/scibert/)에서 확인 가능하다.     



----
----


# 1 Introduction
scientific publications의 양이 매우 많아, NLP는 이러한 문서의 대규모 지식 추출 및 기계 판독을 위한 필수 도구가 됨.   

<span style="background-color:#F5F5F5">**[문제]**</span>         
* NLP의 모델들을 훈련하려면 많은 양의 labeled data가 필요한 경우가 많다.     
➡ 이러한 데이터를 얻는 것은 어렵고 비용이 많이 듦     
* [ELMo](https://wikidocs.net/33930), [GPT](https://yerimoh.github.io/Lan4/) 및 [BERT](https://yerimoh.github.io/Lan2/) 통해 알 수 있듯이,  large corpora에서 언어 모델의 **unsupervised pretraining of language models은 많은 NLP 작업에서 성능을 크게 향상**시킨다.     
이러한 모델은 각 토큰에 대한 **contextualized embeddings을 반환**한다.(즉, 문맥 파악이 가능한 토큰을 준다.)     
➡ 즉, 이러한 <span style="background-color:#fff5b1">**unsupervised pretraining을 활용**하는 것은 과학적 NLP에서와 같이 **task-specific annotations을 얻기 어려운 경우에 특히 중요**</span>해졌다.      
* ⚠️ 하지만 BERT와 ELMo는 task-specific과 같은 domain이 아닌 **general domain corpora**(such as news articles and Wikipedia.)**로 훈련을 받은 모델**이다. (science와 같은 task-specific corpora로 훈련을 하지 않음)     


<span style="background-color:#F5F5F5">**[본 논문의 해결]**</span>         
이 작업에서 우리는 다음과 같은 기여를 한다:
* **(i)** 우리는 <span style="background-color:#FFE6E6">**scientific domain**에서 NLP tasks의 범위에서 성능을 향상</span>시키는 것으로 입증된 새로운 리소스인 SCIBERT를 출시한다.    
➡ SCIBERT는 BERT를 기반으로 한 pretrained language model이지만 **large corpus of scientific text**에 대해 훈련되었다.         
* **(ii)** 우리는 frozen embeddings 위의  <span style="background-color:#FFE6E6">**fine tuning VS task-specific architecture**의 성능과  in-domain vocabulary의 영향을 조사</span>하기 위해 광범위한 실험을 수행한다       
* **(iii)** 우리는 scientific domain의 일련의 작업에 대해 SCIBERT를 evaluate하고, SOTA 결과를 달성한다.        




---
----


# 2 Methods
## Background 
* [BERT](https://yerimoh.github.io/Lan2/)는 [multilayer bidirectional Transformer](https://yerimoh.github.io/Lan2/#1-pre-training-bert)를 기반으로 한다.      
* 기존의 왼쪽에서 오른쪽으로 언어 모델링 하는 대신,     
BERT는 [무작위로 마스킹된 토큰을 예측(Masked Language Model, MLM)](https://yerimoh.github.io/Lan2/#1-pre-training-bert)하고,    
[두 문장이 서로 따르는지 여부를 예측(Next sentence prediction, NSP)](https://yerimoh.github.io/Lan2/#1-pre-training-bert)하는 두 가지 작업에 대해 훈련된다.    

SCIBERT는 BERT와 **동일한 아키텍처**를 따르지만 대신 **scientific text에 대해 pretrained**을 받는다.     

---

## Vocabulary
BERT는 입력 텍스트의 unsupervised tokenization를 위해 [WordPiece](https://yerimoh.github.io/Lan2/#token-embeddings)를 사용한다.      
vocabulary는 가장 자주 사용되는 words or subword units를 포함하도록 구성된다.    

우리는 BERT와 함께 발표된 원래의 어휘를 **BASEVOCAB**라고 부른다.      


우리는 [SentencePiece 라이브러리](https://github.com/google/sentencepiece)를 사용하여,    
**scientific corpus에 새로운 WordPiece 어휘인 SCIVOCAB**를 구성한다.     
* 본 논문은 대문자와 소문자가 없는 어휘를 모두 생성하고 BASE VOCAB의 크기에 맞게 어휘 크기를 30K로 설정한다.   
* BASEVOCAB와 SCIVOCAB 사이의 결과 일반 VOCAB의 토큰과 SCIVOACB의 같은 토큰은 42%로 **scientific and general domain texts에서 자주 사용되는 단어의 상당한 차이**를 보여준다    


<details>
<summary>📜 SentencePiece란? </summary>
<div markdown="1">

논문 : https://arxiv.org/pdf/1808.06226.pdf
구글이 BPE 알고리즘과 Unigram Language Model Tokenizer를 구현한 것.   

SentencePiece는 **pre-tokenization을 필요로 하지 않는 tokenizer**의 하나로, 어떤 언어에도 자유롭게 적용될 수 있고 속도도 굉장히 빠르기 때문에 NLP에서 널리 사용되는 tokenizer이다. 

SetencePiece는 기존에 존재하던 unigram, BPE와 같은 tokenizer들을 모든 언어에 대해 적용이 가능하도록 generalize하고 약간의 추가적인 기능들을 더해서 구현한 것이다.    
특별히 새로운 알고리즘을 제시했다기보다는, 기존의 tokenizer들을 좀 더 사용하기 편하고 성능이 좋게 개선했다고 생각하면 되겠다. 
 
 
**[SentencePiece의 특징]**         
* **Pre-tokenization을 필요로 하지 않는다.**    
 기존의 tokenizer들은 영어를 기준으로 학습되었다면 영어가 띄어쓰기를 기반으로 해서 단어들이 대부분 분리되기 때문에, **띄어쓰기가 없는 일본어나 중국어와 같은 언어들에는 적용될 수 없다**는 한계점을 가지고 있었다.     
 하지만 SentencePiece는 **띄어쓰기를 다른 알파벳 혹은 글자처럼 하나의 character로 취급**한다.      
 ➡ 그렇기 때문에 모든 종류의 언어에 대해 general하게 적용이 가능하고, pre-tokenization이 필요하지 않다는 장점을 가진다.    
* **Character-coverage 설정이 가능하다.**     
 영어의 경우에는 알파벳의 개수가 30개 내외로 적기 때문에 괜챃지만, 중국어나 일본어와 같은 문자들의 경우에는 굉장히 많은 종류의 한자를 포함하고 있고, 이에 따라 **엄청나게 많은 개수의 character들을 다 다루어야 해서 tokenizer를 학습하는 데에 시간이 오래 걸린다**는 한계점을 가진다.     
하지만 SentencePiece는 모든 character를 다 고려하는 것이 아닌, **빈도에 따라 상위 99%의 character들만 고려**하는 등의 옵션을 주는 것이 가능하다.
* **Subword generalzation이 가능하다.**      
 우리가 학습한 tokenizer가 항상 완벽하다면 좋겠지만, 이것이 **overfitting되어 있을 가능성**을 배제할 수는 없다. 그렇기 때문에 SentencePiece에서는 우리가 학습한 tokenizer에서 나올 수 있는 token들만을 사용하는 것이 아니라, **가끔은 다른 token들도 샘플링**하면서 사용하여 overfitting하는 것이 가능하다.       
* **속도가 빠르다.**      
 직접 구현해보면 알겠지만, 알고리즘이 굉장히 최적화되어 있어서 같은 BPE와 unigram을 실행하더라도 굉장히 빠른 속도를 보여준다.

</div>
</details>  

---

## Corpus 
우리는 Semantic Scholar의 1.14M 논문의 무작위 샘플에 대해 SCIBERT를 훈련한다.      
이 corpus의 구성    
* 컴퓨터 과학 영역의 논문 18%           
* 광범위한 생물 의학 영역의 논문 82%       

우리는 논문의 전문을 사용한다. (요약본을 사용하지 않는다.)    

평균 논문 길이는 154개의 문장(2,769개의 토큰)으로, **BERT가 훈련된 3.3B 토큰과 유사한 corpus 크기**가 된다.      
우리는 과학적 텍스트에 최적화된 **[ScispaCy](https://github.com/allenai/SciSpaCy)**를 사용하여 문장을 분할한다.    




<details>
<summary>📜 Semantic Scholar란?  </summary>
<div markdown="1">


**적합한 논문**을 신속하게 검색하여 이용자들이 논문의 내용을 이해하는데 도움이 되고자 PDF파일로 간략한 발표 PPT형식으로 **요약한 내용을 열람할 수 있는 서비스를 제공**하고있는 것.  
 
 또한 특정한 연구분야를 연구하는 학자로서 최근 발표된 논문을 Semantic Scholar 논문 검색 엔진을 사용하게 되면 논문의 내용을 파악하는데 걸리는 시간을 단축할 수 있다


이 회사는 Allen Institute for Artificial Intelligence에서 설립한 비영리 기업이며 무료로 서비스를 제공하고 있다. **인공지능은 논문의 줄거리 내용을 파악하여 “Abstractive”기법을 통해 요약된 내용을 생성**하고 있다.
 
 
</div>
</details>  


---
---


# Experimental Setup
## 3.1 Tasks
우리는 아래의 core NLP tasks로 평가한다:        
**1.** Named Entity Recognition (NER)   
**2.** PICO Extraction (PICO)   
**3**. Text Classification (CLS)   
**4.** Relation Classification (REL)  
**5.** Dependency Parsing (DEP)   


PICO, like NER, is a sequence labeling task where
the model extracts spans describing the Participants, Interventions, Comparisons, and Outcomes
in a clinical trial paper (Kim et al., 2011). REL
is a special case of text classification where the
model predicts the type of relation expressed between two entities, which are encapsulated in the
sentence by inserted special tokens.


PICO는 NER와 마찬가지로 모델이 [Participants, Interventions, Comparisons, and Outcomes를 설명하는 범위를 추출](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-12-S2-S5)하는 sequence labeling task이다.    
그리고 REL은 모델이 두 엔티티 간에 표현되는 관계의 유형을 예측하는 텍스트 분류의 특별한 경우이며, 삽입된 특수 토큰에 의해 문장에 캡슐화(encapsulated)된다.   


<details>
<summary>📜 PICO란?  </summary>
<div markdown="1">
 
**PICO란?**        
과학 실험에서 아래 요인들을 뽑아내는 것    
PICO 프레임워크는 structuring clinical question 질문에 필요한 각 핵심 요소를 포착하기 때문에 임상 질문을 구성하는 데 가장 일반적으로 사용되는 모델입니다. PICO는 다음을 의미합니다.    
* Participants/Problem (P)   
* Intervention (I)   
* Comparison (C)   
* Outcome (O)   

**evidence-based medicine (EBM) 애플리케이션은 대규모 의학 문헌 데이터베이스를 분석하여 임상 질문에 답하는 데 의존**합니다.   

잘 정의되고 집중된 임상 질문을 공식화하기 위해 PICO라는 프레임워크가 널리 사용되며, 이는 참가자/문제(P), 개입(I), 비교(C) 및 결과(O)의 네 가지 구성 요소에 속하는 주어진 의학 텍스트의 문장을 식별합니다. 
 
아래 표는 질문의 유형에 따른 PICO이다.   

![image](https://user-images.githubusercontent.com/76824611/225283752-14153e36-7d6c-405f-8eeb-ca5347553eee.png)
![image](https://user-images.githubusercontent.com/76824611/225284104-08723df3-75bf-47b0-a336-133c3201b7f2.png)

 
 https://libguides.mssm.edu/ebm/ebp_pico
 
 
</div>
</details>  


<details>
<summary>📜 NER이란?  </summary>
<div markdown="1">
 
NER(Named Entity Recognition)은 말 그대로 Named Entity(이름을 가진 개체)를 Recognition(인식)하는 것을 의미하며, 개체명 인식이라고 합니다.
 
즉 개체마다 뭔지 태깅을 해주는 것다.

예를 들면 아래와 같다.   
![image](https://user-images.githubusercontent.com/76824611/225285580-0b1ac9d7-75d7-4412-b71a-fa9d60144b1c.png)

‍[출처](https://www.letr.ai/blog/tech-20210723)   
 
</div>
</details>  

<details>
<summary>📜 Automatic classification of sentences to support Evidence Based Medicine?  </summary>
<div markdown="1">
 
Automatic classification of sentences to support Evidence Based Medicine 란?

**Aim**     
Evidence Based Medicine에서 사용되는 미리 정의된 의료 범주 세트가 주어지면 **의학 초록의 문장에 이러한 레이블을 자동으로 추가하는 것**을 목표로 합니다. 
 
**방법**            
우리는 특정 **의료 범주(예: 개입 , 결과) 로 손으로 주석**을 단 1,000개의 의료 초록 모음을 구성했습니다 . 우리는 분류를 위해 CRF(Conditional Random Fields)를 사용하여 데이터의 어휘, 의미, 구조 및 순차 정보를 기반으로 다양한 기능의 사용을 조사했습니다.
 

**결과**    
모든 레이블에 대한 분류 작업의 경우, 우리 시스템은 sequential feature을 사용하여 구조화 및 구조화되지 않은 초록의 데이터 세트에 대해 각각 80.9% 및 66.9%의 micro-averaged f-scores를 달성했습니다. 핵심 문장에만 레이블을 지정하는 경우, 우리 시스템은 동일한 순차적 기능을 사용하여 구조화된 초록과 구조화되지 않은 초록에 대해 각각 89.3%와 74.0%의  f-scores를 생성했습니다. 외부 데이터 세트에 대한 결과는 더 낮았습니다(모든 레이블의 경우 f-점수 63.1%, 주요 문장의 경우 83.8%).

**결론**         
우리가 사용한 기능 중 abstract 에서 주어진 문장을 분류하는 데 가장 좋은 기능은 unigrams, section headings, and sequential information의 순차적 정보를 기반으로 했습니다. 이러한 기능으로 인해 simple bag-of-words 접근 방식에 비해 성능이 향상되었으며 이전 작업에서 사용된 feature sets다 성능이 뛰어났습니다.
 
</div>
</details>  





----




## 3.2 Datasets


**[Table 1]**     
![image](https://user-images.githubusercontent.com/76824611/225572942-78914c0c-d6e1-4437-a1cb-b4bd6a95cc3e.png)
* 모든 tasks 및  datasets에서 모든 BERT 변형의 성능을 테스트한다.        
* 굵은 글씨는 SOTA 결과를 나타낸다. (95% bootstrap 신뢰 구간 내에서 차이가 있을 경우 여러 개의 결과가 굵은 글씨로 표시됨).          
* 과거 작업과 함께, 우리는 아래와 같은 요소를 보고한다.        
    * NER (span-level)에 대한 macro F1 scores   
    * REL 및 CLS(문장 레벨, 문장의 시작 토큰)에 대한 macro F1 scores    
    * PICO(토큰 레벨)에 대한 macro F1 scores     
    * ChemPro에 대한 micro F1 scores       
    * DEP의 경우, 우리는 LAS에 맞게 조정된 hyperparameters를 사용하여 동일한 모델에 대한 레이블링된(LAS) 및 레이블링되지 않은(UAS) attachment scores를 보고한다.     
* 모든 결과는 서로 다른 random seeds를 사용한 여러 runs의 평균이다.         




<details>
<summary>📜 F1 scores  </summary>
<div markdown="1">
 
 
![image](https://user-images.githubusercontent.com/76824611/225508975-89b576ed-3457-433c-a7c6-7e91167e9e1f.png)

![image](https://user-images.githubusercontent.com/76824611/225509055-2a261c2e-bfb8-4fcf-83d1-04bd79b6a4ee.png)

각 범주별 Precision, Recall, F1 Score
![image](https://user-images.githubusercontent.com/76824611/225509111-83dd797e-039c-460c-b734-84374d44ada6.png)

Macro Average F1 Score
![image](https://user-images.githubusercontent.com/76824611/225510862-71947c9c-2e64-451d-b627-ae50d17d9e51.png)

Micro Average F1 Score = accuracy = micro-precision = micro-recall
![image](https://user-images.githubusercontent.com/76824611/225510902-105ad68e-a594-424c-b7a4-2f6c4d86fe2e.png)

</div>
</details>  




위 표 외의 데이터를 설명하자면,    
* [EBM-NLP](https://aclanthology.org/P18-1019/)는 임상 시험 요약에서 PICO 범위에 주석을 달았다.     
* SCIERC(Luan et al., 2018)는 relations from computer science abstracts의 entities와  relations에 주석을 달았다.     
* ACL-ARC(Jurgens et al., 2018)와 SciCite(Cohan et al., 2019)는 다른 논문을 인용하는 과학 논문의 문장에  intent labels(예: Comparison, Extension 등)을 할당한다.     
* 논문 분야 dataset는 Microsoft Academic Graph(Sinha et al., 2015)에서 구축되었으며 논문 제목을 7개 연구 분야 중 하나에 매핑한다.     
각 연구 분야( geography, politics, economics, business, sociology, medicine, and psychology)에는 약 12K의 training example이 있습니다


---- 

## 3.3 Pretrained BERT Variants
### BERT-Base 
원래 BERT 코드와 함께 릴리스된 BERT-Base에 대해 사전 훈련된 가중치를 사용한다.4 어휘는 BASE VOCAB입니다. 우리는 이 모델의 사례 버전과 사례가 없는 버전을 모두 평가한다.

We use the pretrained weights for
BERT-Base (Devlin et al., 2019) released with the
original BERT code.4 The vocabulary is BASEVOCAB. We evaluate both cased and uncased versions of this model.


### SCIBERT 
우리는 BERT-Base와 동일한 구성과 크기로 말뭉치에서 SCIBERT를 훈련하기 위해 원래 BERT 코드를 사용한다. 우리는 (i) 사례 또는 사례가 없는 SCIBERT의 4가지 다른 버전, (ii) BASE VOCAB 또는 SCIVOCAB를 교육한다. BASE VOCAB을 사용하는 두 모델은 해당 BERT-Base 모델에서 미세 조정됩니다. 새로운 SCIVOCAB를 사용하는 다른 두 모델은 처음부터 훈련된다.

긴 문장에 대한 BERT 사전 훈련은 느릴 수 있다. 원래 BERT 코드에 따라 최대 문장 길이를 128개 토큰으로 설정하고 훈련 손실이 줄어들지 않을 때까지 모델을 훈련시킨다. 그런 다음 최대 512개의 토큰까지 문장 길이를 허용하는 모델을 계속 훈련한다.

우리는 8개의 코어가 있는 단일 TPU v3를 사용한다. 말뭉치에서 SCIVOCAB 모델을 처음부터 훈련하는 데는 1주 5일(최대 길이 128을 사용한 5일, 최대 길이 512를 사용한 2일)이 걸린다. BASE VOCAB 모델은 처음부터 훈련을 받지 않기 때문에 훈련 기간이 2일 더 적습니다.

모든 사전 훈련된 BERT 모델은 PyTorch 트랜스포머 라이브러리를 사용하여 PyTorch와 호환되도록 변환됩니다.6 우리의 모든 모델(섹션 3.4 및 3.5)은 AllenNLP를 사용하여 PyTorch에서 구현된다(Gardner et al., 2017).

We use the original BERT code to
train SCIBERT on our corpus with the same configuration and size as BERT-Base. We train 4
different versions of SCIBERT: (i) cased or uncased and (ii) BASEVOCAB or SCIVOCAB. The
two models that use BASEVOCAB are finetuned
from the corresponding BERT-Base models. The
other two models that use the new SCIVOCAB are
trained from scratch.

Pretraining BERT for long sentences can be
slow. Following the original BERT code, we set a
maximum sentence length of 128 tokens, and train
the model until the training loss stops decreasing.
We then continue training the model allowing sentence lengths up to 512 tokens.


We use a single TPU v3 with 8 cores. Training
the SCIVOCAB models from scratch on our corpus
takes 1 week5
(5 days with max length 128, then
2 days with max length 512). The BASEVOCAB
models take 2 fewer days of training because they
aren’t trained from scratch.


All pretrained BERT models are converted to
be compatible with PyTorch using the pytorchtransformers library.6 All our models (Sections 3.4 and 3.5) are implemented in PyTorch using AllenNLP (Gardner et al., 2017).


### Casing 
우리는 NER에 케이스 모델을 사용하고 다른 모든 작업에 케이스 모델을 사용하는 데 데브린 등(2019)을 따른다. 우리는 또한 파싱을 위해 케이스 모델을 사용한다. 일부 빛 실험은 케이스가 없는 모델이 케이스 모델보다 (때로는 NER에서도) 약간 더 나은 성능을 보인다는 것을 보여주었다.

We follow Devlin et al. (2019) in using
the cased models for NER and the uncased models
for all other tasks. We also use the cased models
for parsing. Some light experimentation showed
that the uncased models perform slightly better
(even sometimes on NER) than cased models.




---






















