---
title: "SCIBERT: A Pretrained Language Model for Scientific Text 정리"
date:   2023-02-20
excerpt: "SCIBERT: A Pretrained Language Model for Scientific Text"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---


# Abstract

<span style="background-color:#F5F5F5">**[문제]**</span>         
과학 영역에서 NLP 작업에 대한 large-scale annotated data를 얻는 것은 어렵고 비용이 많이 든다.     

<span style="background-color:#F5F5F5">**[해결: SCIBERT]**</span>         
* 우리는 high-quality의  large-scale labeled scientific data의 부족을 해결하기 위해,     
 [BERT](https://yerimoh.github.io/Lan2/)를 기반으로 pre-train된 SCIBERT를 만들었    
 * <span style="background-color:#FFE6E6">SCIBERT는 **scientific publications**의  **large multi-domain corpus에서 unsupervised pretraining 을 활용**하여 downstream scientific NLP tasks의 성능을 향상</span>시킨다.     
 * 이는 BERT에 비해 더 좋은 성능을 낸다.    
 * 코드 및 사전 교육된 모델은 [링크](https://github.com/allenai/scibert/)에서 확인 가능하다.     



----
----


# 1 Introduction
scientific publications의 양이 매우 많아, NLP는 이러한 문서의 대규모 지식 추출 및 기계 판독을 위한 필수 도구가 됨.   

<span style="background-color:#F5F5F5">**[문제]**</span>         
* NLP의 모델들을 훈련하려면 많은 양의 labeled data가 필요한 경우가 많다.     
➡ 이러한 데이터를 얻는 것은 어렵고 비용이 많이 듦     
* [ELMo](https://wikidocs.net/33930), [GPT](https://yerimoh.github.io/Lan4/) 및 [BERT](https://yerimoh.github.io/Lan2/) 통해 알 수 있듯이,  large corpora에서 언어 모델의 **unsupervised pretraining of language models은 많은 NLP 작업에서 성능을 크게 향상**시킨다.     
이러한 모델은 각 토큰에 대한 **contextualized embeddings을 반환**한다.(즉, 문맥 파악이 가능한 토큰을 준다.)     
➡ 즉, 이러한 <span style="background-color:#fff5b1">**unsupervised pretraining을 활용**하는 것은 과학적 NLP에서와 같이 **task-specific annotations을 얻기 어려운 경우에 특히 중요**</span>해졌다.      
* ⚠️ 하지만 BERT와 ELMo는 task-specific과 같은 domain이 아닌 **general domain corpora**(such as news articles and Wikipedia.)**로 훈련을 받은 모델**이다. (science와 같은 task-specific corpora로 훈련을 하지 않음)     


<span style="background-color:#F5F5F5">**[본 논문의 해결]**</span>         
이 작업에서 우리는 다음과 같은 기여를 한다:
* **(i)** 우리는 <span style="background-color:#FFE6E6">**scientific domain**에서 NLP tasks의 범위에서 성능을 향상</span>시키는 것으로 입증된 새로운 리소스인 SCIBERT를 출시한다.    
➡ SCIBERT는 BERT를 기반으로 한 pretrained language model이지만 **large corpus of scientific text**에 대해 훈련되었다.         
* **(ii)** 우리는 frozen embeddings 위의  <span style="background-color:#FFE6E6">**fine tuning VS task-specific architecture**의 성능과  in-domain vocabulary의 영향을 조사</span>하기 위해 광범위한 실험을 수행한다       
* **(iii)** 우리는 scientific domain의 일련의 작업에 대해 SCIBERT를 evaluate하고, SOTA 결과를 달성한다.        




---
----


# 2 Methods
## Background 
* [BERT](https://yerimoh.github.io/Lan2/)는 [multilayer bidirectional Transformer](https://yerimoh.github.io/Lan2/#1-pre-training-bert)를 기반으로 한다.      
* 기존의 왼쪽에서 오른쪽으로 언어 모델링 하는 대신,     
BERT는 [무작위로 마스킹된 토큰을 예측(Masked Language Model, MLM)](https://yerimoh.github.io/Lan2/#1-pre-training-bert)하고,    
[두 문장이 서로 따르는지 여부를 예측(Next sentence prediction, NSP)](https://yerimoh.github.io/Lan2/#1-pre-training-bert)하는 두 가지 작업에 대해 훈련된다.    

SCIBERT는 BERT와 **동일한 아키텍처**를 따르지만 대신 **scientific text에 대해 pretrained**을 받는다.     

---

## Vocabulary
BERT는 입력 텍스트의 unsupervised tokenization를 위해 [WordPiece](https://yerimoh.github.io/Lan2/#token-embeddings)를 사용한다.      
vocabulary는 가장 자주 사용되는 words or subword units를 포함하도록 구성된다.    

우리는 BERT와 함께 발표된 원래의 어휘를 **BASEVOCAB**라고 부른다.      


우리는 [SentencePiece 라이브러리](https://github.com/google/sentencepiece)를 사용하여 과학 말뭉치에 새로운 WordPiece 어휘인 SCIVOCAB를 구성한다. 우리는 대문자와 소문자가 없는 어휘를 모두 생성하고 BASE VOCAB의 크기에 맞게 어휘 크기를 30K로 설정한다. BASEVOCAB와 SCIVOCAB 사이의 결과 토큰 오버랩은 42%로 과학과 일반 도메인 텍스트 사이에서 자주 사용되는 단어의 상당한 차이를 보여준다

SentencePiece란?



BERT uses WordPiece (Wu et al.
,
2016) for unsupervised tokenization of the input
text. The vocabulary is built such that it contains
the most frequently used words or subword units.
We refer to the original vocabulary released with BERT as BASEVOCAB.'

We construct SCIVOCAB, a new WordPiece vocabulary on our scientific corpus using the SentencePiece1
library. We produce both cased and
uncased vocabularies and set the vocabulary size
to 30K to match the size of BASEVOCAB. The resulting token overlap between BASEVOCAB and
SCIVOCAB is 42%, illustrating a substantial difference in frequently used words between scientific and general domain texts

## Corpus 
우리는 시맨틱 스콜라(Ammar et al., 2018)의 1.14M 논문의 무작위 샘플에 대해 SCIBERT를 훈련한다. 이 말뭉치는 컴퓨터 과학 영역에서 18%의 논문과 광범위한 생물 의학 영역에서 82%의 논문으로 구성된다. 우리는 논문의 전문을 사용한다. 단지 요약만 사용하는 것이 아닙니다. 평균 용지 길이는 154개의 문장(2,769개의 토큰)으로 BERT가 훈련된 3.3B 토큰과 유사한 말뭉치 크기가 된다. 우리는 과학적 텍스트에 최적화된 ScispaCy(Neumann et al., 2019),2를 사용하여 문장을 분할한다.

We train SCIBERT on a random
sample of 1.14M papers from Semantic
Scholar (Ammar et al., 2018). This corpus
consists of 18% papers from the computer science
domain and 82% from the broad biomedical
domain. We use the full text of the papers, not
just the abstracts. The average paper length is
154 sentences (2,769 tokens) resulting in a corpus
size of 3.17B tokens, similar to the 3.3B tokens
on which BERT was trained. We split sentences
using ScispaCy (Neumann et al., 2019),2 which is
optimized for scientific text.









