---
title: "SCIBERT: A Pretrained Language Model for Scientific Text 정리"
date:   2023-02-20
excerpt: "SCIBERT: A Pretrained Language Model for Scientific Text"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---


# Abstract

<span style="background-color:#F5F5F5">**[문제]**</span>         
과학 영역에서 NLP 작업에 대한 large-scale annotated data를 얻는 것은 어렵고 비용이 많이 든다.     

<span style="background-color:#F5F5F5">**[해결: SCIBERT]**</span>         
* 우리는 high-quality의  large-scale labeled scientific data의 부족을 해결하기 위해,     
 [BERT](https://yerimoh.github.io/Lan2/)를 기반으로 pre-train된 SCIBERT를 만들었    
 * <span style="background-color:#FFE6E6">SCIBERT는 **scientific publications**의  **large multi-domain corpus에서 unsupervised pretraining 을 활용**하여 downstream scientific NLP tasks의 성능을 향상</span>시킨다.     
 * 이는 BERT에 비해 더 좋은 성능을 낸다.    
 * 코드 및 사전 교육된 모델은 [링크](https://github.com/allenai/scibert/)에서 확인 가능하다.     



----
----


# 1 Introduction
scientific publications의 양이 매우 많아, NLP는 이러한 문서의 대규모 지식 추출 및 기계 판독을 위한 필수 도구가 됨.   

<span style="background-color:#F5F5F5">**[문제]**</span>         
* NLP의 모델들을 훈련하려면 많은 양의 labeled data가 필요한 경우가 많다.     
➡ 이러한 데이터를 얻는 것은 어렵고 비용이 많이 듦     
* [ELMo](https://wikidocs.net/33930), [GPT](https://yerimoh.github.io/Lan4/) 및 [BERT](https://yerimoh.github.io/Lan2/) 통해 알 수 있듯이,  large corpora에서 언어 모델의 **unsupervised pretraining of language models은 많은 NLP 작업에서 성능을 크게 향상**시킨다.     
이러한 모델은 각 토큰에 대한 **contextualized embeddings을 반환**한다.(즉, 문맥 파악이 가능한 토큰을 준다.)     
➡ 즉, 이러한 <span style="background-color:#fff5b1">**unsupervised pretraining을 활용**하는 것은 과학적 NLP에서와 같이 **task-specific annotations을 얻기 어려운 경우에 특히 중요**</span>해졌다.      
* ⚠️ 하지만 BERT와 ELMo는 task-specific과 같은 domain이 아닌 **general domain corpora**(such as news articles and Wikipedia.)**로 훈련을 받은 모델**이다. (science와 같은 task-specific corpora로 훈련을 하지 않음)     


<span style="background-color:#F5F5F5">**[본 논문의 해결]**</span>         
이 작업에서 우리는 다음과 같은 기여를 한다:
* **(i)** 우리는 <span style="background-color:#FFE6E6">**scientific domain**에서 NLP tasks의 범위에서 성능을 향상</span>시키는 것으로 입증된 새로운 리소스인 SCIBERT를 출시한다.    
➡ SCIBERT는 BERT를 기반으로 한 pretrained language model이지만 **large corpus of scientific text**에 대해 훈련되었다.         
* **(ii)** 우리는 frozen embeddings 위의  <span style="background-color:#FFE6E6">**fine tuning VS task-specific architecture**의 성능과  in-domain vocabulary의 영향을 조사</span>하기 위해 광범위한 실험을 수행한다       
* **(iii)** 우리는 scientific domain의 일련의 작업에 대해 SCIBERT를 evaluate하고, SOTA 결과를 달성한다.        




---
----


# 2 Methods
## Background 
* [BERT](https://yerimoh.github.io/Lan2/)는 [multilayer bidirectional Transformer](https://yerimoh.github.io/Lan2/#1-pre-training-bert)를 기반으로 한다.      
* 기존의 왼쪽에서 오른쪽으로 언어 모델링 하는 대신,     
BERT는 [무작위로 마스킹된 토큰을 예측(Masked Language Model, MLM)](https://yerimoh.github.io/Lan2/#1-pre-training-bert)하고,    
[두 문장이 서로 따르는지 여부를 예측(Next sentence prediction, NSP)](https://yerimoh.github.io/Lan2/#1-pre-training-bert)하는 두 가지 작업에 대해 훈련된다.    

SCIBERT는 BERT와 **동일한 아키텍처**를 따르지만 대신 **scientific text에 대해 pretrained**을 받는다.     

---

## Vocabulary
BERT는 입력 텍스트의 unsupervised tokenization를 위해 [WordPiece](https://yerimoh.github.io/Lan2/#token-embeddings)를 사용한다.      
vocabulary는 가장 자주 사용되는 words or subword units를 포함하도록 구성된다.    

우리는 BERT와 함께 발표된 원래의 어휘를 **BASEVOCAB**라고 부른다.      


우리는 [SentencePiece 라이브러리](https://github.com/google/sentencepiece)를 사용하여,    
**scientific corpus에 새로운 WordPiece 어휘인 SCIVOCAB**를 구성한다.     
* 본 논문은 대문자와 소문자가 없는 어휘를 모두 생성하고 BASE VOCAB의 크기에 맞게 어휘 크기를 30K로 설정한다.   
* BASEVOCAB와 SCIVOCAB 사이의 결과 일반 VOCAB의 토큰과 SCIVOACB의 같은 토큰은 42%로 **scientific and general domain texts에서 자주 사용되는 단어의 상당한 차이**를 보여준다    


<details>
<summary>📜 SentencePiece란? </summary>
<div markdown="1">

논문 : https://arxiv.org/pdf/1808.06226.pdf
구글이 BPE 알고리즘과 Unigram Language Model Tokenizer를 구현한 것.   

SentencePiece는 **pre-tokenization을 필요로 하지 않는 tokenizer**의 하나로, 어떤 언어에도 자유롭게 적용될 수 있고 속도도 굉장히 빠르기 때문에 NLP에서 널리 사용되는 tokenizer이다. 

SetencePiece는 기존에 존재하던 unigram, BPE와 같은 tokenizer들을 모든 언어에 대해 적용이 가능하도록 generalize하고 약간의 추가적인 기능들을 더해서 구현한 것이다.    
특별히 새로운 알고리즘을 제시했다기보다는, 기존의 tokenizer들을 좀 더 사용하기 편하고 성능이 좋게 개선했다고 생각하면 되겠다. 
 
 
**[SentencePiece의 특징]**         
* **Pre-tokenization을 필요로 하지 않는다.**    
 기존의 tokenizer들은 영어를 기준으로 학습되었다면 영어가 띄어쓰기를 기반으로 해서 단어들이 대부분 분리되기 때문에, **띄어쓰기가 없는 일본어나 중국어와 같은 언어들에는 적용될 수 없다**는 한계점을 가지고 있었다.     
 하지만 SentencePiece는 **띄어쓰기를 다른 알파벳 혹은 글자처럼 하나의 character로 취급**한다.      
 ➡ 그렇기 때문에 모든 종류의 언어에 대해 general하게 적용이 가능하고, pre-tokenization이 필요하지 않다는 장점을 가진다.    
* **Character-coverage 설정이 가능하다.**     
 영어의 경우에는 알파벳의 개수가 30개 내외로 적기 때문에 괜챃지만, 중국어나 일본어와 같은 문자들의 경우에는 굉장히 많은 종류의 한자를 포함하고 있고, 이에 따라 **엄청나게 많은 개수의 character들을 다 다루어야 해서 tokenizer를 학습하는 데에 시간이 오래 걸린다**는 한계점을 가진다.     
하지만 SentencePiece는 모든 character를 다 고려하는 것이 아닌, **빈도에 따라 상위 99%의 character들만 고려**하는 등의 옵션을 주는 것이 가능하다.
* **Subword generalzation이 가능하다.**      
 우리가 학습한 tokenizer가 항상 완벽하다면 좋겠지만, 이것이 **overfitting되어 있을 가능성**을 배제할 수는 없다. 그렇기 때문에 SentencePiece에서는 우리가 학습한 tokenizer에서 나올 수 있는 token들만을 사용하는 것이 아니라, **가끔은 다른 token들도 샘플링**하면서 사용하여 overfitting하는 것이 가능하다.       
* **속도가 빠르다.**      
 직접 구현해보면 알겠지만, 알고리즘이 굉장히 최적화되어 있어서 같은 BPE와 unigram을 실행하더라도 굉장히 빠른 속도를 보여준다.

</div>
</details>  

---

## Corpus 
우리는 Semantic Scholar의 1.14M 논문의 무작위 샘플에 대해 SCIBERT를 훈련한다.      
이 corpus의 구성    
* 컴퓨터 과학 영역의 논문 18%           
* 광범위한 생물 의학 영역의 논문 82%       

우리는 논문의 전문을 사용한다. (요약본을 사용하지 않는다.)    

평균 논문 길이는 154개의 문장(2,769개의 토큰)으로, **BERT가 훈련된 3.3B 토큰과 유사한 corpus 크기**가 된다.      
우리는 과학적 텍스트에 최적화된 **[ScispaCy](https://github.com/allenai/SciSpaCy)**를 사용하여 문장을 분할한다.    




<details>
<summary>📜 Semantic Scholar란?  </summary>
<div markdown="1">


**적합한 논문**을 신속하게 검색하여 이용자들이 논문의 내용을 이해하는데 도움이 되고자 PDF파일로 간략한 발표 PPT형식으로 **요약한 내용을 열람할 수 있는 서비스를 제공**하고있는 것.  
 
 또한 특정한 연구분야를 연구하는 학자로서 최근 발표된 논문을 Semantic Scholar 논문 검색 엔진을 사용하게 되면 논문의 내용을 파악하는데 걸리는 시간을 단축할 수 있다


이 회사는 Allen Institute for Artificial Intelligence에서 설립한 비영리 기업이며 무료로 서비스를 제공하고 있다. **인공지능은 논문의 줄거리 내용을 파악하여 “Abstractive”기법을 통해 요약된 내용을 생성**하고 있다.
 
 
</div>
</details>  


---
---


# Experimental Setup
## 3.1 Tasks
우리는 아래의 core NLP tasks로 평가한다:        
**1.** Named Entity Recognition (NER)   
**2.** PICO Extraction (PICO)   
**3**. Text Classification (CLS)   
**4.** Relation Classification (REL)  
**5.** Dependency Parsing (DEP)   


PICO, like NER, is a sequence labeling task where
the model extracts spans describing the Participants, Interventions, Comparisons, and Outcomes
in a clinical trial paper (Kim et al., 2011). REL
is a special case of text classification where the
model predicts the type of relation expressed between two entities, which are encapsulated in the
sentence by inserted special tokens.


PICO는 NER와 마찬가지로 모델이 [Participants, Interventions, Comparisons, and Outcomes를 설명하는 범위를 추출](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-12-S2-S5)하는 sequence labeling task이다.    
그리고 REL은 모델이 두 엔티티 간에 표현되는 관계의 유형을 예측하는 텍스트 분류의 특별한 경우이며, 삽입된 특수 토큰에 의해 문장에 캡슐화(encapsulated)된다.   


<details>
<summary>📜 PICO란?  </summary>
<div markdown="1">
 
**PICO란?**        
과학 실험에서 아래 요인들을 뽑아내는 것    
PICO 프레임워크는 structuring clinical question 질문에 필요한 각 핵심 요소를 포착하기 때문에 임상 질문을 구성하는 데 가장 일반적으로 사용되는 모델입니다. PICO는 다음을 의미합니다.    
* Participants/Problem (P)   
* Intervention (I)   
* Comparison (C)   
* Outcome (O)   

**evidence-based medicine (EBM) 애플리케이션은 대규모 의학 문헌 데이터베이스를 분석하여 임상 질문에 답하는 데 의존**합니다.   

잘 정의되고 집중된 임상 질문을 공식화하기 위해 PICO라는 프레임워크가 널리 사용되며, 이는 참가자/문제(P), 개입(I), 비교(C) 및 결과(O)의 네 가지 구성 요소에 속하는 주어진 의학 텍스트의 문장을 식별합니다. 
 
아래 표는 질문의 유형에 따른 PICO이다.   

![image](https://user-images.githubusercontent.com/76824611/225283752-14153e36-7d6c-405f-8eeb-ca5347553eee.png)
![image](https://user-images.githubusercontent.com/76824611/225284104-08723df3-75bf-47b0-a336-133c3201b7f2.png)

 
 https://libguides.mssm.edu/ebm/ebp_pico
 
 
</div>
</details>  


<details>
<summary>📜 NER이란?  </summary>
<div markdown="1">
 
NER(Named Entity Recognition)은 말 그대로 Named Entity(이름을 가진 개체)를 Recognition(인식)하는 것을 의미하며, 개체명 인식이라고 합니다.
 
즉 개체마다 뭔지 태깅을 해주는 것다.

예를 들면 아래와 같다.   
![image](https://user-images.githubusercontent.com/76824611/225285580-0b1ac9d7-75d7-4412-b71a-fa9d60144b1c.png)

‍[출처](https://www.letr.ai/blog/tech-20210723)   
 
</div>
</details>  

<details>
<summary>📜 Automatic classification of sentences to support Evidence Based Medicine?  </summary>
<div markdown="1">
 
Automatic classification of sentences to support Evidence Based Medicine 란?

**Aim**     
Evidence Based Medicine에서 사용되는 미리 정의된 의료 범주 세트가 주어지면 **의학 초록의 문장에 이러한 레이블을 자동으로 추가하는 것**을 목표로 합니다. 
 
**방법**            
우리는 특정 **의료 범주(예: 개입 , 결과) 로 손으로 주석**을 단 1,000개의 의료 초록 모음을 구성했습니다 . 우리는 분류를 위해 CRF(Conditional Random Fields)를 사용하여 데이터의 어휘, 의미, 구조 및 순차 정보를 기반으로 다양한 기능의 사용을 조사했습니다.
 

**결과**    
모든 레이블에 대한 분류 작업의 경우, 우리 시스템은 sequential feature을 사용하여 구조화 및 구조화되지 않은 초록의 데이터 세트에 대해 각각 80.9% 및 66.9%의 micro-averaged f-scores를 달성했습니다. 핵심 문장에만 레이블을 지정하는 경우, 우리 시스템은 동일한 순차적 기능을 사용하여 구조화된 초록과 구조화되지 않은 초록에 대해 각각 89.3%와 74.0%의  f-scores를 생성했습니다. 외부 데이터 세트에 대한 결과는 더 낮았습니다(모든 레이블의 경우 f-점수 63.1%, 주요 문장의 경우 83.8%).

**결론**         
우리가 사용한 기능 중 abstract 에서 주어진 문장을 분류하는 데 가장 좋은 기능은 unigrams, section headings, and sequential information의 순차적 정보를 기반으로 했습니다. 이러한 기능으로 인해 simple bag-of-words 접근 방식에 비해 성능이 향상되었으며 이전 작업에서 사용된 feature sets다 성능이 뛰어났습니다.
 
</div>
</details>  





----




## 3.2 Datasets


**[Table 1]**     
![image](https://user-images.githubusercontent.com/76824611/225572942-78914c0c-d6e1-4437-a1cb-b4bd6a95cc3e.png)
* 모든 tasks 및  datasets에서 모든 BERT 변형의 성능을 테스트한다.        
* 굵은 글씨는 SOTA 결과를 나타낸다. (95% bootstrap 신뢰 구간 내에서 차이가 있을 경우 여러 개의 결과가 굵은 글씨로 표시됨).          
* 과거 작업과 함께, 우리는 아래와 같은 요소를 보고한다.        
    * NER (span-level)에 대한 macro F1 scores   
    * REL 및 CLS(문장 레벨, 문장의 시작 토큰)에 대한 macro F1 scores    
    * PICO(토큰 레벨)에 대한 macro F1 scores     
    * ChemPro에 대한 micro F1 scores       
    * DEP의 경우, 우리는 LAS에 맞게 조정된 hyperparameters를 사용하여 동일한 모델에 대한 레이블링된(LAS) 및 레이블링되지 않은(UAS) attachment scores를 보고한다.     
* 모든 결과는 서로 다른 random seeds를 사용한 여러 runs의 평균이다.         




<details>
<summary>📜 F1 scores  </summary>
<div markdown="1">
 
 
![image](https://user-images.githubusercontent.com/76824611/225508975-89b576ed-3457-433c-a7c6-7e91167e9e1f.png)

![image](https://user-images.githubusercontent.com/76824611/225509055-2a261c2e-bfb8-4fcf-83d1-04bd79b6a4ee.png)

각 범주별 Precision, Recall, F1 Score
![image](https://user-images.githubusercontent.com/76824611/225509111-83dd797e-039c-460c-b734-84374d44ada6.png)

Macro Average F1 Score
![image](https://user-images.githubusercontent.com/76824611/225510862-71947c9c-2e64-451d-b627-ae50d17d9e51.png)

Micro Average F1 Score = accuracy = micro-precision = micro-recall
![image](https://user-images.githubusercontent.com/76824611/225510902-105ad68e-a594-424c-b7a4-2f6c4d86fe2e.png)

</div>
</details>  




위 표 외의 데이터를 설명하자면,    
* [EBM-NLP](https://aclanthology.org/P18-1019/)는 임상 시험 요약에서 PICO 범위에 주석을 달았다.     
* [SCIERC](https://aclanthology.org/D18-1360.pdf)는 relations from computer science abstracts의 entities와  relations에 주석을 달았다.     
* [ACL-ARC](https://aclanthology.org/Q18-1028.pdf)와 [SciCite](https://aclanthology.org/N19-1361.pdf)는 다른 논문을 인용하는 과학 논문의 문장에  intent labels(예: Comparison, Extension 등)을 할당한다.     
* 논문 분야 dataset는 [Microsoft Academic Graph](https://www.microsoft.com/en-us/research/publication/overview-microsoft-academic-service-mas-applications/)에서 구축되었으며 논문 제목을 7개 연구 분야 중 하나에 매핑한다.     
각 연구 분야( geography, politics, economics, business, sociology, medicine, and psychology)에는 약 12K의 training example이 있다.    


---- 


## 3.3 Pretrained BERT Variants
### BERT-Base 
원래 [BERT 코드](https://github.com/google-research/bert)와 함께 릴리스된 [BERT-Base](https://yerimoh.github.io/Lan2/)에 대해 pretrained weights를 사용한다.      

어휘는 BASE VOCAB이다.   

우리는 이 모델의 cased 버전과 uncased 버전을 모두 평가한다.




### SCIBERT 
우리는 **BERT-Base와 동일한 구성과 크기로 말뭉치에서 SCIBERT를 훈련**하기 위해 원래 BERT 코드를 사용한다.    

우리는 아래 4가지 different SCIBERT case에 대해 train한다.(i와 ii에서 하나씩 조합)      
**(i)** cased or uncased      
**(ii)** BASEVOCAB or SCIVOCAB         

**BASEVOCAB**을 사용하는 두 모델은 해당 **BERT-Base 모델에서 finetuning**된다.       
**SCIVOCAB**를 사용하는 두 모델은 **처음(scratch)부터 train**된다.      

<span style="background-color:#F5F5F5">**[train 방법]**</span>              
* **1)** original BERT 코드에 따라 최대 문장 길이를 128개 토큰으로 설정함     
long sentences에 대한 BERT Pretraining은 느릴 수 있기 때문      
* **2)** training loss가 줄어들지 않을 때까지 모델을 train시킨다.  
* **3)** 그런 다음 최대 512개의 토큰까지 문장 길이를 허용하는 모델을 계속 훈련한다.   

<span style="background-color:#F5F5F5">**[train time]**</span>              
* 우리는 8개의 코어가 있는 single TPU v3를 사용한다.    
* corpus에서 SCIVOCAB 모델을 처음부터 훈련하는 데는 1주일 하고 5일이 걸렸다.   
 최대 길이 128을 사용하면 5일, 최대 길이 512를 사용하면 2일이 걸린다.      
* BASE VOCAB 모델은 처음부터 훈련을 받지 않기 때문에 훈련 기간이 2일 더 적다.    

모든 사전 훈련된 BERT 모델은 [PyTorch transformer 라이브러리](https://github.com/huggingface/transformers)를 사용하여 PyTorch와 호환되도록 변환된다.    

우리의 모든 모델은 AllenNLP를 사용하여 PyTorch에서 구현된다


### Casing 
우리는 NER에 cased models을 사용하고,    
다른 모든 작업에 cased models을 사용하는 데 [BERT](https://yerimoh.github.io/Lan2/)를 따른다.          
우리는 또한 parsing을 위해 cased models을 사용한다.    

일부 light experimentation은 uncased model이 cased model보다 (때로는 NER에서도) 약간 더 나은 성능을 보인다는 것을 보여주었다.    




<details>
<summary>📜 uncased vs cased </summary>
<div markdown="1">

BERT uncased와 BERT cased는 WordPiece 토큰화 단계에서 텍스트의 **대소문자 사용** 여부, **악센트 마커의 존재 여부**에서 다릅니다.   
 
```
# BERT uncased
OpenGenus -> opengenus
OpènGènus -> opengenus 

# BERT cased
OpenGenus
OpènGènus
```

</div>
</details>   
 



---



## 3.4 Finetuning BERT

[base BERT와 같은 점]
* 우리는 오리지널 BERT에서 사용된 same architecture, optimization, and hyperparameter을 따른다.   
* **텍스트 분류**(즉, CLS및 REL)의 경우 ```[CLS] 토큰```에 대한 최종 BERT 벡터를 **linear classification layer**에 공급한다.    
* **sequence labeling**(즉, NER 및 PICO)의 경우 각 토큰에 대한 최종 BERT 벡터를 **softmax output이 있는  linear classification layer**으로 공급한다.        

[base BERT와 다른 점]
* additional conditional random field를 사용하는 것에 약간 차이가 있는데, 이는 well-formed entities를 보장함으로써 평가를 더 쉽게 만들었다.     
* DEP의 경우, stacked BiLSTMs 대신, BERT size 100의  dependency tag, arc embeddings과 BERT 벡터에 대한 이진 행렬(biaffine matrix) attention를 가진 [모델](https://arxiv.org/abs/1611.01734)을 사용한다.




---


## 3.5 Frozen BERT Embeddings
또한 동결된 BERT embeddings 위에서 간단한 task별 모델을 교육하여 ELMo와 같은 pretrained contextualized word embeddings으로 BERT의 사용을 탐구한다      

* text classification를 위해, BERT 벡터의 각 문장을 크기 200의 2층 BiLSTM에 공급한 후,    
연결된 첫 번째와 마지막 BiLSTM 벡터에 a multilayer perceptron (with hidden size 200)을 적용     
* sequence labeling의 경우 동일한 BiLSTM 레이어를 사용하고  conditional random field를 사용하여 well-formed predictions을 보장한다.      
* DEP의 경우, 우리는 크기가 100인 tag and arc embeddings과 다른 작업과 동일한 BiLSTM 설정을 모델을 사용한다.      
* BiLSTM의 깊이나 크기 변경은 결과에 큰 영향을 미치지는 않았다(Reimers and Gurevych, 2017).             
* Adam을 사용하여 cross entropy loss을 최적화하지만 BERT 가중치를 동결하고 0.5의 dropout을 적용한다.     
* 우리는 배치 크기 32와 학습률 0.001을 사용하여 development se(10명의 환자)에서 early stopping를 사용하여 train한다.    
* 우리는 광범위한 hyperparameter search을 수행하지 않았지만,   
 최적의 하이퍼 파라미터는 작업에 따라 달라질 것이다.     



----
----


# 4 Results
Table 1 summarizes the experimental results. We
observe that SCIBERT outperforms BERT-Base
on scientific tasks (+2.11 F1 with finetuning and
+2.43 F1 without)8
. We also achieve new SOTA
results on many of these tasks using SCIBERT.



표 1은 실험 결과를 요약한 것이다. 우리는 SCIBERT가 과학적 작업에서 BERT-Base를 능가한다는 것을 관찰한다. (+2.11 F1은 미세 조정을, +2.43 F1은 +2.43 F1은 +). 우리는 또한 SCIBERT를 사용하여 이러한 작업 중 많은 작업에서 새로운 SOTA 결과를 얻는다.


---

## 4.1 Biomedical Domain

우리는 SCIBERT가 생물 의학 작업에서 BERTBase(+1.92F1)를 능가한다는 것을 관찰한다. 또한, SCIBERT는 BC5CDR과 ChemProt(Lee et al., 2019) 및 EBMNLP(Nye et al., 2018)에서 새로운 SOTA 결과를 달성한다.

SCIBERT는 3개의 데이터 세트에서 SOTA보다 성능이 약간 떨어진다. JNLPBA에 대한 SOTA 모델은 JNLPBA뿐만 아니라 여러 NER 데이터 세트에 대해 훈련된 BiLSTM-CRF 앙상블이다(Yoon et al., 2018). NCBI 질병의 SOTA 모델은 바이오버트(Lee et al., 2019)로, 생물의학 논문의 18B 토큰에 미세 조정된 BERTBase이다. GENIA에 대한 SOTA 결과는 Nguyen과 Verspoor(2019)에 있으며, 이는 우리가 사용하지 않는 음성 부분(POS) 기능과 함께 Dozat과 Manning(2017)의 모델을 사용한다.

표 2에서, 우리는 SCIBERT 결과를 에 포함된 데이터 세트의 하위 집합에 대한 보고된 BIOBERT 결과와 비교한다(Lee et al., 2019). 흥미롭게도, SCIBERT는 BC5CDR 및 ChemProt에서 BIBERT 결과를 능가하며, 상당히 작은 생물 의학 말뭉치에서 훈련되었음에도 불구하고 JNLPBA에서 유사한 성능을 발휘한다.


We observe that SCIBERT outperforms BERTBase on biomedical tasks (+1.92 F1 with finetuning and +3.59 F1 without). In addition, SCIBERT achieves new SOTA results on BC5CDR
and ChemProt (Lee et al., 2019), and EBMNLP (Nye et al., 2018).

SCIBERT performs slightly worse than SOTA
on 3 datasets. The SOTA model for JNLPBA
is a BiLSTM-CRF ensemble trained on multiple NER datasets not just JNLPBA (Yoon et al.,
2018). The SOTA model for NCBI-disease
is BIOBERT (Lee et al., 2019), which is BERTBase finetuned on 18B tokens from biomedical papers. The SOTA result for GENIA is
in Nguyen and Verspoor (2019) which uses the
model from Dozat and Manning (2017) with partof-speech (POS) features, which we do not use.



In Table 2, we compare SCIBERT results
with reported BIOBERT results on the subset of
datasets included in (Lee et al., 2019). Interesting, SCIBERT outperforms BIOBERT results on
BC5CDR and ChemProt, and performs similarly
on JNLPBA despite being trained on a substantially smaller biomedical corpus.




![image](https://user-images.githubusercontent.com/76824611/225625651-79fc0bfa-08f5-4b3f-afe0-9016e1444b63.png)

Table 2: Comparing SCIBERT with the reported
BIOBERT results on biomedical datasets.
표 2: SCIBERT와 생물의학 데이터 세트에 대한 보고된 BIOBERT 결과를 비교한다.


----

## 4.2 Computer Science Domain
우리는 SCIBERT가 컴퓨터 과학 작업에서 BERTBase(+미세 조정이 있는 경우 +3.55F1, 없는 경우 +1.13 F1)를 능가한다는 것을 관찰한다. 또한 SCIBERT는 ACLARC(Cohan et al., 2019)와 SCIERC의 NER 부분에서 새로운 SOTA 결과를 달성한다(Luan et al., 2018). SIERC의 관계의 경우, 우리의 결과는 루안 외 연구진(2018)의 결과와 비교할 수 없다. 왜냐하면 우리는 주어진 금 실체가 있는 관계 분류를 수행하는 반면 그들은 공동 실체와 관계 추출을 수행하기 때문이다
We observe that SCIBERT outperforms BERTBase on computer science tasks (+3.55 F1 with
finetuning and +1.13 F1 without). In addition,
SCIBERT achieves new SOTA results on ACLARC (Cohan et al., 2019), and the NER part of
SciERC (Luan et al., 2018). For relations in SciERC, our results are not comparable with those in
Luan et al. (2018) because we are performing relation classification given gold entities, while they
perform joint entity and relation extraction


---


## 4.3 Multiple Domains
우리는 SCIBERT가 다중 도메인 작업에서 BERTBase(+0.49F1)를 능가한다는 것을 관찰한다. 또한, SCIBERT는 Sci Cite에서 SOTA를 능가한다(Cohan et al., 2019). 종이 필드 데이터 세트에 대해 이전에 게시된 SOTA 결과가 없습니다.

We observe that SCIBERT outperforms BERTBase on the multidomain tasks (+0.49 F1 with
finetuning and +0.93 F1 without). In addition, SCIBERT outperforms the SOTA on Sci
Cite (Cohan et al., 2019). No prior published
SOTA results exist for the Paper Field dataset.



---
----




#  5 Discussion

## 5.1 Effect of Finetuning
We observe improved results via BERT finetuning
rather than task-specific architectures atop frozen
embeddings (+3.25 F1 with SCIBERT and +3.58
with BERT-Base, on average). For each scientific
domain, we observe the largest effects of finetuning on the computer science (+5.59 F1 with SCIBERT and +3.17 F1 with BERT-Base) and biomedical tasks (+2.94 F1 with SCIBERT and +4.61 F1
with BERT-Base), and the smallest effect on multidomain tasks (+0.7 F1 with SCIBERT and +1.14
F1 with BERT-Base). On every dataset except
BC5CDR and SciCite, BERT-Base with finetuning
outperforms (or performs similarly to) a model using frozen SCIBERT embedding

---

## 5.2 Effect of SCIVOCAB
We assess the importance of an in-domain scientific vocabulary by repeating the finetuning experiments for SCIBERT with BASEVOCAB. We
find the optimal hyperparameters for SCIBERTBASEVOCAB often coincide with those of SCIBERT-SCIVOCAB.

Averaged across datasets, we observe +0.60 F1
when using SCIVOCAB. For each scientific do-
main, we observe +0.76 F1 for biomedical tasks,
+0.61 F1 for computer science tasks, and +0.11 F1
for multidomain tasks.

Given the disjoint vocabularies (Section 2) and
the magnitude of improvement over BERT-Base
(Section 4), we suspect that while an in-domain
vocabulary is helpful, SCIBERT benefits most
from the scientific corpus pretraining.


---
----

# Related Work
Recent work on domain adaptation of BERT includes BIOBERT (Lee et al., 2019) and CLINICALBERT (Alsentzer et al., 2019; Huang et al.,
2019). BIOBERT is trained on PubMed abstracts and PMC full text articles, and CLINICALBERT is trained on clinical text from the
MIMIC-III database (Johnson et al., 2016). In
contrast, SCIBERT is trained on the full text of
1.14M biomedical and computer science papers
from the Semantic Scholar corpus (Ammar et al.,
2018). Furthermore, SCIBERT uses an in-domain
vocabulary (SCIVOCAB) while the other abovementioned models use the original BERT vocabulary (BASEVOCAB).




---
--


# 7 Conclusion and Future Work
We released SCIBERT, a pretrained language
model for scientific text based on BERT. We evaluated SCIBERT on a suite of tasks and datasets from
scientific domains. SCIBERT significantly outperformed BERT-Base and achieves new SOTA results on several of these tasks, even compared to
some reported BIOBERT (Lee et al., 2019) results
on biomedical tasks.

For future work, we will release a version of
SCIBERT analogous to BERT-Large, as well as experiment with different proportions of papers from
each domain. Because these language models are
costly to train, we aim to build a single resource
that’s useful across multiple domains.




---
---

# Acknowledgment
We thank the anonymous reviewers for their comments and suggestions. We also thank Waleed
Ammar, Noah Smith, Yoav Goldberg, Daniel
King, Doug Downey, and Dan Weld for their helpful discussions and feedback. All experiments
were performed on beaker.org and supported
in part by credits from Google Cloud








