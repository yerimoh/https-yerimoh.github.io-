---
title: "SCIBERT: A Pretrained Language Model for Scientific Text 정리"
date:   2023-02-20
excerpt: "SCIBERT: A Pretrained Language Model for Scientific Text"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---


# Abstract

<span style="background-color:#F5F5F5">**[문제]**</span>         
과학 영역에서 NLP 작업에 대한 large-scale annotated data를 얻는 것은 어렵고 비용이 많이 든다.     

<span style="background-color:#F5F5F5">**[해결: SCIBERT]**</span>         
* 우리는 high-quality의  large-scale labeled scientific data의 부족을 해결하기 위해,     
 [BERT](https://yerimoh.github.io/Lan2/)를 기반으로 pre-train된 SCIBERT를 만들었    
 * <span style="background-color:#FFE6E6">SCIBERT는 **scientific publications**의  **large multi-domain corpus에서 unsupervised pretraining 을 활용**하여 downstream scientific NLP tasks의 성능을 향상</span>시킨다.     
 * 이는 BERT에 비해 더 좋은 성능을 낸다.    
 * 코드 및 사전 교육된 모델은 [링크](https://github.com/allenai/scibert/)에서 확인 가능하다.     



----
----


# 1 Introduction
scientific publications의 양이 매우 많아, NLP는 이러한 문서의 대규모 지식 추출 및 기계 판독을 위한 필수 도구가 됨.   

<span style="background-color:#F5F5F5">**[문제]**</span>         
* NLP의 모델들을 훈련하려면 많은 양의 labeled data가 필요한 경우가 많다.     
➡ 이러한 데이터를 얻는 것은 어렵고 비용이 많이 듦     
* [ELMo](https://wikidocs.net/33930), [GPT](https://yerimoh.github.io/Lan4/) 및 [BERT](https://yerimoh.github.io/Lan2/) 통해 알 수 있듯이,  large corpora에서 언어 모델의 **unsupervised pretraining of language models은 많은 NLP 작업에서 성능을 크게 향상**시킨다.     
이러한 모델은 각 토큰에 대한 **contextualized embeddings을 반환**한다.(즉, 문맥 파악이 가능한 토큰을 준다.)     
➡ 즉, 이러한 <span style="background-color:#fff5b1">**unsupervised pretraining을 활용**하는 것은 과학적 NLP에서와 같이 **task-specific annotations을 얻기 어려운 경우에 특히 중요**</span>해졌다.      
* ⚠️ 하지만 BERT와 ELMo는 task-specific과 같은 domain이 아닌 **general domain corpora**(such as news articles and Wikipedia.)**로 훈련을 받은 모델**이다. (science와 같은 task-specific corpora로 훈련을 하지 않음)     


<span style="background-color:#F5F5F5">**[본 논문의 해결]**</span>         
이 작업에서 우리는 다음과 같은 기여를 한다:
* **(i)** 우리는 <span style="background-color:#FFE6E6">**scientific domain**에서 NLP tasks의 범위에서 성능을 향상</span>시키는 것으로 입증된 새로운 리소스인 SCIBERT를 출시한다.    
➡ SCIBERT는 BERT를 기반으로 한 pretrained language model이지만 **large corpus of scientific text**에 대해 훈련되었다.         
* **(ii)** 우리는 frozen embeddings 위의  <span style="background-color:#FFE6E6">**fine tuning VS task-specific architecture**의 성능과  in-domain vocabulary의 영향을 조사</span>하기 위해 광범위한 실험을 수행한다       
* **(iii)** 우리는 scientific domain의 일련의 작업에 대해 SCIBERT를 evaluate하고, SOTA 결과를 달성한다.        




---
----


# 2 Methods
## Background 
* [BERT](https://yerimoh.github.io/Lan2/)는 [multilayer bidirectional Transformer](https://yerimoh.github.io/Lan2/#1-pre-training-bert)를 기반으로 한다.      
* 기존의 왼쪽에서 오른쪽으로 언어 모델링 하는 대신,     
BERT는 [무작위로 마스킹된 토큰을 예측(Masked Language Model, MLM)](https://yerimoh.github.io/Lan2/#1-pre-training-bert)하고,    
[두 문장이 서로 따르는지 여부를 예측(Next sentence prediction, NSP)](https://yerimoh.github.io/Lan2/#1-pre-training-bert)하는 두 가지 작업에 대해 훈련된다.    

SCIBERT는 BERT와 **동일한 아키텍처**를 따르지만 대신 **scientific text에 대해 pretrained**을 받는다.     

---

## Vocabulary
BERT는 입력 텍스트의 unsupervised tokenization를 위해 [WordPiece](https://yerimoh.github.io/Lan2/#token-embeddings)를 사용한다.      
vocabulary는 가장 자주 사용되는 words or subword units를 포함하도록 구성된다.    

우리는 BERT와 함께 발표된 원래의 어휘를 **BASEVOCAB**라고 부른다.      


우리는 [SentencePiece 라이브러리](https://github.com/google/sentencepiece)를 사용하여,    
**scientific corpus에 새로운 WordPiece 어휘인 SCIVOCAB**를 구성한다.     
* 본 논문은 대문자와 소문자가 없는 어휘를 모두 생성하고 BASE VOCAB의 크기에 맞게 어휘 크기를 30K로 설정한다.   
* BASEVOCAB와 SCIVOCAB 사이의 결과 일반 VOCAB의 토큰과 SCIVOACB의 같은 토큰은 42%로 **scientific and general domain texts에서 자주 사용되는 단어의 상당한 차이**를 보여준다    


<details>
<summary>📜 SentencePiece란? </summary>
<div markdown="1">

논문 : https://arxiv.org/pdf/1808.06226.pdf
구글이 BPE 알고리즘과 Unigram Language Model Tokenizer를 구현한 것.   

SentencePiece는 **pre-tokenization을 필요로 하지 않는 tokenizer**의 하나로, 어떤 언어에도 자유롭게 적용될 수 있고 속도도 굉장히 빠르기 때문에 NLP에서 널리 사용되는 tokenizer이다. 

SetencePiece는 기존에 존재하던 unigram, BPE와 같은 tokenizer들을 모든 언어에 대해 적용이 가능하도록 generalize하고 약간의 추가적인 기능들을 더해서 구현한 것이다.    
특별히 새로운 알고리즘을 제시했다기보다는, 기존의 tokenizer들을 좀 더 사용하기 편하고 성능이 좋게 개선했다고 생각하면 되겠다. 
 
 
**[SentencePiece의 특징]**         
* **Pre-tokenization을 필요로 하지 않는다.**    
 기존의 tokenizer들은 영어를 기준으로 학습되었다면 영어가 띄어쓰기를 기반으로 해서 단어들이 대부분 분리되기 때문에, **띄어쓰기가 없는 일본어나 중국어와 같은 언어들에는 적용될 수 없다**는 한계점을 가지고 있었다.     
 하지만 SentencePiece는 **띄어쓰기를 다른 알파벳 혹은 글자처럼 하나의 character로 취급**한다.      
 ➡ 그렇기 때문에 모든 종류의 언어에 대해 general하게 적용이 가능하고, pre-tokenization이 필요하지 않다는 장점을 가진다.    
* **Character-coverage 설정이 가능하다.**     
 영어의 경우에는 알파벳의 개수가 30개 내외로 적기 때문에 괜챃지만, 중국어나 일본어와 같은 문자들의 경우에는 굉장히 많은 종류의 한자를 포함하고 있고, 이에 따라 **엄청나게 많은 개수의 character들을 다 다루어야 해서 tokenizer를 학습하는 데에 시간이 오래 걸린다**는 한계점을 가진다.     
하지만 SentencePiece는 모든 character를 다 고려하는 것이 아닌, **빈도에 따라 상위 99%의 character들만 고려**하는 등의 옵션을 주는 것이 가능하다.
* **Subword generalzation이 가능하다.**      
 우리가 학습한 tokenizer가 항상 완벽하다면 좋겠지만, 이것이 **overfitting되어 있을 가능성**을 배제할 수는 없다. 그렇기 때문에 SentencePiece에서는 우리가 학습한 tokenizer에서 나올 수 있는 token들만을 사용하는 것이 아니라, **가끔은 다른 token들도 샘플링**하면서 사용하여 overfitting하는 것이 가능하다.       
* **속도가 빠르다.**      
 직접 구현해보면 알겠지만, 알고리즘이 굉장히 최적화되어 있어서 같은 BPE와 unigram을 실행하더라도 굉장히 빠른 속도를 보여준다.

</div>
</details>  

---

## Corpus 
우리는 Semantic Scholar의 1.14M 논문의 무작위 샘플에 대해 SCIBERT를 훈련한다.      
이 corpus의 구성    
* 컴퓨터 과학 영역의 논문 18%           
* 광범위한 생물 의학 영역의 논문 82%       

우리는 논문의 전문을 사용한다. (요약본을 사용하지 않는다.)    

평균 논문 길이는 154개의 문장(2,769개의 토큰)으로, **BERT가 훈련된 3.3B 토큰과 유사한 corpus 크기**가 된다.      
우리는 과학적 텍스트에 최적화된 **[ScispaCy](https://github.com/allenai/SciSpaCy)**를 사용하여 문장을 분할한다.    




<details>
<summary>📜 Semantic Scholar란?  </summary>
<div markdown="1">


**적합한 논문**을 신속하게 검색하여 이용자들이 논문의 내용을 이해하는데 도움이 되고자 PDF파일로 간략한 발표 PPT형식으로 **요약한 내용을 열람할 수 있는 서비스를 제공**하고있는 것.  
 
 또한 특정한 연구분야를 연구하는 학자로서 최근 발표된 논문을 Semantic Scholar 논문 검색 엔진을 사용하게 되면 논문의 내용을 파악하는데 걸리는 시간을 단축할 수 있다


이 회사는 Allen Institute for Artificial Intelligence에서 설립한 비영리 기업이며 무료로 서비스를 제공하고 있다. **인공지능은 논문의 줄거리 내용을 파악하여 “Abstractive”기법을 통해 요약된 내용을 생성**하고 있다.
 
 
</div>
</details>  


---
---


# Experimental Setup
## 3.1 Tasks
우리는 아래의 core NLP tasks로 평가한다:        
**1.** Named Entity Recognition (NER)   
**2.** PICO Extraction (PICO)   
**3**. Text Classification (CLS)   
**4.** Relation Classification (REL)  
**5.** Dependency Parsing (DEP)   


PICO, like NER, is a sequence labeling task where
the model extracts spans describing the Participants, Interventions, Comparisons, and Outcomes
in a clinical trial paper (Kim et al., 2011). REL
is a special case of text classification where the
model predicts the type of relation expressed between two entities, which are encapsulated in the
sentence by inserted special tokens.


----




## 3.2 Datasets
For brevity, we only describe the newer datasets
here, and refer the reader to the references in Table 1 for the older datasets. EBM-NLP (Nye et al.,
2018) annotates PICO spans in clinical trial abstracts. SciERC (Luan et al., 2018) annotates entities and relations from computer science ab
stracts. ACL-ARC (Jurgens et al., 2018) and SciCite (Cohan et al., 2019) assign intent labels (e.g.
Comparison, Extension, etc.) to sentences from
scientific papers that cite other papers. The Paper
Field dataset is built from the Microsoft Academic
Graph (Sinha et al., 2015)
3
and maps paper titles
to one of 7 fields of study. Each field of study
(i.e. geography, politics, economics, business, sociology, medicine, and psychology) has approximately 12K training examples



---- 

## 3.3 Pretrained BERT Variants
### BERT-Base 
We use the pretrained weights for
BERT-Base (Devlin et al., 2019) released with the
original BERT code.4 The vocabulary is BASEVOCAB. We evaluate both cased and uncased versions of this model.


### SCIBERT 
We use the original BERT code to
train SCIBERT on our corpus with the same configuration and size as BERT-Base. We train 4
different versions of SCIBERT: (i) cased or uncased and (ii) BASEVOCAB or SCIVOCAB. The
two models that use BASEVOCAB are finetuned
from the corresponding BERT-Base models. The
other two models that use the new SCIVOCAB are
trained from scratch.

Pretraining BERT for long sentences can be
slow. Following the original BERT code, we set a
maximum sentence length of 128 tokens, and train
the model until the training loss stops decreasing.
We then continue training the model allowing sentence lengths up to 512 tokens.


We use a single TPU v3 with 8 cores. Training
the SCIVOCAB models from scratch on our corpus
takes 1 week5
(5 days with max length 128, then
2 days with max length 512). The BASEVOCAB
models take 2 fewer days of training because they
aren’t trained from scratch.


All pretrained BERT models are converted to
be compatible with PyTorch using the pytorchtransformers library.6 All our models (Sections 3.4 and 3.5) are implemented in PyTorch using AllenNLP (Gardner et al., 2017).


### Casing 
We follow Devlin et al. (2019) in using
the cased models for NER and the uncased models
for all other tasks. We also use the cased models
for parsing. Some light experimentation showed
that the uncased models perform slightly better
(even sometimes on NER) than cased models.




---






















