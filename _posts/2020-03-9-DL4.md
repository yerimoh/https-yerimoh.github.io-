---
title: "[04] Deep learning 1 (밑바닥 부터 시작하는 딥러닝 1) "
date:   2020-03-9
excerpt: "데이터 주도 학습,손실 함수(loss function),오차 제곱 합(sum of squares for error, SSE),교차 엔트로피 오차(cross entropy error, CEE), 미니배치, 수치 미분, 학습 알고리즘 구현하기"
category: [Deep Learning]
layout: post
tag:
- Deep Learning
order: 0

comments: true
---


# 오차역전법(backpropagation)(=‘역전파법’,‘역전파’)

**[기존 미분의 문제점]**    
**[수치미분](https://yerimoh.github.io//DL3/#%EC%88%98%EC%B9%98-%EB%AF%B8%EB%B6%84)**은 단순하고 구현하기도 쉽지만 **계산 시간이 오래 걸린**다는 게 단점    
오차역전법은 이를 보완하는 방법(가중치 매개변수의 기울기를 효율적으로 계산)        


# 계산그래프 computational graph      
계산 과정을 그래프(수의 **노드 node** 와 **에지 edge**<노드 사이의 직선> 로 표현)로 나타낸 것     
**국소적 계산에 집중**     
전체 계산이 복잡하더라도 각 단계에서 하는 일은 해당 노드의 ‘국소적 계산’.        
국소적인 계산은 단순하지만, 그 결과를 전달함으로서 전체를 구성하는 복잡한 계산을 해낼 수 있음       


## 계산그래프로 풀기
![image](https://user-images.githubusercontent.com/76824611/119589259-67c8d780-be0d-11eb-9be3-26eee73882fc.png)
   
계산 결과를 위에(에지)에 적음    
![image](https://user-images.githubusercontent.com/76824611/119589265-6bf4f500-be0d-11eb-800e-5e4f4dae4940.png)
 
'×'만을 연산으로 생각   

**[계산그래프의 문제풀이 흐름]**     
**1.** 계산 그래프를 구성      
**2.** 순전파(forward propagation)     
* 그래프에서 계산을 **왼쪽**에서 **오른쪽**으로 진행   
* 계산 그래프의 출발점부터 종착점으로의 전파          
* cf) **역전파 backward propagation**: 반대 방향(그림에서 말하면 오른쪽에서 왼쪽)의 전파-> **미분**을 계산할 때 중요한 역할    

## 국소적 계산
**국소적**: ‘자신과 직접 관계된 작은 범위’    
**국소적 계산**: 전체에서 어떤 일이 벌어지든 상관X       
* 자신과 관계된 정보만으로 결과를 출력할 수 있다는 것      
![image](https://user-images.githubusercontent.com/76824611/119589831-998e6e00-be0e-11eb-856f-b6ad04c2763a.png)
각 노드에서의 계산은 국소적 계산     
각 노드는 **자신만 신경 씀**      

**[계산그래프 사용 이유]**    
전체가 아무리 복잡해도 각 노드에서는 단순한 계산에 집중하여 문제 **단순화** 가능    
**중간 계산 결과**를 모두 보관가능   
역전파를 통해 **‘미분’의 효율적 계산**   




## 연쇄법칙 chain rule

[정의](https://ko.wikipedia.org/wiki/%EC%97%B0%EC%87%84_%EB%B2%95%EC%B9%99): 합성함수의 미분법       

**‘국소적 미분’을 전달**하는 원리는 연쇄법칙 에 따른 것         

**[연쇄법칙(겉미분 속미분)]**      
* 연쇄법칙: **합성 함수**의 미분에 대한 성질           
  * 합성함수: **여러 함수로 구성**된 함수    
     * Ex) $$z = (x + y)^2  -> z = t^2  and  t = x + y$$
  
**[계산 그래프의 역전파]**            
$$y = f (x)$$라는 계산의 역전파     
![image](https://user-images.githubusercontent.com/76824611/119590739-52a17800-be10-11eb-84d1-b708a2fafe17.png)
* 신호 E에 노드의 국소적 미분 &&ay/ax&&를 곱한 후 다음노드로 전달      
* 이것이 가능한 이유는 연쇄적법칙 때문                          



## 역전파

### 덧셈 노드의 역전파
**그대로** 출력    
* **1을 곱하기만 할 뿐**이므로 입력된 값을 그대로 다음 노드로 보내게 됨     
![image](https://user-images.githubusercontent.com/76824611/119591265-42d66380-be11-11eb-9090-bfba94b15148.png)

   
* $$L$$:  순전파 시 최종적인 출력 값          
* $$z$$     
  * $$x + y$$ 계산결과로, 그 큰 계산 그래프의 중간 어딘가에 존재    
  * 상류로부터 값이 전해진 것    
  * 하류로는 $$∂L/∂x$$과 $$∂L/∂y$$값을 전달하는 것     

### 곱셈 노드의 역전파
**서로 바꾼값** 곱하기  
* $$∂L/∂x=y$$   
* $$∂L/∂y=x$$    
![image](https://user-images.githubusercontent.com/76824611/119591587-d7d95c80-be11-11eb-8d2c-b3b83b75819a.png)
**[ex]**  
![image](https://user-images.githubusercontent.com/76824611/119591609-e162c480-be11-11eb-9335-148799f3dec8.png)
* $$1.3 * 5 = 6.5$$    
* $$1.3 * 10 = 13$$
   

## 코드 구현
**[단순한 계층 구현하기]**        
밑의 기능을 **계층 단위**로 구현       
* 곱셈 노드를 **‘MulLayer’**
* 덧셈 노드를 **‘AddLayer’**    
* 시그모이드 함수를 **Sigmoid**    
* 행렬 곱을  **Affine**   


**[EX]**    
![image](https://user-images.githubusercontent.com/76824611/119591810-3f8fa780-be12-11eb-93d7-8679a8c7989b.png)
소비세와 사과 가격이 같은 양만큼 오를 때 영향      
* 최종 금액에는 소비세가 200의 크기로,   
* 사과 가격이 2.2 크기로 영향을 줌     

주의) 이 예에서 소비세와 사과 가격은 **단위가 다름**    
* 소비세 1은 100%
* 사과 가격 1은 1원

### 곱셈 계층 
모든 계층은 ```forward ( )```<순전파>와 ```backward ( )```<역전파>라는 공통의 메서드(인터페이스)를 갖도록 구현할 것          
```python
class MulLayer:
    # 인스턴스 변수인 x와 y를 초기화
    # 두 변수는 순전파시의 입력 값을 유지하기 위해서 사용
    def__init__(self):
        self.x = None 
        self.y = None

    # x와 y를 인수로 받고 두 값을 곱해서 반환
    def forward(self, x, y):
        self.x = x 
        self.y = y 
        out = x * y

        return out

    #상류에서 넘어온 미분(dout)에 순전파 때의 값을 ‘서로 바꿔’ 곱한 후 하류로
    def backward(self, dout):
        dx = dout * self.y # x 와 y 를 바꾼다.
        dy = dout * self.x
    
        return dx, dy
```

**[예시 적용]**  
![image](https://user-images.githubusercontent.com/76824611/119592742-05270a00-be14-11eb-96fe-396d1b56520e.png)
 
```python
apple = 100
apple_num = 2
tax = 1.1

# 계층들
mul_apple_layer = MulLayer() 
mul_tax_layer = MulLayer()

# 순전파
apple_price = mul_apple_layer.forward(apple, apple_num) 
price = mul_tax_layer.forward(apple_price, tax)

print(price) # 220

# 역전파
dprice = 1
## dapple_price로 dapple, dapple_num에 앞의 역전파 결과 전달
## dapple_price = 1.1
dapple_price, dtax = mul_tax_layer.backward(dprice) 
dapple, dapple_num = mul _apple _layer.backward(dapple _price)

print(dapple, dapple _num, dtax) # 2.2 110 200
   
# backward ( )가 받는 인수는 ‘순전파의 출력에 대한 미분’
```

### 덧셈 계층 구현
```python 
class AddLayer:
  def__init__(self):
    pass

  def forward(self, x, y):
    out = x + y 
    return out

  def backward(self, dout): 
    dx = dout * 1
    dy = dout * 1
    return dx, dy

```
**[예시 적용]**   
![image](https://user-images.githubusercontent.com/76824611/119593500-099ff280-be15-11eb-805e-9525abf996a7.png)

```python
apple = 100
apple _num = 2
orange = 150
orange _num = 3
tax = 1.1

# 계층들
mul_apple_layer = MulLayer() 
mul_orange_layer = MulLayer() 
add_apple_orange_layer = AddLayer() 
mul_tax_layer = MulLayer()

# 순전파
apple_price = mul_appl _layer.forward(apple, apple_num)  
orange_price = mul_orange_layer.forward(orange, orange_num) 
all_price = add_apple_orange_layer.forward(apple_price,orange_price) 
price = mul_tax_layer.forward(all_price, tax) 

# 역전파
dprice = 1
dall_price, dtax = mul_tax_layer.backward(dprice) 
dapple_price, dorange_price = add_apple _orange_layer.backward(dall _price)
dorange, dorange _num = mul_orange_layer.backward(dorange_price) 
dapple, dapple_num = mul_apple_layer.backward(dapple_price) 

print(price) # 715
print(dapple_num, dapple, dorange, dorange_num, dtax) 
# 110 2.2 3.3 165 650
```

# 활성화 함수 계층 구현
계산 그래프를 신경망에 적용

## ReLU 계층
![image](https://user-images.githubusercontent.com/76824611/119595895-39e99000-be19-11eb-9ccb-1f26fc095af8.png)
**미분**
![image](https://user-images.githubusercontent.com/76824611/119595914-4241cb00-be19-11eb-872c-1d222bf5e6bc.png)

### 역전파     
![image](https://user-images.githubusercontent.com/76824611/119596119-92b92880-be19-11eb-9a05-175028a3441f.png)
* 순전파 시 **$$x> 0$$**: 역전파는 상류의 값을 **그대로** 하류로 흘림
* 순전파 시 **$$x<= 0$$**: 역전파 때는 하류로 **신호를 보내지X**(0을 보냄)

### 구현
```python
class Relu:
  # mask라는 인스턴스 변수를 가집니다
  # mask는 True/False로 구성된 넘파이 배열
  # 순전파의 입력인 x의 원소 값이 0 이하인 인덱스는 True,
  # 그 외(0보다 큰 원소)는 False로 유지
  def __init __(self):
    self.mask = None

  def forward(self, x): 
    # "순전파 때의 입력 값이 0 이하면" 순전파 시의 값은 0이 돼야 함.
    self.mask = (x < = 0) 
    # 0이상 이면 그냥 똑같은거 보냄
    out = x.copy() 
    # 순전파 때의 입력 값이 0 이하면 "순전파 시의 값은 0이 돼야 함".
    out[self.mask] = 0
    return out

  def backward(self, dout):
    # "순전파 때의 입력 값이 0 이하면" 역전파 때의 값은 0이 돼야 함.
    ## 역전파 때는 순전파 때 만들어둔 mask를 써서 mask의 원소가 True인 곳에는 
    ## 상류에서 전파된 dout을 0으로 설정
    dout[self.mask] = 0
    dx = dout
    return dx
```



## Sigmoid 계층
**[시그모이드 함수](https://yerimoh.github.io//DL2/#sigmoid-function%EC%8B%9C%EA%B7%B8%EB%AA%A8%EC%9D%B4%EB%93%9C-%ED%95%A8%EC%88%98)**       

### 역전파
[순전파]
![image](https://user-images.githubusercontent.com/76824611/119597493-196f0500-be1c-11eb-8c09-b5e88ff05831.png)

[역전파]
![image](https://user-images.githubusercontent.com/76824611/119597500-1d028c00-be1c-11eb-9589-de894e3b65e7.png)

![image](https://user-images.githubusercontent.com/76824611/119598028-204a4780-be1d-11eb-84d3-99684f5298b3.png)

**[1단계]**      
**‘/’ 노드**, 즉 $$y= 1/x$$ 미분
* 역으로 바꾼 것, 즉 $$* x^{-1}$$한 것 미분
* 역전파 때는 상류에서 흘러온 값에 -y 2을 곱해서 하류로 전달
  
**[2단계]**      
:‘+’ 노드는 상류의 값을 여과 없이 하류로 내보냄. 

**[3단계]**      
:‘exp’ 노드는 y = exp (x ) 연산을 수행
    -> exp (-x )
    
**[4단계]**      
: ‘×’ 노드는 순전파 때의 값을 ‘서로 바꿔’ 곱함. 
이 예에서는 -1을 곱하면 되겠습니다.

   
   
  구현
  
    class Sigmoid:
def __init __(self):
self.out = None

def forward(self, x):
out = 1 / (1 + np.exp(-x)) 
self.out = out

return out

	# self.out = y
def backward(self, dout):
dx = dout * (1.0 - self.out) * self.out

return dx

   -> 순전파의 출력을 인스턴스 변수 out에 보관했다가, 
역전파 계산 때 그 값을 사용합니다

Affine/Softmax 계층 구현하기

> Affine 계층
 : 신경망의 순전파에서는 가중치 신호의 총합을 계산하기 때문에 행렬의 곱(넘파이에서는 np.dot ( ) )을 사용
  - (신경망)순전파의 흐름: Y = np.dot (X, W ) + B처럼 계산. 
      Y를 활성화 함수로 변환해 다음 층으로 전파하는 것
- 어파인 변환 (affine transformation): 신경망의 순전파 때 수행하는 행
렬의 곱
- Affine 계층: 어파인 변환을 수행하는 처리
  -> 노드계산시 각변수의 이름 위에 그 변수의 형상도 표기

지금까지의 계산 그래프: 노드 사이에 ‘스칼라값’이 흐름

이 예:‘행렬’이 흐름 



- 역전파



: 곱할 때 다른거 곱(전치해서)

      
> 배치용 Affine 계층
: 지금까진  입력 데이터로 X 하나만을 고려
 배치-묶은 데이터
: 데이터 N개를 묶어 순전파하는 경우
  
 : 기존과 다른 부분은 입력인 X의 형상이 (N, 2 )가 된 것뿐
 - 구현

     class Affine:
def _ _init _ _(self, W, b):
self.W = W 
self.b = b 
self.x = None 
self.dW = None 
self.db = None

def forward(self, x):
self.x = x 
out = np.dot(x, self.W) + self.b
return out

	def backward(self, dout):
dx = np.dot(dout, self.W.T) 
self.dW = np.dot(self.x.T, dout) 
#[3]구현 얘는 열로 더해버려
self.db = np.sum(dout, axis = 0)
return dx

> Softmax-with-Loss 계층
 : 출력층에서 사용하는 소프트맥스 함수
 - 소프트맥스 함수는 입력 값을 정규화하여 출력
    
   -> 입력 이미지가 Affine 계층과 ReLU 계층을 통과하며 변환되고, 마지막 
Softmax 계층에 의해서 10개의입력이 정규화
   -> 손글씨 숫자는 가짓수가 10개(10클래스 분류)이므로 Softmax 계층의 
입력은 10개
 - 소프트맥스 함수의 이용: 신경망에서 학습과 추론중 학습에 주로 이용 
<추론에는 제일 높은 값만 필요하기 때문>
 - 점수(score): 경망에서 정규화하지 않는 출력 결과(그림에선 Softmax 앞
 Affine 계층의 출력)-> 추론엔 가장 높은 점수만 필요
 - Softmaxwith-Loss 계층 
: 손실 함수인 교차 엔트로피 오차도 포함
: ‘Softmax’ 계층+‘Cross Entropy Error’
    

 간소화
   
    -> 입력은 (a 1 , a 2 , a 3 )
 Softmax 계층은 (y 1 , y 2 , y 3 )를 출력
 또, 정답 레이블은 (t 1 , t 2 , t 3 )
 Cross Entropy Error 계층은 손실 L을 출력
    -> 위 그림
        Softmax-with-Loss 계층의 역전파 결과
: (y 1 - t 1 , y 2 -t 2 , y 3 - t 3 )

 구체화 1. 순전파

 1) Softmax 계층
   
     
     -> 최종 출력은 (y 1 , y 2 , y 3 )

 2) Cross Entropy Error 계층
     
     





 구체화 2.역전파
 
 1) Cross Entropy Error 계층
    
    -> 주의
       : 역전파의 초깃값, 즉 가장 오른쪽 역전파의 값은 1(aL/aL =1)
: ‘×’ 노드의 역전파는 순전파 시의 입력들의 값을 ‘서로 바꿔’
상류의 미분에 곱하고 하류로 흘림
: ‘+’ 노드에서는 상류에서 전해지는 미분을 그대로 흘립니다.
: ‘log’ 노드의 역전파는 다음 식을 따릅니다.
   

2) Softmax 계층
  (1)

‘x’-> 바꿔서 곱
Y1 대입
   
   (2)
    ‘/’ 노드의 역전파:‘상류에서 흘러온 값’에 ‘순전파 때의 출력을 
제곱한 후 마이너스를 붙인 값’을 곱해 하류 전달. 
->‘상류에서 흘러온 값’_ (-t 1 S) + (-t 2 S) + (-t 3 S) 
= -S(t 1 + t 2 + t 3 )
‘순전파 때의 출력’_ 1/S
역전파의 출력=  -S(t 1 + t 2 + t 3 ) * -1/S^2
                = (t 1 + t 2 + t 3 )*1/S
    
   (3) 
    ‘x’는 바꿔서 연산
    

 (4)
  ‘exp’ 노드 역전파: 두 갈래의 입력의 합에 exp (a 1 )을 곱한 수치
      
    

정리
 
: (y 1 - t 1 , y 2 - t 2 , y 3 - t 3 )는 Softmax 계층의 출력과 정답 레이블의 차.
-> 신경망의 역전파에서는 이 차이인 오차가 앞 계층에 전해지는 것
: 신경망 학습의 목적은 신경망의 출력(Softmax의 출력)이 정답 레이블과 가까워지도록 가중치 매개변수의 값을 조정하는 것-> 그래서 신경망의 출력과 정답 레이블의 오차를 효율적으로 앞 계층에 전달
-> 구체적인 예
: 가령 정답 레이블이 (0, 1, 0 )일 때 Softmax 계층이 (0.3, 0.2, 0.5 )
를 출력
정답 레이블을 보면 정답의 인덱스는 1. 그런데 출력에서는 이때의 확률
겨우 0.2 (20%)라서, 이 시점의 신경망은 제대로 인식X
이 경우 Softmax 계층의 역전파는 (0.3, -0.8, 0.5 )라는 커다란 오차를 전파-> 결과적으로 Softmax 계층의 앞 계층들은 그 큰 오차로부터 큰 깨달음을 얻게 됩니다.

 구현 

    class SoftmaxWithLoss: 
def __init __(self):
self.loss = None # 손실
self.y = None # softmax 의 출력
self.t = None # 정답 레이블(원-핫 벡터)

def forward(self, x, t):
self.t = t 
self.y = softmax(x) 
self.loss = cross _entropy _error(self.y, 
self.t) 

return self.loss

	# 역전파 때는 전파하는 값을 배치의 수(batch_size ) 로 나눠서 
데이터 1개당 오차를 앞 계층으로 전파하는 점에 주의
def backward(self, dout = 1):
batch _size = self.t.shape[0] 
dx = (self.y - self.t) / batch _size

return dx












> 오차역전파법 구현
: 신경망 학습의 전체 그림(4단계)의 [2단계 기울기 산출]에서 오래걸리는 
수치미분 대신 기울기를 효율적이고 빠르게구할 수 있음
 - 구현
   : 2층 신경망을 TwoLayerNet 클래스로 구현
 - 변수
  : 2층 신경망을 TwoLayerNet 클래스로 구현

인스턴스 변수	설명
params	딕셔너리 변수로, 신경망의 매개변수를 보관 
params['W1']은 1번째 층의 가중치, params['b1']은 1번째 층의 편향 
params['W2']는 2번째 층의 가중치, params['b2']는 2번째 층의 편향
layers	순서가 있는 딕셔너리 변수로, 신경망의 계층을 보관 layers['Affine1'], layers['Relu1'], layers['Affine2']와 같이 각 계층을 순서대로 유지

lastLayer	신경망의 마지막 계층
이 예에서는 SoftmaxWithLoss 계층

메서드	설명
__init__(self,
input_size,
hidden_ size,
output_size,
weight_init_ std)	초기화를 수행한다./ 인수는 앞에서부터 
입력층 뉴런 수, 
은닉층 뉴런 수, 
출력층 뉴런 수, 
가중치 초기화 시 정규분포의 스케일

predict(self, x)	
예측(추론)을 수행한다.
인수 x는 이미지 데이터
loss(self, x, t)	손실 함수의 값을 구한다.
인수 x는 이미지 데이터, t는 정답 레이블
accuracy(self, x, t)	정확도를 구한다
numerical_gradient(self, x, t)	가중치 매개변수의 기울기를 수치미분방식으로 구한다.
gradient(self, x, t)	가중치 매개변수의 기울기를 오차역전파법으로 구한다.
-> 계층을 사용함으로써 인식 결과를 얻는 처리(predict ( ) )와 기울기를 구하는 처리(gradient ( ) ) 계층의 전파만 으로 동작이 이루어지는 것

코드 구현

    import sys, os 
sys.path.append(os.pardir) 
import numpy as np

from common.layers import * 
from common.gradient import numerical _gradient 
# 입력된 아이템들(items)의 순서를 기억하는 Dictionary 클래스 
from collections import OrderedDict

class TwoLayerNet:
def _ _init _ _(self, input _size, hidden _size, output 
_size, weight _init _std = 0.01):

# 가중치 초기화
self.params = {} 
self.params['W1'] = weight _init _std *\ 
np.random.randn(input_size, hidden_size) self.params['b1'] = np.zeros(hidden _size) 
self.params['W2'] = weight _init _std * \ 
np.random.randn(hidden_size,output_size) 
self.params['b2'] = np.zeros(output _size)

# 계층 생성
self.layers = OrderedDict() 
self.layers['Affine1'] = \ 
Affine(self.params['W1'], self.params['b1']) self.layers['Relu1'] = Relu() 
self.layers['Affine2'] = \
 Affine(self.params['W2'], self.params['b2'])

self.lastLayer = SoftmaxWithLoss()
def predict(self, x):
for layer in self.layers.values():
x = layer.forward(x)

return x

# x : 입력 데이터 , t : 정답 레이블
def loss(self, x, t):
y = self.predict(x) 

return self.lastLayer.forward(y, t)

def accuracy(self, x, t):
y = self.predict(x)
y = np.argmax(y, axis = 1)
if t.ndim ! = 1 : t = np.argmax(t, axis = 1)

accuracy = np.sum(y = = t) / float(x.shape[0])

return accuracy

# x : 입력 데이터 , t : 정답 레이블
def numerical _gradient(self, x, t):
loss _W = lambda W: self.loss(x, t)
grads = {} 
grads['W1'] = numerical _gradient(loss _W, 
self.params['W1']) 
grads['b1'] = numerical _gradient(loss _W, 
self.params['b1']) 
grads['W2'] = numerical _gradient(loss _W, 
self.params['W2']) 
grads['b2'] = numerical _gradient(loss _W, 
self.params['b2']) return grads

def gradient(self, x, t):
# 순전파
self.loss(x, t)

# 역전파
dout = 1
dout = self.lastLayer.backward(dout)

layers = list(self.layers.values()) 
layers.reverse() 
for layer in layers:
dout = layer.backward(dout)

# 결과 저장
grads = {}
grads['W1'] = self.layers['Affine1'].dW 
grads['b1'] = self.layers['Affine1'].db 
grads['W2'] = self.layers['Affine2'].dW 
grads['b2'] = self.layers['Affine2'].db

return grads
    
    -> OrderedDict에 보관하는 점이 중요_ OrderedDict은 순서가 있는 딕셔너리
  그래서 순전파 때는 추가한 순서대로 각 계층의 forward ( ) 메서드를 
호출하기만 하면 처리가 완료. 
마찬가지로 역전파 때는 계층을 반대 순서로 호출하기만 하면 됨
-> Affine 계층과 ReLU 계층이 각자의 내부에서 순전파와 역전파를 제대
로 처리하고 있으니, 여기에서는 그냥 계층을 올바른 순서로 연결한 
다음 순서대로 (혹은 역순으로) 호출해주면 끝

> 오차역전파법으로 구한 기울기 검증
 : 수치 미분은 오차역전 파법을 정확히 구현했는지 확인하기 위해 필요
 - gradient check
: 두 방식으로 구한 기울기가 일치함(엄밀히 말하면 거의 
같음)을 확인하는 작업을 기울기 확인 
 - 크기 비교 유의점
   : 수치 미분과 오차역전파법의 결과 오차가 0이 되는 일은 드뭄.
 (이는 컴퓨터가 할 수 있는 계산의 정밀도가 유한하기 때문)
이 정밀도의 한계 때문에 오차는 대부분 0이 되지는 않지만, 올바르게 
구현했다면 0에 아주 가까운 작은 값이 됩니다. 
만약 그 값이 크면 오차역전파법을 잘못 구현했다고 의심 필요
