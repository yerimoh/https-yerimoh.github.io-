---
title: "[07] Deep learning 1: 경사 하강법 gradient descent method"
date:   2020-03-6
excerpt: "경사 하강법 gradient descent method의 개념, 쓰임을 예를 통해서 쉽게 이해하기/ MSE(Mean Squared Error)/ 손실함수 쉽게 "
category: [Deep Learning]
layout: post
tag:
- Deep Learning
order: 0

comments: true
---

# 경사하강법 목차
- [경사 하강법 gradient descent method](#경사-하강법-gradient-descent-method)
  * [시작하기 전 읽어보면 좋을 개념(필수 개념)](#시작하기-전-읽어보면-좋을-개념-필수-개념-)
- [목표](#목표)
- [bulid-up 1](#bulid-up-1)
  * [MSE(Mean Squared Error)](#mse-mean-squared-error-)
    + [MSE의 수식](#mse의-수식)
    + [MSE 구현](#mse-구현)
- [bulid-up 2](#bulid-up-2)
  * [손실 곡선(The Loss Curve)](#손실-곡선-the-loss-curve-)
- [경사 하강법 gradient descent method](#경사 하강법-gradient-descent-method-1)
  * [예시](#예시)
    + [1) MSE에 대입](#1--mse에-대입)
    + [2) MSE를 확장](#2--mse를-확장)
    + [3) 이동방향 설정](#3--이동방향-설정)
    + [4) 학습률(λ)도입](#4--학습률---도입)
    + [5) 학습](#5--학습)

---

# 경사 하강법 gradient descent method   

## 시작하기 전 읽어보면 좋을 개념(필수 개념)
* [경사하강법이 딥러닝의 전체적인 과정에서 어디에 왜 쓰이나요?](https://yerimoh.github.io/DL3/)       
* [손실 곡선](https://yerimoh.github.io/DL3/#%EC%86%90%EC%8B%A4-%ED%95%A8%EC%88%98loss-function)     

---

# 목표
[손실 함수](https://yerimoh.github.io/DL3/#%EC%86%90%EC%8B%A4-%ED%95%A8%EC%88%98loss-function)에서 경사하강법을 이용하여,    
현재위치 ➡ 타깃위치 로 이동
![image](https://user-images.githubusercontent.com/76824611/128706696-c8306e64-9a7a-4b29-801d-b489e17cb5b9.png)



---
# bulid-up 1

## MSE(Mean Squared Error)
먼저 경사하강법에 쓰일 손실함수부터 알아보자!   

앞의 링크에서 볼 수 있는 **오차 제곱 합(sum of squares for error, SSE)**, **교차 엔트로피 오차(cross entropy error, CEE)** 와 같은 손실 함수     
즉 손실함수의 또 다른 종류     

이 손실함수가 왜 쓰이는지 간단하게 설명해 보겠다.     

이를 위해 매우 단순한 모델을 가져와보자.     

![image](https://user-images.githubusercontent.com/76824611/128722387-33a7260b-da3b-4da4-865f-5bbb55da8735.png)

* 각 원 🟢: 뉴런      
* 목표: 두 뉴런을 잇는 **회귀선(y = mx + b)** 만들기      


**[뉴런(y)로 들어오는 각 변수]**      
**회귀선(y = mx + b)**       
* **m**: 기울기, 가중치       
* **b**: y의 절편으로 뉴런과 함께 저장됨      

 
**[m과 b를 구할 수 있는 좀 더 확정적인 방법]**       
 * 시작은 무작위 값         
 * 처음의 y_hat은 x에 대한 정답인 y를 무작위로 추측한다   
 * 기존의 회귀선: ✨최소제곱오차✨(참값과 각 근사치의 차를 제곱)에 기반     

맞다.. 이 ✨최소제곱오차✨의 방법이 **MSE(Mean Squared Error)** 이다.    

### MSE의 수식
![image](https://user-images.githubusercontent.com/76824611/128714016-18cac55d-f080-4763-906f-63167de24611.png)
* 이 방법은 **오차를 양수로 유지**    
* 총 오차를 파악할 때 서로를 **상쇄 안 함**    
* 잘못된 추측을 **제곱**을 통해 **강하게 처벌**     
![image](https://user-images.githubusercontent.com/76824611/128714148-b81d8b94-facf-44f5-805c-e90eeab2975e.png)

➡ 두 오차값이 -,+ 여도 제곱을 하여 서로 상쇄 안함   
➡ 이를 위의 두 식을 이용하여 y_hat을 y에 가깝게 만들어 m과 b를 제대로 추측한다     

### MSE 구현

```python
data = [(1, 3), (2, 5)]
m = -1
b = 5
def get_mse(data, m, b):
    """평균제곱오차 계산"""
    n = len(data)
    squared_error = 0
    for x, y in data:
        # 예측된 y 찾기
    	y_hat = m*x+b
    	# 예측과 참값 사이의 
    	# 차이를 제곱
    	squared_error += (
    	    y - y_hat)**2
    # 평균 제곱 차이 구하기
    mse = squared_error / n
    return mse
```

---
# bulid-up 2

## 손실 곡선(The Loss Curve)
그럼 이 경사하강법이 작동할 배경인 손실 곡선(The Loss Curve)에 대해서 간단하게 설명해보겠다 
.

평균제곱오차에 관한 m과 b를 그래프로 그리면,

**[손실함수의 표면도]**    
모델을 평가하기 위해 우리가 선택한 오차 함수 즉, MSE의 표면도는
![image](https://user-images.githubusercontent.com/76824611/128714866-7876302b-e0e0-41de-8ec7-b84c2188c764.png)

**[손실함수의 등고선도]**    
이를 더 직관적으로 보기위해 등고선도로 그려보겠다.    
![image](https://user-images.githubusercontent.com/76824611/128714942-07851f3a-df3e-49a2-8d92-31a0c9b9fa41.png)
➡ 이를 통해 최소값을 갖는 영역이 b=1과 m=2인 포물선 타원과 같다는 사실을 확인 가능하다    
➡ 가장 진한 곳이 가장 손실이 적은곳, 타깃이기 때문이다     

**[PROBLELM & 경사하강법]**    
* 컴퓨터에는 이 그래프의 바닥에 도달하기 위해 뭘 변경해야 할지를 추측할 수 있는 능력이 없음       
![image](https://user-images.githubusercontent.com/76824611/128715294-34a15621-0577-493f-9235-b4a8dc4139c0.png)
➡주어진 현재 점에서 그래디언트를 계산할 것    

----

# 경사 하강법 gradient descent method
**그래디언트(gradient)**: 어떤 방향으로 손실이 가장 많이 감소하는가      
* m에 대한 b의 어떤 비율      
* 타깃에서 멀어질 수 있으므로 한번에 많이씩 가면 안됨(step 이 너무 크면 안됨)      

- **단계(step)**: 가중치 매개변수에 대한 업데이트    
- **λ(학습률)(learning rate)**: 이동거리   
   * 방향(그래디언트)이 확보되면 얼마나 멀리 이동할지를 파악     



- **에포크(epoch)**: **전체 데이터 세트**로 모델 업데이트     
  * 새 위치에 도달한 후 위의 과정 반복    
- **배치(batch)**: 전체 데이터 세트 중 특정 샘플    
➡ 전체 데이터세트를 사용하든 배치를 사용하든 어떤 경우든, 위의 단계에서 그래디언트를 계산한 다음 해당 그래디언트와 학습률로 매개변수를 업데이트합니다.


## 예시
**[에러를 통한 학습]**   
![image](https://user-images.githubusercontent.com/76824611/128715294-34a15621-0577-493f-9235-b4a8dc4139c0.png)
위와 같은 상황에서 MSE를 이용하여 타겟으로 가보자     

### 1) MSE에 대입
먼저 MSE에 위의 상태를 대입해보면
![image](https://user-images.githubusercontent.com/76824611/128713894-93663e11-6052-4e87-977d-2e14e0cdb686.png)

### 2) MSE를 확장
위의 현재 점을 지나는 수많은 직선 중 x,y 데이터포인트인 (1,3)과 (2,5)를 포함하도록 MSE를 확장해보자  
![image](https://user-images.githubusercontent.com/76824611/128715960-61b9985d-cd76-4e53-802a-5536728cba65.png)

$$𝜕𝑀𝑆𝐸/𝜕𝑚=9𝑚+5𝑏−23$$
$$𝜕𝑀𝑆𝐸/𝜕𝑚=−7$$

$$𝜕𝑀𝑆𝐸/𝜕𝑏=5𝑚+3𝑏−13$$
$$𝜕𝑀𝑆𝐸/𝜕𝑏=−3$$    


### 3) 이동방향 설정
$$𝜕𝑀𝑆𝐸/𝜕𝑚=−7$$, $$𝜕𝑀𝑆𝐸/𝜕𝑏=−3$$  둘다 음수이다 -> 실제 값이 더 작으니 키워야 한다.      
➡ 0에 근접해지게 하기 위해 둘을 늘려야 함
![image](https://user-images.githubusercontent.com/76824611/128718260-0fc50fe8-7832-4f16-b1c5-3394fd7e6a4a.png)
🚫 문제점: 그래디언트가 올바른 방향은 제시하지만 **규모가 너무 큼**   

### 4) 학습률(λ)도입  
매개변수의 변경 규모를 조정!     
데이터 사이언티스트는 직접 이를 정의하지만, 다행히 이 주제에 대한 충분한 연구가 있었던 덕분에 머신 러닝 라이브러리에 나름의 알고리즘이 포함되어 있

![image](https://user-images.githubusercontent.com/76824611/128718649-97a0900f-fa5c-4c32-b204-06921f92f11d.png)
![image](https://user-images.githubusercontent.com/76824611/128718671-07d71dc9-e588-49ec-b936-5a0eec6fb1c6.png)

학습률을 너무 높게 설정하면 목표에서 더 멀리 벗어남    
ex) λ=.5   
![image](https://user-images.githubusercontent.com/76824611/128718875-704d92ed-3da5-4c7d-9bcf-a52f2616cde1.png)

너무 작게 설정하면 목표에 도달하는 데 너무 오래걸림     
➡ [안장점 saddle point](https://yerimoh.github.io/DL3/) 문제가 일어날 수 있음(지역 최소값에 머물러 있게 된다)   
ex) λ=.005 
![image](https://user-images.githubusercontent.com/76824611/128719413-b6418ca3-8653-47ec-96b8-4d9b34031a4a.png)

### 5) 학습
➡ 이 중간인 0.1의 학습률을 유지해보겠다     
$$λ=.1$$  
$$m≔−1 −7 λ=−1.7$$    
$$b ≔5 −3 λ=5.3$$   
![image](https://user-images.githubusercontent.com/76824611/128720003-86d999ea-7ac0-4147-86f4-0bbb791dc34c.png)

계속해서 과정을 반복하며 **b와 m의 새 값을 적용해** 그래디언트를 찾으면 됨.





