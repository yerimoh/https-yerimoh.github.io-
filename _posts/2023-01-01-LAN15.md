---
title: "Effective Approaches to Attention-based Neural Machine Translation 정리"
date:   2023-01-1
excerpt: "Effective Approaches to Attention-based Neural Machine Translation paper review"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---

# 목차
- [Before Read: attention](#before-read--attention)
- [Abstract](#abstract)
- [1. Introduction](#1-introduction)
  * [선행 연구](#선행-연구)
  * [본 논문의 목적](#본-논문의-목적)
- [2. Neural Machine Translation](#2-neural-machine-translation)
- [3. Attention-based Models](#3-attention-based-models)
  * [3.1 Global Attention](#32-global-attention)
  * [3.2 Local Attention](#32-local-attention)
    + [**[local-m]** _Monotonic_ alignment($$p_t$$)](#---local-m-----monotonic--alignment---p-t---)
    + [**[local-p]** _Predictive_ alignment($$p_t$$)](#---local-p-----predictive--alignment---p-t---)
  - [3.3 Input-feeding Approach](#33-input-feeding-approach)
- [4. Experiments](#4-experiments)


----

본 포스트는 [Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/pdf/1508.04025.pdf)을 구현하기 위해 상세히 정리해 둔 것으로,   
논문에 나와있는 내용을 추가 지식과 함께 쉽게 풀어쓰려고 노력하여 정리해두었습니다 :)      


---
---


# **Before Read: attention**
이 논문을 읽기 전, 현 논문에서 가장 많이 언급되는 attention의 개념을 알아둬야 이 논문을 읽기 편하다.    

attention이란 translation task에서 중요한 정보에 더 집중을 할 수 있게 만든 method로, 토큰(각 단어)간의 의미적인 관계를 파악할 수 있게한다.     
특히. **decoder의 각 토큰 예측 시, encoder의 모든 혹은 일부 토큰에 집중하여 task를 수행**하게 하는 것이다.       

이 개념은 3개의 중요 개념을 중심으로 이해할 수 있다.      
* **Query**: 검색어, 즉 번역 대상이 되는 단어이다.    
* **Key**: Query값을 기준으로 output이 될 수 있는 후보들이다.(즉 찾는 대상이다)    
* **Value**: 각각의 key가 갖고있는 실제 값이다.   

  
예를 들어서 아래와 같은 도형들이 존재한다고 가정해보자.    
여기서 검색대상인 Query와 가장 유사도가 높은(여기서는 비슷한 모양인) key 2의 value가 가장 높다는 것을 파악가능하다.     
![image](https://user-images.githubusercontent.com/76824611/214841597-d5091eb0-c8c5-4bfb-9ce3-b0bae7e90220.png)

**attietion은 이렇게 가장 높은 유사도를 반영**해 주는 것이다.     
 
그럼 위의 예시들을 통해 **attention value**를 구하는 과정을 생각해보자.    
 
1️⃣ 먼저 전체 key 중 Query에 대해 각 Key들이 지니는 비중인 유사도를 계산하기위해 임의의 백터 $$<a_1, a_2, a_3>$$가 있다고 가정해보자.      

2️⃣ **$$a_n$$ * Value**를 계산한다.(dot product attention)      
![image](https://user-images.githubusercontent.com/76824611/214841640-20db6bf1-789e-4012-8212-5e5e08bbfe89.png)


3️⃣ 그리고 **각각의 값을 더한다**.     
더한 값은 아래와 같은데,  아래식에 따르면 유사도가 가장 높은 Key2의 값이 가장 많이 반영이 된다는 것을 알 수 있다.         
![image](https://user-images.githubusercontent.com/76824611/214841651-67635f23-5647-49c5-bbd9-a908b1f774e8.png)



즉 위 계산은 결국 **Query와 관련이 깊은 Key의 Value 위주로 모든 값을 가져오는 것**이다.           
 


---
---


# **Abstract**

**[attention-based NMT탐구 배경]**    
최근 번역 중에 원본 문장의 일부에 선택적으로 초점을 맞추어 neural machine translation(NMT)을 개선하기 위해 **attentional mechanism**이 사용되고 있다.     
그러나 attention-based NMT에 대한 유용한 아키텍처를 탐구하는 작업은 거의 없었다.       

**[논문의 목적]**      
이 논문은 **두 가지** 단순하고 효과적인 **attentional mechanism** 클래스를 조사한다.       
* 1) **_global_ attention:** 모든  source words(번역하고자 하는 언어)에 항상 주의를 기울이는 접근방식     
* 2) **_local_ attention:** 한 번에 source words의 하위 집합만 보는 접근 방식     

본 논문은 영어와 독일어 사이의 WMT 번역 작업에 대한 **두 가지 접근 방식의 효과를 입증**한다.       

**_local_ attention**를 사용하면 [dropout](https://yerimoh.github.io/DL17/#%EB%93%9C%EB%A1%AD%EC%95%84%EC%9B%83%EC%97%90-%EC%9D%98%ED%95%9C-%EA%B3%BC%EC%A0%81%ED%95%A9-%EC%96%B5%EC%A0%9C)과 같은 이미 알려진 기술을 통합한 non-attentional systems에 비해 **+ 5.0 BLEU score**의 상당한 이점을 얻을 수 있다.     

서로 다른 attention architectures를 사용하는 앙상블 모델은 25.9 BLEU 포인트가 나왔다.     
➡ WMT'15 영어-독일어 번역 과제에서, NMT와 n-gram reranker를 이용한 기존 보다 1.0 BLEU 포인트 향상된 새로운 state-of-the-art score를 산출한다. 



All our code and models are publicly available at [this link](t
http://nlp.stanford.edu/projects/nmt)


-----
---

# **1. Introduction**

## 선행 연구
Neural Machine Translation (NMT)은 영어에서 프랑스어 및 영어에서 독일어와 같은 대규모 번역 작업에서 state-of-the-art 성능을 달성했다.       

**[NMT의 특징]**   
* **최소한의 도메인 지식을 필요**로 하고 **개념적으로 간단**함    
* NMT는 **end-to-end 방식**으로 훈련됨     
* 매우 **긴 단어 시퀀스까지 잘 일반화**할 수 있는 능력을 가진 **대규모 신경망**임      
➡ 이것은 모델이 표준 MT의 경우처럼 **거대한 phrase table과 언어 모델을 명시적으로 저장할 필요가 없다**는 것을 의미한다.         
* 따라서 NMT는 메모리 공간이 작음      
* NMT 디코더를 구현하는 것은 표준 MT의 매우 복잡한 디코더와는 달리 쉽다      
 


**[NMT 모델의 예시]**    
Luong et al.(2015)의 모델을 예시로 들음     
1️⃣ Figure 1의 파란색 네모와 같이 문장 끝 기호(```<eos>```)에 도달할 때까지 모든 source words를 읽는다.           
2️⃣ 그런 다음 그림의 빨간색 네모 부분과 같이 한 번에 하나의 대상 단어(eg X, Y, Z in Figure 1)를 방출하기 시작한다.        

![image](https://user-images.githubusercontent.com/76824611/216457897-f5e2321a-49b2-4414-87ea-a42e4ec7cc7d.png)

Figure 1: Neural machine translation     
source sequence _A B C D_ (번역하고자 하는 단어)를 target sequence _X, Y, Z_ (번역 대상이 되는 단어)로 변환하기 위한 stacking recurrent architecture. 여기서 ```<eos>```는 문장의 끝을 나타낸다.     
그리고 여기서 각 네모(셀)은 RNN, CNN, LSTM등 으로 구성되어있다.


이와 병행하여, "attention"라는 개념은 최근 신경망 훈련에서 인기를 얻고 있으며,    
모델이 **서로 다른 modalities 간의 [alignments](https://dos-tacos.github.io/concept/alignment-in-rnn/)**을 학습할 수 있다.      
예를 들어,  
* 동적 제어 문제에서 이미지 객체와 에이전트 동작 간의 alignments    
* 음성 인식 작업에서 음성 프레임과 텍스트 간의 alignments     
* 시각적 features및 이미지 캡션 생성 작업의 텍스트 설명 사이의 alignments을 학습할 수 있다   

NMT의 맥락에서, **단어를 공동으로 번역하고 alignments**하기 위해 이러한 attention을 성공적으로 적용했다.       
우리가 아는 한, 위에서 언급한 연구 이외의 NMT에 대한 **attention 기반 아키텍처의 사용을 탐구**하는 다른 연구는 없었다.


## 본 논문의 목적

1️⃣ 본 연구에서는 **단순성과 효율성**을 염두에 두고, **두 가지 새로운 유형의 attention based 모델(_global_, _local_)** 방식을 설계한다.     
* **_global_ 접근 방식**: [Bahdanau et al., 2015의 모델](https://arxiv.org/abs/1409.0473)과 유사하지만 **구조적으로 더 단순**하다.      
* **_local_ 접근 방식**: [Xu et al., 2015](https://arxiv.org/abs/1502.03044)에서 제안된 _hard_ 및 _soft_ attention 모델 간의 흥미로운 혼합으로 볼 수 있다.  
   * _global_ model이나 _soft_ attention보다 **계산적으로 비용이 적게** 든다.            
   * _hard_ attention과 달리 _local_ attention은 거의 모든 곳에서 차별화되어 **구현 및 훈련이 용이**하다    


2️⃣ 또한, 우리는 attention-based models에 대한 **다양한 alignment 기능도 조사**한다.     



3️⃣ 우리는 실험을 통해 우리의 **두 가지 접근 방식이** 영어와 독일어 사이의 **WMT 번역 작업에서 효과적이라는 것**을 보여준다.     
* 우리의 attentional models은 [dropout](https://yerimoh.github.io/DL17/#%EB%93%9C%EB%A1%AD%EC%95%84%EC%9B%83%EC%97%90-%EC%9D%98%ED%95%9C-%EA%B3%BC%EC%A0%81%ED%95%A9-%EC%96%B5%EC%A0%9C)과 같은 이미 알려진 기술을 통합한 **non-attentional systems에 비해 최대 5.0 BLEU의 증가**를 보여준다.         
* 영어에서 독일어로의 번역의 경우, 우리는 WMT'14와 WMT'15 모두에 대한 새로운 최첨단(SOTA) 결과를 달성하여 **NMT 모델과 n-gram LM rerankers가 지원하는 이전 SOTA 시스템을 1.0 BLEU 이상 능가**한다.     


4️⃣ learning, 긴 문장 처리 능력, attentional architectures 선택, alignment 품질 및 번역 outputs에서 모델을 평가하기 위해 **광범위한 분석**을 수행한다. 



------
-----

# **2. Neural Machine Translation**
neural machine translation(NMT) system은 문장 $$x_1, ., x_n$$을 문장 $$y_1, . ., y_m$$으로 **번역하는 조건부 확률 p(y|x)를 직접 모델링하는 신경망**이다.     

NMT의 기본 형태는 **Encoder-Decoder**형태이다.    
* **Encoder**: 각 source sentence(번역하고자 하는 문장)에 대한 표현을 계산을 통한 정보 추출, 이는 RNN, CNN 등으로 구성할 수 있으며 타겟 생성 방식에 따라서도 여러 방법론이 존재함.      
* **Decoder**: 추출한 정보를 바탕으로 target 문장을 생성(번역)         

그런데 우리가 아는 **가장 일반적인 NMT의 모델**은 아래와 같다.    

![image](https://user-images.githubusercontent.com/76824611/214825162-f0bb19b3-805f-4dea-b932-cb0612a650ff.png)


한 번에 하나의 target 단어를 생성하므로 조건부 확률을 다음과 같다:        


(1)
$$log p(y|x) =$$ $$\displaystyle\sum_{j=1}^{m}{log p(y_j |y<j , s)}$$ 


* **y**: target sentence (번역해서 사용하고자하는 문장)    
* **x**: source sentence (번역하고자하는 대상)    
* **s**: (= source) 번역에서 사용되는 원 문장의 정보    
* **j**: Decoder의 토큰 시점      
* 식의 의미    
   * source sentence가 주어졌을 때, target y가 생성될 확률은,   
     Decoder의 토큰 시점 j까지의 정보들을 합하여 최종적으로 출력이 될 토큰의 확률을 구하는 것이다.    



디코더에서 이러한 분해를 모델링하려면 대부분 **recurrent neural network([RNN](https://yerimoh.github.io/DL16/)) 아키텍처**를 사용하는데, 이 아키텍처는 (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015; Luong et al., 2015)과 같은 **최근 NMT 대부분의 작업에서 공통적으로 수행**된다.     

이렇게 같은 방식은 사용하는 위의 각 논문들은 아래 **두가지 방법에서 차이**가 있다.      
**1)** Decoder에 사용되는 RNN 아키텍처의 종류    
**2)** 인코더가 소스 문장 표현(**s**)을 계산하는 방법        


더 자세히 말하면,    
**각 단어 $$y_j$$의 디코딩 확률**을 아래와 같이 매개변수화할 수 있다.         

(2)    
$$p(y_j \| y<j, s) = softmax (g(h_j))$$ 

* **$$g$$**: vocabulary-sized 벡터를 출력하는 변환 함수이다.     
* **$$h_j$$**: RNN hidden unit이며, 추상적으로 다음과 같이 계산된다:

(3)     
$$h_j = f(h_{j-1}, s)$$ 

* **$$f$$**: 이전 hidden state를 기반으로, 주어진 현재 hidden state를 계산한다. 이는 바닐라 RNN 단위, GRU 또는 LSTM 단위가 될 수 있다.      


**[source representation s 의 사용 횟수]**      
* Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Luong et al., 2015 에서 source representation s 은 **decoder hidden state를 초기화하는 데 한 번만 사용**된다.      
* Bahdanau et al., 2015; Jean et al., 2015과 **본 논문**는 **번역 과정의 전 과정**에 걸쳐 사용된다.    
➡ 이러한 접근 방식을 **attention mechanism**이라고 하며, 이에 대해서는 다음 챕터에서 설명하겠다.    


본 연구에서는 (Sutskever et al., 2014; Luong et al., 2015)에 이어 Figure 1에 나와 있는 것처럼 NMT 시스템에 대한 stacking LSTM 아키텍처를 사용한다.      
우리는 ([Zaremba et al., 2015](https://arxiv.org/abs/1409.2329))에 정의된 LSTM 단위를 사용한다. trainig 목표는 다음과 같이 공식화된다:        

(4)    
$$J_t =$$ $$\displaystyle\sum_{(x,y)∈D}{-log p(y\|x)}$$ 
 
* **D**: 우리의 병렬 training corpus이다.         

----
---




# **3. Attention-based Models**


우리의 다양한 주의 기반 모델은 **_global_** 과 **_local_** 로 크게 분류된다.      
두 attention은 앞서 말했다시피 "attention"이 모든 source positions에 배치되는지, 아니면 소수의 source positions에만 배치되는지의 관점에서 다르다.      
우리는 이 두 가지 모델 유형을 Figure 2와 Figure 3에 각각 설명할 예정이다.    




**[두 attention의 공통점]**    
* **Decoding 단계의 입력:** Decoding 단계에서 각 단계 $$t$$에서 먼저 **stacking LSTM의 최상위 계층에서** hidden state **$$h_t$$를 입력으로 받아들인다**는 사실이다.       
* **공통 목표:** 위와 같이 입력을 받은 다음, 현재 target word $$y_t$$를 예측하는 데 도움이 되는 source-side information를 캡처하는 context vector **$$c_t$$를 도출하는 것이 목표**이다.    
* 이 모델들은 context vector **$$c_t$$가 도출되는 방법이 다르지만** **동일한 후속 단계를 공유**한다


구체적으로, 대상 숨겨진 상태 $$h_t$$와 source-side information를 캡처하는 context vector $$c_t$$를 고려할 때,    
우리는 다음과 같이 **attentional hidden state를 생성하기 위한 두 벡터의 정보를 결합**하기 위해 간단한 연결 레이어를 사용한다:

(5)       
$$h˜_t = tanh(W_c[c_t;h_t])$$     

attention 벡터 **$$h˜_t$$는** 다음과 같이 공식화된 **예측 분포를 생성**하기 위해 **소프트맥스 층을 통해 공급**된다:   

(6)    
$$p(y_t |y<t, x) = softmax(W_sh˜_t)$$    

이제 각 모델 유형이 source-side information context vector $$c_t$$를 계산하는 방법을 자세히 설명한다.


---

## 3.1 Global Attention

**[_global_ attentional model]**    
* context vector $$c_t$$를 도출할 때 encoder의 **모든 hidden state를 고려**하는 것이다.     
* 즉, decoder가 토큰을 예측할 때 source문장의 모든 토큰을 반영할 수 있다는 의미이다.    



![image](https://user-images.githubusercontent.com/76824611/216458003-e2dfa5a6-cd40-4be2-b712-88cd1a77a2fe.png)
**Figure 2:** Global attentional model   
* **$$t$$**: 각 단계    
* **$$h_t$$**: 현재 목표 상태    
* **$$h¯_s$$**: 모든 source 상태    
* **$$a_t$$**: 가변 길이 정렬 가중치 벡터(a variable-length alignment weight vect)    
* **$$c_t$$**: A global context vector       
* 각 시간 단계 t에서 모델은 **$$h_t$$와 $$h¯s$$를 기반**으로 **$$a_t$$를 추론**한다.        
* $$c_t$$는 **모든 source 상태에 대한 가중 평균으로 계산**된다     
* 모든 $$h¯_s$$가 $$c_t$$에 연결되어있어 $$c_t$$는 모든 source의 정보를 갖게된다는 것을 그림에서 확인 가능하다.     


**[도출 방법]**    
* **1) $$h_t 계산$$**  
  * Decoder의 특정 시점(t)에서 **현재 목표상태인 hidden state $$h_t$$를 계산**한다.    
* **2) $$score(h_t,h¯_s)$$ 계산**    
  * 정의된 score 방법(아래 여러개의 방법을 제시할 예정이다)을 이용해 **$$h_t$$를 각 encoder 시점에서의 hidden state ($$h¯_s$$)와 계산**한다            
* **3) $$a_t$$ 계산**     
  * 계산된 $$h¯_s$$들에 **softmax를 적용하여 $$a_t$$를 구한다**.    
  * 이 때 $$a_t$$는 현 시점 단어를 출력하는데 있어 **인코더가 갖는 가중치(중요도)**라고 할 수 있다.   
![image](https://user-images.githubusercontent.com/76824611/216457668-bb7a454a-dc59-42e9-b66b-9f8549d1e300.png)
* **4) $$c_t$$ 계산**     
  * $$a_t$$를 각 encoder의 hidden stste(value)에 [곱해서 더하여](#before-read--attention) **context vector을 얻게된다**.     
이는 decoder가 단어를 예측할 때 **source 문장의 어느 부분에 attention을 준 결과**가 된다.    
* **5) $$h˜t$$ 계산**   
  * **attentional hidden state를 생성하기 위한 두 벡터의 정보를 결합**하기 위해 간단한 연결 레이어를 사용한다 (5):    
  $$h˜_t = tanh(W_c[c_t;h_t])$$       
  * 최종적으로 $$h_t$$와 $$c_t$$를 이용하여 $$h˜_t$$를 얻게된다.      
  attention 벡터 **$$h˜_t$$는** 다음과 같이 공식화된 **예측 분포를 생성**하기 위해 **소프트맥스 층을 통해 공급**된다 (6):   
  $$p(y_t |y<t, x) = softmax(W_sh˜_t)$$    
  즉 확률이 가장 높은걸 예측 단어라고 도출한다.


**[score 방법]**    
여기서 score는 아래 세가지 방법으로, content-based 함수이다.   
![image](https://user-images.githubusercontent.com/76824611/216457751-01cfc32f-ff41-41fa-afc2-0e2e6e654df0.png)


**[_global_ attention 단점]**   
* 각 target 단어에 대해 **source 측면의 모든 단어에 주의해야 한다**는 단점 존재    
* 이는 비용이 많이 듦    
* 또한 문단 또는 문서와 같은 **더 긴 시퀀스를 번역하는 것을 실용적이지 않게** 만들 수 있음 (e.g., paragraphs or document)    



----

## 3.2 Local Attention

**[_Local_ attentional model]**    
* _global_ attention의 단점을 해결하기 위해, **target 단어당 source 위치의 작은 subset에만 초점을 맞추기로 선택**하는 _Local_ Attention을 제안.     
* 즉 **encoding된 토큰 중 일부에 대해서만 attention 수행**    
* 이 모델은 Xu et al.(2015)이 이미지 캡션 생성 작업을 해결하기 위해 제안한 **soft and hard attentional models의 절충**에서 영감을 얻었다.   
* 본 모델은 soft Attention에서 발생하는 값비싼 계산을 피할 수 있는 장점이 있으며, 동시에 hard Attention 접근법보다 훈련하기가 더 쉽다.    
* 이 방법은 작은 컨텍스트 창에 선택적으로 초점을 맞추고 차별화할 수 있다.  


※ Soft attention은 가중치를 연속적인 값이 되게하는 반면, Hard attention은 0 혹은 1과 같은 2진 값을 사용하는 것

![image](https://user-images.githubusercontent.com/76824611/216458103-c059d7a9-6994-4fa3-b57e-7e4880e99ebb.png)
**Figure 3:** Local attention model  




**[도출 방법]**    
* **1)** 모델은 먼저 time $$t$$의 각 target 단어에 대해 **정렬된 위치(aligned position) $$p_t$$**를 생성한다.    
* **2)** 그런 다음 context vector $$c_t$$는 **window[$$p_t$$-D, $$p_t$$+D]의 set of source hidden states**에 대한 **가중 평균**으로 도출된다. 
➡ 여기서 D는 경험적으로 선택되는 것으로 어디까지 집중할 것이냐를 생각하는 것이다.     
➡ 이는 _global_ attention과 달리, 고정길이, 고정차원인 $$2D+1$$개의 토큰이다.    
![image](https://user-images.githubusercontent.com/76824611/216458226-6cd54211-4e0d-4d0c-a8ce-f26d6c047688.png) 
* **3)** 여기서 weight은 현재 target state $$h_t$$와 window 안의 source state $$h¯s$$로부터 추론된다.    


또한 Ailgnment($$p_t$$) 는 아래와 같이 두가지 변형을 고려한다.      

### **[local-m]** _Monotonic_ alignment($$p_t$$)    
* 단순히 $$p_t = t$$로 설정하여 비슷한 위치의 단어가 비슷한 의미를 지닐 것으로 가정한다.   
* $$a_t$$ 는 기존과 동일한 수식으로 정의한다.    
![image](https://user-images.githubusercontent.com/76824611/216458283-7859fabe-ed54-4761-b7c2-9fc74f3f352a.png)


### **[local-p]** _Predictive_ alignment($$p_t$$)   
* $$p_t$$의 적절한 위치를 찾기위한 수식 도입     
![image](https://user-images.githubusercontent.com/76824611/216458321-1297f8a4-2ce4-4515-8996-d64ec7b914c0.png)
  * **$$W_p$$, $$v_p$$**: 위치를 예측하기 위해 학습될 모델 매개변수    
  * **S**: 원본 문장의 길이  
  
* **정렬 가중치** 재설정      
  * **주변단어들이 중요하다는 전제** 하에 $$a_t$$에 가우시한 분보 term을 추가하여 $$p_t$$근처에 ailgnment point를 갖게함      
  * local-p는 Eq. (10)와 같이 원래 정렬 가중치 정렬($$h_t,h¯_s$$)을 수정하기 위해 truncated 가우스 분포를 동적으로 계산하고 사용한다는 점을 제외하고 local-m 모델과 유사함    
  * 도출할 $$p_t$$를 활용하여 $$W_p$$와 $$v_p$$에 대한 backprop gradients를 계산할 수 있다.      
  * 이 모델은 거의 모든 곳에서 차별화될 수 있다.      
  ![image](https://user-images.githubusercontent.com/76824611/216458368-28095126-8285-466f-aafe-94383821e3a0.png)
  * 여기서 우리는 local-p와 같은 align식(7)을 사용한다.       
  * standard deviation은 실험에 의해 $$σ= D/2$$로 설정    
  * $$p_t$$는 실수(real nummber)이고, $$s$$는 $$p_t$$를 중심으로 한 window의 integer이다.   
  
  
----
 
## 3.3 Input-feeding Approach
**[기존 방법의 문제점]**    
* 위에서 제안된 _Local_ 및 _global_ Approach에서 attentional decisions은 **독립적**으로 계산되며 이용된다.     
* 이는 기존 기계번역이 **이전 stat에 대한 정보를 이용하여 새로운 시점 정보를 예측**하는 것에 비해 정보 손실을 야기할 수 있다.     
   * 표준 MT에서는 번역 과정 중에, 어떤 **원본 단어가 번역되었는지 추적**하기 위해 _coverage_ 세트가 유지되는 경우가 많다.     
   * 마찬가지로, attentional NMT에서 alignment 결정은 **과거 alignment 정보를 고려**하여 공동으로 이루어져야 한다.      
   
**[Input-feeding Approach의 도입]**       
* 이를 해결하기 위해, 번역 시 새로운 토큰을 입력할 때, **Decoder의 이전 시점 state 정보를 concat하여 입력**한다.    
* 즉 아래 Figure 4와 같이 다음 단계에서 attention 벡터 **&&h˜_t$$가 입력과 연결**되는 **Input-feeding Approach**을 제안한다.     
* 이 방법의 효과 효과는 두 가지이다. 
   * (a) 모델이 **이전 시점의 alignment 선택 정보를 반영**      
   * (b) **수평/수직으로 확장**된 deep network를 구축 가능     


![image](https://user-images.githubusercontent.com/76824611/216458499-47aacdfc-eca1-4264-8b19-ba8e7bc28b65.png)
**Figure 4**: Input-feeding approach   
$$Attentional vectors h˜_t$$은 다음 time steps에 대한 입력으로 제공됨     
이를 통해 모델에게 과거 alignment 결정에 대해 알릴 수 있음    



----
----

# **4. Experiments**
![image](https://user-images.githubusercontent.com/76824611/216456243-cec75941-2dc1-4e0a-9b9e-74df89421733.png)
![image](https://user-images.githubusercontent.com/76824611/216458545-587d3715-db46-4d17-b9b7-4d07587ef4aa.png)
영문- 독일어 번역 task에서 SoTA 달성

