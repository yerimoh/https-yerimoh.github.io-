---
title: "New Intent Discovery with Pre-training and Contrastive Learning 정리"
date:   2022-08-31
excerpt: "Between words and characters:A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---


   

# 원 논문
[Enriching Word Vectors with Subword Information](https://arxiv.org/abs/2112.10508)


# 소스코드
[git adress](https://github.com/ zhang-yu-wei/MTP-CLNN)


---

# Abstract
새로운 의도(intent) 발견은 지원되는 **의도(intent) 클래스 세트**를 확장하기 위해,       
**사용자 발화에서 새로운 의도 범주를 발견**하는 것을 목표로 한다.       

이 발견은 실질적인 대화 시스템의 개발과 서비스 확대를 위한 중요한 과제이다.   

이러한 중요성에도 불구하고, 이 문제는 여전히 문헌에서 충분히 연구되지 않고 있다.  

**[기존 접근 방식의 문제]**        
* 일반적으로 레이블이 지정된 발화(labeled utterances)에 의존     
* 표현 학습 및 클러스터링을 위해 pseudo-labeling 방법을 사용 ➡ 이는 레이블 집약적이고 비효율적이며 부정확하다.     



**[본 논문의 해결책]**    
* 우리는 새로운 의도 발견을 위한 두 가지 중요한 연구 질문에 대한 새로운 해결책을 제공한다:    
* **(1)** 의미론적 발화 표현을 배우는 방법     
* **(2)** 발화를 더 잘 클러스터링하는 방법      
* 특히, 우리는 먼저 표현 학습을 위해 외부 레이블이 지정된 데이터와 함께 레이블이 지정되지 않은 풍부한 데이터를 활용하는 multi-task pre-training 전략을 제안한다.     
* 그런 다음 클러스터링을 위해 레이블이 지정되지 않은 데이터에서 self-supervisory 신호를 이용하기 위해 새로운 대조 손실(contrastive loss)을 설계한다.    
* 세 가지 의도 인식 벤치마크에 대한 광범위한 실험은 제안된 방법의 높은 효과를 입증하는데, 이는 비지도 및 준지도 시나리오 모두에서 최첨단 방법을 큰 폭으로 능가한다. 


---
---


# **1 Introduction**

## Why Study New Intent Discovery (NID)      
최근 몇 년 동안 대화형 AI 애플리케이션의 급속한 성장을 목격했다.   

**[문제]**      
자연어 이해 시스템(NLU)을 설계하기 위해, 의도 인식 모델(intent recognition model)을 훈련시키기 위해 **미리** **예상 고객의 의도 세트(predefined intents)를 수집**한다.    
그러나 미리 정의된 의도(predefined intents)는 고객의 요구를 완전히 충족시킬 수 없다.      
➡ 이는 레이블이 지정되지 않은 사용자 발화에서 발견된 새로운 의도를 반복적으로 통합하여 **의도 인식 모델을 확장할 필요성**을 시사한다
![image](https://user-images.githubusercontent.com/76824611/187633563-3b928c35-4da8-42d9-b82b-b4f7be06c397.png)


**[해결책]**      
다수의 발화에서 알려지지 않은 의도를 수동으로 식별하는 노력을 줄이기 위해, 이전 연구는 일반적으로 유사한 의도를 가진 발화를 그룹화하기 위해 clustering algorithms을 사용한다(Cheung and Li, 2012; Hakkani-Tür et al., 2015; Padmasundari, 2018).     
➡ 이후 클러스터 할당은 **새로운 의도 레이블로 직접 사용**하거나 **더 빠른 주석을 위한 휴리스틱**으로 사용가능함



----


## Research Questions (RQ) and Challenges

NID centers에 대한 현재 연구는 두 가지 기본 연구 질문을 중심으로 이루어진다.     

**1)** 클러스터링을 위한 적절한 단서를 제공하기 위해 **의미론적 발화 표현을 배우는** 방법?    
**2)** 발화를 **더 잘 클러스터링**하는 방법은 무엇인가?   

그 두 질문에 대한 연구는 종종 기존 연구에서 서로 얽혀 있다.    
발화는 다양한 요소에 따라 표현된다 (언어의 스타일, 관련된 주제 또는 심지어 문장의 길이와 같은 다양한 측면)     

**[사전 연구의 한계]**    
클러스터링에 대한 적절한 단서를 제공하기 위해 **의미론적 발성(semantic utterance) 표현을 배우는 것**이 중요하다.     
* **vanilla pre-trained language model:**     
    * 발성 표현을 생성하기 위해 vanilla pre-trained language model (PLM)을 단순히 적용하는 것은 실행 가능한 해결책이 아님      
    * 이는 4.2절의 실험 결과에서 알 수 있듯이 NID에서 성능이 저하된다.      
* **known intents의 labeled utterances 사용:**     
    * 일부 최근 연구는 표현 학습(representation learning)을 위해 known intents의 labeled utterances 사용할 것을 제안했지만,     
      특히 개발의 초기단계에서 그들은 상당한 양의 이미 알려진 의도와 항상 사용 가능한 각 의도의 **이미 충분히 레이블링된 발화**를 필요로 한다.          
      (Forman 등, 2015; Haponchyk 등, 2018; Lin 등, 2020; Zhang 등, 2021c; Haponchyk 및 Moschitti, 2021)
* **pseudo-labeling 접근 방식:**        
    * 또한 pseudo-labeling 접근 방식은 표현 학습 및 클러스터링을 위한 감독 신호(supervision signals)를 생성하기 위해 종종 사용된다.      
    * **Lin et al. (2020):** 레이블이 지정된 발화에 대한 **발화 유사성 예측 작업**(utterance similarity prediction task)으로,      
    vanilla pre-trained language model(PLM)을 미세 조정하여 **유사 레이블이 있는 레이블이 없는 데이터를 train**한다.      
    * **Zhang et al.(2021c):**은 k-means 클러스터링을 사용하여 **pseudo-labels을 생성하는 심층 클러스터링 방법**(Caron et al., 2018)을 채택한다.          
    ➡ 그러나 pseudo-labels은 종종 노이즈가 많고 오류 전파를 초래할 수 있다.      









---



## Our Solutions    


 In this work, we propose a simple yet effective solution for each research question.
Solution to RQ 1: multi-task pre-training. We
propose a multi-task pre-training strategy that takes
advantage of both external data and internal data
for representation learning. Specifically, we leverage publicly available, high-quality intent detection
datasets, following Zhang et al. (2021d), as well
as the provided labeled and unlabeled utterances
in the current domain, to fine-tune a pre-trained
PLM to learn task-specific utterance representations for NID. The multi-task learning strategy enables knowledge transfer from general intent detection tasks and adaptation to a specific application
domain. Solution to RQ 2: contrastive learning
with nearest neighbors. We propose to use a contrastive loss to produce compact clusters, which is
motivated by the recent success of contrastive learning in both computer vision (Bachman et al., 2019;
He et al., 2019; Chen et al., 2020; Khosla et al.,
2020) and natural language processing (Gunel et al.,
2021; Gao et al., 2021; Yan et al., 2021). Contrastive learning usually maximizes the agreement
between different views of the same example and
minimize that between different examples. However, the commonly used instance discrimination
task may push away false negatives and hurts the
clustering performance. Inspired by a recent work
in computer vision (Van Gansbeke et al., 2020), we
introduce neighborhood relationship to customize
the contrastive loss for clustering in both unsupervised (i.e., without any labeled utterances of known
intents) and semi-supervised scenarios. Intuitively,
in a semantic feature space, neighboring utterances
should have a similar intent, and pulling together
neighboring samples makes clusters more compact.
Our main contributions are three-fold.
• We show that our proposed multi-task pretraining method already leads to large performance gains over state-of-the-art models for
both unsupervised and semi-supervised NID.
• We propose a self-supervised clustering
method for NID by incorporating neighborhood relationship into the contrastive learning
objective, which further boosts performance.
• We conduct extensive experiments and ablation studies on three benchmark datasets to
verify the effectiveness of our methods.









