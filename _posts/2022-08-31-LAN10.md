---
title: "New Intent Discovery with Pre-training and Contrastive Learning 정리"
date:   2022-08-31
excerpt: "Between words and characters:A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---


   

# 원 논문
[Enriching Word Vectors with Subword Information](https://arxiv.org/abs/2112.10508)

**[소스코드]**     
* [git adress](https://github.com/ zhang-yu-wei/MTP-CLNN)


**[사전 학습]**
* 읽기 전 아래의 포스트들을 읽어야 무슨소린지 알아듣기 편하다..!   
* [contrastive learning](https://daebaq27.tistory.com/97)    

---

# Abstract
새로운 의도(intent) 발견은 지원되는 **의도(intent) 클래스 세트**를 확장하기 위해,       
**사용자 발화에서 새로운 의도 범주를 발견**하는 것을 목표로 한다.       

이 발견은 실질적인 대화 시스템의 개발과 서비스 확대를 위한 중요한 과제이다.   

이러한 중요성에도 불구하고, 이 문제는 여전히 문헌에서 충분히 연구되지 않고 있다.  

**[기존 접근 방식의 문제]**        
* 일반적으로 레이블이 지정된 발화(labeled utterances)에 의존     
* 표현 학습 및 클러스터링을 위해 pseudo-labeling 방법을 사용 ➡ 이는 레이블 집약적이고 비효율적이며 부정확하다.     



**[본 논문의 해결책]**    
* 우리는 새로운 의도 발견을 위한 두 가지 중요한 연구 질문에 대한 새로운 해결책을 제공한다:    
* **(1)** 의미론적 발화 표현을 배우는 방법     
* **(2)** 발화를 더 잘 클러스터링하는 방법      
* 특히, 우리는 먼저 표현 학습을 위해 외부 레이블이 지정된 데이터와 함께 레이블이 지정되지 않은 풍부한 데이터를 활용하는 multi-task pre-training 전략을 제안한다.     
* 그런 다음 클러스터링을 위해 레이블이 지정되지 않은 데이터에서 self-supervisory 신호를 이용하기 위해 새로운 대조 손실(contrastive loss)을 설계한다.    
* 세 가지 의도 인식 벤치마크에 대한 광범위한 실험은 제안된 방법의 높은 효과를 입증하는데, 이는 비지도 및 준지도 시나리오 모두에서 최첨단 방법을 큰 폭으로 능가한다. 


---
---


# **1 Introduction**

## Why Study New Intent Discovery (NID)      
최근 몇 년 동안 대화형 AI 애플리케이션의 급속한 성장을 목격했다.   

**[문제]**      
자연어 이해 시스템(NLU)을 설계하기 위해, 의도 인식 모델(intent recognition model)을 훈련시키기 위해 **미리** **예상 고객의 의도 세트(predefined intents)를 수집**한다.    
그러나 미리 정의된 의도(predefined intents)는 고객의 요구를 완전히 충족시킬 수 없다.      
➡ 이는 레이블이 지정되지 않은 사용자 발화에서 발견된 새로운 의도를 반복적으로 통합하여 **의도 인식 모델을 확장할 필요성**을 시사한다
![image](https://user-images.githubusercontent.com/76824611/187633563-3b928c35-4da8-42d9-b82b-b4f7be06c397.png)


**[해결책]**      
다수의 발화에서 알려지지 않은 의도를 수동으로 식별하는 노력을 줄이기 위해, 이전 연구는 일반적으로 유사한 의도를 가진 발화를 그룹화하기 위해 clustering algorithms을 사용한다(Cheung and Li, 2012; Hakkani-Tür et al., 2015; Padmasundari, 2018).     
➡ 이후 클러스터 할당은 **새로운 의도 레이블로 직접 사용**하거나 **더 빠른 주석을 위한 휴리스틱**으로 사용가능함



----


## Research Questions (RQ) and Challenges

NID centers에 대한 현재 연구는 두 가지 기본 연구 질문을 중심으로 이루어진다.     

**1)** 클러스터링을 위한 적절한 단서를 제공하기 위해 **의미론적 발화 표현을 배우는** 방법?    
**2)** 발화를 **더 잘 클러스터링**하는 방법은 무엇인가?   

그 두 질문에 대한 연구는 종종 기존 연구에서 서로 얽혀 있다.    
발화는 다양한 요소에 따라 표현된다 (언어의 스타일, 관련된 주제 또는 심지어 문장의 길이와 같은 다양한 측면)     

**[사전 연구의 한계]**    
클러스터링에 대한 적절한 단서를 제공하기 위해 **의미론적 발성(semantic utterance) 표현을 배우는 것**이 중요하다.     
* **vanilla pre-trained language model:**     
    * 발성 표현을 생성하기 위해 vanilla pre-trained language model (PLM)을 단순히 적용하는 것은 실행 가능한 해결책이 아님      
    * 이는 4.2절의 실험 결과에서 알 수 있듯이 NID에서 성능이 저하된다.      
* **known intents의 labeled utterances 사용:**     
    * 일부 최근 연구는 표현 학습(representation learning)을 위해 known intents의 labeled utterances 사용할 것을 제안했지만,     
      특히 개발의 초기단계에서 그들은 상당한 양의 이미 알려진 의도와 항상 사용 가능한 각 의도의 **이미 충분히 레이블링된 발화**를 필요로 한다.          
      (Forman 등, 2015; Haponchyk 등, 2018; Lin 등, 2020; Zhang 등, 2021c; Haponchyk 및 Moschitti, 2021)
* **pseudo-labeling 접근 방식:**        
    * 또한 pseudo-labeling 접근 방식은 표현 학습 및 클러스터링을 위한 감독 신호(supervision signals)를 생성하기 위해 종종 사용된다.      
    * **Lin et al. (2020):** 레이블이 지정된 발화에 대한 **발화 유사성 예측 작업**(utterance similarity prediction task)으로,      
    vanilla pre-trained language model(PLM)을 미세 조정하여 **유사 레이블이 있는 레이블이 없는 데이터를 train**한다.      
    * **Zhang et al.(2021c):**은 k-means 클러스터링을 사용하여 **pseudo-labels을 생성하는 심층 클러스터링 방법**(Caron et al., 2018)을 채택한다.          
    ➡ 그러나 pseudo-labels은 종종 노이즈가 많고 오류 전파를 초래할 수 있다.      









---



## Our Solutions    

본 연구에서는 각 연구 질문에 대한 간단하면서도 효과적인 해결책을 제안한다.      

**[Solution to RQ 1: multi-task pre-training]**          
* representation learning을 위해 외부 데이터와 내부 데이터를 모두 활용하는 a multi-task pre-training 전략을 제안한다.      
* **방법**: NID에 대한 task-specific utterance representations을 학습하기 위해 pre-trained PLM을 fine-tune한다.      
   * **fine-tune 방법 1:** 본 논문에선 Zhang et al. (2021d)에 이어 공개적으로 사용 가능한 고품질 의도 탐지 데이터 세트( high-quality intent detection datasets) 이용          
   * **fine-tune 방법 2:** 현재 도메인에서 제공되는 레이블이 지정된 및 레이블이 지정되지 않은 발화를 활용 
* **효과:** multi-task 학습 전략은 일반적인 의도 탐지 작업에서 **특정 응용 프로그램 영역으로 지식을 전달**할 수 있도록 한다.     

**[Solution to RQ 2: contrastive learning with nearest neighbors]**         
* 본 논문에선 **대조적 손실을 사용하여 소형 클러스터(contrastive loss to produce compact clusters)를 생성**할 것을 제안한다.           
   * 이 학습은 컴퓨터 비전(Bachman et al., 2019; He et al., 2019; Chen et al., 2020; Khosla et al., 2020)과 자연어 처리(Gunel et al., 2021; Gao et al., 2021; Yan et al. 2021) 모두에서 최근 대조 학습의 성공에 동기를 부여할 정도로 유의미한 방법임.       
* **contrastive learning(대조 학습):**   
   * 사전에 정답 데이터를 구축하지 않는 판별 모델     
   * 학습된 표현 공간 상에서 **"비슷한" 데이터는 가깝게**, "다른" 데이터는 멀게 존재하도록 **표현 공간을 학습**         
   * 이후, classification 등 다양한 downstream task에 대해서 네트워크를 fine-tuning시키는 방향으로 활용함.    
   * **장점:**     
      * 데이터 구축 비용이 들지 않음     
      * 학습 과정에 있어서 보다 용이      
      * label이 없기 때문에 보다 일반적인 feature representation 가능      
      * 새로운 class가 들어와도 대응이 가능    
   * **단점:** 그러나 일반적으로 사용되는 인스턴스 식별 작업(instance discrimination task)은 잘못된 음성을 밀어내고 클러스터링 성능을 해칠 수 있다.           
   * 아래 두 개의 방법 모두에 따라 **클러스터링을 위한 contrastive loss을 맞춤화**하기 위한 **neighborhood relationship를 소개**한다.
     * **방법 1:** unsupervised(즉, 알려진 intents의 labeled utterances없이)     
     * **방법 2:** semi-supervised scenarios 
   * 직관적으로, 의미론적 특징 공간에서, 이웃한 발화는 유사한 의도를 가져야 하며, 이웃한 샘플을 함께 당기면 클러스터가 더 콤팩트해진다.      
* **본 논문의 3가지  main contributions**    
   * 우리는 제안된 multi-task pretraining 방법이 이미 unsupervised NID와 semi-supervised NID 모두에 대한 최첨단 모델에 비해 큰 성능 향상으로 이어진다는 것을 보여준다.     
   * 우리는 이웃 관계를 contrastive 학습 목표에 통합하여 NID에 대한 self-supervised clustering 방법을 제안하여 성능을 더욱 향상시킨다.       
   * 우리는 방법의 효과를 검증하기 위해 세 가지 벤치마크 데이터 세트에 대한 광범위한 실험과 절제 연구를 수행한다.     




----
----

# **2. Related Works**

New Intent Discovery. The study of NID is still
in an early stage. Pioneering works focus on unsupervised clustering methods. Shi et al. (2018) leveraged auto-encoder to extract features. Perkins and
Yang (2019) considered the context of an utterance
in a conversation. Chatterjee and Sengupta (2020)
proposed to improve density-based models. Some
recent works (Haponchyk et al., 2018; Haponchyk
and Moschitti, 2021) studied supervised clustering
algorithms for intent labeling, yet it can not handle
new intents. Another line of works (Forman et al.,
2015; Lin et al., 2020; Zhang et al., 2021c) investigated a more practical case where some known
intents are provided to support the discovery of
unknown intents, which is often referred to as semisupervised NID.
To tackle semi-supervised NID, Lin et al. (2020)
proposed to first perform supervised training on
known intents with a sentence similarity task and
then use pseudo labeling on unlabeled utterances
to learn a better embedding space. Zhang et al.
(2021c) proposed to first pre-train on known intents and then perform k-means clustering to assign pseudo labels on unlabeled data for representation learning following Deep Clustering (Caron
et al., 2018). They also proposed to align clusters
to accelerate the learning of top layers. Another
approach is to first classify the utterances as known
and unknown and then uncover new intents with
the unknown utterances (Vedula et al., 2020; Zhang
et al., 2021b). Hence, it relies on accurate classification in the first stage.
In this work, we address NID by proposing a
multi-task pre-training method for representation
learning and a contrastive learning method for clustering. In contrast to previous methods that rely
on ample annotated data in the current domain for
pre-training, our method can be used in an unsupervised setting and work well in data-scarce scenarios
(Section 4.3).








