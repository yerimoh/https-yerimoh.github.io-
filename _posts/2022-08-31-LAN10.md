---
title: "New Intent Discovery with Pre-training and Contrastive Learning 정리"
date:   2022-08-31
excerpt: "Between words and characters:A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---


   

# 원 논문
[Enriching Word Vectors with Subword Information](https://arxiv.org/abs/2112.10508)

**[소스코드]**     
* [git adress](https://github.com/ zhang-yu-wei/MTP-CLNN)


**[사전 학습]**
* 읽기 전 아래의 포스트들을 읽어야 무슨소린지 알아듣기 편하다..!   
* [contrastive learning](https://daebaq27.tistory.com/97)    

---

# Abstract
새로운 의도(intent) 발견은 지원되는 **의도(intent) 클래스 세트**를 확장하기 위해,       
**사용자 발화에서 새로운 의도 범주를 발견**하는 것을 목표로 한다.       

이 발견은 실질적인 대화 시스템의 개발과 서비스 확대를 위한 중요한 과제이다.   

이러한 중요성에도 불구하고, 이 문제는 여전히 문헌에서 충분히 연구되지 않고 있다.  

**[기존 접근 방식의 문제]**        
* 일반적으로 레이블이 지정된 발화(labeled utterances)에 의존     
* 표현 학습 및 클러스터링을 위해 pseudo-labeling 방법을 사용 ➡ 이는 레이블 집약적이고 비효율적이며 부정확하다.     



**[본 논문의 해결책]**    
* 우리는 새로운 의도 발견을 위한 두 가지 중요한 연구 질문에 대한 새로운 해결책을 제공한다:    
* **(1)** 의미론적 발화 표현을 배우는 방법     
* **(2)** 발화를 더 잘 클러스터링하는 방법      
* 특히, 우리는 먼저 표현 학습을 위해 외부 레이블이 지정된 데이터와 함께 레이블이 지정되지 않은 풍부한 데이터를 활용하는 multi-task pre-training 전략을 제안한다.     
* 그런 다음 클러스터링을 위해 레이블이 지정되지 않은 데이터에서 self-supervisory 신호를 이용하기 위해 새로운 대조 손실(contrastive loss)을 설계한다.    
* 세 가지 의도 인식 벤치마크에 대한 광범위한 실험은 제안된 방법의 높은 효과를 입증하는데, 이는 비지도 및 준지도 시나리오 모두에서 최첨단 방법을 큰 폭으로 능가한다. 


---
---


# **1 Introduction**

## Why Study New Intent Discovery (NID)      
최근 몇 년 동안 대화형 AI 애플리케이션의 급속한 성장을 목격했다.   

**[문제]**      
자연어 이해 시스템(NLU)을 설계하기 위해, 의도 인식 모델(intent recognition model)을 훈련시키기 위해 **미리** **예상 고객의 의도 세트(predefined intents)를 수집**한다.    
그러나 미리 정의된 의도(predefined intents)는 고객의 요구를 완전히 충족시킬 수 없다.      
➡ 이는 레이블이 지정되지 않은 사용자 발화에서 발견된 새로운 의도를 반복적으로 통합하여 **의도 인식 모델을 확장할 필요성**을 시사한다
![image](https://user-images.githubusercontent.com/76824611/187633563-3b928c35-4da8-42d9-b82b-b4f7be06c397.png)


**[해결책]**      
다수의 발화에서 알려지지 않은 의도를 수동으로 식별하는 노력을 줄이기 위해, 이전 연구는 일반적으로 유사한 의도를 가진 발화를 그룹화하기 위해 clustering algorithms을 사용한다(Cheung and Li, 2012; Hakkani-Tür et al., 2015; Padmasundari, 2018).     
➡ 이후 클러스터 할당은 **새로운 의도 레이블로 직접 사용**하거나 **더 빠른 주석을 위한 휴리스틱**으로 사용가능함


**[개요]**    
아래의 2가지 방법으로 NID를 해결
1) multi-task pre-training method     
2) 클러스터링을 위한 contrastive learning      


----


## Research Questions (RQ) and Challenges

NID centers에 대한 현재 연구는 두 가지 기본 연구 질문을 중심으로 이루어진다.     

**1)** 클러스터링을 위한 적절한 단서를 제공하기 위해 **의미론적 발화 표현을 배우는** 방법?    
**2)** 발화를 **더 잘 클러스터링**하는 방법은 무엇인가?   

그 두 질문에 대한 연구는 종종 기존 연구에서 서로 얽혀 있다.    
발화는 다양한 요소에 따라 표현된다 (언어의 스타일, 관련된 주제 또는 심지어 문장의 길이와 같은 다양한 측면)     

**[사전 연구의 한계]**    
클러스터링에 대한 적절한 단서를 제공하기 위해 **의미론적 발성(semantic utterance) 표현을 배우는 것**이 중요하다.     
* **vanilla pre-trained language model:**     
    * 발성 표현을 생성하기 위해 vanilla pre-trained language model (PLM)을 단순히 적용하는 것은 실행 가능한 해결책이 아님      
    * 이는 4.2절의 실험 결과에서 알 수 있듯이 NID에서 성능이 저하된다.      
* **known intents의 labeled utterances 사용:**     
    * 일부 최근 연구는 표현 학습(representation learning)을 위해 known intents의 labeled utterances 사용할 것을 제안했지만,     
      특히 개발의 초기단계에서 그들은 상당한 양의 이미 알려진 의도와 항상 사용 가능한 각 의도의 **이미 충분히 레이블링된 발화**를 필요로 한다.          
      (Forman 등, 2015; Haponchyk 등, 2018; Lin 등, 2020; Zhang 등, 2021c; Haponchyk 및 Moschitti, 2021)
* **pseudo-labeling 접근 방식:**        
    * 또한 pseudo-labeling 접근 방식은 표현 학습 및 클러스터링을 위한 감독 신호(supervision signals)를 생성하기 위해 종종 사용된다.      
    * **Lin et al. (2020):** 레이블이 지정된 발화에 대한 **발화 유사성 예측 작업**(utterance similarity prediction task)으로,      
    vanilla pre-trained language model(PLM)을 미세 조정하여 **유사 레이블이 있는 레이블이 없는 데이터를 train**한다.      
    * **Zhang et al.(2021c):**은 k-means 클러스터링을 사용하여 **pseudo-labels을 생성하는 심층 클러스터링 방법**(Caron et al., 2018)을 채택한다.          
    ➡ 그러나 pseudo-labels은 종종 노이즈가 많고 오류 전파를 초래할 수 있다.      









---



## Our Solutions    

본 연구에서는 각 연구 질문에 대한 간단하면서도 효과적인 해결책을 제안한다.      

**[Solution to RQ 1: multi-task pre-training]**          
* representation learning을 위해 외부 데이터와 내부 데이터를 모두 활용하는 a multi-task pre-training 전략을 제안한다.      
* **방법**: NID에 대한 task-specific utterance representations을 학습하기 위해 pre-trained PLM을 fine-tune한다.      
   * **fine-tune 방법 1:** 본 논문에선 Zhang et al. (2021d)에 이어 공개적으로 사용 가능한 고품질 의도 탐지 데이터 세트( high-quality intent detection datasets) 이용          
   * **fine-tune 방법 2:** 현재 도메인에서 제공되는 레이블이 지정된 및 레이블이 지정되지 않은 발화를 활용 
* **효과:** multi-task 학습 전략은 일반적인 의도 탐지 작업에서 **특정 응용 프로그램 영역으로 지식을 전달**할 수 있도록 한다.     

**[Solution to RQ 2: contrastive learning with nearest neighbors]**         
* 본 논문에선 **대조적 손실을 사용하여 소형 클러스터(contrastive loss to produce compact clusters)를 생성**할 것을 제안한다.           
   * 이 학습은 컴퓨터 비전(Bachman et al., 2019; He et al., 2019; Chen et al., 2020; Khosla et al., 2020)과 자연어 처리(Gunel et al., 2021; Gao et al., 2021; Yan et al. 2021) 모두에서 최근 대조 학습의 성공에 동기를 부여할 정도로 유의미한 방법임.       
* **contrastive learning(대조 학습):**   
   * 사전에 정답 데이터를 구축하지 않는 판별 모델     
   * 학습된 표현 공간 상에서 **"비슷한" 데이터는 가깝게**, "다른" 데이터는 멀게 존재하도록 **표현 공간을 학습**         
   * 이후, classification 등 다양한 downstream task에 대해서 네트워크를 fine-tuning시키는 방향으로 활용함.    
   * **장점:**     
      * 데이터 구축 비용이 들지 않음     
      * 학습 과정에 있어서 보다 용이      
      * label이 없기 때문에 보다 일반적인 feature representation 가능      
      * 새로운 class가 들어와도 대응이 가능    
   * **단점:** 그러나 일반적으로 사용되는 인스턴스 식별 작업(instance discrimination task)은 false negatives을 밀어내고 클러스터링 성능을 해칠 수 있다.           
   * 아래 두 개의 방법 모두에 따라 **클러스터링을 위한 contrastive loss을 맞춤화**하기 위한 **neighborhood relationship를 소개**한다.
     * **방법 1:** unsupervised(즉, 알려진 intents의 labeled utterances없이)     
     * **방법 2:** semi-supervised scenarios 
   * 직관적으로, 의미론적 특징 공간에서, 이웃한 발화는 유사한 의도를 가져야 하며, 이웃한 샘플을 함께 당기면 클러스터가 더 콤팩트해진다.      
* **본 논문의 3가지  main contributions**    
   * 우리는 제안된 multi-task pretraining 방법이 이미 unsupervised NID와 semi-supervised NID 모두에 대한 최첨단 모델에 비해 큰 성능 향상으로 이어진다는 것을 보여준다.     
   * 우리는 이웃 관계를 contrastive 학습 목표에 통합하여 NID에 대한 self-supervised clustering 방법을 제안하여 성능을 더욱 향상시킨다.       
   * 우리는 방법의 효과를 검증하기 위해 세 가지 벤치마크 데이터 세트에 대한 광범위한 실험과 절제 연구를 수행한다.     




----
----

# **2. Related Works**

## New Intent Discovery.    

NID에 대한 연구는 아직 초기 단계에 있다. 선구적인 작업은 unsupervised clustering 방법에 중점을 둔다.     

**[선행연구]**    
* **Shi et al. (2018):** 자동 인코더를 활용하여 기능을 추출.   
* **Perkins and Yang (2019):** 대화에서 발언의 맥락을 고려            
* **Chatterjee and Sengupta (2020):** 밀도 기반 모델(density-based models)을 개선할 것을 제안             
* **일부 최근 연구(Haponchyk et al., 2018; Haponchyk and Moschitti, 2021):** intent labeling을 위한  supervised clustering 알고리즘을 연구했지만 **새로운 의도를 처리할 수 없다**.         
* **다른 작업 라인(Forman et al., 2015; Lin et al., 2020; Zhang et al., 2021c):** 종종 semisupervised NID로 불리는 **알려지지 않은 intents의 발견을 지원**하기 위해 알려진 일부 intents가 제공되는 보다 실용적인 사례를 조사했다.   
* **Lin et al.(2020)**:  semi-supervised NID를 다루기 위해 먼저 문장 **유사성 작업**으로 **알려진 intents**에 대한 지도 교육을 수행한 다음 **레이블이 지정되지 않은 발화**에 대한 pseudo labeling을 사용하여 더 나은 임베딩 공간을 학습할 것을 제안      
* **Zhang et al.(2021c)**: 먼저 알려진 intents를 pre-train한 다음 k-means clustering을 수행하여 Deep Clustering에 이어 representation learning을 위해 레이블이 지정되지 않은 데이터에 pseudo labeling을 할당할 것을 제안했다      
* **Caron et al., (2018):** 최상위 계층의 학습을 가속화하기 위해 **클러스터를 정렬**할 것을 제안했다.     
* **Vedula et al., 2020; Zhang et al., 2021b:** 먼저 발화를 알려진 것과 알려지지 않은 것으로 분류한 다음 알려지지 않은 발화로 새로운 의도를 밝히는 것이다. 따라서, 그것은 **첫 번째 단계에서 정확한 분류에 의존**한다.      


**[본 연구]**      
본 연구에서, 우리는 표현 학습을 위한 multi-task pre-training method와 클러스터링을 위한 contrastive learning 방법을 제안하여 NID를 해결한다.    
pre-train을 위해 현재 도메인의 주석이 달린 충분한 데이터에 의존하는 이전 방법과 달리, 우리의 방법은 unsupervised한 환경에서 사용될 수 있고 data-scarce scenarios에서 잘 작동할 수 있다(섹션 4.3).     



------



## Pre-training for Intent Recognition.    
**[문제]**    
large-scale pre-trained 언어 모델의 효과에도 불구하고,     
pre-trained 데이터 세트와 대화 사이의 언어 행동의 고유한 불일치의 문제가 지속되고있다.        


**[사전 연구]**    
대부분의 이전 연구는 selfsupervised 방식으로 개방형 도메인 대화에 대한 pre-train을 제안했다      
최근 여러 작품에서 **관련 과제를 가진 pre-train**이 **intent 인식에 효과적**일 수 있다는 지적이 나왔다.     
* Zhang et al. (2020): intent 인식을 문장 유사성 과제로 공식화하고 자연어 추론(NLI) 데이터 세트에 대해 pre-train을 받았다.       
* Vulic et al.(2021); Zhang et al.(2021e): intent 탐지 작업에서 contrastive loss로 사전 훈련을 받았다.      


**[본 논문]**     
본 논문의 multi-task pre-training method는 현재 **도메인의 레이블이 없는 데이터**와 **공개적으로 사용 가능한 intent 데이터 세트**를 활용하여 few-shot intent 탐지 성능을 향상시키는 Zhang 외(2021d)에서 영감을 받았다.      
✨ 그러나 **레이블이 없는 발화의 자연스러운 존재**로 인해 본 논문이 제안한 방법이 NID에 적용하기에 더 적합하다고 주장한다.






-----


## Contrastive Representation Learning.   
Contrastive learning은 컴퓨터 비전에서 유망한 결과를 보여주었고 자연어 처리에서 인기를 얻었다.      


**[사전 연구]**    
* **일부 최근 연구:** 문장 임베딩을 학습하기 위해 unsupervised Contrastive learning을 사용했다    
구체적으로, contrastive loss이 비등방성(anisotropic) 임베딩 공간을 피할 수 있음을 보여주었다.    
* **Kim et al.(2021)**: BERT 표현의 품질을 향상시키기 위한 self-guided contrastive training을 제안했다.    
* **Giorgi et al.(2021):** 인근 문장에서 무작위로 샘플링된 텍스트 세그먼트를 대조하여 universal sentence encoder를 사전 교육할 것을 제안했다.     
* **Zhang 외(2021e):** self-supervised contrastive pre-training과 supervised contrastive fine-tuning이 few-shot intent 인식에 도움이 될 수 있음을 입증했다.    
* **Zhang et al. (2021a):** contrastive loss을 클러스터링 목표와 결합하면 짧은 텍스트 클러스터링을 개선할 수 있음을 보여주었다.    


<details>
<summary>📜 anisotropic 설명 보기</summary>
<div markdown="1">
   
**Isotropic:** 벡터가 주어진 공간에서 모든 방향으로 존재하는 상황을 의미한다. 임베딩에 대입해보면 주어진 보캡의 임베딩 벡터들이 사방팔방으로 향해 있는 상황이다.


**anisotropic:** 벡터가 주어진 공간에서 특정 방향으로만 향해 있어서 일종의 cone 형태를 이루고 있는 것을 말한다  


**이상적인** 상황이라면 임베딩 벡터는 **Isotropic**해야 한다.    
그래야 각 단어의 의미가 명확히 구분되고, 유사한 단어는 서로 가깝게, 영 뚱딴지인 두 단어는 서로 멀리 분포하게 될 것이다. 만약 Anisotropic하게 임베딩 벡터가 분포해 있다면, 단어의 의미/문법적 유사도와 관계없이 모든 단어들이 가깝게 분포하기 때문에 각 단어의 의미가 명확히 구분되지 않을 우려가 있다.   
![image](https://user-images.githubusercontent.com/76824611/187747971-fbdb3733-a539-4858-a2ef-6c6dcc4a1488.png)

[출처](https://velog.io/@stapers/Contextual-Embedding-How-Contextual-are-Contextualized-Word-Representations)

  
</div>
</details>  

**[본 논문]**     
본 논문이 제안한 contrastive loss는 클러스터링에 맞게 조정되어,     
**유사한 의미론**을 가진 **발화**를 함께 **그룹화**하고     
**기존의 대조 손실**에서처럼 **false negatives을 밀어내는 것을 피한다**.    



-----
-----

# **3 Method**    



## Problem Statement. 
To develop an intent recognition model, we usually prepare a set of expected
intents Ck along with a few annotated utterances
Dlabeled
known = {(xi
, yi)|yi ∈ Ck} for each intent. After deployed, the system will encounter utterances
Dunlabeled = {xi
|yi ∈ {Ck, Cu}} from both predefined (known) intents Ck and unknown intents
Cu. The aim of new intent discovery (NID) is
to identify the emerging intents Cu in Dunlabeled
.
NID can be viewed as a direct extension of out-ofdistribution (OOD) detection, where we not only
need to identify OOD examples but also discover
the underlying clusters. NID is also different from
zero-shot learning in that we do not presume access to any kind of class information during training. In this work, we consider both unsupervised
and semi-supervised NID, which are distinguished
by the existence of Dlabeled
known , following Zhang et al.
(2021c)
 
