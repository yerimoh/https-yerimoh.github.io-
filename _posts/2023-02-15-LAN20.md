---
title: "Specializing Multi-domain NMT via Penalizing Low Mutual Information 정리"
date:   2023-02-15
excerpt: "Specializing Multi-domain NMT via Penalizing Low Mutual Information paper review"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---



# Abstract

<span style="background-color:#F5F5F5">**[multi-domain NMT의 중요성]**</span>         
Multi-domain Neural Machine Translation(NMT)은 **여러 domains**으로 **single model을 훈련**시킨다.              
이와 같이 훈련시키면, 한 model 내에서 여러 domains을 처리할 수 있어 매우 효과적이다.     

<span style="background-color:#F5F5F5">**[multi-domain NMT의 한계]**</span>      
이상적인 Multi-domain NMT는 고유한 domain 특성들을 동시에 학습해야 하지만,   
**domain 특성을 파악하는 것**은 매우 중요하며, 어려운 작업(non-trivial task)이다.      


<span style="background-color:#F5F5F5">**[본 논문의 목표]**</span>      

본 논문에서는 <span style="background-color:#FFE6E6">**mutual information(MI)의 렌즈**</span>를 통해 **domain별 정보를 조사**하고,    
<span style="background-color:#FFE6E6">**낮은 MI가 더 높아지도록 패널티를 주는**</span> 새로운 목표를 제안한다.      

본 논문의 방법은 현재 경쟁력 있는 multi-domain NMT 모델 중에서 SOTA를 달성했다.     
또한, 본 논문은 **low MI를 더 높게 promote**하여 **domain-specialized multi-domain NMT를 초래**한다는 것을 경험적으로 보여준다.          


# 1 Introduction
Multi-domain Neural Machine Translation (NMT)
(Sajjad et al., 2017; Farajian et al., 2017) has been
an attractive topic due to its efficacy in handling
multiple domains with a single model. Ideally,
a multi-domain NMT should capture both general knowledge (e.g., sentence structure, common
words) and domain-specific knowledge (e.g., domain terminology) unique in each domain. While
the shared knowledge can be easily acquired via
sharing parameters across domains (Kobus et al.,
2017), obtaining domain specialized knowledge
is a challenging task. Haddow and Koehn (2012)
demonstrate that a model trained on multiple domains sometimes underperforms the one trained
on a single domain. Pham et al. (2021) shows that
separate domain-specific adaptation modules are
not sufficient to fully-gain specialized knowledge.


In this paper, we reinterpret domain specialized
knowledge from mutual information (MI) perspective and propose a method to strengthen it. Given
a source sentence X, target sentence Y , and corresponding domain D, the MI between D and the
translation Y |X (i.e., MI(D; Y |X)) measures the
dependency between the domain and the translated sentence. Here, we assume that the larger
MI(D; Y |X), the more the translation incorporates
domain knowledge. Low MI is undesirable because
it indicates the model is not sufficiently utilizing domain characteristics in translation. In other words,
low MI can be interpreted as a domain-specific information the model has yet to learn. For example,
as shown in Fig. 1, we found that a model with low
MI translates an IT term ‘computing totals’ to the
vague and plain term ‘calculation’. However, once
we force the model to have high MI, ‘computing
totals’ is correctly retained in its translation. Thus,
maximizing MI promotes multi-domain NMT to
be domain-specialized.


Motivated by this idea, we introduce a new
method that specializes multi-domain NMT by
penalizing low MI. We first theoretically derive
MI(D; Y |X), and formulate a new objective that
weights more penalty on subword-tokens with low
MI. Our results show that the proposed method improves the translation quality in all domains. Also,
the MI visualization ensures that our method is effective in maximizing MI. We also observed that
our model performs particularly better on samples
with strong domain characteristics

The main contributions of our paper are as follows:     

• We investigate MI in multi-domain NMT and
present a new objective that penalizes low MI
to have higher value.   

• Extensive experiment results prove that our
method truly yields high MI, resulting in
domain-specialized model.

