---
title: "Specializing Multi-domain NMT via Penalizing Low Mutual Information 정리"
date:   2023-02-15
excerpt: "Specializing Multi-domain NMT via Penalizing Low Mutual Information paper review"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---



# Abstract
Multi-domain Neural Machine Translation
(NMT) trains a single model with multiple domains. It is appealing because of its efficacy in
handling multiple domains within one model.
An ideal multi-domain NMT should learn distinctive domain characteristics simultaneously,
however, grasping the domain peculiarity is
a non-trivial task. In this paper, we investigate domain-specific information through the
lens of mutual information (MI) and propose a
new objective that penalizes low MI to become
higher. Our method achieved the state-of-theart performance among the current competitive multi-domain NMT models. Also, we empirically show our objective promotes low MI
to be higher resulting in domain-specialized
multi-domain NMT.

다중 도메인 신경 기계 번역(NMT)은 여러 도메인으로 단일 모델을 훈련시킨다. 한 모델 내에서 여러 도메인을 처리하는 효과 때문에 매력적이다. 이상적인 다중 도메인 NMT는 고유한 도메인 특성을 동시에 학습해야 하지만 도메인 특성을 파악하는 것은 중요하지 않은 작업이다. 본 논문에서는 상호 정보(MI)의 렌즈를 통해 도메인별 정보를 조사하고 낮은 MI가 더 높아지도록 불이익을 주는 새로운 목표를 제안한다. 우리의 방법은 현재 경쟁력 있는 다중 도메인 NMT 모델 중에서 최첨단 성능을 달성했다. 또한, 우리는 우리의 목표가 낮은 MI를 더 높게 촉진하여 도메인 특화 다중 도메인 NMT를 초래한다는 것을 경험적으로 보여준다.


# 1 Introduction
Multi-domain Neural Machine Translation (NMT)
(Sajjad et al., 2017; Farajian et al., 2017) has been
an attractive topic due to its efficacy in handling
multiple domains with a single model. Ideally,
a multi-domain NMT should capture both general knowledge (e.g., sentence structure, common
words) and domain-specific knowledge (e.g., domain terminology) unique in each domain. While
the shared knowledge can be easily acquired via
sharing parameters across domains (Kobus et al.,
2017), obtaining domain specialized knowledge
is a challenging task. Haddow and Koehn (2012)
demonstrate that a model trained on multiple domains sometimes underperforms the one trained
on a single domain. Pham et al. (2021) shows that
separate domain-specific adaptation modules are
not sufficient to fully-gain specialized knowledge.


In this paper, we reinterpret domain specialized
knowledge from mutual information (MI) perspective and propose a method to strengthen it. Given
a source sentence X, target sentence Y , and corresponding domain D, the MI between D and the
translation Y |X (i.e., MI(D; Y |X)) measures the
dependency between the domain and the translated sentence. Here, we assume that the larger
MI(D; Y |X), the more the translation incorporates
domain knowledge. Low MI is undesirable because
it indicates the model is not sufficiently utilizing domain characteristics in translation. In other words,
low MI can be interpreted as a domain-specific information the model has yet to learn. For example,
as shown in Fig. 1, we found that a model with low
MI translates an IT term ‘computing totals’ to the
vague and plain term ‘calculation’. However, once
we force the model to have high MI, ‘computing
totals’ is correctly retained in its translation. Thus,
maximizing MI promotes multi-domain NMT to
be domain-specialized.


Motivated by this idea, we introduce a new
method that specializes multi-domain NMT by
penalizing low MI. We first theoretically derive
MI(D; Y |X), and formulate a new objective that
weights more penalty on subword-tokens with low
MI. Our results show that the proposed method improves the translation quality in all domains. Also,
the MI visualization ensures that our method is effective in maximizing MI. We also observed that
our model performs particularly better on samples
with strong domain characteristics

The main contributions of our paper are as follows:     

• We investigate MI in multi-domain NMT and
present a new objective that penalizes low MI
to have higher value.   

• Extensive experiment results prove that our
method truly yields high MI, resulting in
domain-specialized model.

