---
title: "Specializing Multi-domain NMT via Penalizing Low Mutual Information 정리"
date:   2023-02-15
excerpt: "Specializing Multi-domain NMT via Penalizing Low Mutual Information paper review"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---



# Abstract

<span style="background-color:#F5F5F5">**[multi-domain NMT의 중요성]**</span>         
Multi-domain Neural Machine Translation(NMT)은 **여러 domains**으로 **single model을 훈련**시킨다.              
이와 같이 훈련시키면, 한 model 내에서 여러 domains을 처리할 수 있어 매우 효과적이다.     

<span style="background-color:#F5F5F5">**[multi-domain NMT의 한계]**</span>      
이상적인 Multi-domain NMT는 고유한 domain 특성들을 동시에 학습해야 하지만,   
**domain 특성을 파악하는 것**은 매우 중요하며, 어려운 작업(non-trivial task)이다.      


<span style="background-color:#F5F5F5">**[본 논문의 목표]**</span>      
본 논문에서는 <span style="background-color:#FFE6E6">**mutual information(MI)의 렌즈**</span>를 통해 **domain별 정보를 조사**하고,    
<span style="background-color:#FFE6E6">**낮은 MI가 더 높아지도록 패널티를 주는**</span> 새로운 목표를 제안한다.      

본 논문의 방법은 현재 경쟁력 있는 multi-domain NMT 모델 중에서 SOTA를 달성했다.     
또한, 본 논문은 **low MI를 더 높게 promote**하여 **domain-specialized multi-domain NMT를 초래**한다는 것을 경험적으로 보여준다.          


# 1 Introduction
<span style="background-color:#F5F5F5">**[multi-domain NMT]**</span>        
Multi-domain Neural Machine Translation (NMT)은 **single 모델로 여러 domain을 처리**할 수 있어 인해 매력적인 주제였다.      
이상적으로는 multi-domain NMT는 아래의 두가지 지식을 모두 알아야 한다.     
* **general knowledge (e.g., sentence structure, common words)**: 여러 domain의 공통 지식 (domain 간 parameter를 공유하면 얻기 **쉬움**)             
* **domain-specific knowledge (e.g., domain terminology)**: 각 도메인에서의 specific한 지식 (**어렵**)           



<span style="background-color:#F5F5F5">**[multi-domain NMT 학습 한계]**</span>        
* Haddow와 Koehn(2012)은 multi-domain NMT이 **single-domain NMT보다 성능이 떨어지는 경우**가 있음을 보여준다.         
* Pham et al.(2021)은 separate domain-specific adaptation modules이 **전문 지식을 완전히 얻기에 충분하지 않음**을 보여준다.             



     
<span style="background-color:#F5F5F5">**[기존 domain-specialized 정도 측정 방법]**</span>        
본 논문에서는 domain 전문 지식을 **mutual information(MI) 관점에서 재해석**하고, 이를 **강화하는 방법**을 제안한다.       
<center>**$$MI(D; Y \|X)$$**</center>        
* **역할:** domain과 번역된 문장 사이의 의존성을 측정한다.                 
* **X:** source sentence          
* **Y:** target sentence      
* **D:** corresponding domain     
* **$$MI(D; Y \|X)$$**: D 와 translation $$Y\|X$$ 사이의 MI   

여기서, 우리는 <span style="background-color:#fff5b1">**$$MI(D; Y \|X)$$** 가 **클수록** **번역이 domain 지식을 더 많이 통합**한다(**domain-specific knowledge를 잘 학습**했다.)고 가정</span>한다.      

**low MI는 모델**이 번역에서 **domain 특성을 충분히 활용하고 있지 않다**는 것을 나타내기 때문에 **바람직하지 않다**.     
➡ 즉, low MI는 모델은 domain-specific를 잘 알지 못한다고 해석될 수 있다.       

예를 들어 IT 용어 ‘computing totals’ 번역을 보자,           
* **low MI model(도메인에  지식 습득률 낮음):** "calculation"으로 번역(IT분야에 맞지않는 너무 general한 단어이다)        
* **high MI(도메인 별 지식 습득률 낮음):** ‘computing totals’는 번역에서 올바르게 유지된다 (IT분야에 맞는 해석임).     

✨ 따라서 **MI를 최대화**하면, multi-domain NMT이 **domain-specialized** 된다.     



<span style="background-color:#F5F5F5">**[본 논문의 목표]**</span>      
위의 아이디어에 자극을 받아 <span style="background-color:#fff5b1">**low MI를 페널티화**하여 multi-domain NMT를 **specializes**하는 새로운 방법을 소개</span>한다.       

먼저 **$$MI(D; Y \|X)$$를 이론적으로 도출**하고,   
**low MI를 가진 subword-tokens에 더 많은 페널티**를 부여하는 **새로운 목표를 공식화**한다.       


본 논문의 결과는,   
* 제안된 방법이 모든 도메인에서 번역 품질을 향상시킨다는 것을 보여준다.     
* MI 시각화는 우리의 방법이 MI를 최대화하는 데 효과적임을 보장한다.    
* 본 논문은 본 논문의 모델이 강한 도메인 특성을 가진 샘플에서 특히 더 잘 수행된다는 것을 관찰했다.       



<span style="background-color:#F5F5F5">**[본 논문의 main contributions]**</span>      
* 본 논문은 multi-domain NMT에서 MI를 조사하고 **low MI가 더 높은 가치를 갖도록 패널티를 주는** 새로운 목표를 제시한다.    
* 광범위한 실험 결과는 우리의 방법이 실제로 **high MI를 산출**하여 multi-domain을 생성한다는 것을 증명한다.





----
----


# 2 Related Works
Multi-Domain Neural Machine Translation  

Multi-Domain NMT focuses on developing a
proper usage of domain information to improve
translation. Early studies had two main approaches:
injecting source domain information and adding
a domain classifier. For adding source domain information, Kobus et al. (2017) inserts a source domain label as an additional tag with input or as a
complementary feature. For the second approach,
Britz et al. (2017) trains the sentence embedding to
be domain-specific by updating using the gradient
from the domain-classifier


While previous work leverages domain information by injection or implementing an auxiliary
classifier, we view domain information from MI
perspective and propose a loss that promotes model
to explore domain specific knowledge




Information-Theoretic Approaches in NMT   

Mutual information in NMT is primarily used either as metrics or a loss function. For metrics,
Bugliarello et al. (2020) proposes cross-mutual information (XMI) to quantify the difficulty of translating between languages. Fernandes et al. (2021)
modifies XMI to measure the usage of the given
context during translation. For the loss function, Xu
et al. (2021) proposes bilingual mutual information
(BMI) which calculates the word mapping diversity, further applied in NMT training. Zhang et al.
(2022) improves the model translation by maximizing the MI between a target token and its source
sentence based on its context.



Above work only considers general machine
translation scenarios. Our work differs in that
we integrate mutual information in multi-domain
NMT to learn domain-specific information. Unlike
other methods that require training of an additional
model, our method can calculate MI within a single
model which is more computation-efficient.










