---
title: "Specializing Multi-domain NMT via Penalizing Low Mutual Information 정리"
date:   2023-02-15
excerpt: "Specializing Multi-domain NMT via Penalizing Low Mutual Information paper review"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---



# Abstract

<span style="background-color:#F5F5F5">**[multi-domain NMT의 중요성]**</span>         
* Multi-domain Neural Machine Translation(NMT)은 **여러 domains**으로 **single model을 훈련**시킨다.              
이와 같이 훈련시키면, 한 model 내에서 여러 domains을 처리할 수 있어 매우 효과적이다.     

<span style="background-color:#F5F5F5">**[multi-domain NMT의 한계]**</span>      
* 이상적인 Multi-domain NMT는 고유한 domain 특성들을 동시에 학습해야 하지만,   
**domain 특성을 파악하는 것**은 매우 중요하며, 어려운 작업(non-trivial task)이다.      


<span style="background-color:#F5F5F5">**[본 논문의 목표]**</span>      
* 본 논문에서는 <span style="background-color:#FFE6E6">**mutual information(MI)의 렌즈**</span>를 통해 **domain별 정보를 조사**하고,    
<span style="background-color:#FFE6E6">**낮은 MI가 더 높아지도록 패널티를 주는**</span> 새로운 목표를 제안한다.      


본 논문의 방법은 현재 경쟁력 있는 multi-domain NMT 모델 중에서 SOTA를 달성했다.     
또한, 본 논문은 **low MI를 더 높게 promote**하여 **domain-specialized multi-domain NMT를 초래**한다는 것을 경험적으로 보여준다.          

---
---

# 1 Introduction
<span style="background-color:#F5F5F5">**[multi-domain NMT]**</span>        
Multi-domain Neural Machine Translation (NMT)은 **single 모델로 여러 domain을 처리**할 수 있어 인해 매력적인 주제였다.      
이상적으로는 multi-domain NMT는 아래의 두가지 지식을 모두 알아야 한다.     
* **general knowledge (e.g., sentence structure, common words)**: 여러 domain의 공통 지식 (domain 간 parameter를 공유하면 얻기 **쉬움**)             
* **domain-specific knowledge (e.g., domain terminology)**: 각 도메인에서의 specific한 지식 (**어렵**)           

---

<span style="background-color:#F5F5F5">**[multi-domain NMT 학습 한계]**</span>        
* Haddow와 Koehn(2012)은 multi-domain NMT이 **single-domain NMT보다 성능이 떨어지는 경우**가 있음을 보여준다.         
* Pham et al.(2021)은 separate domain-specific adaptation modules이 **전문 지식을 완전히 얻기에 충분하지 않음**을 보여준다.             


---
     
<span style="background-color:#F5F5F5">**[기존 domain-specialized 정도 측정 방법]**</span>        
본 논문에서는 domain 전문 지식을 **mutual information(MI) 관점에서 재해석**하고, 이를 **강화하는 방법**을 제안한다.       
<center>**$$MI(D; Y \|X)$$**</center>        
* **역할:** domain과 번역된 문장 사이의 의존성을 측정한다.                 
* **X:** source sentence          
* **Y:** target sentence      
* **D:** corresponding domain     
* **$$MI(D; Y \|X)$$**: D 와 translation $$Y\|X$$ 사이의 MI   

여기서, 우리는 <span style="background-color:#fff5b1">**$$MI(D; Y \|X)$$** 가 **클수록** **번역이 domain 지식을 더 많이 통합**한다(**domain-specific knowledge를 잘 학습**했다.)고 가정</span>한다.      

**low MI는 모델**이 번역에서 **domain 특성을 충분히 활용하고 있지 않다**는 것을 나타내기 때문에 **바람직하지 않다**.     
➡ 즉, low MI는 모델은 domain-specific를 잘 알지 못한다고 해석될 수 있다.       

예를 들어 IT 용어 ‘computing totals’ 번역을 보자,           
* **low MI model(도메인에  지식 습득률 낮음):** "calculation"으로 번역(IT분야에 맞지않는 너무 general한 단어이다)        
* **high MI(도메인 별 지식 습득률 낮음):** ‘computing totals’는 번역에서 올바르게 유지된다 (IT분야에 맞는 해석임).     

✨ 따라서 **MI를 최대화**하면, multi-domain NMT이 **domain-specialized** 된다.     

---

<span style="background-color:#F5F5F5">**[본 논문의 목표]**</span>      
위의 아이디어에 자극을 받아 <span style="background-color:#fff5b1">**low MI를 페널티화**하여 multi-domain NMT를 **specializes**하는 새로운 방법을 소개</span>한다.       

먼저 **$$MI(D; Y \|X)$$를 이론적으로 도출**하고,   
**low MI를 가진 subword-tokens에 더 많은 페널티**를 부여하는 **새로운 목표를 공식화**한다.       


본 논문의 결과는,   
* 제안된 방법이 모든 도메인에서 번역 품질을 향상시킨다는 것을 보여준다.     
* MI 시각화는 우리의 방법이 MI를 최대화하는 데 효과적임을 보장한다.    
* 본 논문은 본 논문의 모델이 강한 도메인 특성을 가진 샘플에서 특히 더 잘 수행된다는 것을 관찰했다.       

----

<span style="background-color:#F5F5F5">**[본 논문의 main contributions]**</span>      
* 본 논문은 multi-domain NMT에서 MI를 조사하고 **low MI가 더 높은 가치를 갖도록 패널티를 주는** 새로운 목표를 제시한다.    
* 광범위한 실험 결과는 우리의 방법이 실제로 **high MI를 산출**하여 multi-domain을 생성한다는 것을 증명한다.





----
----


# 2 Related Works

<span style="background-color:#F5F5F5">**[Multi-Domain Neural Machine Translation]**</span>   
Multi-Domain NMT는 번역을 개선하기 위해 **domain 정보의 적절한 사용을 개발**하는 데 중점을 둔다.    
초기 연구에서는 **source domain 정보**를 주입하고, **domain 분류기를 추가**하는 두 가지 주요 접근 방식이 있었다.        
* source domain 정보를 추가하기 위해, Kobus 등(2017)은 소스 domain label을 입력이 있는 **additional tag** 또는 **complementary feature**으로 삽입한다.           
* Britz 등(2017)은 **domain 분류기의 그레이디언트**를 사용하여 **domain별 문장 임베딩**을 train한다


**[previous work과 본 논문의 차이]**         
* **previous work:**     
**auxiliary classifier**를 주입하거나 구현하여 domain 정보를 활용하지만,      
* **본 논문:**    
MI 관점에서 domain 정보를 보고, domain별 지식을 탐색하기 위해 모델을 promotes하는 **손실**을 제안한다.       


-----


<span style="background-color:#F5F5F5">**[Information-Theoretic Approaches in NMT]**</span>   
NMT의 Mutual information는 주로 **메트릭** 또는 **손실 함수**로 사용된다.       
* 메트릭의 경우,        
     * Bugliarello et al.(2020)은 언어 간 번역의 어려움을 정량화하기 위해 **cross-mutual information(XMI**)를 제안한다.      
     * Fernandes et al. (2021)은 번역 중 주어진 컨텍스트의 사용을 측정하기 위해 **XMI를 수정**한다.      
* 손실 함수의 경우,         
     * Xu et al.(2021)은 **단어 매핑 다양성을 계산**하는  **cross-mutual information(BMI)를 제안**하며, NMT 훈련에 추가로 적용된다.          
     *  Zhang et al.(2022)은 컨텍스트를 기반으로 **target 토큰과 source sentence 간의 MI를 최대화**하여 모델 번역을 개선한다.     


**[previous work과 본 논문의 차이]**         
* **previous work:**         
일반적인 기계 번역 시나리오만 고려한다.     
* **본 논문:**    
multi-domain NMT에서 **상호 정보를 통합**하여 **domain-specific 정보를 학습**한다는 점에서 다르다.         추가 모델의 훈련이 필요한 다른 방법과 달리, 우리의 방법은 **단일 모델 내에서 MI를 계산할 수 있어** 계산 효율이 더 높다.          

----
-----

# 3 Proposed Method

이 섹션에서는 새로운 방법을 소개하여 **domain-specialized model**을 만든다.       
새로운 방법은 전체적으로 아래와 같다.     
**1)** 먼저 multi-domain NMT에서 **MI를 도출**한다.    
**2)** 그 다음 **low MI**가 높은 값을 갖도록 **패널티**을 준다.     



---


## 3.1 Mutual Information in Multi-Domain

<span style="background-color:#F5F5F5">**[MI]**</span>     
**Mutual Information(MI)** 는 두 랜덤 variables 간의 **상호 의존성을 측정**한다.      
* $$MI(D; Y \| X)$$: 다중 domain NMT에서의 의미는, domain($$D$$)과 $$translation(Y \| X)$$ 사이의 MI는 **번역에 포함된 도메인별 정보의 양**을 나타낸다.      
* $$MI(D; Y \| X)$$는 다음과 같이 쓸 수 있다:
![image](https://user-images.githubusercontent.com/76824611/224192782-a68bfc2f-23f5-4025-a473-456131bbe283.png)


<details>
<summary>📜 Full Derivation of Domain-Aware Mutual Information</summary>
<div markdown="1">
  
음... 그러니까 분자는 grneral하게 해석될 확룰,    
분모는 domain쪼긍로 해석 될 확률이라고 간단하게 해서하면 될 것 이다.     
     
![image](https://user-images.githubusercontent.com/76824611/224193609-a9862f33-71cc-46ae-b6c2-4ce267939b1f.png)
     
The proof from the first to the second line is
provided below.
     
     
![image](https://user-images.githubusercontent.com/76824611/224193647-1edb77ff-978b-4c75-b473-348c77edec30.png)
     
     
</div>
</details> 


즉, $$MI(D; Y \| X)$$의 최종 형태는 **domain과 domain이 없는 번역을 고려한 번역의 로그 지수**이다.     


<span style="background-color:#F5F5F5">**[XMI]**</span>     
그런데 실제 분포를 알 수 없기 때문에,    
매개 변수화된 모델, 즉 **cross-MI (XMI)** 로 근사한다.       
$$XMI(D; Y \| X)$$는 아래의 각 모델 출력과 함께 아래와 같이 표현될 수 있다.
* **generic domain-agnostic model**(generic 도메인에 구애받지 않는 모델(*general*으로 더 언급되고 **G**로 약칭됨)의 출력은 **$$p(Y \| X)$$** 의 적절한 근사치가 될 것이다.       
* **domain-adapted(*DA*)** 모델 출력은 **$$p(Y \| X, D)$$** 에 적합할 것이다.     
![image](https://user-images.githubusercontent.com/76824611/224195320-b3892d25-c9f4-4d82-8582-2f7b28bc975f.png)

----

# 3.2 MI-based Token Weighted Loss
To calculate XMI, we need outputs from both general and domain-adapted models. Motivated by
the success of adapters (Houlsby et al., 2019) in
multi-domain NMT (Pham et al., 2021), we assign
adapters φ1, · · · φN for each domain (N number of domains) and have an extra adapter φG
for general. We will denote the shared parameter
(e.g., self-attention and feed-forward layer) as θ.
For a source sentence x from domain d, x passes
the model twice, once through the corresponding
domain adapter, φd, and the other through the general adapter, φG. Then, we treat the output probability from domain adapter as pDA and from general
adapter as pG. For the i
th target token, yi
,we calculate XMI as in Eq. (3),


식   


, where y<i is the target subword-tokens up to,
but excluding yi
. For simplicity, we will denote
Eq. (3) as XMI(i). Low XMI(i) means that our
domain adapted model is not thoroughly utilizing
domain information during translation. Therefore,
we weight more on the tokens with low XMI(i),
resulting in minimizing Eq. (4),


식    
, where nT is the number of subword-tokens in the
target sentence.


The final loss of our method is in Eq. (7), where
λ1 and λ2 are hyperparameters


식
