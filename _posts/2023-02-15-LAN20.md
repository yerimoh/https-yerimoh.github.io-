---
title: "Specializing Multi-domain NMT via Penalizing Low Mutual Information 정리"
date:   2023-02-15
excerpt: "Specializing Multi-domain NMT via Penalizing Low Mutual Information paper review"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---



# Abstract
Multi-domain Neural Machine Translation
(NMT) trains a single model with multiple domains. It is appealing because of its efficacy in
handling multiple domains within one model.
An ideal multi-domain NMT should learn distinctive domain characteristics simultaneously,
however, grasping the domain peculiarity is
a non-trivial task. In this paper, we investigate domain-specific information through the
lens of mutual information (MI) and propose a
new objective that penalizes low MI to become
higher. Our method achieved the state-of-theart performance among the current competitive multi-domain NMT models. Also, we empirically show our objective promotes low MI
to be higher resulting in domain-specialized
multi-domain NMT.


# 1 Introduction
Multi-domain Neural Machine Translation (NMT)
(Sajjad et al., 2017; Farajian et al., 2017) has been
an attractive topic due to its efficacy in handling
multiple domains with a single model. Ideally,
a multi-domain NMT should capture both general knowledge (e.g., sentence structure, common
words) and domain-specific knowledge (e.g., domain terminology) unique in each domain. While
the shared knowledge can be easily acquired via
sharing parameters across domains (Kobus et al.,
2017), obtaining domain specialized knowledge
is a challenging task. Haddow and Koehn (2012)
demonstrate that a model trained on multiple domains sometimes underperforms the one trained
on a single domain. Pham et al. (2021) shows that
separate domain-specific adaptation modules are
not sufficient to fully-gain specialized knowledge.


In this paper, we reinterpret domain specialized
knowledge from mutual information (MI) perspective and propose a method to strengthen it. Given
a source sentence X, target sentence Y , and corresponding domain D, the MI between D and the
translation Y |X (i.e., MI(D; Y |X)) measures the
dependency between the domain and the translated sentence. Here, we assume that the larger
MI(D; Y |X), the more the translation incorporates
domain knowledge. Low MI is undesirable because
it indicates the model is not sufficiently utilizing domain characteristics in translation. In other words,
low MI can be interpreted as a domain-specific information the model has yet to learn. For example,
as shown in Fig. 1, we found that a model with low
MI translates an IT term ‘computing totals’ to the
vague and plain term ‘calculation’. However, once
we force the model to have high MI, ‘computing
totals’ is correctly retained in its translation. Thus,
maximizing MI promotes multi-domain NMT to
be domain-specialized.


Motivated by this idea, we introduce a new
method that specializes multi-domain NMT by
penalizing low MI. We first theoretically derive
MI(D; Y |X), and formulate a new objective that
weights more penalty on subword-tokens with low
MI. Our results show that the proposed method improves the translation quality in all domains. Also,
the MI visualization ensures that our method is effective in maximizing MI. We also observed that
our model performs particularly better on samples
with strong domain characteristics

The main contributions of our paper are as follows:     

• We investigate MI in multi-domain NMT and
present a new objective that penalizes low MI
to have higher value.   

• Extensive experiment results prove that our
method truly yields high MI, resulting in
domain-specialized model.

