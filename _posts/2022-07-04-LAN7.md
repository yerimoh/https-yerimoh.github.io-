---
title: "FastText: Enriching Word Vectors with Subword Information 정리"
date:   2022-07-04
excerpt: "Enriching Word Vectors with Subword Information, FastText"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---

# intro
 
      

## 핵심  

## 읽기 위해 필요한 지식
* [word2vec](https://yerimoh.github.io/DL14/): baseline 모델이기 때문에 꼭 알아야 한다.        
* [Embedding](https://yerimoh.github.io/DL15/): word2vec 속도 개선으로 이 포스팅도 꼭 알아야 한다.      

## 원 논문
[Enriching Word Vectors with Subword Information](https://arxiv.org/pdf/1607.04606.pdf)


---

# 목차  


---


# INTRO
단어를 벡터로 만드는 방법이다.     
기존 방법인 Word2Vec의 **형태학을 무시(단어의 내부구조 무시)** 한다는 단점을 보완하기 위해 나온 것 이기 때문에 메커니즘 자체는 Word2Vec의 확장이라고 볼 수 있다.      

Word2Vec와 패스트텍스트와의 가장 큰 차이점은 **Word2Vec**은 단어를 **한단위**로 생각한다면,     
**패스트텍스트**는 **하나의 단어 안에도 여러 단어들이 존재**하는 것으로 간주한다. 즉 **내부 단어(subword)를 고려하여 학습**한다.

이 방법은 **빠르고**, **대규모 말뭉치**에서 모델을 빠르게 훈련할 수 있으며, 훈련 데이터에 나타나지 않은 단어에 대한 단어 표현을 계산할 수 있다(OOV문제 개선). 


----


# **1. Introduction** 

**[기존 연구: 분포 의미론(distribution semantics)]**    
기존 자연어 처리 분야는 분포 의미론(distribution semantics)으로 알려진 많은 연구가 진행되었음.       
* 분포 의미론(distribution semantics): 단어의 **연속적인 표현을 학습**하는 것      
   * ex) 신경에 네트워크 커뮤니티 Colovert와 Weston(2008)은 왼쪽의 두 단어와 오른쪽의 두 단어를 기반으로 단어를 예측하여 피드포워드 신경망을 사용하여 단어 임베딩을 학습할 것을 제안    
   * ex) 미콜로프 외 연구진(2013b)은 매우 큰 말뭉치에서 단어의 연속적인 표현을 효율적으로 학습하기 위해 간단한 로그-이선형 모델을 제안  


**[기존 연구의 한계 1: OOV 문제]**    
연속적 표현은 일반적으로 **공동 발생 통계**를 사용한 **레이블이 없는 대형 말뭉치에서 파생**된다    
➡ 이러한 기법의 대부분은 매개 변수 공유 없이 **개별 벡터**로 어휘의 **각 단어를 나타냄**      
➡ 이렇게 1:1로 개별백터로 단어를 나타내게 하면 **처음보는 새로운 단어**(학습 데이터에 없는)를 **표현하지 못하는**(vector embedding하지 못하는) **OOV문제**가 나타난다.      

 
   


**[기존 연구의 한계: 단어 자체의 내부구조 무시]**    
이는 **형태학적**으로 **풍부**한 언어(ex 핀란드)를 잘 나타내지 못한다.       
+ 형태학적으로 풍부: 적은 단어를 많이 응용하여 여러 표현을 만들 수 있는 언어         
+ ex) work, working, worked, worker      
훈련 말뭉치에서 거의 발생하지 않는(또는 전혀 발생하지 않는) 많은 단어 형식이 포함되어 있어 **좋은 단어 표현을 배우기가 어렵다**.          


**[Solution]**
형태학적으로 풍부한 언어에는  많은 **단어 형성이 규칙**을 따름     
➡ **문자 수준 정보를 사용**하여 형태학적으로 풍부한 언어의 벡터 표현을 개선 가능     

본 논문에서는 문자 n-gram에 대한 표현을 배우고 단어를 n-gram 벡터의 합으로 나타낼 것을 제안한다.      
➡ 주요 기여는 **하위 단어 정보를 고려**한 연속 [Skip-gram model](https://yerimoh.github.io/DL14/#skip-gram-%EB%AA%A8%EB%8D%B8)**의 확장**을 도입하는 것     
➡ 다양한 형태를 보이는 9개 언어에 대해 이 모델을 평가하여 접근 방식의 이점을 보여줌        



-----
-----



# **3. Model**
이 섹션에서는 형태학을 고려하면서 단어 표현을 학습하는 모델을 제안한다.      

**[모델 목표]**     
* **하위 단어** 단위 고려     
* 단어를 **문자 n-그램의 합**으로 표현하여 형태학을 모델링       


**[제안 순서]**
1. 단어 벡터를 훈련시키는 데 사용하는 **일반적인 프레임워크를 제시**        
2. 하위 단어 모델을 제시    
3. 최종적으로 문자 n-그램 사전을 처리하는 방법을 설명       


---



## 3.1  General model
FastText의 balse model은 **skip-gram**이다.      

[자세히 보기](https://yerimoh.github.io/DL14/#skip-gram-%EB%AA%A8%EB%8D%B8)

요약하자면 현재 단어($$W_t$$)(eat)로 맥락단어들($$W_c$$)(I, You, She ...)(snack, rice, cake...)을 유추하는 것이다,       
이를 통해 단어의 분산표현을 얻는다.     

![image](https://user-images.githubusercontent.com/76824611/177746087-c7df7756-7fa1-48be-900a-3b4f9e2ad155.png)

----


## 3.2 Subword model
기존 Skip-gram모델의 **문장 자체의 구조를 무시한다는 문제를 해결**하기 위해 본 논문에서는 새로운 방법을 제안한다.     

단어 **내부 구조**를 보다 잘 반영하기 위하여,      
FastText는 단어 w를 **n-gram character의 bag**으로 표현한다.        


패스트텍스트에서는 우선 각 단어는 글자들의 **n-gram**으로 나타낸다.    
n을 몇으로 결정하는지에 따라서 단어들이 얼마나 분리되는지 결정한다. 

예를 들어서 n을 3으로 잡은 트라이그램(tri-gram)의 경우,      
먼저 각 단어를 구분하기 위해 단어의 시작과 끝에 ```<```,```>```을 붙인다.      
그 다음 ```where```은 ```wh, her, ere, re```로 분리하고 이들 또한 임베딩을 한다.     
마지막으로 여기에 **special sequence**로 **word 자체**가 포함된다.       
따라서 bag of n-gram으로 표현된 단어는 다음과 같다.    
![image](https://user-images.githubusercontent.com/76824611/177518555-36e5f509-365c-4fea-a33d-c70dd5e7bdb4.png)


**[n의 범위 설정]**       
* 실제 사용할 때는 n의 최소값과 최대값을 설정가능       
* n은 3이상 6이하의 범위의 숫자로 설정해야한다.         
![image](https://user-images.githubusercontent.com/76824611/177522694-59f02607-4d3c-41db-80a9-0f2ff070f1c8.png)

   




**[Scoring Function]**     
* g 크기의 n그램 dictionary가 주어졌다고 가정하자.      
* w 단어가 주어지면, Gw ⊂ {1, . . . , G}로 w에 나타나는 n그램 집합을 나타내자.     
* 우리는 각 n-gram에 벡터 표현 $$z_g$$를 연관시킨다.     
* 우리는 단어를 n-그램의 벡터 표현의 합으로 표현한다. 따라서 스코어링 함수를 얻는다.
![image](https://user-images.githubusercontent.com/76824611/177741695-bfce9e2e-7126-43bb-9146-34df6e37c3c0.png)




이 간단한 모델을 통해 단어 간에 **representation을 공유(=가중치 공유)** 할 수 있으므로 rare한 단어(단어 집합 내 빈도 수가 적었던 단어)에 대한 신뢰할 수 있는 표현을 학습할 수 있다.    
예를 들면 work, working, worked, worker에서 work의 가중치를 공유시키는 것이다.     


