---
title: "FastText: Enriching Word Vectors with Subword Information 정리"
date:   2022-07-04
excerpt: "Enriching Word Vectors with Subword Information, FastText"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---

# intro
 
      

## 핵심  

## 읽기 위해 필요한 지식
* [word2vec](https://yerimoh.github.io/DL14/): baseline 모델이기 때문에 꼭 알아야 한다.        
* [Embedding](https://yerimoh.github.io/DL15/): word2vec 속도 개선으로 이 포스팅도 꼭 알아야 한다.      

## 원 논문
[Enriching Word Vectors with Subword Information](https://arxiv.org/pdf/1607.04606.pdf)

---

# 목차  


---


# INTRO

레이블이 지정되지 않은 큰 말뭉치에 대해 훈련된 연속 단어 표현은 많은 자연어 처리 작업에 유용하다. 이러한 표현을 학습하는 인기 모델은 각 단어에 고유한 벡터를 할당하여 단어의 형태학을 무시한다. 이는 특히 어휘가 많고 희귀한 단어가 많은 언어에서 제한적이다. 본 논문에서, 우리는 각 단어가 문자 n-그램의 가방으로 표현되는 스킵그램 모델을 기반으로 하는 새로운 접근법을 제안한다. 벡터 표현은 각 문자 n-그램과 연관되어 있으며, 단어들은 이러한 표현의 합으로 표현된다. 우리의 방법은 빠르고, 대규모 말뭉치에서 모델을 빠르게 훈련할 수 있으며, 훈련 데이터에 나타나지 않은 단어에 대한 단어 표현을 계산할 수 있다. 우리는 단어 유사성과 유사성 작업 모두에서 9개의 다른 언어에 대한 단어 표현을 평가한다. 최근에 제안된 형태학적 단어 표현과 비교함으로써, 우리의 벡터가 이러한 작업에서 최첨단 성능을 달성한다는 것을 보여준다.

----


# **1. Introduction** 

**[기존 연구: 분포 의미론(distribution semantics)]**    
기존 자연어 처리 분야는 분포 의미론(distribution semantics)으로 알려진 많은 연구가 진행되었음.       
* 분포 의미론(distribution semantics): 단어의 **연속적인 표현을 학습**하는 것      
   * ex) 신경에 네트워크 커뮤니티 Colovert와 Weston(2008)은 왼쪽의 두 단어와 오른쪽의 두 단어를 기반으로 단어를 예측하여 피드포워드 신경망을 사용하여 단어 임베딩을 학습할 것을 제안    
   * ex) 미콜로프 외 연구진(2013b)은 매우 큰 말뭉치에서 단어의 연속적인 표현을 효율적으로 학습하기 위해 간단한 로그-이선형 모델을 제안  


**[기존 연구의 한계 1: OOV 문제]**    
연속적 표현은 일반적으로 **공동 발생 통계**를 사용한 **레이블이 없는 대형 말뭉치에서 파생**된다    
➡ 이러한 기법의 대부분은 매개 변수 공유 없이 **개별 벡터**로 어휘의 **각 단어를 나타냄**      
➡ 이렇게 1:1로 개별백터로 단어를 나타내게 하면 **처음보는 새로운 단어**(학습 데이터에 없는)를 **표현하지 못하는**(vector embedding하지 못하는) **OOV문제**가 나타난다.      

 
   


**[기존 연구의 한계: 단어 자체의 내부구조 무시]**    
이는 **형태학적**으로 **풍부**한 언어(ex 핀란드)를 잘 나타내지 못한다.       
+ 형태학적으로 풍부: 적은 단어를 많이 응용하여 여러 표현을 만들 수 있는 언어         
+ ex) work, working, worked, worker      
훈련 말뭉치에서 거의 발생하지 않는(또는 전혀 발생하지 않는) 많은 단어 형식이 포함되어 있어 **좋은 단어 표현을 배우기가 어렵다**.          


**[Solution]**
형태학적으로 풍부한 언어에는  많은 **단어 형성이 규칙**을 따름     
➡ **문자 수준 정보를 사용**하여 형태학적으로 풍부한 언어의 벡터 표현을 개선 가능     

본 논문에서는 문자 n-gram에 대한 표현을 배우고 단어를 n-gram 벡터의 합으로 나타낼 것을 제안한다.      
➡ 주요 기여는 **하위 단어 정보를 고려**한 연속 [Skip-gram model](https://yerimoh.github.io/DL14/#skip-gram-%EB%AA%A8%EB%8D%B8)**의 확장**을 도입하는 것     
➡ 다양한 형태를 보이는 9개 언어에 대해 이 모델을 평가하여 접근 방식의 이점을 보여줌        



-----



