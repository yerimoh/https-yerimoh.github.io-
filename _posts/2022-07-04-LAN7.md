---
title: "FastText: Enriching Word Vectors with Subword Information 정리"
date:   2022-07-04
excerpt: "Enriching Word Vectors with Subword Information, FastText"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---

# intro
 
      

## 핵심  

## 읽기 위해 필요한 지식
* [word2vec](https://yerimoh.github.io/DL14/): baseline 모델이기 때문에 꼭 알아야 한다.        
* [Embedding](https://yerimoh.github.io/DL15/): word2vec 속도 개선으로 이 포스팅도 꼭 알아야 한다.      

## 원 논문
[Enriching Word Vectors with Subword Information](https://arxiv.org/pdf/1607.04606.pdf)

---

# 목차  


---


# INTRO
단어를 벡터로 만드는 방법이다.     
기존 방법인 Word2Vec의 **형태학을 무시(단어의 내부구조 무시)** 한다는 단점을 보완하기 위해 나온 것 이기 때문에 메커니즘 자체는 Word2Vec의 확장이라고 볼 수 있다.      

Word2Vec와 패스트텍스트와의 가장 큰 차이점은 **Word2Vec**은 단어를 **한단위**로 생각한다면,     
**패스트텍스트**는 **하나의 단어 안에도 여러 단어들이 존재**하는 것으로 간주한다. 즉 **내부 단어(subword)를 고려하여 학습**한다.

이 방법은 **빠르고**, **대규모 말뭉치**에서 모델을 빠르게 훈련할 수 있으며, 훈련 데이터에 나타나지 않은 단어에 대한 단어 표현을 계산할 수 있다(OOV문제 개선). 


----


# **1. Introduction** 

**[기존 연구: 분포 의미론(distribution semantics)]**    
기존 자연어 처리 분야는 분포 의미론(distribution semantics)으로 알려진 많은 연구가 진행되었음.       
* 분포 의미론(distribution semantics): 단어의 **연속적인 표현을 학습**하는 것      
   * ex) 신경에 네트워크 커뮤니티 Colovert와 Weston(2008)은 왼쪽의 두 단어와 오른쪽의 두 단어를 기반으로 단어를 예측하여 피드포워드 신경망을 사용하여 단어 임베딩을 학습할 것을 제안    
   * ex) 미콜로프 외 연구진(2013b)은 매우 큰 말뭉치에서 단어의 연속적인 표현을 효율적으로 학습하기 위해 간단한 로그-이선형 모델을 제안  


**[기존 연구의 한계 1: OOV 문제]**    
연속적 표현은 일반적으로 **공동 발생 통계**를 사용한 **레이블이 없는 대형 말뭉치에서 파생**된다    
➡ 이러한 기법의 대부분은 매개 변수 공유 없이 **개별 벡터**로 어휘의 **각 단어를 나타냄**      
➡ 이렇게 1:1로 개별백터로 단어를 나타내게 하면 **처음보는 새로운 단어**(학습 데이터에 없는)를 **표현하지 못하는**(vector embedding하지 못하는) **OOV문제**가 나타난다.      

 
   


**[기존 연구의 한계: 단어 자체의 내부구조 무시]**    
이는 **형태학적**으로 **풍부**한 언어(ex 핀란드)를 잘 나타내지 못한다.       
+ 형태학적으로 풍부: 적은 단어를 많이 응용하여 여러 표현을 만들 수 있는 언어         
+ ex) work, working, worked, worker      
훈련 말뭉치에서 거의 발생하지 않는(또는 전혀 발생하지 않는) 많은 단어 형식이 포함되어 있어 **좋은 단어 표현을 배우기가 어렵다**.          


**[Solution]**
형태학적으로 풍부한 언어에는  많은 **단어 형성이 규칙**을 따름     
➡ **문자 수준 정보를 사용**하여 형태학적으로 풍부한 언어의 벡터 표현을 개선 가능     

본 논문에서는 문자 n-gram에 대한 표현을 배우고 단어를 n-gram 벡터의 합으로 나타낼 것을 제안한다.      
➡ 주요 기여는 **하위 단어 정보를 고려**한 연속 [Skip-gram model](https://yerimoh.github.io/DL14/#skip-gram-%EB%AA%A8%EB%8D%B8)**의 확장**을 도입하는 것     
➡ 다양한 형태를 보이는 9개 언어에 대해 이 모델을 평가하여 접근 방식의 이점을 보여줌        



-----
-----



# Model
이 섹션에서는 형태학을 고려하면서 단어 표현을 학습하는 모델을 제안한다.      

**[모델 목표]**     
* **하위 단어** 단위 고려     
* 단어를 **문자 n-그램의 합**으로 표현하여 형태학을 모델링       


**[제안 순서]**
1. 단어 벡터를 훈련시키는 데 사용하는 **일반적인 프레임워크를 제시**        
2. 하위 단어 모델을 제시    
3. 최종적으로 문자 n-그램 사전을 처리하는 방법을 설명       


---



## 3.1  General model
FastText의 balse model은 **skip-gram**이다.      

[자세히 보기](https://yerimoh.github.io/DL14/#skip-gram-%EB%AA%A8%EB%8D%B8)

요약하자면 현재 단어($$W_t$$)(eat)로 맥락단어들($$W_c$$)(I, You, She ...)(snack, rice, cake...)을 유추하는 것이다,       
이를 통해 단어의 분산표현을 얻는다.     

![image](https://user-images.githubusercontent.com/76824611/177247624-68915062-0a33-4b8f-807b-8d618370d8dc.png)

----


## 3.2 Subword model
기존 Skip-gram모델의 **문장 자체의 구조를 무시한다는 문제를 해결**하기 위해 본 논문에서는 새로운 방법을 제안한다.     

단어 **내부 구조**를 보다 잘 반영하기 위하여,      
FastText는 단어 w를 **n-gram character의 bag**으로 표현한다.        


패스트텍스트에서는 우선 각 단어는 글자들의 **n-gram**으로 나타낸다.    
n을 몇으로 결정하는지에 따라서 단어들이 얼마나 분리되는지 결정한다. 

예를 들어서 n을 3으로 잡은 트라이그램(tri-gram)의 경우,      
먼저 각 단어를 구분하기 위해 단어의 시작과 끝에 ```<```,```>```을 붙인다.      
그 다음 ```where```은 ```wh, her, ere, re```로 분리하고 이들 또한 임베딩을 한다.     
마지막으로 여기에 **special sequence**로 **word 자체**가 포함된다.       
따라서 bag of n-gram으로 표현된 단어는 다음과 같다.    
![image](https://user-images.githubusercontent.com/76824611/177518555-36e5f509-365c-4fea-a33d-c70dd5e7bdb4.png)



실제 사용할 때는 n의 최소값과 최대값을 설정할 수 있는데, 기본값으로는 각각 3과 6으로 설정되어져 있다.    
패스트텍스트의 인공 신경망을 학습한 후에는 데이터 셋의 모든 단어의 각 n-gram에 대해서 워드 임베딩이 된다. 이렇게 되면 장점은 데이터 셋만 충분하다면 위와 같은 내부 단어(subword)를 통해 모르는 단어(OOV)에 대해서도 다른 단어와의 유사도를 계산할 수 있다는 점이다.

가령, 패스트텍스트에서 birthplace(출생지)란 단어를 학습하지 않은 상태라고 해보자. 하지만 다른 단어의 n-gram으로서 birth와 place를 학습한 적이 있다면 birthplace의 임베딩 벡터(Embedding Vector)를 만들어낼 수 있다. 이는 모르는 단어에 대처할 수 없었던 Word2Vec와는 다른 점이다.

2. 단어 집합 내 빈도 수가 적었던 단어(Rare Word)에 대한 대응
Word2Vec의 경우에는 단어 집합에서 등장 빈도 수가 높으면 비교적 정확하게 임베딩 되지만, 등장 빈도 수가 적은 단어(Rare word)에 대해서는 임베딩의 정확도가 높지 않다는 단점이 있다. 참고할 수 있는 경우의 수가 적다보니 정확하게 임베딩이 되지 않는 것이다.

하지만 패스트텍스트는 등장 빈도 수가 적은 단어라 하더라도, n-gram으로 임베딩을 하는 특성상 참고할 수 있는 경우의 수가 많아지므로 Word2Vec와 비교하여 정확도가 높은 경향이 있다.

패스트텍스트가 노이즈가 많은 코퍼스에서 강점을 가진 것 또한 이와 같은 이유이다. 모든 훈련 코퍼스에 오타(Typo)나 맞춤법이 틀린 단어가 없으면 이상적이겠지만, 실제 많은 비정형 데이터에는 오타가 섞여 있다. 그리고 오타가 섞인 단어는 당연히 등장 빈도수가 매우 적으므로 일종의 희귀 단어가 된다. 즉, Word2Vec에서는 오타가 섞인 단어는 임베딩이 제대로 되지 않지만 패스트텍스트는 이에 대해서도 일정 수준의 성능을 보인다.







