---
title: "Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP 정리"
date:   2022-07-12
excerpt: "Between words and characters:A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---

# intro
 
      

## 핵심  

## 읽기 위해 필요한 지식
* [word2vec](https://yerimoh.github.io/DL14/): baseline 모델이기 때문에 꼭 알아야 한다.        
* [Embedding](https://yerimoh.github.io/DL15/): word2vec 속도 개선으로 이 포스팅도 꼭 알아야 한다.      

## 원 논문
[Enriching Word Vectors with Subword Information](https://arxiv.org/abs/2112.10508)


---

# 목차  
  * [comments: true](#comments--true)
- [intro](#intro)
  * [핵심](#--)
  * [읽기 위해 필요한 지식](#------------)
  * [원 논문](#----)
- [목차](#--)
- [**Abstract**](#abstract)
- [**1 Introduction**](#--1-introduction--)
  * [논문 개요](#-----)
- [**2 Tokens, word-forms, and sub-words**](#--2-tokens--word-forms--and-sub-words--)
- [**3 Pre-tokenization yields word-like typographic units**](#--3-pre-tokenization-yields-word-like-typographic-units--)
- [**4. Augmenting word-level pretokenizer tokens with character information**](#--4-augmenting-word-level-pretokenizer-tokens-with-character-information--)
  * [4.1 Augmenting word-level models with spelling information](#41-augmenting-word-level-models-with-spelling-information)
  * [**4.2 Open-vocabulary language modeling with (tokenizer-defined) words made of characters**](#--42-open-vocabulary-language-modeling-with--tokenizer-defined--words-made-of-characters--)
- [**5 Learning segmentations to find concatenative word-like pretokenizer tokens**](#--5-learning-segmentations-to-find-concatenative-word-like-pretokenizer-tokens--)
  * [5.1 Character-level neural models that learn to skip steps at higher levels](#51-character-level-neural-models-that-learn-to-skip-steps-at-higher-levels)
  * [**5.2 Marginalization over all possible segmentations**](#--52-marginalization-over-all-possible-segmentations--)
  * [5.3 Finding words through Bayesian non-parametrics](#53-finding-words-through-bayesian-non-parametrics)
  * [5.4 Related task: Unsupervised Chinese Word Segmentation](#54-related-task--unsupervised-chinese-word-segmentation)
- [6 Learning subword vocabularies and segmentations](#6-learning-subword-vocabularies-and-segmentations)
  * [6.1 Manually constructed linguistic analyzers](#61-manually-constructed-linguistic-analyzers)
  * [6.2 Other Language-Specific Methods](#62-other-language-specific-methods)
  * [6.3 Unsupervised morphological segmentation](#63-unsupervised-morphological-segmentation)
  * [6.4 Modern fast subword segmentation algorithms](#64-modern-fast-subword-segmentation-algorithms)
  * [6.5 Comparing morphological segmentation to BPE and friends](#65-comparing-morphological-segmentation-to-bpe-and-friends)
  * [6.6 How many units do we need?](#66-how-many-units-do-we-need-)
- [7 Shared vocabularies in multilingual models](#7-shared-vocabularies-in-multilingual-models)
- [**8 “Tokenization-free” character- and byte-level modeling**](#--8--tokenization-free--character--and-byte-level-modeling--)
  * [8.1 Characters?](#81-characters-)
  * [8.2 Character hashes?](#82-character-hashes-)
  * [8.3 Bytes?](#83-bytes-)
  * [8.4 So are these maximal decompositions the solution then?](#84-so-are-these-maximal-decompositions-the-solution-then-)
  * [8.5 Visual featurization: Pixels?](#85-visual-featurization--pixels-)
- [**9 Discussion and Conclusion**](#--9-discussion-and-conclusion--)



---


# **Abstract**
우리가 모델링하고자 하는 텍스트의 단위는  
bytes에서 다중 단어 표현식(multi-word expressions)에 이르기까지, 텍스트는 다양한 세분성으로 분석되고 생성될 수 있다.      

최근까지 대부분의 자연어 처리(NLP) 모델은 단어를 통해 작동하여 discrete 및  atomic tokens으로 처리했지만,   
byte-pair encoding(BPE)을 시작으로 많은 영역에서 하위 단어 기반 접근 방식이 우세하여 **작은 어휘를 사용**하면서도 **빠른 추론**을 허용하고 있다.      

이 토크나이징 방법들에 대해 학습된 세분화를 기반으로 한 **subword-based approaches**뿐만 아니라 **단어와 문자의 하이브리드 접근 방식**이 **어떻게 제안되고 평가**되었는지를 보여줌으로써,     
사전 신경 및 신경 시대의 여러 작업 라인을 연결한다.     

본 논문은 모든 애플리케이션에 대해 **특별한 해결책**이 결코 **없을 것**이며,   
토큰화 방법이 애플리케이션에 여전히 중요하다고 결론짓는다.    


-----
-----

# **1 Introduction**    


_“‘tokens’ are not a real thing. they are a computer generated illusion created by a clever engineer” dril_gpt1         


우리가 사람들에게 NLP 모델을 처음 소개할 때,    
우리는 종종 텍스트가 컴퓨터에 작은 조각들로 잘려나간다는 생각을 당연하게 받아들인다.    
➡ 결국 그것은 단지 일련의 정수일 뿐이다. 따라서 우리는 이것들을 (보통) 연속 **sub-strings tokens**이라고 부른다. 


교육 환경에서, 그리고 사실 역사적으로 NLP에서 이러한 토큰은 다소 자연스럽게 단어(처음에는 영어로 “space-separated substrings”)로 암시된다. 


**[단어에서 구두점 분리의 어려움]**            
* "don't"라는 단어를 예로 들면,    
* 이 단어의 구두점 분할의 경우, ```"don"``` ```"'"``` ```"t"```라는 다소 무의미한 세 개의 토큰을 얻게 될 것이다.            
* 그런데 더 효과적인 토크나이징을 하려면 ```"do"```와 ```"n't"```의 두 단위를 산출해야 한다고 주장할 수 있다.       

---

## 논문 개요    
이 survey는 $$tokenization$$에 대한 위와 같은 질문을 다루며,      
본 논문은 의 각 챕터의 핵심내용은 아래와 같다,    

**[2: Tokens, word forms, and sub words](# 2-Tokens,-word-forms,-and-sub-words)**      
* 근본적인 질문과 용어를 상세히 설명     

**[3: Pre-tokenization yields word-like typographic units](# 3-Pre-tokenization-yields-word-like-typographic-units)**      
* 모든 NLP 작업에서 다소 매력적이지 않은 부분이 역사적으로 어떻게 다루어졌는지를 보여줌     

특히 지난 5년 동안, 다소 원자적인 단어와 같은 공간 분리 단위로서의 **"토큰"에 대한 직관적인 정의를 넘어서는 것**에 대한 관심이 새로워졌다.  
* **4**: 이를 위한 한 가지 방법인 **단어 내부 정보를 사용하여 단어와 같은 단위를 보강**하는 방법.    
* **5**: 신경 모델링은 이를 그 어느 때보다 쉽게 만들어 명백한 표시 없이 단어 경계까지 학습할 수 있는 모델로 이어진다(예: 공백이 있는 경우).  unsupervised word segmentation or discovery의 개념은 수십 년의 작업에서 자체적으로 존재했지만,    
* **6**: **subword** 단위를 원자 토큰으로 사용하는 현재 널리 퍼져 있는 아이디어를 최종적으로 살펴볼 때 고려하는 것이 유용하다는 것을 알게 될 것이다.      
* **7**: 다국어 어휘의 공유와 경쟁의 몇 가지 문제 살펴봄     
* **8**: 문자, 바이트 또는 심지어 픽셀로 최대 분해할 수 있는 가장 간단한 토큰화를 사용해봄        


이 논문이 끝난 후,       
* 바이트의 손쉬운 솔루션이 도달해 보이는 2021년 현재에도 "복잡한" 토큰화가 의 중요성을 논증하여 마무리 한다.     
* 최근의 발전은 특정 도메인과 사용 사례에 대해 최대 분해 처리를 가능하게 하지만, 그것들은 광범위한 NLP 시나리오를 다루지 않고 **자체의 단점과 편견**을 가지고 있다고 주장할 것이다.     


![image](https://user-images.githubusercontent.com/76824611/178999180-03d114b4-8ba7-4748-afce-5115faccc2b5.png)
A taxonomy of segmentation and tokenization algorithms and research directions


------
-----











# **2 Tokens, word-forms, and sub-words** 
이 챕터에서는 token에 대한 근본적인 질문과 용어를 설명한다.      

토큰, 단어 형태 및 하위 단어 NLP에서 텍스트 데이터는 
"문장"(sentences)과 "단어(words)"로 분할되었다. 

**[sentences]**     
* 거시적 단위(macroscopic units)는 종종 서로 독립적으로 고려되며, 그 자체가 미시적 단위로 세분된다.        




**[tokens]**    
* 일반적으로 토큰이라고 불리는 타이포그래피 단위(typographic units)는 언어적으로 동기화된 단위에 대한 근사치로 사용되어 왔다.     
* MAF(형태학적 주석 프레임워크)는 토큰을 "“non-empty contiguous sequence of graphemes or phonemes in a document."으로 정의한다.    
* 예를 들어, 현재 라틴어 스크립트와 함께 보편적으로 사용되는 **공백**과 같은 **타이포그래피 구분자**를 사용하는 쓰기 시스템의 경우, 토큰은 널리 사용되었으며 **비문구 비백색 공간 마크** 또는 **구두점의 연속 시퀀스**로 정의되었다.    
* 특정 문장 부호(예: 하이픈 또는 아포스트로피)를 토큰으로 임의로 정하면, 그러한 정의는 **문장**을 토큰을 기준으로 **원자 단위로 분할**한다.     
* 토큰과 단어 형태 사이에 **일대일 대응이 없다**("멀티톡 단어", "멀티워드 토큰")    
    * 단어 형태는 여러 개의 토큰(예:French or English sine die)으로 만들 수 있다.     
    * 반면, 여러 개의 단어 형태는 동일한 토큰(예: 영어 don't = do + not, 스페인어 damélo + da + lo)으로 나타낼 수 있다.     

**[tokens의 재정의의 필요성: word-forms을 위해]**      
* 최근 문장이 원자 단위로 분할되는 방식이 진화하여 **토큰화 개념이 재정의되**었다.    
* **필요성:** 과학적 결과(예: 하위 단어 분할이 기계 번역 성능에 미치는 영향와 기술적 요구 사항(예: 고정 크기 어휘가 필요한 BERT와 같은 언어 모델) 모두에 기초하여, 원자 처리 장치(아직 토큰이라고 함)가 **단어 형태(word-forms)의 근사치가 될 필요**가 있다.    
* ❌ **문제**: 퇴색한 결과적으로, 현재의 NLP에서 토큰의 개념은 여전히 MAF 정의와 완벽하게 일치하지만, 더 이상 **타이포그래피 단위의 전통적인 정의와 일치하지 않는다**.     


**[토큰화(Tokenization)]**     
* **정의:** 문장을 정형적이지 않은 (그리고 실제로 언어적이지 않은) 동기화된 단위로 분할하는 작업     
* 이는 종종 고전적인 토큰과 단어 형태보다 작기 때문에 종종 **subword**라고 불린다.     
* Typographic 단위("오래된" 토큰)는 현재 종종 "pre-tokens"라고 불린다.      
➡ 따라서 "tokenization"라고 불리던 것은 오늘날 "pretokenization"라고 불린다.      
* **pretokenization**: 이 용어는 토큰화의 새로운 개념에 대한 첫 번째 접근법이 **resulting 단위**(이전에는 “tokens”, 지금은  “pretokens”)를 “sub-words”로 **분할하기 전**에 **문장을 적절한 Typographic 단위**(예: 토큰화의 "옛" 개념)**로 분할하는 것을 포함**했다는 사실에 의해 동기 부여된다.     


-----
------


# **3 Pre-tokenization yields word-like typographic units**    
모든 NLP 작업에서 다소 매력적이지 않은 부분이 역사적으로 어떻게 다루어졌는지를 보여줌

**[옛 토큰의 사용방식]**       
* purely typographic token의 언어적 무관함(linguistic irrelevance)     
* 텍스트를 linguistically motivated된 단어 형태로 자동 분할하는 것의 어려움(difficulty of automatically splitting a text into linguistically motivated word-forms)     
* **위 두 문제의 절충안:**  purely typographic token과 purely linguistic word-forms의 중간 단위가 널리 사용되어 왔다.   

**[“pre-tokenizers”의 역사]**         
* **용어 변화**     
   * 초기에는 “tokenizers”로 알려짐     
   * sub-word 토큰화의 확산 이후 “pretoken”으로 바뀜    
   * 오늘날에는 “pre-tokenizers”로 명칭 바뀜         
* **대표 모델**      
   * 이러한 “pre-tokenizers” 중 일부는 비교적 단순하고 typographic token에 충실하다.      
   * Hugging Face의 토큰라이저 패키지에 있는 오래된 ```Moses (Kohn et al., 2007) tokenizer6``` 가 초기에 널리 사용됨           
   * 그리고 더 최근은 Hugging Face의 Tokenizers package에 있는```pre-tokenizers package```가 있다.      
 
**[“pre-tokenization” 개념의의 탄생과 문제점]**     
* 위의 대표 모델들의 사용은 "tokenization(현재는 “pre-tokenization”)"라는 단어를 만들어냈다.     
* **"tokenization(pre-tokenization)"의 의미:** 일반적으로 문장을 원자 단위로 분할하는 작업을 나타내게 됨.       
    * 즉, "후속 처리에서 분해될 필요가 없는 기본 단위" 단위가 typographic 단위보다 단어 형태에 더 가까울 때에도 "tokenization"이라고 불린다.       
    * 더 나아가, 토큰화자가 **문장을 분할**할 뿐만 아니라 **정규화**, **철자 수정** 또는 **named entity detection** 목적과 같은 원시 텍스트를 수정하는 데 공통적으로 사용되므로 현재 토큰의 표준 정의에서 벗어난다.      
* ❌ **문제점**     
    * 이러한 초기 토큰의 정의와 역할은 정규화 작업이나 다른 공백 기호의 통합 및 병합은 **토큰화 전으로 되돌릴 수 없게한다**.     
    * 토큰화된 출력에서 원시 텍스트를 확실하게 복구할 수 없다.   
  
  
 
------
-----


# **4. Augmenting word-level pretokenizer tokens with character information**
특히 지난 5년 동안, 다소 원자적인 단어와 같은 공간 분리 단위로서의 **"토큰"에 대한 직관적인 정의를 넘어서는 것**에 대한 관심이 새로워졌다.  
이 챕터에서는 이를 위한 한 가지 방법인 **단어 내부 정보를 사용하여 단어와 같은 단위를 보강**하는 방법을 설명한다.     

**[word-level 모델의 특징]**           
* 개념적으로 이해하기 쉬움    
* neural era에서는 해석 가능한 세분화된 기능을 제공      
* **단점**: 폐쇄형 어휘 모델: 훈련 중에 거의 보이지 않는 희귀하고 새로운 단어를 처리할 수 없음    
* **해결책:** 역사적으로 희귀한 단어 유형은 훈련 시 새로운 단어 유형 ```UNK(알 수 없음)```로 대체됨     


**[```UNK```대체 방식의 단점]**         
* 자연어 생성(NLG)을 수행할 때 UNK가 허용되지 않는다   
* ELMo 또는 BERT와 같은 large-scale 모델에 사용될 때 일회성 이벤트뿐만 아니라 유용한 의미의 앵커인 **새로운 단어에 대한 기능**을 **추출할 수 없다**        
* 영어 이외의 언어, 특히 더 생산적인 형태학 및 따라서 유형 토큰 비율이 높은 언어에서 희귀 단어를 제거하는 것은 불가능     

그럼에도 불구하고, **단어**가 언어의 **기본 단위**이기 때문에,       
**단어를 구성하는 문자를 기반**으로 처리함으로써 근본적으로 단어 기반 틀에서 희귀하고 새로운 단어의 **처리를 개선**하는 여러 접근법이 등장했다.      
우리는 단어 표현의 보다 포괄적인 처리를 위해 이 섹션에서 이러한 접근 방식 중 일부를 제시할 것이다.       

-----


## 4.1 Augmenting word-level models with spelling information    

**[단어에 초점]**         
* 언어를 위한 neural models에서,    
90년대와 2000년대의 연구는 **단어에 초점**을 맞추고, 대신 문자의 문자열을 처리했다.      

**[단어 자체에 대한 정보]**        
* 그러나 neural models이 NLP에서 중요해지자마자, 신경 네트워크에서 사용하기 위한 **단어와 문자 수준의 정보의 조합**(word- and character-level information)이 등장.       
* 단어 임베딩 추정을 돕기 위해 단어 자체에 대한 정보를 사용할 것을 처음 제안했다.      
* word’s embedding을 단어의 철자를 통해 구성한다는 아이디어를 **대중화**했다.   


**[CNN 기반 임베딩 기능]**     
* 임베딩 행렬을 CNN(convolutional neural network) 레이어로 대체할 때에도 generative 모델은 여전히 폐쇄 어휘이며, 
이는 훈련 데이터에서 보였던 단어만 예측할 수 있다는 것을 의미하기 때문에,         
CNN 구성은 **새로운 단어가 아닌** **희귀 단어**만 돕는다.      


**[CNN with 토큰의 철자]**      
* **각 토큰에 대한 철자**로 임베딩을 구성하는 CNN 기반 임베딩 기능     
* 새로운 단어를 예상하는 대신 "자주 단어를 정확하게 얻도록" 암묵적으로 훈련한다.  
* **이 기반 모델의 역할:** POS 태깅과 같은 다른 고전적인 NLP 작업의 발전을 이끌었고 궁극적으로 최초의 큰 문맥 단어 임베딩 모델 ELMo에 힘을 실어주었다.        
  * [fastText](https://yerimoh.github.io/LAN7/) 임베딩은 문자가 아니라 겹치는 n-gram에서 단어 임베딩을 구성할 것을 제안하여,     
새로운 단어에 대한 임베딩을 얻을 수 있다(generative 의미는 아니지만 그런 의미에서 "open-vocabulary"로 만든다).    

  * Ataman and Federico도 마찬가지로 문자 대신 (중첩) 𝑛-grams을 사용하여 기계 번역에서 더 나은 성능을 얻는다(형태학적으로 풍부한 언어에서 BPE를 능가함).     
  * 최근에는 철자 오류 또는 전송으로 노이즈가 많은 텍스트를 더 잘 처리함으로써     
구성 문자 임베딩에 대한 BPE 단위의 임베딩을 향상시켰다.      


**[small Transformer]**   
* 의료 텍스트와 같은 새로운 도메인; 동시에 Aguilar 등(2021)은 거의 동일하게 하지만 CNN 대신 small Transformer를 사용한다.     

**[구성 기반 접근 방식]**   
* 구성 기반 접근 방식은 사전 훈련된 단어 수준 입력 모델에도 통합되었다.     
* 특히, Pinter 등(2017)은 테스트 시간 동안 알 수 없는 단어가 나타날 때마다 호출되는 helper RNN 모델을 사용하여 **철자가 주어진 단어의 임베딩을 모방**하도록 훈련된 모델을 학습한다.      


---


## **4.2 Open-vocabulary language modeling with (tokenizer-defined) words made of characters**
**[open-vocabulary language modeling의 어려움]**    
* ```open-vocabulary language modeling```: test time때 **새로운 단어를 예측하고 생성**할 수 있는 모델     
* ```closed-vocabulary generative models```을 ```open-vocabulary language modeling```로 확장하는 것은 어렵다.              
* **어려운 이유**: 완전히 새로운 단어를 포함하는 문장의 무한 집합에 대한 확률 질량을 유지할 수 있어야 하기 때문에 input side의 Open-vocabulary 보다 다소 어렵다.     


**[Mielke와 Eisner(2018)의 모델]**         
* 단어 임베딩을 정규화하여 작은 문자 수준(smaller character-level)의 ```RNNLM```을 사용     
* smaller model을 사용하여 철자를 예측하도록 함으로써 일반적인 폐쇄 음성 단어 수준의 반복 신경망 언어 모델(```RNNLM```) 설정을 근본적으로 향상시키는 확률론적 2단계 모델을 제안한다.       
* 단어 수준(word-level) ```RNNLM```이 ```UNK```를 **예측할 때마다 새로운 단어를 즉시 생성**      
➡ 언어 개념과 직관적 모델링에 의해 동기 부여       
➡ 질적 및 정량적으로 성공적인 것으로 입증된 개방형 어휘 모델을 산출      



**[```cache model```의 확장]**        
* Kawakami et al. (2017)의 모델은 단어 및 문자 수준의 RNN의 유사한 2단계 설정을 따르지만,     
cache model(Grave et al., 2016)을 사용하여 directly copied할 수 없는 경우, **각 단어를 문자 수준의 RNN을 사용하여 철자**해야 한다.         
➡ 이들의 분석은 캐시 모델이 그렇지 않다는 것을 분명히 보여준다.      
* ```cache model```은 “bursty” **미지의 단어**만 복사하지만, 또한 잊어버리지 않도록 하기 위해 극히 **extremely** **common** function words들도 복사한다.       
* 이 아이디어는 ```Ataman``` 등(2019)에 의해 machine translation decoder(creating word embeddings on the encoder side from character-level ```BiRNNs``` as in ```ELMo```)에서 인코더 측에 단어 임베딩을 생성)에 대해 선택됨    
* 나중에 ```Ataman``` 등(2020)에 의해 확장됨          


**[다른 접근 방식]**     
* 더 낮은 속도로 ```multi-layer RNNs```의 higher layer를 실행하는 것이다(hidden state로의 업데이트를 건너뛴다)     
* 이는 El Hihi and Bengio(1995)에서 처음 제시된 오래된 아이디어이며(Schmidhuber (1991, 1992)의 "neural sequence chunker"에 구축됨)      
* 우리는 단어 경계와 그에 따른 segmentations을 실제로 배울 수 있다.    


----
----

# **5 Learning segmentations to find concatenative word-like pretokenizer tokens**
지금까지 우리는 [2](#2-Tokens,-word-forms,-and-sub-words)에 요약된 개념정의의 변화에도 불구하고,    
predefined된 단어(또는 사전 토큰화 출력) 개념을 갖는 것에 의존해왔다.              

**[문제]**      
그러나 영어가 아닌 언어나 robustness문제로 인해 predefined된 정의가 주어지지 않거나, 얻을 수 없거나, 단순히 바람직하지 않다면 어떻게 될까?      
우리의 **데이터 기반 머신 러닝 접근법**이 **토큰화를 학습**할 수 있는 방법이 있을까?     

**[해결]**       
이 절에 설명된 대부분의 접근 방식은 **의미 있는 단위**에 해당하는 세그먼트와 **경계를 찾기 위해**,     
**근사(approximate)** 또는 (더 많은 가정을 사용하여) **정확한 추론**을 수행할 수 있는 **잠재(latent) 변수로 암시적 분할(implied segmentation)을 처리**함으로써 토큰화를 해결할 것을 제안한다.       

이 절에서 설명하는 다양한 기술은 다양한 크기와 품질의 단위를 산출한다.


----

## 5.1 Character-level neural models that learn to skip steps at higher levels

**[Elman(1990)]**     
* 이미 90년대에 Elman(1990)은 character-level의 RNN을 수동으로 분석하고 예측 깜짝 비교를 단어 경계(boundaries)와 관계시켰다.    
* 이 아이디어는  Schmidhuber(1991, 1992)의 “neural sequence chunker”에서 확장되었다.     


**[Doval and GómezRodríguez(2019)]**     
* 최근에는 Doval and GómezRodríguez(2019)의 beam search framework 하에서 character-level neural models 뿐만 아니라 n-gram models에도 분석을 적용하여 공간이 삭제되는 micro blog texts를 분할했다.     


**[```HM-RNN```(Chung et al., 2017)]**      
* post-hoc surprisal thresholding(사후 임계값)을 사용하는 대신 [4.2](# Open-vocabulary-language-modeling-with-(tokenizer-defined)-words-made-of-characters)에서 동기화된 여러 timescales의 아이디어를 취한다.     
* 기울기 하강 최적화 방법        
   * skip이나 update하기 위한 이진 결정(binary decisionbinary decision)을 학습       
   ➡ 단어 경계 감각 제공    
   * 직선 추정기를 사용하여 대략적인 기울기 하강으로 최적화        
* 계층들 사이의 통신은 **양방향적**으로 일어남      
   * the lower network: 그것의 최종 상태를 higher 네트워크에 보고한다      
   * the higher network: 그것의 새로운 상태를 lower 계층에 보고한다.     
   * 이러한 통신을 스스로 계속 진행한다.    
 
 
**[```HM-RNN```(Chung et al., 2017)의 연구]**      
* **Kawakami et al. (2019):** 데이터에 공간을 포함할 때 단어 경계를 “recover(복구)” 하지만,      
Kawakami 외(2019)는 **공간을 포함하지 않을 때** 이 모델로 사용할 수 없는 세그먼트(unusable segments)를 얻었다고 주장한다.     
* **NMT에 ```HM-RNN```을 사용하려고 할 때:**     
   * Cherry 등(2018)은 그것이 훈련되도록 하는 데 **많은 수정 작업이 필요**했으며,      
작업에서 그것의 성능은 경쟁적이지만 **우수하지는 않았다**고 보고했다.     
   * 이 발견은 HM-RNN이 잘 훈련되도록 하고, 이를 완화하며,    
   텍스트 데이터에 대한 하위 세그먼트(subpar segmentation)를 보여주는 논문을 전문으로 하는  Kádár et al. (2018)의 연구 결과를 뒷받침한다.      
* Kreutzer and Sokolov (2018)는 NMT에 대해 **단계를 건너뛰고 하위 레이어로 요약을 생성**하는 유사한 패러다임을 사용하려고 노력하며, **건너뛰기**는 거의 사용되지 않으므로 좋은 성능을 위해 **불필요**해 보인다는 것을 발견한다.        
* **모델의 확장:** 그럼에도 불구하고, 이 모델은 Luo와 Zhu(2021년)에 의해 구와 문장 수준의 경계로 확장된다.      
* 더 coarser한 계산 레이어를 가지고 있음에도 불구하고, 이러한 모델은 여전히 단어가 생성될 때마다  “spell out”해야 한다.       
즉, 토큰을 재사용 가능한 단위로 memoize할 수 없다는 것을 지적할 필요가 있다.       


-------


## **5.2 Marginalization over all possible segmentations**    
마지막으로, 개념적으로 간단한 접근 방식은,    
문자열의 분할을 training과 test time 모두에서 **marginalized해야 하는 잠재 변수(latent variable)** 로 다루는 것이다.     

**[문제]**       
이것은 본질적으로 겹치는 길이가 다른 문자열, 즉 "cat", "at", "foster cat"을 포함하는 어휘를 갖는 것을 의미하며,   
"my foster cat" 문자열은 잠재 단위의 다른 시퀀스에 대응하여 여러 방식으로 분해될 수 있다.    
➡ 분할의 수가 시퀀스 또는 컨텍스트 길이에서 기하급수적이다.            

**[해결]**      
* 잠재 분해에 대한 주변화를 위한 근사치(§ 5.2.1)에 의존       
* n-gram 모델(§ 5.2.2)을 사용하여 독립성 가정으로 모델을 단순화해야함       



<details>
<summary>📜 5.2.1 Approximate marginalization</summary>
<div markdown="1">
  
* **Chan et al. (2017)**     
    * beam search을 통해 **approximate MAP 추론**을 사용하여 관측의 한계 확률을 근사화하는 추정기를 제안한다.     
    * 그들은 그 모델이 훈련시키기가 매우 힘들지만 유망한 결과를 얻는 데 성공한다는 것을 발견했다.    
* **Buckman과 Neubig(2018)**   
    * 이 모델의 **불안정성을 확인**하고 LM 복잡도 측면에서 더 나은 결과를 생성하는 평균 RNN 숨겨진 상태를 기반으로 몇 가지 **근사 추론 체계를 제안**한다.       
* **Hiraoka 등(2020)**      
    * Unigram LM 토큰화 제안 분포(§ 6.4.3 참조)를 기반으로 유사한 모델을 구현하는데,      
    이 분포는 문장의 **𝑛-best 토큰화**가 모든 **문장 인코더 모델에 독립적으로 공급**되며,     
    **결과 문장 임베딩은 우선순위 토큰화 가능성에 따라 평균화**된다.      
* **Hiraoka 등(2021)**      
    * 별도의 손실로 토큰화 및 다운스트림 모델을 훈련시키고,      
    전자는 낮은 다운스트림 손실을 생성하는 토큰화를 보상하며,     
    후자는 조건부(및 강화) LM에서 샘플링된 하나의 토큰화를 사용하여 이 모델을 sequence-to-sequence 설정으로 확장한다. 
 
</div>
</details>  







<details>
<summary>📜 5.2.2 Exact marginalization using additional independence assumptions: segmental neural language models</summary>
<div markdown="1">


     

**[Kong et al.(2016)]**    
* segmental neural language models의 더 인기 있는 솔루션 개척      
* 그는 분할 문제를 **단조로운 13개의 seq2seq 작업으로 캐스팅**하여 characters에서 하위 문자열의 커버 순서(sequence of substrings), 즉 분할로 전환했다.      
* ```BiRNN```을 사용하여 처리되고 포함된 **전체 raw string에 대한 세그먼트 예측을 조건화**      
➡ 세그먼트화 결정/점수는 컨텍스트를 사용 가능             
* 이러한 임베딩을 사용하여 세그먼트 가능한 모든 개별 하위 문자열을 독립적으로 채점한 다음,     
개별 점수를 합산하여 전체 세그먼트를 채점          
➡ 동적 프로그래밍(DP)을 사용하여 covering of the entire input string를 효율적으로 찾을 수 있다    
* **동작 원리**     
   * 중앙 독립성 가정(central independence assumption) 때문이다.       
   * 이 모델은 세그먼트에 점수를 매길 때 다른 세그먼트에 의존하지 않고 **단지 주변 문자에 의존**할 뿐이다.     


**[Wang et al. (2017)]**
* 위의 Kong et al.(2016)에 대한 확장      
* 이 모델은 세그먼트화를 몰라도 실행 가능     
➡ 개별 세그먼트 생성 프로세스에서, 과거 표현을 사용할 수 있는 출력에 대한 문자에 대해 per-segment RNN을 보유      
➡ 동적 프로그래밍을 중단하지 않고 세그먼트에 대한 정보를 왼쪽에서 오른쪽으로 공유 가능      


**[LMing]**         
* 이제 **입력에 대한 조건을 생략**하여 세그먼트 언어 모델이라는 용어를 만든 Sun과 Deng(2018)의 모델을 산출     
* 한자에 대해 훈련     
* 비지도 학습된 세그먼트를 사용하여 중국어 단어 분할에서 경쟁하는 것으로 만들어짐      
* 2차 세그먼트 수의 **계산을 실현 가능하게 유지**하기 위해 세그먼트를 최대 **4자로 제한**한다.      


**[Grave et al. (2019)]**        
* Transformers를 독립적인 character-level global backbone으로 사용하여, **독립**적으로 **동일한 점프**를 한다.     
* 영어 open-vocabulary language modeling에서 평가할 때 perplexity가 개선되었다.    
* 하지만, obtained segmentation을 사용하거나 평가하지 않는 것을 발견      
➡ 원인: 이는 그들 역시 최소 400번 나타나는 4-grams만 사용하기 때문일 가능성이 크다.       

**[Kawakami et al. (2019)]**      
* 위와 동일한 **독립 아이디어를 사용**     
* **다른점:** 문자열 세그먼트의 방출(missions of string segments)은  Kawakami et al. (2017)와 같은 문자 수준 모델(§ 4.2 참조)과 학습된 임베딩이 있는 대규모 하위 문자열 세트(최대 10-grams까지 훈련: 데이터에 충분히 자주 나타남)의 컨텍스트 의존적 혼합에서 비롯됨        
* perplexity뿐만 아니라 일부 baselines(제5.3조 참조)을 능가하는 단어 분할 성능을 갖음     
* 하지만, 여전히 일부 이전 모델보다 훨씬 더 나쁜 성능을 보임      
    * marginal likelihood 대신 분할 성능에 hyperparameters를 조정하여 **불공평한 이점**을 가지고 있다고 주장     
* 이미지 캡션에 대한 훈련을 할 때 모델이 **설명되는 이미지에 액세스**할 수 있을 때,    
 perplexity과 분할 성능이 모두 **향상**된다는 것을 발견       
 ➡ **단일 언어 및 단일 언어 텍스트**(monolingual and unimodal text)가 학습 세분화가 다른 양식, 다른 언어가 존재할 때보다 **더 어려울 수 있음**을 보여준다.    
 
 
**[He et al. (2020)]**    
* NMT 시스템의 대상 측 생성기로 유사한 세그먼트 모델을 구축     
➡ 위와 같이 character backbone 아이디어를 따르는 Transformer-based version모델을 동적 프로그래밍이 가능화를 위해 사용함         
* NMT 시스템을 최종 모델로 사용하지 않고 **학습된 tokenizer로 사용**한다.      
➡ 이는 동적 프로그램을 marginalization에서 **maximization**로 변경하여 BPE 또는 유니그램 LM 대신 사용할 수 있는 **DPE라는 새로운 분할을 얻음**으로써 쉽게 달성 가능(§ 6.4 참조).       
* ```small Transformer-based NMT```모델로 토큰화하는 학습이 **더 큰 모델에서 사용**하기 위해 BPE보다 더 나은 분할을 생성한다는 것을 보여준다.      
* 특히 번역 작업에 토큰화 모델을 훈련하면 **소스 언어에 따라 다른 분할이 생성**되며, 더 중요한 것은 **더 나은 분할(segmentation)이 생성**된다는 것을 보여준다.        


**[Downey et al. (2021)]**     
* 문자를 조건화하고 세그먼트를 예측하는 아이디어는 결과가 ```RNN-based SNLMs```을 일관되게 능가하지는 않지만,     
Downey et al. (2021)의 Transformers 및 ```left-to-right autoregressive Transformers```에서 발견된 **방향 마스크 언어 모델링**(adirectional masked language modeling ) 설정으로 확장된다.       
* 이 모델들 중 많은 것들이 ```UnigramLM```에 기반을 둔 모델들의 변형이라는 점이 중요하다 §6.4.3.       

 </div>
</details>  


-----

## 5.3 Finding words through Bayesian non-parametrics
𝑛-gram과 word-based 언어 모델들의 시대에,   

**[MacKay and Peto (1995)]**       
* 자동 회귀 언어 모델들의 Bayesian 관점 도움이 된다는 것을 알아냄      
* 평균이 저차 분포(lower-order distributions)인 디리클레 분포에서 고차 분포(higher-order distributions)를 도출하는 계층적 모델에서 추론으로 𝑛-gram 모델에서 smoothing 및 backoff를 재해석한다.


**[Teh(2006)]**      
* 위의 생각을 확장     
* 우리가 다시 임의적으로 큰 차수의 𝑛-gram 분포를 갖는 계층적 PYP 언어 모델을 제안        
* PYP 언어 모델      
    * 𝑛-gram 언어 모델 smoothing과 유사      
    * 그러나 𝑛의 선택을 포기하는 원칙적인 방법을 제공      

**[Wood et al. (2011)]**     
* 위 모델링 아이디어의 정점의 달성        
* **성능**    
    * 임의의 이진 데이터에 대해 뛰어난 압축 성능을 자랑함      
    * 언어 모델링 작업 여전히 매우 잘 수행        
    * 하지만 이때 신경 모델이 이미 강력한 경쟁자로 나타남     


**[Goldwater et al. (2006b)]**        
* 이 Bayesian 관점을 확장       
* 새로운 단어들이 어떻게 처음 만들어지고 그리고 그것들이 실행 텍스트에서 어떻게 사용되는지를 설명함       
* **언급한 기술:** 2단계 언어 모델링 (제4.2절 참조)      
    * 1단계: 생성기 (새로운 어휘소를 생성하는)      
    * 2단계: 어댑터 (여기서 재사용을 지배하는, PyP)        
    * 위에서 말한 것과 같이 이 두개의 단계는 서로 상호작용한다.      
* **기능**   
   * 단어 경계를 유추하는 것 가능해짐    
   * = 비지도 단어 분할을 수행하는 것이 가능해짐      
   * 이유: 텍스트를 설명하는 2단계 모델과 무한한 수의 가능한 어휘소에 양의 확률을 할당할 수 있는 Bayesian 비모수 메트릭(nonparametrics)의 사용 덕분       
* 인지 과정, 특히 아동 언어 습득을 설명하고 모델링함으로써 더 많은 동기를 부여받음.       

**[Goldwater et al. (2009)]**      
* transcribe된 유아 지향 음성을 분할하기 위한 Unigram과 Bigram Dirichlet Processs(DP)를 요약하여 오래된 비베이지안 접근 방식보다 우월함을 보여준다.      


**[Mochihashi et al. (2009)]**     
* 아이디어를 bigram DPs에서 ∞-gram nested/hierarchical PYPs로 확장하여 **영어 필기 텍스트에 대한 단어 분할**을 개선한다. 



**[Elsner et al. (2013)]**     
* 일련의 세그먼트를 관찰된 실현으로 변환하는 **음성 프로세스**를 추가로 모델링한다.      


-----



## 5.4 Related task: Unsupervised Chinese Word Segmentation
중국어, 일본어 및 베트남어와 같은 **공백 구분자가 없는 언어**에 대한 단어 세분화는 중요한 연구 영역이며 까다롭기로 악명 높을 수 있다.     

중국어 단어 분할(CWS)에서는 신경 언어 모델을 포함하는 비지도 단어 분할을 탐색하는 데 관심이 높아지고 있다.     

전통적으로, 인기 있는 비지도 접근 방식은 두 가지 주요 경로를 취한다.        
**1)** 차별적 모델(discriminative models)      
**2)** 생성적 모델(generative models)      


**[1) 차별적 모델]**     
* 후보 분할에 대한 선량한 척도(goodness measures)에 의존한다.      
* 통계적 측정 방법     
    * 상호 정보(MI: Mutual Information)     
    * 정규화된 분기 엔트로피 변화(nVBE: normalized Variation of Branching Entropy)    
    * 최소 설명 길이(MDL: Minimum Description Length) 등이 포함된다(§6.3)     


**[2) 생성적 모델]** 
* 가장 높은 생성 확률의 최적의 분할을 찾기 위한 통계 모델 설계에 초점을 맞춤.     
* 모델의 종류     
    * 숨겨진 마르코프 모델(HMM: Hidden Markov Model)    
    * 계층적 디리클레 프로세스(HDP: Hierarchical Dirichlet Process)     
    * 중첩 피트만-요르 프로세스(NPY: Nested Pitman-Yor Process) 등이 포함된다(§5.3) 
* 𝑛-gram 언어 모델을 **neural language 모델**로 대체하여 차별적 접근 방식을 확장하는 것은 사소한 일이다.     
* 생성 접근 방식의 경우, 이전 연구는 **context encoder와 segment decoder**를 사용하여 **neural language 모델**을 구성하면 통계적 대응물에 대한 경쟁력 있는 성능을 달성한다는 것을 보여주었다(un and Deng, 2018, §5.2.2).






---
----


# 6 Learning subword vocabularies and segmentations
     
**[subword 단위의 특징]**     
* word-level 모델과 character-level 모델 사이의 원활한 전환을 가능하게 한다(§3)     
* pre-tokenization를 통해 얻은 단어 유사 토큰(word-like tokens)을 더 작은 단위로 분할한다      
* 위와 같은 분할은 가능한 모든 subword 단위 집합은 **유한**하며 **훈련 데이터에서 결정될 수 있**지만, **모든 문자**(또는 바이트, §8.3 참조)**를 포함**하는 것으로 가정한다.     
* test time에 나타나서, held-out 데이터안의 새로운 단어를 설명할 수 있다.      
* subword 정보에 대해 생각하는 것은 형태학적으로 풍부한 언어를 처리하는 데 더 많은 전통을 가질 수 있음     

**[Mikolov et al. (2012)]**
* OOV(Out of Vorder) 문제를 피하기 위해 언어 모델링 영어를 위한 단어 대신 **subword units18을 사용**할 것을 제안했다.     

**[Sennrich et al. (2016)]**
* 위와 같은 토크나이징에 대한 노력에도 불구하고, Sennrich et al. (2016) 이후 대부분의 현대 NLP 모델은 아니더라도 많은 모델에서 **크고 무한한 어휘 크기와 싸우는 것**이 관례가 되었다.     


위 문제 해결을 위해서 **"무엇을 subword 단위로 사용해야 하는가?"**가 매우 중요해졌다    

**[subword 단위의 종류]**      
* 수동으로 구성되고 언어적으로 정보가 있는 규칙 기반(rule-based) 시스템(§ 6.1)       
* 다른 옵션은 전통적, 언어적으로 동기 부여, 평가된 data-driven segmentation learners(§ 6.3)     
* 빠르고 쉬우며 downstream 성능을 향상시키기 위한 단순한 휴리스틱(§ 6.4)      


합리적인 동기에도 불구하고,     
아래의 언어들에게 **분할은 좋지 않은 아이디어**일 수 있다     
➡ 아랍어와 히브리어와 같은 Semitic 언어(non-concatenative morphological phenomena가 존재)      
➡ 위와 같은 언어는 character-level 모델이나 very small subword inventories한 모델에 의해 더 잘 제공된다(Amrhein and Sennrich (2021))         


---

## 6.1 Manually constructed linguistic analyzers
형태학적 분석은 형태학적(morphologically)으로 풍부한 언어에 매우 중요하다.    
➡ 이를 위해 단어 형태를 lemmata(단어의 기본형)와 inflections(단어의 굴절)들로 분석하기 위한 다양한 도구들이 개발되었다.  



**[Porter, 1980]**     
* 위의 형태학적 방법론을 위한 도구 중 가장 초기이자 가장 유명한 것       
* 언어학자들이 유한 상태(finite-state tools) 도구(FST; Beesley and Karttunen, 2003)를 사용하여 **수동으로 구성**하는 경우가 많다.      
➡ 이는 종종 형태학적 프로세스가 체계적인 설명을 제공함     
➡ 복잡한 데이터 기반 모델을 학습하려고 시도하는 것보다 유한 상태 분석기를 **수동으로 구성하는 것이 더 빠르고 저렴**하기 때문(Beemer et al., 2020).        
* 유한 상태 도구의 사용     
   * lemmata(단어의 기본형)와 다른 subword 단위의 공개(overt) 분할 또는 발견에 사용 가능    
   * 유한 상태 기계가 입력 문자열의 어떤 부분이 출력 선택으로 귀결되었는지를 쉽게 추적가능    
   * 형태론적인 logical tagger를 사용하여 분할을 유도 가능.     
* 위의 기능들은 물론 수동 주석에 의존하지만, 그 이상은 종종 느리고 불필요하게 복잡하다고 간주된다는 점을 지적할 가치가 있다.    


**[Tan et al. (2020)’s BITE]**
* 그럼에도 불구하고 tagging의 lemmatization(단어의 기본형) 조합은 크고 잠재적으로 노이즈가 많은 어휘를 다루는 데 성공적으로 사용되어 왔다.       
* BITE는 노이즈를 방지하고 변증법 데이터를 개선하기 위해 굴절된 형태를 **lemma와 tag로 변환**한다.      
* 이 survey의 나머지 부분에 대한 중요한 차이점은 그러한 접근법이 **더 강해질 가능성이 있다**는 것이다.      
* 앞서 말한 순수 연결 세분화(purely concatenative segmentation)는    
     * 순수 연결 세분화(purely concatenative segmentation)의 예      
     * "hoping”  ➡ “hope V.PTCP;PRS”        
     *  “ate” ➡ “eat PST,”        
     *  각 패러다임의 다른 **형식과 정보를 공유**할 수 있게 함       


**[Hofmann et al. (2021)]**
* 위와 같은 접근 방식의 이점을 보여     
* 토큰화 전에 단어를 형태소로 분할하여 파생 과정을 취소하면 정서 및 주제 분류 결과를 개선할 수 있다고 관찰한다.      


-----


## 6.2 Other Language-Specific Methods

각 언어에 따른 연구는 아래와 같이 진행되었다.     
* **독일어**: 독일어는 compounds가 공백으로 절대 분리되지 않는 특징을 갖음      
➡ compounds 분할에 대한 연구를 촉진(Kohn and Knight, 2003; Brashchler and Riplinger, 2004; Macherey et al., 2011).    
![image](https://user-images.githubusercontent.com/76824611/180714603-9002e888-77d9-4f90-b56a-065ff8e9f043.png)

* **산스크리트어**: 단어 간 경계에서 발생하는 프로세스가 있어 세분화가 복잡하다(Krishna et al., 2017; Huet, 2003)     
➡ ```/```를 통해 단어의 경계를 분리하는 연구를 진행했다       
![image](https://user-images.githubusercontent.com/76824611/180714337-7dfd8a43-9d07-4b7c-94de-bbf877a53b7c.png)

* **아랍어:** neural and subword-based models 시대에, 토큰화에 대해 가장 최근에 연구됨       
➡ 다양한 **언어 무관** 및 **언어별** 토큰화기를 조사했고 **데이터 세트의 크기** 및 **형태학적 복잡성**과 같은 요인에 따라 성능이 다르다는 것을 발견했다. (Alyafeai et al. (2021))       
![image](https://user-images.githubusercontent.com/76824611/180716284-fd86f6c5-ad89-426f-9048-09fc260c4dff.png)



* **중국어:** **glyph(상형문자)** 또는 **발음**을 기반으로 potential sub-character 정보를 포착을 목표로 함      
➡ BPE를 적용하기 전에 **문자를 획순 또는 로마자 순서로 변환**했다. (Si et al. (2021))    
![image](https://user-images.githubusercontent.com/76824611/180716512-c265fad5-2d5c-40a4-a483-4802742e4e3f.png)


* **한국어:** 대부분의 한국 데이터 세트에 대해 BPE(§ 6.4)에 이은 **형태론적 분할의 hybrid 접근법**이 가장 잘 작동한다는 것을 보여준다. (Park et al. (2020))    
![image](https://user-images.githubusercontent.com/76824611/180717173-2bd3eb88-1c70-4d89-b156-81ec693f20a6.png)



-----


## 6.3 Unsupervised morphological segmentation

**[초기연구]**             
* subword 표현은 현재 downstream application의 사용에 기초하여 평가되는게 일반적이지만,      
* 이 space의 초기 연구는 subword19 세그먼트의 언어적 타당성을 **직접 평가**하였다.    
* **직접 평가하는 방법:** CELEX(Baayen et al., 1995)와 같은 **데이터베이스** 또는 **언어 문법의 주석(annotations)** 및 **analyzers의 주석(annotations)** 을 통해 직접 평가   


**[(Brent and Cartwright, 1996)]**      
말뭉치와 화자의 계산 모델 모두에서, "분포 규칙성(distributional regularity)과 **음운론적 제약(phonotactic constraints)** 은 **분할에 유용**하다"는 것이 발견되었다.    


**[de Marcken (1996)]**     
* 문장으로부터 단어를 넘어 텍스트를 재귀적(recursively)으로 분해하고 “composition and perturbation”을 통해 문자로 변형        
 ➡ 언어적으로 그럴듯한 단어를 복구하는 결과를 제시         
 
**[MDL]**     
* = essentially minimum description length(MDL; Rissanen, 1989)        
* Brent et al. (1995)에서 MDL 기반의 접근법을 제안    
➡ **potential 어휘 단위의 길이**와 이 어휘로 **인코딩된** **텍스트 길이의 합이 최소화**되는 형태론적 세분화에 대한essentially minimum description length(MDL; Rissanen, 1989) 기반 접근법    


<details>
<summary>📜 MDL 더 자세히 보기</summary>
<div markdown="1">
  
MDL은 Object를 만들고 가능한 한 적은 Bit로 표현하는 것을 말함        

[ex]    
100만에서 1을 뺀 수를 가정해보자    
이를 숫자로 표현하면 999999 이다       
100만에서 1을 뺀 수와 999999 중에 어떤 방법이 더 간단하게 표현하는 것일까?    
당연히 전자가 더 편리한 표현임을 쉽게 알 수 있다.    

이와 같이 표현할 떄 더 쉽게 표현하는 것이다.     
 
</div>
</details>  


**[MDL 효과]**     
* MDL 기반 접근법은 Linguistica(Goldsmith, 2001)에서와 같이 unsupervised된 형태론적 세분화를 위해 많이 사용되고 인용되었다.   
* 또한 이는 영어 및 로맨스 언어(Romance, 라틴어에서 발달한 프랑스어, 이탈리아어, 스페인어 등을 가리킴)에서 형태소 경계와 높은 상관 관계가 있는 세분화를 생성하는 것으로 밝혀졌다(Baroni, 2000; Goldsmith, 2001).        


**[MDL 단점]**     
* 처음에, 이러한 접근은 가능한 형태학적 구조나 패러다임에 대한 **추가 정보**에 의해 **가볍게 유도**되었다. 
* 하나의 형태소 경계에서만 그것들은 **처음에 테스트된 언어에만** 가장 적절했다.        
* 단어 유형을 접미사나 접두사를 가진 줄기의 집합으로 분할하고, 형태소를 추가 하위 단어로 재귀적으로 분할하거나 조건화된 **문자 변화를 설명할 수 없었다**.          
➡ 그래서 형태소 경계는 단 하나뿐이었고, 그것들은 **처음에 테스트되었던 언어에만 가장 적절**했다. (다른 언어에 적용성 떨어짐)
* ⭕ **SOLUTION**: 형태학적 '카테고리'와 세분화 모델 내의 추가 구조를 사용하여 회수 및 적용 가능성을 확대했다.        



**[MDL 모델 개선: _Morfessor family21_]**            
* **언어적 편향을 통합**하는 것을 목표    
* 몇 가지 unsupervised 및 semi-supervised 분할 모델로 구성된다.     
 
**[_Morfessor 1.0_ (Creutz and Lagus, 2002)]**           
* 나중에 _Morfessor Baseline_ 이라고 불림       
* unigram 문법 형태 단어의 빈도와 길이에 기반한 재귀 MDL 모델이다.     
* **단점:** 이 모델은 추가적인 이론없이 단어를 과도하게 분할하고 생성하는 경향이 뚜렷하다.      
ex) splits such as ‘s + plit.’ 
   

**[Morfessor CatMAP(Creutz and Lagus, 2005)]**     
* **loose한 형태학적 범주**(prefixes, stems, and suffixes)의 **순차적 특성**에 대한 ```hierarchical HMM-based``` 모델을 추가함      
* 여기서 이 모델을 쓰기 전에  wordlists에서 semi-supervised으로 학습할 수 있다.      
* 이 모델은 Morpho Challenge(영어, 핀란드어, 터키어, 독일어)에서 평가한 언어에서 발견되는 ✨**연결 형태학**에 여전히 이상적이다(Kurimo et al., 2010).


**[The _FlatCat_ model (Grönroos et al., 2014)]**         
* unsupervised 조건에서 정확도를 감소시켰지만 semi-supervised을 단순화하고 개선했다.   



**[EM+Prune(Grönroos et al., 2020)]**    
* 가장 최근의 Morfessor 모델      
* 위의 전통을 최근의 기술과 병합하여 Unigram LM의 엄격한 일반화를 했다. (Kudo, 2018, § 6.4.3 참조).      

**[Snover and Brent, 2002; Creutz and Lagus, 2005; Monson 등, 2007, 2009; Lignos, 2010; Narasimhan 등, 2015]**      
* 형태론적 세분화에 대한 많은 접근 방식을 형태론적 패러다임과 **inflection(단어의 변형) 클래스**의 개념을 통합하여 작업    
* 이러한 관점에서, **explicit rules, clustering, or ‘chains’**을 통해 **초기 MDL 모델**에서 제한된 패러다임 구조를 **확장**하는 데 초점을 맞췄다       





**[Shone and Jurafsky, 2001; Snover and Brent, 2001; Yarowsky and Wicentowski, 2000; Bergmani).s and Goldwater, 2017]**    
* **공유 패러다임(shared paradigms)** 으로 형태를 발견            
   * 세분화를 개선하는 데 초점을 맞춤     
   * 표면 형태(surface forms) 전반에 걸쳐 형태학적 관련성을 유도      
   * 불규칙 형태(irregular forms) 전반에 걸쳐 공유 구조의 발견을 개선하기 위한 **철자 변경(spelling changes)을 허용**    
* **장점**         
   * **분할 리콜**이 더 높아짐(higher segmentation recall)      
   * **말뭉치 수준의 일관성**이 더 높아짐    
   ➡ 더 **compact한 말뭉치 표현**이 될 수 있음     
* **단점**         
   * 정확도는 Morfessor 또는 단어의 빈도 기반 기술보다 낮은 경우가 많다.     



✨✨✨✨      
§ 6.5에서 간략하게 논의되었듯이, **downstream작업에 대한 tokenization**으로서 **형태론적 세분화**를 사용하는 것은 § 6.4의 경량화 기법( lighter-weight techniques)에 비해 **일관되지 않은 개선**만을 제공하며, **최근 연구는** 주로 **이러한 접근 방식의 단순성을 선택**한다.
✨✨✨✨      



-----


## 6.4 Modern fast subword segmentation algorithms
앞에서 설명했듯이,     
오늘날 중심적으로 간주되는 subword tokenization는 Sennrich et al. (2016)의 **Byte-Pair-Encoding(BPE; Gage, 1994)의 사용**이었다.          


<details>
<summary>📜 6.4.1 BPE (Gage, 1994; Sennrich et al., 2016) 보기</summary>
<div markdown="1">
  
**[BPE 의미]**     
* 바이트 페어 인코딩(Byte Pair Encoding, BPE)    
* 1994년에 제안된 **데이터 압축 알고리즘**
* **substrings 이 참조(references)로 대체**되는 “macro-schemas” (Storer and Szymanski, 1982)라는 이름의 계열의 압축 알고리즘이다.         
* 이 이름은 Gage(1994년)에서 만들어졌지만, 자연어(Wolff, 1975년)와 유전자 배열(genetic sequences)의 복잡성(Angel Jiménez-Montano, 1984년)에서 적용된 알고리즘이다.          


**[BPE 기능 방법]**     
* 토큰화를 학습할 때 ```BPE```는 인접한 기호 쌍을 해당 쌍을 나타내는 **새로운 기호로 대체**한다.       
➡ 주어진 시간에 **가장 자주 발생하는 모든 쌍의 발생**을 **반복적으로 병합**             
* **test time**에 토큰화 모델의 **train 중에 수행된** 순서대로 기록된 **모든 병합을 실행**함으로써 **train 과 동일한 병합 절차를 수행**할 수 있다.       
* Byte-Level BPE(Wang 등, 2019)는 characters가 아닌 **raw bytes**(§ 8.3 참조)에 BPE를 적용한다.      


**[BPE 사용]**     
* GPT-2(Radford 등, 2019) 및 기타 모델에서 사용된다.     
* BPE-dropout (Provilkov et al., 2020)은 subword 정규화를 허용하는 확장이다(제6.4.3조 참조)   
 
 
[BPE 예시로 더 알아보기](https://wikidocs.net/22592)
 
 
</div>
</details>  


  






<details>
<summary>📜 6.4.2 WordPiece (Schuster and Nakajima, 2012) 보기</summary>
<div markdown="1">
  

**[Wordpiece Model (WPM)]**    
* BPE의 변형 알고리즘        
* 매우 유사한 기법이 일본어와 한국어 텍스트에 대해 "WordPiece"라는 이름으로 제안되었음    
➡ 문자는 공백 없이 작성되므로 공백으로 분리된 토큰에 대한 의존은 불가능하다      
* BERT(Devlin et al., 2018) 및 다른 모델에서도 사용된다.     


**[Wordpiece Model 기능 방법]**     
* BPE와 달리 WordPiece는 **가장 자주 발생하는 쌍을 병합하지 않지만** 업데이트된 어휘로 훈련된 **n-gram 기반 언어 모델이 데이터에 도달할 가능성을 높이는 쌍을 병합**한다.       
➡ BPE가 빈도수에 기반하여 가장 많이 등장한 쌍을 병합하는 것과는 달리, 병합되었을 때 코퍼스의 우도(Likelihood)를 가장 높이는 쌍을 병합한다는 것이다.     
* 텍스트를 분할하기 위해 WordPiece는 단어당 좌우 최장 일치 우선 전략을 따르므로 매우 빠른 선형 시간 처리가 가능하다(Song et al., 2021).


**[예시]**    
* 논문엔 없지만 저게 무슨소린지 몰라서 논문을 찾아봤다...  
* 그래서 논문을 바탕으로 예시를 들어봤다.     
* 미리 학습된 데이터 셋을 통해서 아래와 같이 분리하는 것이다.    
  * 분리는 공백으로 하고 이미 있는 띄어쓰기는 공백을 분리하기 전 ```_```으로 표시해둔다      
 
```
공연은 끝났어 -> ['공연-' + '-은' + '끝-' + '-났어'] -> 공연 은_끝 났어
공연을 끝냈어 -> ['공연-' + '-을' + '끝-' + '-냈어'] -> 공연 을_끝 냈어
```

```
Word : Jet makers feud over seat width with big orders at stake
Wordpieces: _J et _makers _fe ud _over _seat _width _with _big _orders _at _stake
``` 
 

**[단점]**     
* 그런데 문제는 위처럼 토크나이징을 하려면 해당 언어의 언어학적 지식과 학습데이터가 필요하다.      
* 그러나 언어가 다르고, 도메인이 다르면 이를 준비하는 것은 어렵다.      

 
 
</div>
</details>  








<details>
<summary>📜 6.4.3 UnigramLM (Kudo, 2018) 보기</summary>
<div markdown="1">
  


**[방법 over view]**    
* Kudo(2018)는 언어 모델에서 사용을 평가하여 하위 subword 후보를 판단           


**[simple unigram Language Model(unigram LM)의 방법]**       
* **1)** 사용하고 목표하는 것보다 **훨씬 많은 subword 단위**를 포함하는 어휘로 시작한다           
* **2)** subword 단위를 반복적으로 제거한다.      
   * 2-1) 위 반복에서 unigram LM은 EM을 사용하여 훈련됨        
   * 2-2) 그리고 나서 가장 낮은 확률의 항목을 어휘에서 제거해나가는 것임    
* **3)** 원하는 어휘 크기에 도달할 때까지 이 과정을 몇 번 반복 


**[효과]**      
* 이 확률론적 설정은 주어진 문자열과 일치하는 possible 분할이 많다는 사실도 깨끗하게 모델링할 수 있다     
* 하나의 결정론적 분할을 사용하는 대신 **샘플링된 분할("subword 정규화")** 를 사용한 훈련이 실제로 **기계 번역 성능을 향상**시킨다고 보고한다.     


**[향후 연구]**      
* **Provilkov et al. (2020):** BPE 구성에서 individual merges의 건너뛰기가 세분화의 다양성(variety in segmentations)으로 이어지는 **BPE-dropout**을 제안하였다.      
* **Subword regularization:** **단일 언어 도메인 내 작업**에 도움이 될 뿐만 아니라 **다국어 모델**에서 번역을 개선하는 데 도움이 되는 것으로 나타났다. (§7)   
* **sampling segmentation**: 이 방법이 도움이 된다는 관찰은 토큰화를 정의하는 LM으로 Bayesian nonparametric model (§ 5.3 참조)을 사용하는 Hiraoka 등(2019)에 의해 확인되었다.        
* **Wang et al. (2021a)**: unigram LM이 입력의 **character-level 수준 BiLSTM 인코딩**을 기반으로 하는 유사한 모델을 구축하고 unsupervised 중국어 단어 분할에 적용한다.    


</div>
</details>  


<details>
<summary>📜 6.4.4 SentencePiece (Kudo and Richardson,2018) 보기</summary>
<div markdown="1">
 
**[SentencePiece]**     
* google에서 제공하는 Tokenizer tool   
* 위에서 와 같이 자주 assumed된 알고리즘 자체가 아니라 실제로 소프트웨어 패키지이다.    
* BPE와 Unigram LM 알고리즘을 모두 제공한다 (따라서 "SentencePiece"를 지정하는 것은 확실히 충분히 유익하지 않다).   
➡ 즉 각 언어에 따라 원하는 토크나이징 방법을 선택할 수 있다.        
* BPE를 포함하여 기타 서브워드 토크나이징 알고리즘들을 내장한 센텐스피스(SentencePiece)는 일반적으로 실무에서 선택할 수 있는 최선의 선택 중 하나다.       
* Sentencepiece의 기본 tokenize 방식은 unigram    
* 중요한 것은 다른 구현과 달리 **공백을 미리 정해진 특수한 단어 경계로 취급하지 않는다.**     
  * 학습된 단위가 이러한 경계를 넘을 수 있도록 함      
  * 중국어 및 일본어처럼 공백으로 토큰화된 단어가 없는 언어의 사전 토큰화의 필요성을 없앤다   

 
 
</div>
</details>  


-----


## 6.5 Comparing morphological segmentation to BPE and friends
여러 연구에서 linguistically motivated segmentation를 data-driven segmentation과 비교했다.     
* morphological segmentation = linguistically motivated segmentation     
* BPE and friends = data-driven segmentation     
➡ 하지만 결정적인 결과가 없다.            


**[linguistically motivated segmentation(morphological segmentation)을 옹호하는 입장]**      
* **Bostrom and Durrett (2020)**        
   * **UnigramLM**은 **질적으로** BPE보다 더 나은 segmentation을 가져옴     
   ➡ 그들은 형태소와 Morfessor 형태에 더 잘 대응하는 경향이 있음         
   * **UnigramLM**은 **정량적으로** BPE보다 더 나은 segmentation을 가져옴         
   ➡ 그것들은 현대 NLP 작업에서 BERT 스타일 모델의 성능을 영어로 조금, 일본어로 많이 향상시킨다       
* **Matthews et al. (2018)**     
   * 형태학적 분석을 사용할 때,터키어와 같은 agglutinative(교착어) 언어에 대해 언어 모델링이 개선될 수 있음을 보여준다.      
   * **교착어:** 고립어와 굴절어의 중간적 성격을 띠는 것으로 어근과 접사에 의해 단어의 기능이 결정되는 언어의 형태이다.    
* **In Schwartz et al. (2020)**               
   * low-resource 연구에서 **Morfessor 기반 언어 모델**(및 character-based 모델, §8 참조)이 BPE 기반 모델보다 성능이 우수하다는 것을 보여준다.      
* **Pan et al. (2020)**      
   * 위와 마찬가지로 BPE를 적용하기 전에 **형태학적 분석기**를 사용하여 터키어와 위구르어의 NMT(Neural machine translation)를 개선한다.       



**[data-driven segmentation(BPE and friends)을 옹호하는 입장]**      
* **Zhou (2018), Domingo et al. (2018), Machácek et al. ˇ (2018), and Saleva and Lignos (2021)**    
   * unsupervised으로 얻은 "형태학적" subwords를 사용하면, 오직 Ataman과 Federico(2018b)만이 BPE를 능가할 수 있다는 것을 발견한다. 
  ➡ 나머지는 BPE를 능가하지 못함       
* **Banerjee and Bhattacharyya (2018)**     
   * Morfessor 및 BPE로 분할하여 얻은 번역을 분석하고, 개선 가능성이 있는 것은 **언어의 유사성**에 달려 있다는 결론을 내린다.  * **Gallé (2019)**      
   * BPE의 우수한 성능에 대한 가능한 설명       
   * BPE의 성능이 압축 용량과 관련이 있다고 주장한다.       
   * 동일한 클래스의 압축 알고리듬의 멤버에 대해, BPE는 데이터 압축 벤치마크에서 상위 수준에 근접한다.      
   
  
**[둘을 결합하자는 입장]**     
* **Huck et al. (2017)**             
   * 두 접근 방식을 결합할 것을 제안한다.      


-----


## 6.6 How many units do we need?

또 다른 미해결 질문은 최적의 성능을 위해 **몇 개의 merges를 선택**해야 하는지에 대한 문제이다.       

**[Mielke et al., 2019; Domingo et al., 2018]**     
* 최적의 선택 개수는 **작업과 도메인에 따라 다를 수 있으며** **언어**에 따라 다르다     



**[Kharitonov et al., 2021]**      
* **더 많은, 더 큰** subword 단위는 더 많은 memorization를 허용하고 더 많은 memorization를 유도한다       
* 이는 응용 분야에 따라 바람직할 수도 있고 그렇지 않을 수도 있다.      


**[Gowda and May (2020)]**    
* 다른 크기를 시도하지 않고도 가장 잘 작동하는 병합의 수를 예측하는 것이 바람직할 것이라는 생각에서 시작된 논문이다.     
* 이 논문은 그러한 휴리스틱 중 하나를 발견했다고 주장한다. 
* 즉, 하위 단어 단위의 95%가 최소 100회 이상 나타나도록 하면서 전체 시퀀스 길이를 최대한 단축하는 것이다      
* 그들의 동기는 신경 기계 번역 모델이 출력에서 단어의 빈도에 편향되어 있으므로 **더 균일한 단어의 빈도 분포를 유지**하는 것이 더 낫다는 것이다. 

**[Ding et al. (2019)]**        
* Gowda and May (2020)와 유사한 연구      
* 위 연구와 같이 과거 작업에서 merge 수에 대한 제안이 얼마나 모순되는지를 밝힘       
* **low-resource** 시나리오에서는 **merge 수가 훨씬 적다**는 것을 추가한다.     



그리고 대망의 [Transformer](https://yerimoh.github.io/Lan/)가 나왔다    
➡ 트랜스포머의 트렌드는 LSTM의 트렌드와 달라 흥미로운 질문을 이끌어낸다.     
➡ 작은 코퍼스가 **characters(문자)** 를 이해하는게 가능하지 않다는 의미일까요, 아니면 **words(단어)** 를 이해하는게 가능하지 않다는  의미일까요?     
 
**[Salesky et al. (2018)]**       
* merge를 선택하는 방법에 대한 질문에 대한 간단한 해답    
* BPE 세그먼트를 사용하여 NMT 모델을 교육하는 동안, BPE 어휘 항목을 병합하여 **어휘 크기를 점진적으로 증가**시키고, 감소되는 returns을 얻을 때까지 **새롭고 큰 BPE 세그먼트를 추가**한다.           
* 새로 도입된 subwords에 대한 임베딩은 병합된 두 BPE 세그먼트의 임베딩을 auto 인코더와 **병합하여 초기화**된다.      


**[Xu et al. (2021)]**    
* 어휘 선택 문제를 엔트로피가 가장 높은 토큰 세트에 대한 검색으로 공식화           
* language-independent 표준 설정을 종종 능가하는 어휘 병합을 얻는 BPE 단위에서 최적의 전송 중심 선택을 제안한다.     
 

**[Vilar와 Federico (2021)]**    
* 정지 기준(따라서 추가적인 하이퍼 매개 변수를 사용하지 않음)을 제공하는 또 다른 최근의 방법         



---
----


# 7 Shared vocabularies in multilingual models
많은 NLP 응용 프로그램은 한 번에 여러 언어로 텍스트를 처리하는데, 가장 확실한 예는 기계 번역 시스템일 수 있다.    
이러한 경우 언어별 토큰화기를 사용하거나 두 언어에 대해 각각의 단일 tokenizer 를 사용할 수 있다. (원하는 경우 임베딩 공유를 허용한다).   


**[Johnson et al. (2017)]**     
* 두 개 이상의 언어를 번역하는 고도로 다국어 시스템(highly multilingual system)을 구축하면서 데이터 기반 tokenizer 학습을 위해  **low-resource를 오버샘플링해야 하는지 여부**와 **그 경우 어느 정도까지 해야 하는지** 등의 질문에 대해 답한다.       
* 이러한 질문은 ```mBERT```, ```XLM```, ```XLM-R```과 같이 현재 널리 사용되는 **multilingual pre-trained** **트랜스포머**에서 각각 해결된다.          
 
**[Pires et al. (2019)]**      
* 이러한 모델에서 학습된 표현의 공유는 **언어 간 transfer을 돕는다**는 가설을 세웠다         
* **반대 입장**    
  * **Wu and Dredze (2019):** 이러한 주장에 대해 결정적이지 않다(inclonclusive)는 결과를 제공한다.         
  * **K et al. (2020):** 이 논문은 이에 동의하지 않으며 **subword 중첩이 **transfer에 **그다지 중요하지 않다**고 주장한다      

**[Ács (2019) and Rust et al. (2021)]**       
* 이러한 모든 모델이 적어도 어느 정도 low-resource 언어를 오버샘플링하도록 도와줘도, **BERT 기반 Transformers의 토큰화**가 여전히 **high-resource 언어에 편향**되어 있음을 보여준다.               
* 이러한 편향은 단어의 “fertility,”, 즉 단어가 평균적으로 분할되는 subwords의 수에서 확인된다      
* 또한 통제된 (monolingual 토큰화와 multilingual 토큰화를 비교하는) downstream 작업에서 결과에 영향을 미치는 것을 발견하기도 한다.       


**[Maronikolakis et al. (2021)]**             
* 언어 간 토큰화의 이러한 **granularity(얼마나 세밀하게 나눌 것이냐) 차이**도 **의미 표현 공유**에 **큰 영향**을 미친다는 것을 발견함   


**[Chung et al. (2020)]**          
* 다국어 환경에서 적절한 토큰화를 선택하는 것이 목표이다.         
* 할당 및 공유를 위해 명시적으로 제어할 언어 클러스터에 대한 subword 어휘를 선택하여,    
**처음부터 모델을 재교육**하는 접근 방식을 제공한다.    
* 반면에 처음부터 **재교육이 가능하지 않은 경우,**             
한 가지 옵션은 자원이 부족한/과분할된 언어(underresourced/oversegmented)에 대한 새로운 subword 단위를 추가하는 것이다.     


**[한계]**    
Wang et al. (2020b) and Chau et al. (2020)   모두 **무작위로 초기화된 임베딩**으로 subword 단위를 추가할 것을 제안하지만,    Ebrahimi와 Kann(2021)이 연구했을 때 이러한 접근 방식은 **잘 수행되지 않았다**.     
  

**[한계 극복]**       
* **Liu et al. (2021)**     
   * ~~새로 추가된 units를 초기화~~하는 대신 **기존 subword 단위에 대한 정보를 사용**하여 임베딩을 추정할 것을 제안한다.       
* **Wang et al. (2021b)**       
   * 모델이 **영어와 같은 high-resource 언어**에서 더 작은 subword 단위를 사용하도록 하여 **언어 간 분할을 더 유사하게** 만들고 **transfer을 지원**     
   ➡ 분할 방법이나 할당 변경과 함께 오는 **완전한 재교육을 피할 수 있다**.         
   * 특히, 그들은 모델을 미세 조정하여     
      * 1) deterministic segmentation: 결정론적 분할(low-resource 언어 분할에 비해 high-resource 언어를 세분화함)      
      * 2) well with sampled segmentations: 샘플링된 분할을 사용(그래서 high-resource의 단어조차도 더 세분화됨)      
      * 3) 각 출력 분포 간의 divergence 측면에서 두 방법을 동등하게 수행       



-----
----


# **8 “Tokenization-free” character- and byte-level modeling**
섹션 § 4.1과 § 4.2에서, 우리는 폐쇄 및 개방된 어휘 설정에서 character-level 정보로 단어 모델을 augment하는 것에 대해 논의하였다.     

pure character-level 또는 byte-level 모델링의 아이디어는 명백히 **단순화**처럼 보인다.     
* **Sutskever et al. (2011):** ```multiplicative RNN```을 사용하여 **한 번에 한 문자씩 문자열을 모델링**했음        
* **Chrupawa(2013):** character 수준 텍스트 분할(문맥화된 임베딩도 예상)과 같은 **character 정보가 필요한 작업**과 교착어(agglutinative)에 대해 ```character-level RNN```  모델링을 사용할 것을 제안한다.      
* **Conneau et al. (2017):** 원시 문자에서 텍스트 분류를 성공적으로 수행하였다.      
* **Al-Rfou et al:**        
   * 생성 character/byte level 모델의 큰 돌파구(breakthrough)      
   * sufficiently deep Transformers(그들의 경우 64개 레이어)가 이전의 subword-based 및 hybrid(§ 4.2) open-vocabulary language models을 크게 능가할 수 있다는 것을 보여주었다.    
   * **Choe et al. (2019):** 이 연구에 의해 의해 업데이트, previous word- and subword-based 최첨단 언어 모델링 결과와 다시 일치한다.    



----


## 8.1 Characters?
character-level 모델의 채택을 **제한**하는 이유:
character sequences가 word- or subword-level의 대응물보다 훨씬 김    
➡ 훈련과 추론이 느려짐        


**[Libovick and와 Fraser(2020)]**      
* 훈련 속도와 효율성을 향상시키기 위함     
* **subword 기반 모델**로 시작한 다음    
평가된 두 언어 쌍 중 **하나**에서만 **개선점**을 찾지만, 이 **개선점을 character수준**에서 작업하도록 해당 모델을 fine-tuning할 것을 제안한다.      


**[subsampling sequences를 위한 다양한 아키텍처]**          
* 그러나 훈련과 추론 모두를 위한 보다 **일반적인 접근 방식**은 **subsampling sequences를 위한 다양한 아키텍처**이며, 특히 기계 번역에 대한 응용에서 그러하다.    
* **Chung et al. (2016)**     
   * 인코더-디코더 모델    
   * **decoder**가 **character sequences를 생성**할 수 있도록 하기 위해 ```bi-scale recurrent neural network```을 도입한다.       
   * 이들은 subword-level decoder보다 향상된 성능을 보여준다.      
* **Lee et al. (2017) (and later Gao et al. (2020))**      
   * encoder 입력에서 convolution 및 pooling 레이어의 사용을 지지한다.       
* **Cherry et al. (2018)**              
   * Chung et al. (2017) (discussed in §5.1) 의 ```HM-RNN```을 포함한 다양한 **temporal pooling strategies**을 평가하고, 그 중 어느 것도 **성능과 속도의 적절한 균형을 제공하지 않았다**고 결론 내린다.      


**[character-level 모델 지지]**      
* **Gupta et al., (2019)**     
   * **character-level 모델이** noise 및 out-of-distribution data에 **더 robust**하다는 주장을 제기함       
   * 이는 noise가 있을 경우 word or subword-level 토큰 시퀀스가 크게 변경되기 때문일 수 있다.       
* **Gaido et al. (2021)**     
   * character-level processing이 모델에서 성별 편향을 덜 초래할 수 있다고 주장한다.     
   * 데이터 기반 BPE 어휘는 빈도 때문에 남성 형태에 치우쳐 있지만, 프랑스어나 이탈리아어와 같은 언어에서 여성 형태는 종종 남성 형태를 붙임으로써 구성되기 때문이다.      
   * 이러한 특징은 토큰화의 불균형을 가중시키고 있다. 그들은 character-level의 처리가 이러한 차이를 어느 정도 개선할 수 있다는 것을 보여준다.


**[character-level 모델 지지에 대한 반박]**  
* **Libovický et al. (2021)**        
   * Gupta et al., (2019)와 반대로 multiple **character-level MT** 시스템을 조사하여 "being often so motivated에도 불구하고 더 나은 도메인 robustness이나 **더 나은 형태적 일반화를 보여주지 않는다**"는 결론을 내렸다.      
* **Rosales Núñez et al. (2021)**    
   * noisy user-generated text에 대한 이 논문에 의해 Libovický et al. (2021)의 주장이 입증되었다.    



----


## 8.2 Character hashes?
대규모 multilingual 설정에서 character-level 모델을 naïve하게 사용하면 어휘가 매우 많아질 수 있다.      
➡ 이는 training떄 보지 못했던 단어를 실제 사용시 마주칠 수 있는 “out-of vocabulary” 문제를 초래할 수 있다.     

**[해결책: Clark et al. (2021)’s CANINE]**     
* 여기서 각 character에 자체 임베딩을 제공하는 대신,    
여러 해시 함수를 가진 가능한 모든 코드 포인트를 **더 작은 세트로 가져** 코드 포인트 간에 **매개 변수를 공유**한다(Svenstrup 외(2017)와 유사).    
* CANINE는 합리적인 계산 효율성을 달성하기 위해 down sampling 및 upsampling 작업을 추가로 포함     
* non-generative sequence 레이블링 작업에 중점을 둔다.   



-----


## 8.3 Bytes?
**[character-level 모델의 huge-vocabulary 문제를 피하는 다른 방법]**    
* **Unicode 이용**        
  * UTF-8과 같은 표준 인코딩에서 비롯된 **byte sequence를 사용**하여 텍스트를 표현하는 것이다.     
  * 이는 언어적 또는 통계적으로 정보를 제공하는 프로세스가 아닌 **표준 기구(Unicode consortium)** 에 의해 만들어진 고정된 토큰화 체계를 사용하는 것으로 볼 수 있다.    
  * Unicode consortium의 목표:    
      * "기술 기호, 구두점 및 글쓰기에 사용되는 많은 다른 문자들"을 다루는 것         
      * "현대 및 고대 세계의 모든 문자 시스템을 위한 모든 문자들"을 다루는 것             
* **Graves(2013)**       
  * "데이터에서 **가장 작은 의미 단위를 모델링**하는 원칙"을 따를 때 **byte 수준 모델링**을 자연스러운 선택으로 제시한다.     * **ByT5(Xue et al., 2021)**     
  * byte-level 모델의 character-level models과 유사한 **단점:** 더 긴 시퀀스로 인한 더 긴 훈련 및 추론 시간    
  * byte-level 모델의 character-level models과 유사한 **장점:** 노이즈에 대한 더 나은 robustness, out-of-vocabulary 문제 방지            
  * 이와 같이, character-level models과 마찬가지로 byte-level 모델에 대해서도 유사한 결과와 모델이 제안되었다.       
  * 예를 들어, 최근의 Charformer 모델(Tay et al., 2021)은 계산 비용 증가를 피하기 위해 입력 시퀀스를 downsampling하고, Al-Rfou et al.(2019)은 딥deep byte-level Transformer로 강력한 언어 모델링 성능을 보여준다.   


-----

## 8.4 So are these maximal decompositions the solution then?
그럼 위의 byte처럼 분해할 수 있을 만큼 분해하는 것이 해결책인지 알아보자.          


**[byte-level modeling의 단점]**       
* byte-level modeling은 종종 편향되지 않고 토큰이 없는 접근 방식으로 제시된다.      
➡ 하지만 우리는 byte-level modeling을 단순히 **최선의 방법이라고 생각하는 것이 아닌** 그저 fixed, predefined, and standardized된 토큰화 체계를 사용하는 것으로 간주하는 것이 더 건설적이라고 주장한다.        
* 실제로 Unicode standard는 다양한 콘텐츠를 표현할 수 있을 뿐 아니라 **언어적으로 동기부여된 목표를 염두에 두고 만들어진 것이 아니다.**       
* 실제로 유니코드 기반의 바이트 수준 토큰화 체계를 사용하면 표준이 만들어진 방식에 따라 **언어마다 상당히 다른 트레이드오프**를 얻을 수 있다.      
   * 라틴 문자(즉, ASCII)는 단일 바이트로 표현되는 반면 다른 언어의 문자는 여러 바이트로 표현된다.            
   ➡ 즉 위와 같은 특징의 단점은 UTF-8 기반 byte-level 토큰화 체계가 **다른 언어로** 동일한 기본 의미론적 내용을 나타낼 때 **엄청 긴 시퀀스를 초래할 수 있다**는 것이다.      
   * 이것은 ASCII 기호로 통신하지 않는 모델의 downstream 사용자에게 부당하게 계산 비용을 증가시킬 수 있다.       



----


## 8.5 Visual featurization: Pixels?
토큰화 없는 모델링(“tokenization-free”)에 대한 또 다른 접근 방식       
* byte-based 표현이 아닌 **시각적(visual)인 표현**을 사용한다.     


**[byte-based 모델]**     
* 완전한 기본 'vocabulary'를 다루는 것을 목표로 함      
* robustness 부족: 일관성 있는 관찰된 시퀀스에 의존하기 때문에 비슷하게 시각적으로 표현될 수 있는 변동과 노이즈에 민감하다       


**[시각적(visual)인 표현]** 
* 시각적 표현은 인간 **독자들이 텍스트를 처리할 때** 사용할 수 있는 **문자 사이의 유사성을 인코딩**하는 것을 목표로 한다.   * robustness 강함       
* 시각적 기능을 사용하는 많은 작업에 대한 초기 동기는 중국 문자에서 발견되는 **공유 문자 구성 요소를 반영**하는 임베딩을 만드는 것이었다.       
   * 중국어 characters (radicals), 한국어 음절 블록 등 희귀하거나 보이지 않는 공유 문자 구성 요소를 일반화할 수 있었다.    


**[시각적(visual)인 표현의 연구 과정]** 
* **Aldon Mingues et al., 2016; Costa-jussa et al., 2017**     
   * 이 공간(space)의 첫 번째 연구     
   * 문자 또는 단어의 bitmaps을 선형화하여 중국어 임베딩을 초기화했다.      
* **Wang et al., 2020a**     
   * 위 연구의 후속 연구          
   * 선형화된 이미지를 기반으로 **사전 계산된 임베딩, 컨볼루션을 통해** downstream 작업으로 **학습**한 **characters 세분화에 초점**을 맞춰 **희귀 문자에 대한 결과를 개선**했다.     
* **Sun et al. (2018, 2019)**     
   * **기존 연구:** characters 분할은 부분적으로 중국어로의 적용과 sub-character components에 대한 집중에 의해 동기 부여되었었다.     
   * 그러나 이 연구는 이 대신 **단어를 정사각형 이미지로 렌더링**하고 필요에 따라 **문자를 감싸면서** 해결한 문제인 **고정 너비 이미지의 사용을 가능**하게 했다.       
* **Mansimov et al., 2020; Salesky et al., 2021**      
   * 최근 연구는 “tokenization-free” 즉 토크나이저 없는 시각적 표현을 제안했다.     
   * 주어진 분할에 대한 이미지를 단어, 문자 또는 바이트로 렌더링하여 이전에 논의한 과제를 통합하는 대신 **각 문장을 이미지로 렌더링하고 픽셀 분해에서 직접 번역**한다.       
* **Salesky et al. (2021)**      
   * 이러한 모델이 기계 번역을 위한 다양한 언어와 스크립트에 걸쳐 경쟁적으로 수행될 수 있다.     
   * unicode 오류를 포함하지만 이에 국한되지 않는 induced noise에 훨씬 더 robust함을 보여준다.      
* **Mansimov et al. (2020)**     
   * 픽셀 대 픽셀 모델을 탐구.       
   * 아직 경쟁력이 없지만 **text-based**로 많은 우려를 깔끔하게 회피하는 어려운 설정이다.



-----



# **9 Discussion and Conclusion**      
과정과 용어 그 자체인 토큰화는 NLP의 초기부터 크게 발전해 왔다.   

이 논문에서, 우리는 그들의 진화를 추적하고 서로 다른 직관을 연결하면서 수년에 걸친 주요 변화를 강조했다. 

상당한 발전에도 불구하고, 우리는 토큰화를 다루는 **완벽한 방법이 없다**는 것을 보았다.    
➡ 공백으로 구분된 사전 토큰부터 학습된 하위 단어, 바이트까지 **모든 옵션은 단점과 장점이 있다**.      
➡ 많은 데이터는 근본적으로 서로 상충될 수 있다.        
ex)      
* 단순하고 강력한 처리를 위해 최대한 분해하려는 욕구   
* 언어에 걸쳐 공정한 방식으로 계산 효율성을 달성하고자 하는 욕구    


문자, 바이트 또는 픽셀을 선택하는 도구가 될 수 있는 응용 프로그램이 있는 반면,    
해석 가능성과 효율성을 위해 더 큰 이산 토큰이 바람직한 응용 프로그램은 무수히 많다.   


최근의 연구는 이에 대한 새로운 예를 제공한다.    
* Zhang 등(2021년): 토큰화된 다중 방식으로 입력을 공급하여 BERT를 개선   
* Dossou와 Emezue(2021년): low-resource MT를 개선하기 위해 인간 주석의 단어보다 큰 어휘 단위를 사용한다.   
* Itzhak과 Levy(2021년): subword 단위를 사용하는 것이 철자 정보를 포기할 필요가 없다는 것을 보여줌으로써 철자를 보여준다.       
* ngs: 서브워드 기반 사전 훈련 모델에서 복구할 수 있다. 



결론적으로, "end-to-endnes"를 허용한다는 신경망의 모든 약속에도 불구하고, **토큰화는 아직 멀었다**는 증거이다.     

