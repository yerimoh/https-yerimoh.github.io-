---
title: "Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP 정리"
date:   2022-07-12
excerpt: "Between words and characters:A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---

# intro
 
      

## 핵심  

## 읽기 위해 필요한 지식
* [word2vec](https://yerimoh.github.io/DL14/): baseline 모델이기 때문에 꼭 알아야 한다.        
* [Embedding](https://yerimoh.github.io/DL15/): word2vec 속도 개선으로 이 포스팅도 꼭 알아야 한다.      

## 원 논문
[Enriching Word Vectors with Subword Information](https://arxiv.org/abs/2112.10508)


---

# 목차  


---


# **Abstract**
우리가 모델링하고자 하는 텍스트의 단위는  
bytes에서 다중 단어 표현식(multi-word expressions)에 이르기까지, 텍스트는 다양한 세분성으로 분석되고 생성될 수 있다.      

최근까지 대부분의 자연어 처리(NLP) 모델은 단어를 통해 작동하여 discrete 및  atomic tokens으로 처리했지만,   
byte-pair encoding(BPE)을 시작으로 많은 영역에서 하위 단어 기반 접근 방식이 우세하여 **작은 어휘를 사용**하면서도 **빠른 추론**을 허용하고 있다.      

이 토크나이징 방법들에 대해 학습된 세분화를 기반으로 한 **subword-based approaches**뿐만 아니라 **단어와 문자의 하이브리드 접근 방식**이 **어떻게 제안되고 평가**되었는지를 보여줌으로써,     
사전 신경 및 신경 시대의 여러 작업 라인을 연결한다.     

본 논문은 모든 애플리케이션에 대해 **특별한 해결책**이 결코 **없을 것**이며,   
토큰화 방법이 애플리케이션에 여전히 중요하다고 결론짓는다.    


-----
-----

# **1 Introduction**    


_“‘tokens’ are not a real thing. they are a computer generated illusion created by a clever engineer” dril_gpt1         


우리가 사람들에게 NLP 모델을 처음 소개할 때,    
우리는 종종 텍스트가 컴퓨터에 작은 조각들로 잘려나간다는 생각을 당연하게 받아들인다.    
➡ 결국 그것은 단지 일련의 정수일 뿐이다. 따라서 우리는 이것들을 (보통) 연속 **sub-strings tokens**이라고 부른다. 


교육 환경에서, 그리고 사실 역사적으로 NLP에서 이러한 토큰은 다소 자연스럽게 단어(처음에는 영어로 “space-separated substrings”)로 암시된다. 


**[단어에서 구두점 분리의 어려움]**            
* "don't"라는 단어를 예로 들면,    
* 이 단어의 구두점 분할의 경우, ```"don"``` ```"'"``` ```"t"```라는 다소 무의미한 세 개의 토큰을 얻게 될 것이다.            
* 그런데 더 효과적인 토크나이징을 하려면 ```"do"```와 ```"n't"```의 두 단위를 산출해야 한다고 주장할 수 있다.       

---

## 논문 개요    
이 survey는 $$tokenization$$에 대한 위와 같은 질문을 다루며,      
본 논문은 의 각 챕터의 핵심내용은 아래와 같다,    

**[2: Tokens, word forms, and sub words](# 2-Tokens,-word-forms,-and-sub-words)**      
* 근본적인 질문과 용어를 상세히 설명     

**[3: Pre-tokenization yields word-like typographic units](# 3-Pre-tokenization-yields-word-like-typographic-units)**      
* 모든 NLP 작업에서 다소 매력적이지 않은 부분이 역사적으로 어떻게 다루어졌는지를 보여줌     

특히 지난 5년 동안, 다소 원자적인 단어와 같은 공간 분리 단위로서의 **"토큰"에 대한 직관적인 정의를 넘어서는 것**에 대한 관심이 새로워졌다.  
* **4**: 이를 위한 한 가지 방법인 **단어 내부 정보를 사용하여 단어와 같은 단위를 보강**하는 방법.    
* **5**: 신경 모델링은 이를 그 어느 때보다 쉽게 만들어 명백한 표시 없이 단어 경계까지 학습할 수 있는 모델로 이어진다(예: 공백이 있는 경우).  unsupervised word segmentation or discovery의 개념은 수십 년의 작업에서 자체적으로 존재했지만,    
* **6**: **subword** 단위를 원자 토큰으로 사용하는 현재 널리 퍼져 있는 아이디어를 최종적으로 살펴볼 때 고려하는 것이 유용하다는 것을 알게 될 것이다.      
* **7**: 다국어 어휘의 공유와 경쟁의 몇 가지 문제 살펴봄     
* **8**: 문자, 바이트 또는 심지어 픽셀로 최대 분해할 수 있는 가장 간단한 토큰화를 사용해봄        


이 논문이 끝난 후,       
* 바이트의 손쉬운 솔루션이 도달해 보이는 2021년 현재에도 "복잡한" 토큰화가 의 중요성을 논증하여 마무리 한다.     
* 최근의 발전은 특정 도메인과 사용 사례에 대해 최대 분해 처리를 가능하게 하지만, 그것들은 광범위한 NLP 시나리오를 다루지 않고 **자체의 단점과 편견**을 가지고 있다고 주장할 것이다.     


![image](https://user-images.githubusercontent.com/76824611/178999180-03d114b4-8ba7-4748-afce-5115faccc2b5.png)
A taxonomy of segmentation and tokenization algorithms and research directions


------
-----











# **2 Tokens, word-forms, and sub-words** 
이 챕터에서는 token에 대한 근본적인 질문과 용어를 설명한다.      

토큰, 단어 형태 및 하위 단어 NLP에서 텍스트 데이터는 
"문장"(sentences)과 "단어(words)"로 분할되었다. 

**[sentences]**     
* 거시적 단위(macroscopic units)는 종종 서로 독립적으로 고려되며, 그 자체가 미시적 단위로 세분된다.        




**[tokens]**    
* 일반적으로 토큰이라고 불리는 타이포그래피 단위(typographic units)는 언어적으로 동기화된 단위에 대한 근사치로 사용되어 왔다.     
* MAF(형태학적 주석 프레임워크)는 토큰을 "“non-empty contiguous sequence of graphemes or phonemes in a document."으로 정의한다.    
* 예를 들어, 현재 라틴어 스크립트와 함께 보편적으로 사용되는 **공백**과 같은 **타이포그래피 구분자**를 사용하는 쓰기 시스템의 경우, 토큰은 널리 사용되었으며 **비문구 비백색 공간 마크** 또는 **구두점의 연속 시퀀스**로 정의되었다.    
* 특정 문장 부호(예: 하이픈 또는 아포스트로피)를 토큰으로 임의로 정하면, 그러한 정의는 **문장**을 토큰을 기준으로 **원자 단위로 분할**한다.     
* 토큰과 단어 형태 사이에 **일대일 대응이 없다**("멀티톡 단어", "멀티워드 토큰")    
    * 단어 형태는 여러 개의 토큰(예:French or English sine die)으로 만들 수 있다.     
    * 반면, 여러 개의 단어 형태는 동일한 토큰(예: 영어 don't = do + not, 스페인어 damélo + da + lo)으로 나타낼 수 있다.     

**[tokens의 재정의의 필요성: word-forms을 위해]**      
* 최근 문장이 원자 단위로 분할되는 방식이 진화하여 **토큰화 개념이 재정의되**었다.    
* **필요성:** 과학적 결과(예: 하위 단어 분할이 기계 번역 성능에 미치는 영향와 기술적 요구 사항(예: 고정 크기 어휘가 필요한 BERT와 같은 언어 모델) 모두에 기초하여, 원자 처리 장치(아직 토큰이라고 함)가 **단어 형태(word-forms)의 근사치가 될 필요**가 있다.    
* ❌ **문제**: 퇴색한 결과적으로, 현재의 NLP에서 토큰의 개념은 여전히 MAF 정의와 완벽하게 일치하지만, 더 이상 **타이포그래피 단위의 전통적인 정의와 일치하지 않는다**.     


**[토큰화(Tokenization)]**     
* **정의:** 문장을 정형적이지 않은 (그리고 실제로 언어적이지 않은) 동기화된 단위로 분할하는 작업     
* 이는 종종 고전적인 토큰과 단어 형태보다 작기 때문에 종종 **subword**라고 불린다.     
* Typographic 단위("오래된" 토큰)는 현재 종종 "pre-tokens"라고 불린다.      
➡ 따라서 "tokenization"라고 불리던 것은 오늘날 "pretokenization"라고 불린다.      
* **pretokenization**: 이 용어는 토큰화의 새로운 개념에 대한 첫 번째 접근법이 **resulting 단위**(이전에는 “tokens”, 지금은  “pretokens”)를 “sub-words”로 **분할하기 전**에 **문장을 적절한 Typographic 단위**(예: 토큰화의 "옛" 개념)**로 분할하는 것을 포함**했다는 사실에 의해 동기 부여된다.     


-----
------


# **3 Pre-tokenization yields word-like typographic units**    
모든 NLP 작업에서 다소 매력적이지 않은 부분이 역사적으로 어떻게 다루어졌는지를 보여줌

**[옛 토큰의 사용방식]**       
* purely typographic token의 언어적 무관함(linguistic irrelevance)     
* 텍스트를 linguistically motivated된 단어 형태로 자동 분할하는 것의 어려움(difficulty of automatically splitting a text into linguistically motivated word-forms)     
* **위 두 문제의 절충안:**  purely typographic token과 purely linguistic word-forms의 중간 단위가 널리 사용되어 왔다.   

**[“pre-tokenizers”의 역사]**         
* **용어 변화**     
   * 초기에는 “tokenizers”로 알려짐     
   * sub-word 토큰화의 확산 이후 “pretoken”으로 바뀜    
   * 오늘날에는 “pre-tokenizers”로 명칭 바뀜         
* **대표 모델**      
   * 이러한 “pre-tokenizers” 중 일부는 비교적 단순하고 typographic token에 충실하다.      
   * Hugging Face의 토큰라이저 패키지에 있는 오래된 ```Moses (Kohn et al., 2007) tokenizer6``` 가 초기에 널리 사용됨           
   * 그리고 더 최근은 Hugging Face의 Tokenizers package에 있는```pre-tokenizers package```가 있다.      
 
**[“pre-tokenization” 개념의의 탄생과 문제점]**     
* 위의 대표 모델들의 사용은 "tokenization(현재는 “pre-tokenization”)"라는 단어를 만들어냈다.     
* **"tokenization(pre-tokenization)"의 의미:** 일반적으로 문장을 원자 단위로 분할하는 작업을 나타내게 됨.       
    * 즉, "후속 처리에서 분해될 필요가 없는 기본 단위" 단위가 typographic 단위보다 단어 형태에 더 가까울 때에도 "tokenization"이라고 불린다.       
    * 더 나아가, 토큰화자가 **문장을 분할**할 뿐만 아니라 **정규화**, **철자 수정** 또는 **named entity detection** 목적과 같은 원시 텍스트를 수정하는 데 공통적으로 사용되므로 현재 토큰의 표준 정의에서 벗어난다.      
* ❌ **문제점**     
    * 이러한 초기 토큰의 정의와 역할은 정규화 작업이나 다른 공백 기호의 통합 및 병합은 **토큰화 전으로 되돌릴 수 없게한다**.     
    * 토큰화된 출력에서 원시 텍스트를 확실하게 복구할 수 없다.   
  
  
 
------
-----


# **4. Augmenting word-level pretokenizer tokens with character information**
특히 지난 5년 동안, 다소 원자적인 단어와 같은 공간 분리 단위로서의 **"토큰"에 대한 직관적인 정의를 넘어서는 것**에 대한 관심이 새로워졌다.  
이 챕터에서는 이를 위한 한 가지 방법인 **단어 내부 정보를 사용하여 단어와 같은 단위를 보강**하는 방법을 설명한다.     

**[word-level 모델의 특징]**           
* 개념적으로 이해하기 쉬움    
* neural era에서는 해석 가능한 세분화된 기능을 제공      
* **단점**: 폐쇄형 어휘 모델: 훈련 중에 거의 보이지 않는 희귀하고 새로운 단어를 처리할 수 없음    
* **해결책:** 역사적으로 희귀한 단어 유형은 훈련 시 새로운 단어 유형 ```UNK(알 수 없음)```로 대체됨     


**[```UNK```대체 방식의 단점]**         
* 자연어 생성(NLG)을 수행할 때 UNK가 허용되지 않는다   
* ELMo 또는 BERT와 같은 large-scale 모델에 사용될 때 일회성 이벤트뿐만 아니라 유용한 의미의 앵커인 **새로운 단어에 대한 기능**을 **추출할 수 없다**        
* 영어 이외의 언어, 특히 더 생산적인 형태학 및 따라서 유형 토큰 비율이 높은 언어에서 희귀 단어를 제거하는 것은 불가능     

그럼에도 불구하고, **단어**가 언어의 **기본 단위**이기 때문에,       
**단어를 구성하는 문자를 기반**으로 처리함으로써 근본적으로 단어 기반 틀에서 희귀하고 새로운 단어의 **처리를 개선**하는 여러 접근법이 등장했다.      
우리는 단어 표현의 보다 포괄적인 처리를 위해 이 섹션에서 이러한 접근 방식 중 일부를 제시할 것이다.       

-----


## 4.1 Augmenting word-level models with spelling information    

**[단어에 초점]**         
* 언어를 위한 neural models에서,    
90년대와 2000년대의 연구는 **단어에 초점**을 맞추고, 대신 문자의 문자열을 처리했다.      

**[단어 자체에 대한 정보]**        
* 그러나 neural models이 NLP에서 중요해지자마자, 신경 네트워크에서 사용하기 위한 **단어와 문자 수준의 정보의 조합**(word- and character-level information)이 등장.       
* 단어 임베딩 추정을 돕기 위해 단어 자체에 대한 정보를 사용할 것을 처음 제안했다.      
* word’s embedding을 단어의 철자를 통해 구성한다는 아이디어를 **대중화**했다.   


**[CNN 기반 임베딩 기능]**     
* 임베딩 행렬을 CNN(convolutional neural network) 레이어로 대체할 때에도 generative 모델은 여전히 폐쇄 어휘이며, 
이는 훈련 데이터에서 보였던 단어만 예측할 수 있다는 것을 의미하기 때문에,         
CNN 구성은 **새로운 단어가 아닌** **희귀 단어**만 돕는다.      


**[CNN with 토큰의 철자]**      
* **각 토큰에 대한 철자**로 임베딩을 구성하는 CNN 기반 임베딩 기능     
* 새로운 단어를 예상하는 대신 "자주 단어를 정확하게 얻도록" 암묵적으로 훈련한다.  
* **이 기반 모델의 역할:** POS 태깅과 같은 다른 고전적인 NLP 작업의 발전을 이끌었고 궁극적으로 최초의 큰 문맥 단어 임베딩 모델 ELMo에 힘을 실어주었다.        
  * [fastText](https://yerimoh.github.io/LAN7/) 임베딩은 문자가 아니라 겹치는 n-gram에서 단어 임베딩을 구성할 것을 제안하여,     
새로운 단어에 대한 임베딩을 얻을 수 있다(generative 의미는 아니지만 그런 의미에서 "open-vocabulary"로 만든다).    

  * Ataman and Federico도 마찬가지로 문자 대신 (중첩) 𝑛-grams을 사용하여 기계 번역에서 더 나은 성능을 얻는다(형태학적으로 풍부한 언어에서 BPE를 능가함).     
  * 최근에는 철자 오류 또는 전송으로 노이즈가 많은 텍스트를 더 잘 처리함으로써     
구성 문자 임베딩에 대한 BPE 단위의 임베딩을 향상시켰다.      


**[small Transformer]**   
* 의료 텍스트와 같은 새로운 도메인; 동시에 Aguilar 등(2021)은 거의 동일하게 하지만 CNN 대신 small Transformer를 사용한다.     

**[구성 기반 접근 방식]**   
* 구성 기반 접근 방식은 사전 훈련된 단어 수준 입력 모델에도 통합되었다.     
* 특히, Pinter 등(2017)은 테스트 시간 동안 알 수 없는 단어가 나타날 때마다 호출되는 helper RNN 모델을 사용하여 **철자가 주어진 단어의 임베딩을 모방**하도록 훈련된 모델을 학습한다.      


---


## **4.2 Open-vocabulary language modeling with (tokenizer-defined) words made of characters**
**[open-vocabulary language modeling의 어려움]**    
* ```open-vocabulary language modeling```: test time때 **새로운 단어를 예측하고 생성**할 수 있는 모델     
* ```closed-vocabulary generative models```을 ```open-vocabulary language modeling```로 확장하는 것은 어렵다.              
* **어려운 이유**: 완전히 새로운 단어를 포함하는 문장의 무한 집합에 대한 확률 질량을 유지할 수 있어야 하기 때문에 input side의 Open-vocabulary 보다 다소 어렵다.     


**[Mielke와 Eisner(2018)의 모델]**         
* 단어 임베딩을 정규화하여 작은 문자 수준(smaller character-level)의 ```RNNLM```을 사용     
* smaller model을 사용하여 철자를 예측하도록 함으로써 일반적인 폐쇄 음성 단어 수준의 반복 신경망 언어 모델(```RNNLM```) 설정을 근본적으로 향상시키는 확률론적 2단계 모델을 제안한다.       
* 단어 수준(word-level) ```RNNLM```이 ```UNK```를 **예측할 때마다 새로운 단어를 즉시 생성**      
➡ 언어 개념과 직관적 모델링에 의해 동기 부여       
➡ 질적 및 정량적으로 성공적인 것으로 입증된 개방형 어휘 모델을 산출      



**[```cache model```의 확장]**        
* Kawakami et al. (2017)의 모델은 단어 및 문자 수준의 RNN의 유사한 2단계 설정을 따르지만,     
cache model(Grave et al., 2016)을 사용하여 directly copied할 수 없는 경우, **각 단어를 문자 수준의 RNN을 사용하여 철자**해야 한다.         
➡ 이들의 분석은 캐시 모델이 그렇지 않다는 것을 분명히 보여준다.      
* ```cache model```은 “bursty” **미지의 단어**만 복사하지만, 또한 잊어버리지 않도록 하기 위해 극히 extremely **common** function words들도 복사한다.       
* 이 아이디어는 ```Ataman``` 등(2019)에 의해 machine translation decoder(creating word embeddings on the encoder side from character-level ```BiRNNs``` as in ```ELMo```)에서 인코더 측에 단어 임베딩을 생성)에 대해 선택됨    
* 나중에 ```Ataman``` 등(2020)에 의해 확장됨          


**[다른 접근 방식]**     
* 더 낮은 속도로 ```multi-layer RNNs```의 higher layer를 실행하는 것이다(hidden state로의 업데이트를 건너뛴다)     
* 이는 El Hihi and Bengio(1995)에서 처음 제시된 오래된 아이디어이며(Schmidhuber (1991, 1992)의 "neural sequence chunker"에 구축됨)      
* 우리는 단어 경계와 그에 따른 segmentations을 실제로 배울 수 있다.    


----
----

# **5 Learning segmentations to find concatenative word-like pretokenizer tokens**
지금까지 우리는 [2](#2-Tokens,-word-forms,-and-sub-words)에 요약된 개념정의의 변화에도 불구하고,    
predefined된 단어(또는 사전 토큰화 출력) 개념을 갖는 것에 의존해왔다.              

**[문제]**      
그러나 영어가 아닌 언어나 robustness문제로 인해 predefined된 정의가 주어지지 않거나, 얻을 수 없거나, 단순히 바람직하지 않다면 어떻게 될까?      
우리의 **데이터 기반 머신 러닝 접근법**이 **토큰화를 학습**할 수 있는 방법이 있을까?     

**[해결]**       
이 절에 설명된 대부분의 접근 방식은 **의미 있는 단위**에 해당하는 세그먼트와 **경계를 찾기 위해**,     
**근사(approximate)** 또는 (더 많은 가정을 사용하여) **정확한 추론**을 수행할 수 있는 **잠재(latent) 변수로 암시적 분할(implied segmentation)을 처리**함으로써 토큰화를 해결할 것을 제안한다.       

이 절에서 설명하는 다양한 기술은 다양한 크기와 품질의 단위를 산출한다.


----

## 5.1 Character-level neural models that learn to skip steps at higher levels

**[Elman(1990)]**     
* 이미 90년대에 Elman(1990)은 character-level의 RNN을 수동으로 분석하고 예측 깜짝 비교를 단어 경계(boundaries)와 관계시켰다.    
* 이 아이디어는  Schmidhuber(1991, 1992)의 “neural sequence chunker”에서 확장되었다.     


**[Doval and GómezRodríguez(2019)]**     
* 최근에는 Doval and GómezRodríguez(2019)의 beam search framework 하에서 character-level neural models 뿐만 아니라 n-gram models에도 분석을 적용하여 공간이 삭제되는 micro blog texts를 분할했다.     


**[```HM-RNN```(Chung et al., 2017)]**      
* post-hoc surprisal thresholding(사후 임계값)을 사용하는 대신 [4.2](# Open-vocabulary-language-modeling-with-(tokenizer-defined)-words-made-of-characters)에서 동기화된 여러 timescales의 아이디어를 취한다.     
* 기울기 하강 최적화 방법        
   * skip이나 update하기 위한 이진 결정(binary decisionbinary decision)을 학습       
   ➡ 단어 경계 감각 제공    
   * 직선 추정기를 사용하여 대략적인 기울기 하강으로 최적화        
* 계층들 사이의 통신은 **양방향적**으로 일어남      
   * the lower network: 그것의 최종 상태를 higher 네트워크에 보고한다      
   * the higher network: 그것의 새로운 상태를 lower 계층에 보고한다.     
   * 이러한 통신을 스스로 계속 진행한다.    
 
 
**[```HM-RNN```(Chung et al., 2017)의 연구]**      
* **Kawakami et al. (2019):** 데이터에 공간을 포함할 때 단어 경계를 “recover(복구)” 하지만,      
Kawakami 외(2019)는 **공간을 포함하지 않을 때** 이 모델로 사용할 수 없는 세그먼트(unusable segments)를 얻었다고 주장한다.     
* **NMT에 ```HM-RNN```을 사용하려고 할 때:**     
   * Cherry 등(2018)은 그것이 훈련되도록 하는 데 **많은 수정 작업이 필요**했으며,      
작업에서 그것의 성능은 경쟁적이지만 **우수하지는 않았다**고 보고했다.     
   * 이 발견은 HM-RNN이 잘 훈련되도록 하고, 이를 완화하며,    
   텍스트 데이터에 대한 하위 세그먼트(subpar segmentation)를 보여주는 논문을 전문으로 하는  Kádár et al. (2018)의 연구 결과를 뒷받침한다.      
* Kreutzer and Sokolov (2018)는 NMT에 대해 **단계를 건너뛰고 하위 레이어로 요약을 생성**하는 유사한 패러다임을 사용하려고 노력하며, **건너뛰기**는 거의 사용되지 않으므로 좋은 성능을 위해 **불필요**해 보인다는 것을 발견한다.        
* **모델의 확장:** 그럼에도 불구하고, 이 모델은 Luo와 Zhu(2021년)에 의해 구와 문장 수준의 경계로 확장된다.      
* 더 coarser한 계산 레이어를 가지고 있음에도 불구하고, 이러한 모델은 여전히 단어가 생성될 때마다  “spell out”해야 한다.       
즉, 토큰을 재사용 가능한 단위로 memoize할 수 없다는 것을 지적할 필요가 있다.       


-------


## **5.2 Marginalization over all possible segmentations**    
마지막으로, 개념적으로 간단한 접근 방식은,    
문자열의 분할을 training과 test time 모두에서 **marginalized해야 하는 잠재 변수(latent variable)** 로 다루는 것이다.     

**[문제]**       
이것은 본질적으로 겹치는 길이가 다른 문자열, 즉 "cat", "at", "foster cat"을 포함하는 어휘를 갖는 것을 의미하며,   
"my foster cat" 문자열은 잠재 단위의 다른 시퀀스에 대응하여 여러 방식으로 분해될 수 있다.    
➡ 분할의 수가 시퀀스 또는 컨텍스트 길이에서 기하급수적이다.            

**[해결]**      
* 잠재 분해에 대한 주변화를 위한 근사치(§ 5.2.1)에 의존       
* n-gram 모델(§ 5.2.2)을 사용하여 독립성 가정으로 모델을 단순화해야함       



<details>
<summary>📜 5.2.1 Approximate marginalization</summary>
<div markdown="1">
  
</div>
</details>  


5.2.1 Approximate marginalization
Chan et al. (2017) propose an estimator to approximate the marginal probability of observations using
approximate MAP inference through beam search.
They find that the model is very hard to train, but
manage to obtain promising results. Buckman
and Neubig (2018) confirm this model’s instability
and propose some approximate inference schemes
based on averaging RNN hidden states that produce
better results in terms of LM perplexity. Hiraoka
et al. (2020) implement a similar model, based
on a Unigram LM tokenization proposal distribution (see §6.4.3), whose 𝑛-best tokenizations of a
sentence are fed into any sentence encoder model
independently and whose resulting sentence embeddings are averaged in line with their a priori
tokenization likelihood. Hiraoka et al. (2021) extend this model to sequence-to-sequence settings
by training a tokenizer and downstream model with
separate losses, the former by rewarding tokenizations that produced a low downstream loss, and the
latter using just one tokenization sampled from the
conditioned (and tempered) LM.
5.2.2 Exact marginalization using additional
independence assumptions: segmental
neural language models
The more popular solution of segmental neural language models was pioneered by Kong et al. (2016),
who cast the problem of segmentation as a monotonic13 seq2seq task, going from characters to a
covering sequence of substrings, i.e., a segmentation. By conditioning segment prediction on the
entire raw string, processed and embedded using
a BiRNN, segmentation decisions/scores can use
context, but by scoring every individual possible
substring independently as a segment using these
embeddings and then adding up individual scores
to score entire segmentations, they can find a covering of the entire input string with segments efficiently using dynamic programming. The reason
for this ability is the central independence assumption: the model does not depend on any other segments when scoring a segment, but merely on surrounding characters. Wang et al. (2017) extend
this by also having a per-segment RNN over characters for the outputs that can run without knowing
the segmentation and whose past representations
can thus be used by the individual segment generation processes, allowing for left-to-right sharing
of information about segments without breaking
dynamic programming.
The jump to LMing is now made simply by
omitting the conditioning on an input, yielding the
model of Sun and Deng (2018), who coin the term
segmental language model, training on Chinese
characters and using the unsupervisedly learned
segments to compete on Chinese Word Segmentation. To keep the computation of the quadratic number of segments feasible, they restrict the segments
to a maximum length of 4 characters (a sensible
prior for Chinese). Grave et al. (2019) make the
same jump independently, using Transformers as
the independent character-level global backbone.
When evaluating on English open-vocabulary language modeling, Grave et al. (2019) notice improved perplexity, but not using or evaluating the
obtained segmentation, most likely because they,
too, only use 4-grams that appear at least 400
times. Contemporaneously, Kawakami et al. (2019)
use the same independence idea, but have emissions of string segments come from a contextdependent mixture of a character-level model like
in Kawakami et al. (2017) (see §4.2) and a large
set of substrings (up to 10-grams that appear often
enough in training data) with learned embeddings.
They evaluate not only on perplexity, but also on
word segmentation performance, where they do
beat some baselines (see §5.3), but still perform
much worse than some previous models,14 which
they argue tuned their hyperparameters on segmentation performance instead of marginal likelihood
and thus have an unfair advantage.
Interestingly, when training on image captions,
Kawakami et al. (2019) find that both perplexity and segmentation performance improve when
the model also has access to the image that is being described, showing that learning segmentation
only from monolingual and unimodal text may be
harder than when other modalities or languages are
present. This observation is shared by He et al.
(2020), who build a similar segmental model (in
their case, a Transformer-based version that still
follows the character backbone idea to allow for
dynamic programming) as the target-side generator
in an NMT system and use it not as the final model,
but merely as a learned tokenizer. This is easy
to achieve by changing the dynamic program from
marginalization to maximization and thus obtaining
a new segmentation, called DPE, that can be used
in place of BPE or unigram LM (see §6.4). He et al.
(2020) proceed to show that learning to tokenize
with a small Transformer-based NMT model15 produces better segmentations than BPE for use in a
bigger model; in particular, training the tokenizing
model on the translation task produces different
segmentations depending on the source language,
and, more importantly, better segmentations (as
measured through downstream translation performance) than training on target-side language modeling alone.
The idea of conditioning on characters and pre
dicting segments is extended to the adirectional
masked language modeling setting found in Transformers and left-to-right autoregressive Transformers by Downey et al. (2021), though results do not
outperform RNN-based SNLMs consistently.
Note that many of these models can also be seen
as relatives of models based on UnigramLM, which
we will cover in §6.4.3.





5.3 Finding words through Bayesian
non-parametrics
In the era of 𝑛-gram and word-based language
models, MacKay and Peto (1995) noticed that a
Bayesian view of autoregressive language models
may prove beneficial, reinterpreting smoothing and
backoff in 𝑛-gram models as inference in a hierarchical model where higher-order distributions are
drawn from a Dirichlet distribution whose mean is
a lower-order distributions. Teh (2006) extends this
thinking, proposing a hierarchical PYP language
model where we again have 𝑛-gram distributions
of arbitarily large orders, drawn through a hierarchy of PYP distributions that lead to a model that
still bears resemblance to 𝑛-gram language model
smoothing, but offers a principled way to forego
the choice of 𝑛. The pinnacle of this idea of modeling was reached in Wood et al. (2011)’s sequence
memoizer, which boasted great compression performance for arbitrary binary data and still performed
very well on language modeling tasks, although
neural models at this time already proved to be
strong competitors.
At the same time, Goldwater et al. (2006b) extended this Bayesian perspective to also explain
how new words are first coined and how they are
then used in running text: a process they call twostage language modeling (see §4.2), with the two
stages being referred to as generator (which creates
new lexemes) and adaptor (which governs reuse;
here, a Pitman-Yor Process (PYP)), relating the
resulting interaction between types and tokens to
interpolated Kneser-Ney smoothing as presented
in Chen and Goodman (1999).16 Given such a twostage model to explain text and the use of Bayesian
nonparametrics that can assign positive probability
to an infinite number of possible lexemes, it becomes possible to also try to infer word boundaries,
that is to perform unsupervised word segmentation.
Motivated more by trying to explain and model cognitive processes and in particular child language
acquisition, Goldwater et al. (2009)
17 summarize
Unigram and Bigram Dirichlet Processes (DPs)
for segmenting transcribed infant-directed speech,
showing superiority over older non-Bayesian approaches. Mochihashi et al. (2009) extend the idea
from bigram DPs to ∞-gram nested/hierarchical
PYPs to improve word segmentation for English
written text; Elsner et al. (2013) additionally model
phonotactic processes that convert a sequence of
segments into observed realizations.
5.4 Related task: Unsupervised Chinese
Word Segmentation
Word segmentation for languages without whitespace delimiters such as Chinese, Japanese and
Vietnamese (Shao et al., 2018) is an important area
of research and can be notoriously tricky.
In Chinese word segmentation (CWS), there is
growing interest in exploring unsupervised word
segmentation involving neural language models.
Traditionally, popular unsupervised approaches
take on two primary paths: 1) discriminative models and 2) generative models. Discriminative models rely on goodness measures for candidate segmentation. These statistical measures incude Mutual Information (MI), normalized Variation of
Branching Entropy (nVBE) and Minimum Description Length (MDL), etc., see §6.3. Generative
models focus on designing statistical models to
find the optimal segmentation of the highest generative probability. These models include Hidden
Markov Model (HMM), Hierarchical Dirichlet Process (HDP), Nested Pitman-Yor Process (NPY),
etc., see §5.3. It is trivial to extend discriminative
approaches by replacing 𝑛-gram language model
with neural language models. For generative approaches, previous work has shown that constructing a neural language model with a context encoder
and a segment decoder achieves competitive performance to its statistical counterparts (Sun and Deng,
2018, see previous subsection §5.2.2).


















