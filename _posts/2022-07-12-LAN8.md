---
title: "Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP 정리"
date:   2022-07-12
excerpt: "Between words and characters:A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---

# intro
 
      

## 핵심  

## 읽기 위해 필요한 지식
* [word2vec](https://yerimoh.github.io/DL14/): baseline 모델이기 때문에 꼭 알아야 한다.        
* [Embedding](https://yerimoh.github.io/DL15/): word2vec 속도 개선으로 이 포스팅도 꼭 알아야 한다.      

## 원 논문
[Enriching Word Vectors with Subword Information](https://arxiv.org/abs/2112.10508)


---

# 목차  


---


# **Abstract**
우리가 모델링하고자 하는 텍스트의 단위는  
bytes에서 다중 단어 표현식(multi-word expressions)에 이르기까지, 텍스트는 다양한 세분성으로 분석되고 생성될 수 있다.      

최근까지 대부분의 자연어 처리(NLP) 모델은 단어를 통해 작동하여 discrete 및  atomic tokens으로 처리했지만,   
byte-pair encoding(BPE)을 시작으로 많은 영역에서 하위 단어 기반 접근 방식이 우세하여 **작은 어휘를 사용**하면서도 **빠른 추론**을 허용하고 있다.      

이 토크나이징 방법들에 대해 학습된 세분화를 기반으로 한 **subword-based approaches**뿐만 아니라 **단어와 문자의 하이브리드 접근 방식**이 **어떻게 제안되고 평가**되었는지를 보여줌으로써,     
사전 신경 및 신경 시대의 여러 작업 라인을 연결한다.     

본 논문은 모든 애플리케이션에 대해 **특별한 해결책**이 결코 **없을 것**이며,   
토큰화 방법이 애플리케이션에 여전히 중요하다고 결론짓는다.    


-----
-----

# **1 Introduction**    
$$“‘tokens’ are not a real thing. they are a computer generated illusion created by a clever engineer” —@dril_gpt1$$    


우리가 사람들에게 NLP 모델을 처음 소개할 때,    
우리는 종종 텍스트가 컴퓨터에 작은 조각들로 잘려나간다는 생각을 당연하게 받아들인다.    
➡ 결국 그것은 단지 일련의 정수일 뿐이다. 따라서 우리는 이것들을 (보통) 연속 **sub-strings tokens**이라고 부른다. 


교육 환경에서, 그리고 사실 역사적으로 NLP에서 이러한 토큰은 다소 자연스럽게 단어(처음에는 영어로 “space-separated substrings”)로 암시된다. 


**[단어에서 구두점 분리의 어려움]**            
* "don't"라는 단어를 예로 들면,    
* 이 단어의 구두점 분할의 경우, ```"don"``` ```"'"``` ```"t"```라는 다소 무의미한 세 개의 토큰을 얻게 될 것이다.            
* 그런데 더 효과적인 토크나이징을 하려면 ```"do"```와 ```"n't"```의 두 단위를 산출해야 한다고 주장할 수 있다.       


**[논문 개요]**    
이 survey는 $$tokenization$$에 대한 위와 같은 질문을 다루며,      
본 논문은,    
* **2**: 근본적인 질문과 용어를 상세히 설명하고,     
* **3**: 모든 NLP 작업에서 다소 매력적이지 않은 부분이 역사적으로 어떻게 다루어졌는지를 보여줄 것이다.   

특히 지난 5년 동안, 다소 원자적인 단어와 같은 공간 분리 단위로서의 **"토큰"에 대한 직관적인 정의를 넘어서는 것**에 대한 관심이 새로워졌다.  
* **4**: 이를 위한 한 가지 방법인 **단어 내부 정보를 사용하여 단어와 같은 단위를 보강**하는 방법.    
* **5**: 신경 모델링은 이를 그 어느 때보다 쉽게 만들어 명백한 표시 없이 단어 경계까지 학습할 수 있는 모델로 이어진다(예: 공백이 있는 경우).  unsupervised word segmentation or discovery의 개념은 수십 년의 작업에서 자체적으로 존재했지만,    
* **6**: **subword** 단위를 원자 토큰으로 사용하는 현재 널리 퍼져 있는 아이디어를 최종적으로 살펴볼 때 고려하는 것이 유용하다는 것을 알게 될 것이다.      
* **7**: 다국어 어휘의 공유와 경쟁의 몇 가지 문제 살펴봄     
* **8**: 문자, 바이트 또는 심지어 픽셀로 최대 분해할 수 있는 가장 간단한 토큰화를 사용해봄        


이 논문이 끝난 후,       
* 바이트의 손쉬운 솔루션이 도달해 보이는 2021년 현재에도 "복잡한" 토큰화가 의 중요성을 논증하여 마무리 한다.     
* 최근의 발전은 특정 도메인과 사용 사례에 대해 최대 분해 처리를 가능하게 하지만, 그것들은 광범위한 NLP 시나리오를 다루지 않고 **자체의 단점과 편견**을 가지고 있다고 주장할 것이다.     


![image](https://user-images.githubusercontent.com/76824611/178692493-989362fb-6b72-47c4-9bd4-31c7b5e80be5.png)
A taxonomy of segmentation and tokenization algorithms and research directions


------
-----











# **2 Tokens, word-forms, and sub-words** 
2 Tokens, word-forms, and sub-words
In NLP, textual data has been traditionally segmented into “sentences” (or “utterances”, etc.) and
“words” due to linguistic motivations and technical
constraints. The macroscopic units (“sentences”)
are often considered independently from one another and themselves segmented into microscopic
units. The definition of these microscopic units
has always been a matter of approximation and
compromise. On the one hand, these units receive linguistic annotations (e.g. part-of-speech
tags, morphosyntactic annotation, syntactic dependency information), which would require them to
be linguistically motivated units. On the other
hand, a large range of phenomena make it highly
non-trivial to identify and even to consistently define linguistic units, denoted by the Morphological Annotation Framework (MAF) ISO standard
(Clément et al., 2005) as word-forms. Such phenomena include contractions (e.g. English don’t,
cited above, and French aux ‘to thepl’), compounds
(e.g. French copier-coller ‘copy-paste’2
), morpho2Cf. inflected forms copié-collé ‘copy-pasted’, lit. ‘copiedpasted’, but copie-collerai ‘will1sg copy-paste’.
logical derivatives (e.g. English or French antiTrump), as well as numerous classes of named entities and other sequences following type-specific
grammars (e.g. numbers, URLs).
As a result, typographic units, generally called
tokens have been used as an approximation for
such linguistically motivated units. For instance,
MAF defines a token as a “non-empty contiguous
sequence of graphemes or phonemes in a document.” In the case of writing systems using a typographic separator such as the whitespace, now
universally used with the Latin script for instance,
tokens have been widely used and broadly defined
as either contiguous sequences of non-punctuation
non-whitespace marks or punctuation marks. Provided a few arbitrary decisions are made regarding
certain punctuation marks (e.g. the hyphen or the
apostrophe), such a definition makes it possible to
deterministically split a sentence into atomic units,
resulting in a segmentation into tokens that are acceptable approximations of word-forms. Crucially,
as discussed in detail by Clément et al. (2005),
Sagot and Boullier (2008) and elsewhere, there is
no one-to-one correspondence between tokens and
word-forms; a word-form can be made of several
tokens (e.g. French or English sine die) whereas
several word-forms can be represented by the same
token (e.g. English don’t = do + not, Spanish
damélo = da + me + lo). This is what the Universal Dependencies guidelines3
refer to as “multitoken words” and “multiword tokens,” respectively,
a topic further discussed by More et al. (2018). In
fact, both phenomena can interfere in non trivial
ways (e.g. French à l’instar du = à_l’instar_de +
le).4
In recent years, the spread of approaches based
on neural language models resulted in an evolution in how sentences are split into atomic units,
thereby resulting in a redefinition of the notion of
tokenization. Indeed, based both on scientific results (e.g. the impact of sub-word segmentation on
machine translation performance (Sennrich et al.,
2016)) and on technical requirements (e.g. language models such as BERT that require a fixedsize vocabulary), the need for the atomic processing
units (still called tokens) to be an approximation of
word-forms has faded out. As a result, in current
NLP, the notion of token still perfectly matches
its MAF definition, but it no longer corresponds
to the traditional definition of a typographic unit.
“Tokenization” now denotes the task of segmenting a sentence into such non-typographically (and
indeed non-linguistically) motivated units, which
are often smaller than classical tokens and wordforms, and therefore often called sub-words. Typographic units (the “old” tokens) are now often
called “pre-tokens,” and what used to be called
“tokenization” is therefore called nowadays “pretokenization.” This term is motivated by the fact
that the first approaches to the new notion of “tokenization” often involved segmenting sentences
into proper typographic units (i.e. the “old” notion
of tokenization) before further segmenting (some
of) the resulting units (formerly “tokens”, now “pretokens”) into “sub-words”.

