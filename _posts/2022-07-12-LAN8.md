---
title: "Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP 정리"
date:   2022-07-12
excerpt: "Between words and characters:A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---

# intro
 
      

## 핵심  

## 읽기 위해 필요한 지식
* [word2vec](https://yerimoh.github.io/DL14/): baseline 모델이기 때문에 꼭 알아야 한다.        
* [Embedding](https://yerimoh.github.io/DL15/): word2vec 속도 개선으로 이 포스팅도 꼭 알아야 한다.      

## 원 논문
[Enriching Word Vectors with Subword Information](https://arxiv.org/abs/2112.10508)


---

# 목차  


---


# **Abstract**
우리가 모델링하고자 하는 텍스트의 단위는  
bytes에서 다중 단어 표현식(multi-word expressions)에 이르기까지, 텍스트는 다양한 세분성으로 분석되고 생성될 수 있다.      

최근까지 대부분의 자연어 처리(NLP) 모델은 단어를 통해 작동하여 discrete 및  atomic tokens으로 처리했지만,   
byte-pair encoding(BPE)을 시작으로 많은 영역에서 하위 단어 기반 접근 방식이 우세하여 **작은 어휘를 사용**하면서도 **빠른 추론**을 허용하고 있다.      

이 토크나이징 방법들에 대해 학습된 세분화를 기반으로 한 **subword-based approaches**뿐만 아니라 **단어와 문자의 하이브리드 접근 방식**이 **어떻게 제안되고 평가**되었는지를 보여줌으로써,     
사전 신경 및 신경 시대의 여러 작업 라인을 연결한다.     

본 논문은 모든 애플리케이션에 대해 **특별한 해결책**이 결코 **없을 것**이며,   
토큰화 방법이 애플리케이션에 여전히 중요하다고 결론짓는다.    


-----
-----

# **1 Introduction**    


_“‘tokens’ are not a real thing. they are a computer generated illusion created by a clever engineer” dril_gpt1         


우리가 사람들에게 NLP 모델을 처음 소개할 때,    
우리는 종종 텍스트가 컴퓨터에 작은 조각들로 잘려나간다는 생각을 당연하게 받아들인다.    
➡ 결국 그것은 단지 일련의 정수일 뿐이다. 따라서 우리는 이것들을 (보통) 연속 **sub-strings tokens**이라고 부른다. 


교육 환경에서, 그리고 사실 역사적으로 NLP에서 이러한 토큰은 다소 자연스럽게 단어(처음에는 영어로 “space-separated substrings”)로 암시된다. 


**[단어에서 구두점 분리의 어려움]**            
* "don't"라는 단어를 예로 들면,    
* 이 단어의 구두점 분할의 경우, ```"don"``` ```"'"``` ```"t"```라는 다소 무의미한 세 개의 토큰을 얻게 될 것이다.            
* 그런데 더 효과적인 토크나이징을 하려면 ```"do"```와 ```"n't"```의 두 단위를 산출해야 한다고 주장할 수 있다.       

---

## 논문 개요    
이 survey는 $$tokenization$$에 대한 위와 같은 질문을 다루며,      
본 논문은 의 각 챕터의 핵심내용은 아래와 같다,    

**[2. Tokens, word forms, and sub words](# 2-Tokens,-word-forms,-and-sub-words)**      
* 근본적인 질문과 용어를 상세히 설명     

**[3: Pre-tokenization yields word-like typographic units](# 3-Pre-tokenization-yields-word-like-typographic-units)**      
* 모든 NLP 작업에서 다소 매력적이지 않은 부분이 역사적으로 어떻게 다루어졌는지를 보여줌     

특히 지난 5년 동안, 다소 원자적인 단어와 같은 공간 분리 단위로서의 **"토큰"에 대한 직관적인 정의를 넘어서는 것**에 대한 관심이 새로워졌다.  
* **4**: 이를 위한 한 가지 방법인 **단어 내부 정보를 사용하여 단어와 같은 단위를 보강**하는 방법.    
* **5**: 신경 모델링은 이를 그 어느 때보다 쉽게 만들어 명백한 표시 없이 단어 경계까지 학습할 수 있는 모델로 이어진다(예: 공백이 있는 경우).  unsupervised word segmentation or discovery의 개념은 수십 년의 작업에서 자체적으로 존재했지만,    
* **6**: **subword** 단위를 원자 토큰으로 사용하는 현재 널리 퍼져 있는 아이디어를 최종적으로 살펴볼 때 고려하는 것이 유용하다는 것을 알게 될 것이다.      
* **7**: 다국어 어휘의 공유와 경쟁의 몇 가지 문제 살펴봄     
* **8**: 문자, 바이트 또는 심지어 픽셀로 최대 분해할 수 있는 가장 간단한 토큰화를 사용해봄        


이 논문이 끝난 후,       
* 바이트의 손쉬운 솔루션이 도달해 보이는 2021년 현재에도 "복잡한" 토큰화가 의 중요성을 논증하여 마무리 한다.     
* 최근의 발전은 특정 도메인과 사용 사례에 대해 최대 분해 처리를 가능하게 하지만, 그것들은 광범위한 NLP 시나리오를 다루지 않고 **자체의 단점과 편견**을 가지고 있다고 주장할 것이다.     


![image](https://user-images.githubusercontent.com/76824611/178999180-03d114b4-8ba7-4748-afce-5115faccc2b5.png)
A taxonomy of segmentation and tokenization algorithms and research directions


------
-----











# **2 Tokens, word-forms, and sub-words** 
이 챕터에서는 token에 대한 근본적인 질문과 용어를 설명한다.      

토큰, 단어 형태 및 하위 단어 NLP에서 텍스트 데이터는 
"문장"(sentences)과 "단어(words)"로 분할되었다. 

**[sentences]**     
* 거시적 단위(macroscopic units)는 종종 서로 독립적으로 고려되며, 그 자체가 미시적 단위로 세분된다.        




**[tokens]**    
* 일반적으로 토큰이라고 불리는 타이포그래피 단위(typographic units)는 언어적으로 동기화된 단위에 대한 근사치로 사용되어 왔다.     
* MAF(형태학적 주석 프레임워크)는 토큰을 "“non-empty contiguous sequence of graphemes or phonemes in a document."으로 정의한다.    
* 예를 들어, 현재 라틴어 스크립트와 함께 보편적으로 사용되는 **공백**과 같은 **타이포그래피 구분자**를 사용하는 쓰기 시스템의 경우, 토큰은 널리 사용되었으며 **비문구 비백색 공간 마크** 또는 **구두점의 연속 시퀀스**로 정의되었다.    
* 특정 문장 부호(예: 하이픈 또는 아포스트로피)를 토큰으로 임의로 정하면, 그러한 정의는 **문장**을 토큰을 기준으로 **원자 단위로 분할**한다.     
* 토큰과 단어 형태 사이에 **일대일 대응이 없다**("멀티톡 단어", "멀티워드 토큰")    
    * 단어 형태는 여러 개의 토큰(예:French or English sine die)으로 만들 수 있다.     
    * 반면, 여러 개의 단어 형태는 동일한 토큰(예: 영어 don't = do + not, 스페인어 damélo + da + lo)으로 나타낼 수 있다.     

**[tokens의 재정의의 필요성: word-forms을 위해]**      
* 최근 문장이 원자 단위로 분할되는 방식이 진화하여 **토큰화 개념이 재정의되**었다.    
* **필요성:** 과학적 결과(예: 하위 단어 분할이 기계 번역 성능에 미치는 영향와 기술적 요구 사항(예: 고정 크기 어휘가 필요한 BERT와 같은 언어 모델) 모두에 기초하여, 원자 처리 장치(아직 토큰이라고 함)가 **단어 형태(word-forms)의 근사치가 될 필요**가 있다.    
* ❌ **문제**: 퇴색한 결과적으로, 현재의 NLP에서 토큰의 개념은 여전히 MAF 정의와 완벽하게 일치하지만, 더 이상 **타이포그래피 단위의 전통적인 정의와 일치하지 않는다**.     


**[토큰화(Tokenization)]**     
* **정의:** 문장을 정형적이지 않은 (그리고 실제로 언어적이지 않은) 동기화된 단위로 분할하는 작업     
* 이는 종종 고전적인 토큰과 단어 형태보다 작기 때문에 종종 **subword**라고 불린다.     
* Typographic 단위("오래된" 토큰)는 현재 종종 "pre-tokens"라고 불린다.      
➡ 따라서 "tokenization"라고 불리던 것은 오늘날 "pretokenization"라고 불린다.      
* **pretokenization**: 이 용어는 토큰화의 새로운 개념에 대한 첫 번째 접근법이 **resulting 단위**(이전에는 “tokens”, 지금은  “pretokens”)를 “sub-words”로 **분할하기 전**에 **문장을 적절한 Typographic 단위**(예: 토큰화의 "옛" 개념)**로 분할하는 것을 포함**했다는 사실에 의해 동기 부여된다.     


-----
------


# **3 Pre-tokenization yields word-like typographic units**    
모든 NLP 작업에서 다소 매력적이지 않은 부분이 역사적으로 어떻게 다루어졌는지를 보여줌

**[옛 토큰의 사용방식]**       
* purely typographic token의 언어적 무관함(linguistic irrelevance)     
* 텍스트를 linguistically motivated된 단어 형태로 자동 분할하는 것의 어려움(difficulty of automatically splitting a text into linguistically motivated word-forms)     
* **위 두 문제의 절충안:**  purely typographic token과 purely linguistic word-forms의 중간 단위가 널리 사용되어 왔다.   

**[“pre-tokenizers”의 역사]**         
* **용어 변화**     
   * 초기에는 “tokenizers”로 알려짐     
   * sub-word 토큰화의 확산 이후 “pretoken”으로 바뀜    
   * 오늘날에는 “pre-tokenizers”로 명칭 바뀜         
* **대표 모델**      
   * 이러한 “pre-tokenizers” 중 일부는 비교적 단순하고 typographic token에 충실하다.      
   * Hugging Face의 토큰라이저 패키지에 있는 오래된 ```Moses (Kohn et al., 2007) tokenizer6``` 가 초기에 널리 사용됨           
   * 그리고 더 최근은 Hugging Face의 Tokenizers package에 있는```pre-tokenizers package```가 있다.      
 
**[“pre-tokenization” 개념의의 탄생과 문제점]**     
* 위의 대표 모델들의 사용은 "tokenization(현재는 “pre-tokenization”)"라는 단어를 만들어냈다.     
* **"tokenization(pre-tokenization)"의 의미:** 일반적으로 문장을 원자 단위로 분할하는 작업을 나타내게 됨.       
    * 즉, "후속 처리에서 분해될 필요가 없는 기본 단위" 단위가 typographic 단위보다 단어 형태에 더 가까울 때에도 "tokenization"이라고 불린다.       
    * 더 나아가, 토큰화자가 **문장을 분할**할 뿐만 아니라 **정규화**, **철자 수정** 또는 **named entity detection** 목적과 같은 원시 텍스트를 수정하는 데 공통적으로 사용되므로 현재 토큰의 표준 정의에서 벗어난다.      
* ❌ **문제점**     
    * 이러한 초기 토큰의 정의와 역할은 정규화 작업이나 다른 공백 기호의 통합 및 병합은 **토큰화 전으로 되돌릴 수 없게한다**.     
    * 토큰화된 출력에서 원시 텍스트를 확실하게 복구할 수 없다.   
  
  
 
------
-----


# **4. Augmenting word-level pretokenizer tokens with character information**
특히 지난 5년 동안, 다소 원자적인 단어와 같은 공간 분리 단위로서의 **"토큰"에 대한 직관적인 정의를 넘어서는 것**에 대한 관심이 새로워졌다.  
이 챕터에서는 이를 위한 한 가지 방법인 **단어 내부 정보를 사용하여 단어와 같은 단위를 보강**하는 방법을 설명한다.     

**[word-level 모델의 특징]**           
* 개념적으로 이해하기 쉬움    
* neural era에서는 해석 가능한 세분화된 기능을 제공      
* **단점**: 폐쇄형 어휘 모델: 훈련 중에 거의 보이지 않는 희귀하고 새로운 단어를 처리할 수 없음    
* **해결책:** 역사적으로 희귀한 단어 유형은 훈련 시 새로운 단어 유형 ```UNK(알 수 없음)```로 대체됨     


**[```UNK```대체 방식의 단점]**         
* 자연어 생성(NLG)을 수행할 때 UNK가 허용되지 않는다   
* ELMo 또는 BERT와 같은 large-scale 모델에 사용될 때 일회성 이벤트뿐만 아니라 유용한 의미의 앵커인 **새로운 단어에 대한 기능**을 **추출할 수 없다**        
* 영어 이외의 언어, 특히 더 생산적인 형태학 및 따라서 유형 토큰 비율이 높은 언어에서 희귀 단어를 제거하는 것은 불가능     

그럼에도 불구하고, **단어**가 언어의 **기본 단위**이기 때문에,       
**단어를 구성하는 문자를 기반**으로 처리함으로써 근본적으로 단어 기반 틀에서 희귀하고 새로운 단어의 **처리를 개선**하는 여러 접근법이 등장했다.      
우리는 단어 표현의 보다 포괄적인 처리를 위해 이 섹션에서 이러한 접근 방식 중 일부를 제시할 것이다.       

-----


## 4.1 Augmenting word-level models with spelling information    

**[단어에 초점]**         
* 언어를 위한 neural models에서,    
90년대와 2000년대의 연구는 **단어에 초점**을 맞추고, 대신 문자의 문자열을 처리했다.      

**[단어 자체에 대한 정보]**        
* 그러나 neural models이 NLP에서 중요해지자마자, 신경 네트워크에서 사용하기 위한 **단어와 문자 수준의 정보의 조합**(word- and character-level information)이 등장.       
* 단어 임베딩 추정을 돕기 위해 단어 자체에 대한 정보를 사용할 것을 처음 제안했다.      
* word’s embedding을 단어의 철자를 통해 구성한다는 아이디어를 **대중화**했다.   


**[CNN 기반 임베딩 기능]**     
* 임베딩 행렬을 CNN(convolutional neural network) 레이어로 대체할 때에도 generative 모델은 여전히 폐쇄 어휘이며, 
이는 훈련 데이터에서 보였던 단어만 예측할 수 있다는 것을 의미하기 때문에,         
CNN 구성은 **새로운 단어가 아닌** **희귀 단어**만 돕는다.      


**[CNN with 토큰의 철자]**      
* **각 토큰에 대한 철자**로 임베딩을 구성하는 CNN 기반 임베딩 기능     
* 새로운 단어를 예상하는 대신 "자주 단어를 정확하게 얻도록" 암묵적으로 훈련한다.  
* **이 기반 모델의 역할:** POS 태깅과 같은 다른 고전적인 NLP 작업의 발전을 이끌었고 궁극적으로 최초의 큰 문맥 단어 임베딩 모델 ELMo에 힘을 실어주었다.        
  * [fastText](https://yerimoh.github.io/LAN7/) 임베딩은 문자가 아니라 겹치는 n-gram에서 단어 임베딩을 구성할 것을 제안하여,     
새로운 단어에 대한 임베딩을 얻을 수 있다(generative 의미는 아니지만 그런 의미에서 "open-vocabulary"로 만든다).    

  * Ataman and Federico도 마찬가지로 문자 대신 (중첩) 𝑛-grams을 사용하여 기계 번역에서 더 나은 성능을 얻는다(형태학적으로 풍부한 언어에서 BPE를 능가함).     
  * 최근에는 철자 오류 또는 전송으로 노이즈가 많은 텍스트를 더 잘 처리함으로써     
구성 문자 임베딩에 대한 BPE 단위의 임베딩을 향상시켰다.      


**[small Transformer]**   
* 의료 텍스트와 같은 새로운 도메인; 동시에 Aguilar 등(2021)은 거의 동일하게 하지만 CNN 대신 small Transformer를 사용한다.     

**[구성 기반 접근 방식]**   
* 구성 기반 접근 방식은 사전 훈련된 단어 수준 입력 모델에도 통합되었다.     
* 특히, Pinter 등(2017)은 테스트 시간 동안 알 수 없는 단어가 나타날 때마다 호출되는 helper RNN 모델을 사용하여 **철자가 주어진 단어의 임베딩을 모방**하도록 훈련된 모델을 학습한다.      


---


## **4.2 Open-vocabulary language modeling with (tokenizer-defined) words made of characters**
**[open-vocabulary language modeling의 어려움]**    
* ```open-vocabulary language modeling```: test time때 **새로운 단어를 예측하고 생성**할 수 있는 모델     
* ```closed-vocabulary``` 어휘 생성 모델을 ```open-vocabulary language modeling```로 확장하는 것은 어렵다.              
* **어려운 이유**: 완전히 새로운 단어를 포함하는 문장의 무한 집합에 대한 확률 질량을 유지할 수 있어야 하기 때문에 input side의 Open-vocabulary 보다 다소 어렵다.     
   * 루옹과 매닝(2016)에서 영감을 받아 Mielke와 Eisner(2018)는 단어 임베딩을 정규화하여 작은 문자 수준의 RNNLM을 사용하고 작은 m을 사용하여 철자를 예측하도록 함으로써 일반적인 폐쇄 음성 단어 수준의 반복 신경망 언어 모델(RNNLM) 설정을 근본적으로 향상시키는 확률론적 2단계 모델을 제안한다.단어 수준 RNNLM이 UNK를 예측할 때마다 새로운 단어를 즉시 생성하여 언어 개념과 직관적 모델링에 의해 동기 부여되고 질적 및 정량적으로 성공적인 것으로 입증된 개방형 어휘 모델을 산출한다. 독립적으로 개발된 카와카미 외(2017)의 모델은 단어 및 문자 수준의 RNN의 유사한 2단계 설정을 따르지만, 캐시 모델을 사용하여 최근에서 직접 복사할 수 없는 경우 각 단어를 문자 수준의 RNN을 사용하여 철자해야 한다(Grave et al., 2016).11 이들의 분석은 캐시 모델이 그렇지 않다는 것을 분명히 보여준다. 노리에가(Church, 2000)와 같은 "버스트" 미지의 단어만 복사하지만, 또한 잊어버리지 않도록 하기 위해 와 같은 극히 일반적인 기능 단어들도 복사한다. 이 아이디어는 Ataman 등(2019)에 의해 기계 번역 디코더(ELMo(Peters 등, 2018, § 4.1 참조)에서 인코더 측에 단어 임베딩을 생성)에 대해 선택되었으며, 나중에 Ataman 등(2020)에 의해 확장되었으며, 이는 리마타를 픽업하고 감독되지 않은 굴절을 유도하기 위한 추가 확률성으로 확장되었다. 다른 접근 방식은 더 낮은 속도로 다층 RNN의 더 높은 레이어를 실행하는 것이다(숨겨진 상태로의 업데이트를 건너뛴다. 이는 El Hihi와 Bengio(1995)에서 처음 제시된 오래된 아이디어이며(Schmidhuber(1991년, 1992년)의 "신경 시퀀스 청커"에 구축됨), 고정 주파수 건너뛰기를 위한 Koutnik 외(2014)와 단어 경계를 건너뛰기 위한 황과 성(2017)에서 부활되었다. 12 이 접근법은 여러 가지 방법 중 첫 번째로 이어진다. 우리는 단어 경계와 그에 따른 분할을 실제로 배울 수 있다.



Extending closed-vocabulary generative models to
open-vocabulary models, i.e., those that can predict
and generate novel words at test time, is somewhat
more difficult than being open-vocabulary on the
input side because it must be possible to hold out
probability mass for the infinite set of sentences
that contain completely novel words.
Inspired by Luong and Manning (2016), Mielke
and Eisner (2018) propose a probabilistic twostage model that essentially augments the ordinary
closed-vocab word-level recurrent neural network
language model (RNNLM) setup by regularizing
word embeddings to be predictive of their spellings
using a smaller character-level RNNLM and using that smaller model to generate novel words
on the fly whenever the word-level RNNLM pre
dicts UNK, yielding an open-vocabulary model motivated by linguistic notions and intuitive modeling
and proven successful qualitatively and quantitatively.
Independently developed, the model of
Kawakami et al. (2017) follows a similar two-level
setup of word- and character-level RNN, but
where each word has to be spelled out using
a character-level RNN if it cannot be directly
copied from the recent past using a cache model
(Grave et al., 2016).11 Their analysis shows clearly
that the cache model not only copies “bursty”
unknown words like Noriega (Church, 2000),
but also extremely common function words like
the in an attempt to keep itself from forgetting
them. The idea is picked up by Ataman et al.
(2019) for a machine translation decoder (creating
word embeddings on the encoder side from
character-level BiRNNs as in ELMo (Peters et al.,
2018, see §4.1)) and later extended by Ataman
et al. (2020) with some additional stochasticity that
is intended to pick up on lemmata and inflections
unsupervisedly.
A different approach is having higher layers of
multi-layer RNNs run at lower speed (skipping updates to the hidden state) This is an old idea, first
present in El Hihi and Bengio (1995) (building
on Schmidhuber (1991, 1992)’s “neural sequence
chunker”) and revived in Koutnik et al. (2014) for
fixed-frequency skipping and Hwang and Sung
(2017) for skipping on word boundaries (which are
assumed to be observed).12 This approach leads
to the first of a number of ways in which we can
actually learn word boundaries and thus segmentations.










