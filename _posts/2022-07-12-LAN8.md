---
title: "Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP 정리"
date:   2022-07-12
excerpt: "Between words and characters:A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---

# intro
 
      

## 핵심  

## 읽기 위해 필요한 지식
* [word2vec](https://yerimoh.github.io/DL14/): baseline 모델이기 때문에 꼭 알아야 한다.        
* [Embedding](https://yerimoh.github.io/DL15/): word2vec 속도 개선으로 이 포스팅도 꼭 알아야 한다.      

## 원 논문
[Enriching Word Vectors with Subword Information](https://arxiv.org/abs/2112.10508)


---

# 목차  


---


# **Abstract**
우리가 모델링하고자 하는 텍스트의 단위는  
bytes에서 다중 단어 표현식(multi-word expressions)에 이르기까지, 텍스트는 다양한 세분성으로 분석되고 생성될 수 있다.      

최근까지 대부분의 자연어 처리(NLP) 모델은 단어를 통해 작동하여 discrete 및  atomic tokens으로 처리했지만,   
byte-pair encoding(BPE)을 시작으로 많은 영역에서 하위 단어 기반 접근 방식이 우세하여 **작은 어휘를 사용**하면서도 **빠른 추론**을 허용하고 있다.      

이 토크나이징 방법들에 대해 학습된 세분화를 기반으로 한 **subword-based approaches**뿐만 아니라 **단어와 문자의 하이브리드 접근 방식**이 **어떻게 제안되고 평가**되었는지를 보여줌으로써,     
사전 신경 및 신경 시대의 여러 작업 라인을 연결한다.     

본 논문은 모든 애플리케이션에 대해 **특별한 해결책**이 결코 **없을 것**이며,   
토큰화 방법이 애플리케이션에 여전히 중요하다고 결론짓는다.    


-----
-----

# **1 Introduction**    
$$“‘tokens’ are not a real thing. they are a computer generated illusion created by a clever engineer” @dril_gpt1$$    


우리가 사람들에게 NLP 모델을 처음 소개할 때,    
우리는 종종 텍스트가 컴퓨터에 작은 조각들로 잘려나간다는 생각을 당연하게 받아들인다.    
➡ 결국 그것은 단지 일련의 정수일 뿐이다. 따라서 우리는 이것들을 (보통) 연속 **sub-strings tokens**이라고 부른다. 


교육 환경에서, 그리고 사실 역사적으로 NLP에서 이러한 토큰은 다소 자연스럽게 단어(처음에는 영어로 “space-separated substrings”)로 암시된다. 


**[단어에서 구두점 분리의 어려움]**            
* "don't"라는 단어를 예로 들면,    
* 이 단어의 구두점 분할의 경우, ```"don"``` ```"'"``` ```"t"```라는 다소 무의미한 세 개의 토큰을 얻게 될 것이다.            
* 그런데 더 효과적인 토크나이징을 하려면 ```"do"```와 ```"n't"```의 두 단위를 산출해야 한다고 주장할 수 있다.       

---

## 논문 개요    
이 survey는 $$tokenization$$에 대한 위와 같은 질문을 다루며,      
본 논문은 의 각 챕터의 핵심내용은 아래와 같다,    

**[2. Tokens, word forms, and sub words](# 2-Tokens,-word-forms,-and-sub-words)**      
* 근본적인 질문과 용어를 상세히 설명     

**[3: Pre-tokenization yields word-like typographic units](# 3-Pre-tokenization-yields-word-like-typographic-units)**      
* 모든 NLP 작업에서 다소 매력적이지 않은 부분이 역사적으로 어떻게 다루어졌는지를 보여줌     

특히 지난 5년 동안, 다소 원자적인 단어와 같은 공간 분리 단위로서의 **"토큰"에 대한 직관적인 정의를 넘어서는 것**에 대한 관심이 새로워졌다.  
* **4**: 이를 위한 한 가지 방법인 **단어 내부 정보를 사용하여 단어와 같은 단위를 보강**하는 방법.    
* **5**: 신경 모델링은 이를 그 어느 때보다 쉽게 만들어 명백한 표시 없이 단어 경계까지 학습할 수 있는 모델로 이어진다(예: 공백이 있는 경우).  unsupervised word segmentation or discovery의 개념은 수십 년의 작업에서 자체적으로 존재했지만,    
* **6**: **subword** 단위를 원자 토큰으로 사용하는 현재 널리 퍼져 있는 아이디어를 최종적으로 살펴볼 때 고려하는 것이 유용하다는 것을 알게 될 것이다.      
* **7**: 다국어 어휘의 공유와 경쟁의 몇 가지 문제 살펴봄     
* **8**: 문자, 바이트 또는 심지어 픽셀로 최대 분해할 수 있는 가장 간단한 토큰화를 사용해봄        


이 논문이 끝난 후,       
* 바이트의 손쉬운 솔루션이 도달해 보이는 2021년 현재에도 "복잡한" 토큰화가 의 중요성을 논증하여 마무리 한다.     
* 최근의 발전은 특정 도메인과 사용 사례에 대해 최대 분해 처리를 가능하게 하지만, 그것들은 광범위한 NLP 시나리오를 다루지 않고 **자체의 단점과 편견**을 가지고 있다고 주장할 것이다.     


![image](https://user-images.githubusercontent.com/76824611/178999180-03d114b4-8ba7-4748-afce-5115faccc2b5.png)
A taxonomy of segmentation and tokenization algorithms and research directions


------
-----











# **2 Tokens, word-forms, and sub-words** 
이 챕터에서는 token에 대한 근본적인 질문과 용어를 설명한다.      

토큰, 단어 형태 및 하위 단어 NLP에서 텍스트 데이터는 
"문장"(sentences)과 "단어(words)"로 분할되었다. 

**[sentences]**     
* 거시적 단위(macroscopic units)는 종종 서로 독립적으로 고려되며, 그 자체가 미시적 단위로 세분된다.        




**[tokens]**    
* 일반적으로 토큰이라고 불리는 타이포그래피 단위(typographic units)는 언어적으로 동기화된 단위에 대한 근사치로 사용되어 왔다.     
* MAF(형태학적 주석 프레임워크)는 토큰을 "“non-empty contiguous sequence of graphemes or phonemes in a document."으로 정의한다.    
* 예를 들어, 현재 라틴어 스크립트와 함께 보편적으로 사용되는 **공백**과 같은 **타이포그래피 구분자**를 사용하는 쓰기 시스템의 경우, 토큰은 널리 사용되었으며 **비문구 비백색 공간 마크** 또는 **구두점의 연속 시퀀스**로 정의되었다.    
* 특정 문장 부호(예: 하이픈 또는 아포스트로피)를 토큰으로 임의로 정하면, 그러한 정의는 **문장**을 토큰을 기준으로 **원자 단위로 분할**한다.     
* 토큰과 단어 형태 사이에 **일대일 대응이 없다**("멀티톡 단어", "멀티워드 토큰")    
    * 단어 형태는 여러 개의 토큰(예:French or English sine die)으로 만들 수 있다.     
    * 반면, 여러 개의 단어 형태는 동일한 토큰(예: 영어 don't = do + not, 스페인어 damélo + da + lo)으로 나타낼 수 있다.     

**[tokens의 재정의의 필요성: word-forms을 위해]**      
* 최근 문장이 원자 단위로 분할되는 방식이 진화하여 **토큰화 개념이 재정의되**었다.    
* **필요성:** 과학적 결과(예: 하위 단어 분할이 기계 번역 성능에 미치는 영향와 기술적 요구 사항(예: 고정 크기 어휘가 필요한 BERT와 같은 언어 모델) 모두에 기초하여, 원자 처리 장치(아직 토큰이라고 함)가 **단어 형태(word-forms)의 근사치가 될 필요**가 있다.    
* ❌ **문제**: 퇴색한 결과적으로, 현재의 NLP에서 토큰의 개념은 여전히 MAF 정의와 완벽하게 일치하지만, 더 이상 **타이포그래피 단위의 전통적인 정의와 일치하지 않는다**.     


**[토큰화(Tokenization)]**     
* **정의:** 문장을 정형적이지 않은 (그리고 실제로 언어적이지 않은) 동기화된 단위로 분할하는 작업     
* 이는 종종 고전적인 토큰과 단어 형태보다 작기 때문에 종종 **subword**라고 불린다.     
* Typographic 단위("오래된" 토큰)는 현재 종종 "pre-tokens"라고 불린다.      
➡ 따라서 "tokenization"라고 불리던 것은 오늘날 "pretokenization"라고 불린다.      
* **pretokenization**: 이 용어는 토큰화의 새로운 개념에 대한 첫 번째 접근법이 **resulting 단위**(이전에는 “tokens”, 지금은  “pretokens”)를 “sub-words”로 **분할하기 전**에 **문장을 적절한 Typographic 단위**(예: 토큰화의 "옛" 개념)**로 분할하는 것을 포함**했다는 사실에 의해 동기 부여된다.     


-----
------


# **3 Pre-tokenization yields word-like typographic units**    
모든 NLP 작업에서 다소 매력적이지 않은 부분이 역사적으로 어떻게 다루어졌는지를 보여줌

**[옛 토큰의 사용방식]**       
* purely typographic token의 언어적 무관함(linguistic irrelevance)     
* 텍스트를 linguistically motivated된 단어 형태로 자동 분할하는 것의 어려움(difficulty of automatically splitting a text into linguistically motivated word-forms)     
* **위 두 문제의 절충안:**  purely typographic token과 purely linguistic word-forms의 중간 단위가 널리 사용되어 왔다.   

**[“pre-tokenizers”의 역사]**         
* **용어 변화**     
   * 초기에는 “tokenizers”로 알려짐     
   * sub-word 토큰화의 확산 이후 “pretoken”으로 바뀜    
   * 오늘날에는 “pre-tokenizers”로 명칭 바뀜         
* **대표 모델**      
   * 이러한 “pre-tokenizers” 중 일부는 비교적 단순하고 typographic token에 충실하다.      
   * Hugging Face의 토큰라이저 패키지에 있는 오래된 ```Moses (Kohn et al., 2007) tokenizer6``` 가 초기에 널리 사용됨           
   * 그리고 더 최근은 Hugging Face의 Tokenizers package에 있는```pre-tokenizers package```가 있다.      
 
**[“pre-tokenization” 개념의의 탄생과 문제점]**     
* 위의 대표 모델들의 사용은 "tokenization(현재는 “pre-tokenization”)"라는 단어를 만들어냈다.     
* **"tokenization(pre-tokenization)"의 의미:** 일반적으로 문장을 원자 단위로 분할하는 작업을 나타내게 됨.       
    * 즉, "후속 처리에서 분해될 필요가 없는 기본 단위" 단위가 typographic 단위보다 단어 형태에 더 가까울 때에도 "tokenization"이라고 불린다.       
    * 더 나아가, 토큰화자가 **문장을 분할**할 뿐만 아니라 **정규화**, **철자 수정** 또는 **named entity detection** 목적과 같은 원시 텍스트를 수정하는 데 공통적으로 사용되므로 현재 토큰의 표준 정의에서 벗어난다.      
* ❌ **문제점**     
    * 이러한 초기 토큰의 정의와 역할은 정규화 작업이나 다른 공백 기호의 통합 및 병합은 **토큰화 전으로 되돌릴 수 없게한다**.     
    * 토큰화된 출력에서 원시 텍스트를 확실하게 복구할 수 없다.       
