---
title: "Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP 정리"
date:   2022-07-12
excerpt: "Between words and characters:A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---

# intro
 
      

## 핵심  

## 읽기 위해 필요한 지식
* [word2vec](https://yerimoh.github.io/DL14/): baseline 모델이기 때문에 꼭 알아야 한다.        
* [Embedding](https://yerimoh.github.io/DL15/): word2vec 속도 개선으로 이 포스팅도 꼭 알아야 한다.      

## 원 논문
[Enriching Word Vectors with Subword Information](https://arxiv.org/abs/2112.10508)


---

# 목차  


---


# **Abstract**
우리가 모델링하고자 하는 텍스트의 단위는  
bytes에서 다중 단어 표현식(multi-word expressions)에 이르기까지, 텍스트는 다양한 세분성으로 분석되고 생성될 수 있다.      

최근까지 대부분의 자연어 처리(NLP) 모델은 단어를 통해 작동하여 discrete 및  atomic tokens으로 처리했지만,   
byte-pair encoding(BPE)을 시작으로 많은 영역에서 하위 단어 기반 접근 방식이 우세하여 **작은 어휘를 사용**하면서도 **빠른 추론**을 허용하고 있다.      

이 토크나이징 방법들에 대해 학습된 세분화를 기반으로 한 **subword-based approaches**뿐만 아니라 **단어와 문자의 하이브리드 접근 방식**이 **어떻게 제안되고 평가**되었는지를 보여줌으로써,     
사전 신경 및 신경 시대의 여러 작업 라인을 연결한다.     

본 논문은 모든 애플리케이션에 대해 **특별한 해결책**이 결코 **없을 것**이며,   
토큰화 방법이 애플리케이션에 여전히 중요하다고 결론짓는다.    


-----
-----

# **1 Introduction**    


_“‘tokens’ are not a real thing. they are a computer generated illusion created by a clever engineer” dril_gpt1         


우리가 사람들에게 NLP 모델을 처음 소개할 때,    
우리는 종종 텍스트가 컴퓨터에 작은 조각들로 잘려나간다는 생각을 당연하게 받아들인다.    
➡ 결국 그것은 단지 일련의 정수일 뿐이다. 따라서 우리는 이것들을 (보통) 연속 **sub-strings tokens**이라고 부른다. 


교육 환경에서, 그리고 사실 역사적으로 NLP에서 이러한 토큰은 다소 자연스럽게 단어(처음에는 영어로 “space-separated substrings”)로 암시된다. 


**[단어에서 구두점 분리의 어려움]**            
* "don't"라는 단어를 예로 들면,    
* 이 단어의 구두점 분할의 경우, ```"don"``` ```"'"``` ```"t"```라는 다소 무의미한 세 개의 토큰을 얻게 될 것이다.            
* 그런데 더 효과적인 토크나이징을 하려면 ```"do"```와 ```"n't"```의 두 단위를 산출해야 한다고 주장할 수 있다.       

---

## 논문 개요    
이 survey는 $$tokenization$$에 대한 위와 같은 질문을 다루며,      
본 논문은 의 각 챕터의 핵심내용은 아래와 같다,    

**[2: Tokens, word forms, and sub words](# 2-Tokens,-word-forms,-and-sub-words)**      
* 근본적인 질문과 용어를 상세히 설명     

**[3: Pre-tokenization yields word-like typographic units](# 3-Pre-tokenization-yields-word-like-typographic-units)**      
* 모든 NLP 작업에서 다소 매력적이지 않은 부분이 역사적으로 어떻게 다루어졌는지를 보여줌     

특히 지난 5년 동안, 다소 원자적인 단어와 같은 공간 분리 단위로서의 **"토큰"에 대한 직관적인 정의를 넘어서는 것**에 대한 관심이 새로워졌다.  
* **4**: 이를 위한 한 가지 방법인 **단어 내부 정보를 사용하여 단어와 같은 단위를 보강**하는 방법.    
* **5**: 신경 모델링은 이를 그 어느 때보다 쉽게 만들어 명백한 표시 없이 단어 경계까지 학습할 수 있는 모델로 이어진다(예: 공백이 있는 경우).  unsupervised word segmentation or discovery의 개념은 수십 년의 작업에서 자체적으로 존재했지만,    
* **6**: **subword** 단위를 원자 토큰으로 사용하는 현재 널리 퍼져 있는 아이디어를 최종적으로 살펴볼 때 고려하는 것이 유용하다는 것을 알게 될 것이다.      
* **7**: 다국어 어휘의 공유와 경쟁의 몇 가지 문제 살펴봄     
* **8**: 문자, 바이트 또는 심지어 픽셀로 최대 분해할 수 있는 가장 간단한 토큰화를 사용해봄        


이 논문이 끝난 후,       
* 바이트의 손쉬운 솔루션이 도달해 보이는 2021년 현재에도 "복잡한" 토큰화가 의 중요성을 논증하여 마무리 한다.     
* 최근의 발전은 특정 도메인과 사용 사례에 대해 최대 분해 처리를 가능하게 하지만, 그것들은 광범위한 NLP 시나리오를 다루지 않고 **자체의 단점과 편견**을 가지고 있다고 주장할 것이다.     


![image](https://user-images.githubusercontent.com/76824611/178999180-03d114b4-8ba7-4748-afce-5115faccc2b5.png)
A taxonomy of segmentation and tokenization algorithms and research directions


------
-----











# **2 Tokens, word-forms, and sub-words** 
이 챕터에서는 token에 대한 근본적인 질문과 용어를 설명한다.      

토큰, 단어 형태 및 하위 단어 NLP에서 텍스트 데이터는 
"문장"(sentences)과 "단어(words)"로 분할되었다. 

**[sentences]**     
* 거시적 단위(macroscopic units)는 종종 서로 독립적으로 고려되며, 그 자체가 미시적 단위로 세분된다.        




**[tokens]**    
* 일반적으로 토큰이라고 불리는 타이포그래피 단위(typographic units)는 언어적으로 동기화된 단위에 대한 근사치로 사용되어 왔다.     
* MAF(형태학적 주석 프레임워크)는 토큰을 "“non-empty contiguous sequence of graphemes or phonemes in a document."으로 정의한다.    
* 예를 들어, 현재 라틴어 스크립트와 함께 보편적으로 사용되는 **공백**과 같은 **타이포그래피 구분자**를 사용하는 쓰기 시스템의 경우, 토큰은 널리 사용되었으며 **비문구 비백색 공간 마크** 또는 **구두점의 연속 시퀀스**로 정의되었다.    
* 특정 문장 부호(예: 하이픈 또는 아포스트로피)를 토큰으로 임의로 정하면, 그러한 정의는 **문장**을 토큰을 기준으로 **원자 단위로 분할**한다.     
* 토큰과 단어 형태 사이에 **일대일 대응이 없다**("멀티톡 단어", "멀티워드 토큰")    
    * 단어 형태는 여러 개의 토큰(예:French or English sine die)으로 만들 수 있다.     
    * 반면, 여러 개의 단어 형태는 동일한 토큰(예: 영어 don't = do + not, 스페인어 damélo + da + lo)으로 나타낼 수 있다.     

**[tokens의 재정의의 필요성: word-forms을 위해]**      
* 최근 문장이 원자 단위로 분할되는 방식이 진화하여 **토큰화 개념이 재정의되**었다.    
* **필요성:** 과학적 결과(예: 하위 단어 분할이 기계 번역 성능에 미치는 영향와 기술적 요구 사항(예: 고정 크기 어휘가 필요한 BERT와 같은 언어 모델) 모두에 기초하여, 원자 처리 장치(아직 토큰이라고 함)가 **단어 형태(word-forms)의 근사치가 될 필요**가 있다.    
* ❌ **문제**: 퇴색한 결과적으로, 현재의 NLP에서 토큰의 개념은 여전히 MAF 정의와 완벽하게 일치하지만, 더 이상 **타이포그래피 단위의 전통적인 정의와 일치하지 않는다**.     


**[토큰화(Tokenization)]**     
* **정의:** 문장을 정형적이지 않은 (그리고 실제로 언어적이지 않은) 동기화된 단위로 분할하는 작업     
* 이는 종종 고전적인 토큰과 단어 형태보다 작기 때문에 종종 **subword**라고 불린다.     
* Typographic 단위("오래된" 토큰)는 현재 종종 "pre-tokens"라고 불린다.      
➡ 따라서 "tokenization"라고 불리던 것은 오늘날 "pretokenization"라고 불린다.      
* **pretokenization**: 이 용어는 토큰화의 새로운 개념에 대한 첫 번째 접근법이 **resulting 단위**(이전에는 “tokens”, 지금은  “pretokens”)를 “sub-words”로 **분할하기 전**에 **문장을 적절한 Typographic 단위**(예: 토큰화의 "옛" 개념)**로 분할하는 것을 포함**했다는 사실에 의해 동기 부여된다.     


-----
------


# **3 Pre-tokenization yields word-like typographic units**    
모든 NLP 작업에서 다소 매력적이지 않은 부분이 역사적으로 어떻게 다루어졌는지를 보여줌

**[옛 토큰의 사용방식]**       
* purely typographic token의 언어적 무관함(linguistic irrelevance)     
* 텍스트를 linguistically motivated된 단어 형태로 자동 분할하는 것의 어려움(difficulty of automatically splitting a text into linguistically motivated word-forms)     
* **위 두 문제의 절충안:**  purely typographic token과 purely linguistic word-forms의 중간 단위가 널리 사용되어 왔다.   

**[“pre-tokenizers”의 역사]**         
* **용어 변화**     
   * 초기에는 “tokenizers”로 알려짐     
   * sub-word 토큰화의 확산 이후 “pretoken”으로 바뀜    
   * 오늘날에는 “pre-tokenizers”로 명칭 바뀜         
* **대표 모델**      
   * 이러한 “pre-tokenizers” 중 일부는 비교적 단순하고 typographic token에 충실하다.      
   * Hugging Face의 토큰라이저 패키지에 있는 오래된 ```Moses (Kohn et al., 2007) tokenizer6``` 가 초기에 널리 사용됨           
   * 그리고 더 최근은 Hugging Face의 Tokenizers package에 있는```pre-tokenizers package```가 있다.      
 
**[“pre-tokenization” 개념의의 탄생과 문제점]**     
* 위의 대표 모델들의 사용은 "tokenization(현재는 “pre-tokenization”)"라는 단어를 만들어냈다.     
* **"tokenization(pre-tokenization)"의 의미:** 일반적으로 문장을 원자 단위로 분할하는 작업을 나타내게 됨.       
    * 즉, "후속 처리에서 분해될 필요가 없는 기본 단위" 단위가 typographic 단위보다 단어 형태에 더 가까울 때에도 "tokenization"이라고 불린다.       
    * 더 나아가, 토큰화자가 **문장을 분할**할 뿐만 아니라 **정규화**, **철자 수정** 또는 **named entity detection** 목적과 같은 원시 텍스트를 수정하는 데 공통적으로 사용되므로 현재 토큰의 표준 정의에서 벗어난다.      
* ❌ **문제점**     
    * 이러한 초기 토큰의 정의와 역할은 정규화 작업이나 다른 공백 기호의 통합 및 병합은 **토큰화 전으로 되돌릴 수 없게한다**.     
    * 토큰화된 출력에서 원시 텍스트를 확실하게 복구할 수 없다.   
  
  
 
------
-----


# **4. Augmenting word-level pretokenizer tokens with character information**
특히 지난 5년 동안, 다소 원자적인 단어와 같은 공간 분리 단위로서의 **"토큰"에 대한 직관적인 정의를 넘어서는 것**에 대한 관심이 새로워졌다.  
이 챕터에서는 이를 위한 한 가지 방법인 **단어 내부 정보를 사용하여 단어와 같은 단위를 보강**하는 방법을 설명한다.     

**[word-level 모델의 특징]**           
* 개념적으로 이해하기 쉬움    
* neural era에서는 해석 가능한 세분화된 기능을 제공      
* **단점**: 폐쇄형 어휘 모델: 훈련 중에 거의 보이지 않는 희귀하고 새로운 단어를 처리할 수 없음    
* **해결책:** 역사적으로 희귀한 단어 유형은 훈련 시 새로운 단어 유형 ```UNK(알 수 없음)```로 대체됨     


**[```UNK```대체 방식의 단점]**         
* 자연어 생성(NLG)을 수행할 때 UNK가 허용되지 않는다   
* ELMo 또는 BERT와 같은 large-scale 모델에 사용될 때 일회성 이벤트뿐만 아니라 유용한 의미의 앵커인 **새로운 단어에 대한 기능**을 **추출할 수 없다**        
* 영어 이외의 언어, 특히 더 생산적인 형태학 및 따라서 유형 토큰 비율이 높은 언어에서 희귀 단어를 제거하는 것은 불가능     

그럼에도 불구하고, **단어**가 언어의 **기본 단위**이기 때문에,       
**단어를 구성하는 문자를 기반**으로 처리함으로써 근본적으로 단어 기반 틀에서 희귀하고 새로운 단어의 **처리를 개선**하는 여러 접근법이 등장했다.      
우리는 단어 표현의 보다 포괄적인 처리를 위해 이 섹션에서 이러한 접근 방식 중 일부를 제시할 것이다.       

-----


## 4.1 Augmenting word-level models with spelling information    

**[단어에 초점]**         
* 언어를 위한 neural models에서,    
90년대와 2000년대의 연구는 **단어에 초점**을 맞추고, 대신 문자의 문자열을 처리했다.      

**[단어 자체에 대한 정보]**        
* 그러나 neural models이 NLP에서 중요해지자마자, 신경 네트워크에서 사용하기 위한 **단어와 문자 수준의 정보의 조합**(word- and character-level information)이 등장.       
* 단어 임베딩 추정을 돕기 위해 단어 자체에 대한 정보를 사용할 것을 처음 제안했다.      
* word’s embedding을 단어의 철자를 통해 구성한다는 아이디어를 **대중화**했다.   


**[CNN 기반 임베딩 기능]**     
* 임베딩 행렬을 CNN(convolutional neural network) 레이어로 대체할 때에도 generative 모델은 여전히 폐쇄 어휘이며, 
이는 훈련 데이터에서 보였던 단어만 예측할 수 있다는 것을 의미하기 때문에,         
CNN 구성은 **새로운 단어가 아닌** **희귀 단어**만 돕는다.      


**[CNN with 토큰의 철자]**      
* **각 토큰에 대한 철자**로 임베딩을 구성하는 CNN 기반 임베딩 기능     
* 새로운 단어를 예상하는 대신 "자주 단어를 정확하게 얻도록" 암묵적으로 훈련한다.  
* **이 기반 모델의 역할:** POS 태깅과 같은 다른 고전적인 NLP 작업의 발전을 이끌었고 궁극적으로 최초의 큰 문맥 단어 임베딩 모델 ELMo에 힘을 실어주었다.        
  * [fastText](https://yerimoh.github.io/LAN7/) 임베딩은 문자가 아니라 겹치는 n-gram에서 단어 임베딩을 구성할 것을 제안하여,     
새로운 단어에 대한 임베딩을 얻을 수 있다(generative 의미는 아니지만 그런 의미에서 "open-vocabulary"로 만든다).    

  * Ataman and Federico도 마찬가지로 문자 대신 (중첩) 𝑛-grams을 사용하여 기계 번역에서 더 나은 성능을 얻는다(형태학적으로 풍부한 언어에서 BPE를 능가함).     
  * 최근에는 철자 오류 또는 전송으로 노이즈가 많은 텍스트를 더 잘 처리함으로써     
구성 문자 임베딩에 대한 BPE 단위의 임베딩을 향상시켰다.      


**[small Transformer]**   
* 의료 텍스트와 같은 새로운 도메인; 동시에 Aguilar 등(2021)은 거의 동일하게 하지만 CNN 대신 small Transformer를 사용한다.     

**[구성 기반 접근 방식]**   
* 구성 기반 접근 방식은 사전 훈련된 단어 수준 입력 모델에도 통합되었다.     
* 특히, Pinter 등(2017)은 테스트 시간 동안 알 수 없는 단어가 나타날 때마다 호출되는 helper RNN 모델을 사용하여 **철자가 주어진 단어의 임베딩을 모방**하도록 훈련된 모델을 학습한다.      


---


## **4.2 Open-vocabulary language modeling with (tokenizer-defined) words made of characters**
**[open-vocabulary language modeling의 어려움]**    
* ```open-vocabulary language modeling```: test time때 **새로운 단어를 예측하고 생성**할 수 있는 모델     
* ```closed-vocabulary generative models```을 ```open-vocabulary language modeling```로 확장하는 것은 어렵다.              
* **어려운 이유**: 완전히 새로운 단어를 포함하는 문장의 무한 집합에 대한 확률 질량을 유지할 수 있어야 하기 때문에 input side의 Open-vocabulary 보다 다소 어렵다.     


**[Mielke와 Eisner(2018)의 모델]**         
* 단어 임베딩을 정규화하여 작은 문자 수준(smaller character-level)의 ```RNNLM```을 사용     
* smaller model을 사용하여 철자를 예측하도록 함으로써 일반적인 폐쇄 음성 단어 수준의 반복 신경망 언어 모델(```RNNLM```) 설정을 근본적으로 향상시키는 확률론적 2단계 모델을 제안한다.       
* 단어 수준(word-level) ```RNNLM```이 ```UNK```를 **예측할 때마다 새로운 단어를 즉시 생성**      
➡ 언어 개념과 직관적 모델링에 의해 동기 부여       
➡ 질적 및 정량적으로 성공적인 것으로 입증된 개방형 어휘 모델을 산출      



**[```cache model```의 확장]**        
* Kawakami et al. (2017)의 모델은 단어 및 문자 수준의 RNN의 유사한 2단계 설정을 따르지만,     
cache model(Grave et al., 2016)을 사용하여 directly copied할 수 없는 경우, **각 단어를 문자 수준의 RNN을 사용하여 철자**해야 한다.         
➡ 이들의 분석은 캐시 모델이 그렇지 않다는 것을 분명히 보여준다.      
* ```cache model```은 “bursty” **미지의 단어**만 복사하지만, 또한 잊어버리지 않도록 하기 위해 극히 extremely **common** function words들도 복사한다.       
* 이 아이디어는 ```Ataman``` 등(2019)에 의해 machine translation decoder(creating word embeddings on the encoder side from character-level ```BiRNNs``` as in ```ELMo```)에서 인코더 측에 단어 임베딩을 생성)에 대해 선택됨    
* 나중에 ```Ataman``` 등(2020)에 의해 확장됨          


**[다른 접근 방식]**     
* 더 낮은 속도로 ```multi-layer RNNs```의 higher layer를 실행하는 것이다(hidden state로의 업데이트를 건너뛴다)     
* 이는 El Hihi and Bengio(1995)에서 처음 제시된 오래된 아이디어이며(Schmidhuber (1991, 1992)의 "neural sequence chunker"에 구축됨)      
* 우리는 단어 경계와 그에 따른 segmentations을 실제로 배울 수 있다.    


----
----

# **5 Learning segmentations to find concatenative word-like pretokenizer tokens**
지금까지 우리는 [2](#2-Tokens,-word-forms,-and-sub-words)에 요약된 개념정의의 변화에도 불구하고,    
predefined된 단어(또는 사전 토큰화 출력) 개념을 갖는 것에 의존해왔다.              

**[문제]**      
그러나 영어가 아닌 언어나 robustness문제로 인해 predefined된 정의가 주어지지 않거나, 얻을 수 없거나, 단순히 바람직하지 않다면 어떻게 될까?      
우리의 **데이터 기반 머신 러닝 접근법**이 **토큰화를 학습**할 수 있는 방법이 있을까?     

**[해결]**       
이 절에 설명된 대부분의 접근 방식은 **의미 있는 단위**에 해당하는 세그먼트와 **경계를 찾기 위해**,     
**근사(approximate)** 또는 (더 많은 가정을 사용하여) **정확한 추론**을 수행할 수 있는 **잠재(latent) 변수로 암시적 분할(implied segmentation)을 처리**함으로써 토큰화를 해결할 것을 제안한다.       

이 절에서 설명하는 다양한 기술은 다양한 크기와 품질의 단위를 산출한다.


----

## 5.1 Character-level neural models that learn to skip steps at higher levels

**[Elman(1990)]**     
* 이미 90년대에 Elman(1990)은 character-level의 RNN을 수동으로 분석하고 예측 깜짝 비교를 단어 경계(boundaries)와 관계시켰다.    
* 이 아이디어는  Schmidhuber(1991, 1992)의 “neural sequence chunker”에서 확장되었다.     


**[Doval and GómezRodríguez(2019)]**     
* 최근에는 Doval and GómezRodríguez(2019)의 beam search framework 하에서 character-level neural models 뿐만 아니라 n-gram models에도 분석을 적용하여 공간이 삭제되는 micro blog texts를 분할했다.     


**[```HM-RNN```(Chung et al., 2017)]**      
* post-hoc surprisal thresholding(사후 임계값)을 사용하는 대신 [4.2](# Open-vocabulary-language-modeling-with-(tokenizer-defined)-words-made-of-characters)에서 동기화된 여러 timescales의 아이디어를 취한다.     
* 기울기 하강 최적화 방법        
   * skip이나 update하기 위한 이진 결정(binary decisionbinary decision)을 학습       
   ➡ 단어 경계 감각 제공    
   * 직선 추정기를 사용하여 대략적인 기울기 하강으로 최적화        
* 계층들 사이의 통신은 **양방향적**으로 일어남      
   * the lower network: 그것의 최종 상태를 higher 네트워크에 보고한다      
   * the higher network: 그것의 새로운 상태를 lower 계층에 보고한다.     
   * 이러한 통신을 스스로 계속 진행한다.    
 
 
**[```HM-RNN```(Chung et al., 2017)의 연구]**      
* **Kawakami et al. (2019):** 데이터에 공간을 포함할 때 단어 경계를 “recover(복구)” 하지만,      
Kawakami 외(2019)는 **공간을 포함하지 않을 때** 이 모델로 사용할 수 없는 세그먼트(unusable segments)를 얻었다고 주장한다.     
* **NMT에 ```HM-RNN```을 사용하려고 할 때:**     
   * Cherry 등(2018)은 그것이 훈련되도록 하는 데 **많은 수정 작업이 필요**했으며,      
작업에서 그것의 성능은 경쟁적이지만 **우수하지는 않았다**고 보고했다.     
   * 이 발견은 HM-RNN이 잘 훈련되도록 하고, 이를 완화하며,    
   텍스트 데이터에 대한 하위 세그먼트(subpar segmentation)를 보여주는 논문을 전문으로 하는  Kádár et al. (2018)의 연구 결과를 뒷받침한다.      
* Kreutzer and Sokolov (2018)는 NMT에 대해 **단계를 건너뛰고 하위 레이어로 요약을 생성**하는 유사한 패러다임을 사용하려고 노력하며, **건너뛰기**는 거의 사용되지 않으므로 좋은 성능을 위해 **불필요**해 보인다는 것을 발견한다.        
* **모델의 확장:** 그럼에도 불구하고, 이 모델은 Luo와 Zhu(2021년)에 의해 구와 문장 수준의 경계로 확장된다.      
* 더 coarser한 계산 레이어를 가지고 있음에도 불구하고, 이러한 모델은 여전히 단어가 생성될 때마다  “spell out”해야 한다.       
즉, 토큰을 재사용 가능한 단위로 memoize할 수 없다는 것을 지적할 필요가 있다.       


-------


## **5.2 Marginalization over all possible segmentations**    
마지막으로, 개념적으로 간단한 접근 방식은,    
문자열의 분할을 training과 test time 모두에서 **marginalized해야 하는 잠재 변수(latent variable)** 로 다루는 것이다.     

**[문제]**       
이것은 본질적으로 겹치는 길이가 다른 문자열, 즉 "cat", "at", "foster cat"을 포함하는 어휘를 갖는 것을 의미하며,   
"my foster cat" 문자열은 잠재 단위의 다른 시퀀스에 대응하여 여러 방식으로 분해될 수 있다.    
➡ 분할의 수가 시퀀스 또는 컨텍스트 길이에서 기하급수적이다.            

**[해결]**      
* 잠재 분해에 대한 주변화를 위한 근사치(§ 5.2.1)에 의존       
* n-gram 모델(§ 5.2.2)을 사용하여 독립성 가정으로 모델을 단순화해야함       



<details>
<summary>📜 5.2.1 Approximate marginalization</summary>
<div markdown="1">
  
* **Chan et al. (2017)**     
    * beam search을 통해 **approximate MAP 추론**을 사용하여 관측의 한계 확률을 근사화하는 추정기를 제안한다.     
    * 그들은 그 모델이 훈련시키기가 매우 힘들지만 유망한 결과를 얻는 데 성공한다는 것을 발견했다.    
* **Buckman과 Neubig(2018)**   
    * 이 모델의 **불안정성을 확인**하고 LM 복잡도 측면에서 더 나은 결과를 생성하는 평균 RNN 숨겨진 상태를 기반으로 몇 가지 **근사 추론 체계를 제안**한다.       
* **Hiraoka 등(2020)**      
    * Unigram LM 토큰화 제안 분포(§ 6.4.3 참조)를 기반으로 유사한 모델을 구현하는데,      
    이 분포는 문장의 **𝑛-best 토큰화**가 모든 **문장 인코더 모델에 독립적으로 공급**되며,     
    **결과 문장 임베딩은 우선순위 토큰화 가능성에 따라 평균화**된다.      
* **Hiraoka 등(2021)**      
    * 별도의 손실로 토큰화 및 다운스트림 모델을 훈련시키고,      
    전자는 낮은 다운스트림 손실을 생성하는 토큰화를 보상하며,     
    후자는 조건부(및 강화) LM에서 샘플링된 하나의 토큰화를 사용하여 이 모델을 sequence-to-sequence 설정으로 확장한다. 
 
</div>
</details>  







<details>
<summary>📜 5.2.2 Exact marginalization using additional independence assumptions: segmental neural language models</summary>
<div markdown="1">


     

**[Kong et al.(2016)]**    
* segmental neural language models의 더 인기 있는 솔루션 개척      
* 그는 분할 문제를 **단조로운 13개의 seq2seq 작업으로 캐스팅**하여 characters에서 하위 문자열의 커버 순서(sequence of substrings), 즉 분할로 전환했다.      
* ```BiRNN```을 사용하여 처리되고 포함된 **전체 raw string에 대한 세그먼트 예측을 조건화**      
➡ 세그먼트화 결정/점수는 컨텍스트를 사용 가능             
* 이러한 임베딩을 사용하여 세그먼트 가능한 모든 개별 하위 문자열을 독립적으로 채점한 다음,     
개별 점수를 합산하여 전체 세그먼트를 채점          
➡ 동적 프로그래밍(DP)을 사용하여 covering of the entire input string를 효율적으로 찾을 수 있다    
* **동작 원리**     
   * 중앙 독립성 가정(central independence assumption) 때문이다.       
   * 이 모델은 세그먼트에 점수를 매길 때 다른 세그먼트에 의존하지 않고 **단지 주변 문자에 의존**할 뿐이다.     


**[Wang et al. (2017)]**
* 위의 Kong et al.(2016)에 대한 확장      
* 이 모델은 세그먼트화를 몰라도 실행 가능     
➡ 개별 세그먼트 생성 프로세스에서, 과거 표현을 사용할 수 있는 출력에 대한 문자에 대해 per-segment RNN을 보유      
➡ 동적 프로그래밍을 중단하지 않고 세그먼트에 대한 정보를 왼쪽에서 오른쪽으로 공유 가능      


**[LMing]**         
* 이제 **입력에 대한 조건을 생략**하여 세그먼트 언어 모델이라는 용어를 만든 Sun과 Deng(2018)의 모델을 산출     
* 한자에 대해 훈련     
* 비지도 학습된 세그먼트를 사용하여 중국어 단어 분할에서 경쟁하는 것으로 만들어짐      
* 2차 세그먼트 수의 **계산을 실현 가능하게 유지**하기 위해 세그먼트를 최대 **4자로 제한**한다.      


**[Grave et al. (2019)]**        
* Transformers를 독립적인 character-level global backbone으로 사용하여, **독립**적으로 **동일한 점프**를 한다.     
* 영어 open-vocabulary language modeling에서 평가할 때 perplexity가 개선되었다.    
* 하지만, obtained segmentation을 사용하거나 평가하지 않는 것을 발견      
➡ 원인: 이는 그들 역시 최소 400번 나타나는 4-grams만 사용하기 때문일 가능성이 크다.       

**[Kawakami et al. (2019)]**      
* 위와 동일한 **독립 아이디어를 사용**     
* **다른점:** 문자열 세그먼트의 방출(missions of string segments)은  Kawakami et al. (2017)와 같은 문자 수준 모델(§ 4.2 참조)과 학습된 임베딩이 있는 대규모 하위 문자열 세트(최대 10-grams까지 훈련: 데이터에 충분히 자주 나타남)의 컨텍스트 의존적 혼합에서 비롯됨        
* perplexity뿐만 아니라 일부 baselines(제5.3조 참조)을 능가하는 단어 분할 성능을 갖음     
* 하지만, 여전히 일부 이전 모델보다 훨씬 더 나쁜 성능을 보임      
    * marginal likelihood 대신 분할 성능에 hyperparameters를 조정하여 **불공평한 이점**을 가지고 있다고 주장     
* 이미지 캡션에 대한 훈련을 할 때 모델이 **설명되는 이미지에 액세스**할 수 있을 때,    
 perplexity과 분할 성능이 모두 **향상**된다는 것을 발견       
 ➡ **단일 언어 및 단일 언어 텍스트**(monolingual and unimodal text)가 학습 세분화가 다른 양식, 다른 언어가 존재할 때보다 **더 어려울 수 있음**을 보여준다.    
 
 
**[He et al. (2020)]**    
* NMT 시스템의 대상 측 생성기로 유사한 세그먼트 모델을 구축     
➡ 위와 같이 character backbone 아이디어를 따르는 Transformer-based version모델을 동적 프로그래밍이 가능화를 위해 사용함         
* NMT 시스템을 최종 모델로 사용하지 않고 **학습된 tokenizer로 사용**한다.      
➡ 이는 동적 프로그램을 marginalization에서 **maximization**로 변경하여 BPE 또는 유니그램 LM 대신 사용할 수 있는 **DPE라는 새로운 분할을 얻음**으로써 쉽게 달성 가능(§ 6.4 참조).       
* ```small Transformer-based NMT```모델로 토큰화하는 학습이 **더 큰 모델에서 사용**하기 위해 BPE보다 더 나은 분할을 생성한다는 것을 보여준다.      
* 특히 번역 작업에 토큰화 모델을 훈련하면 **소스 언어에 따라 다른 분할이 생성**되며, 더 중요한 것은 **더 나은 분할(segmentation)이 생성**된다는 것을 보여준다.        


**[Downey et al. (2021)]**     
* 문자를 조건화하고 세그먼트를 예측하는 아이디어는 결과가 ```RNN-based SNLMs```을 일관되게 능가하지는 않지만,     
Downey et al. (2021)의 Transformers 및 ```left-to-right autoregressive Transformers```에서 발견된 **방향 마스크 언어 모델링**(adirectional masked language modeling ) 설정으로 확장된다.       
* 이 모델들 중 많은 것들이 ```UnigramLM```에 기반을 둔 모델들의 변형이라는 점이 중요하다 §6.4.3.       

 </div>
</details>  


-----

## 5.3 Finding words through Bayesian non-parametrics
𝑛-gram과 word-based 언어 모델들의 시대에,   

**[MacKay and Peto (1995)]**       
* 자동 회귀 언어 모델들의 Bayesian 관점 도움이 된다는 것을 알아냄      
* 평균이 저차 분포(lower-order distributions)인 디리클레 분포에서 고차 분포(higher-order distributions)를 도출하는 계층적 모델에서 추론으로 𝑛-gram 모델에서 smoothing 및 backoff를 재해석한다.


**[Teh(2006)]**      
* 위의 생각을 확장     
* 우리가 다시 임의적으로 큰 차수의 𝑛-gram 분포를 갖는 계층적 PYP 언어 모델을 제안        
* PYP 언어 모델      
    * 𝑛-gram 언어 모델 smoothing과 유사      
    * 그러나 𝑛의 선택을 포기하는 원칙적인 방법을 제공      

**[Wood et al. (2011)]**     
* 위 모델링 아이디어의 정점의 달성        
* **성능**    
    * 임의의 이진 데이터에 대해 뛰어난 압축 성능을 자랑함      
    * 언어 모델링 작업 여전히 매우 잘 수행        
    * 하지만 이때 신경 모델이 이미 강력한 경쟁자로 나타남     


**[Goldwater et al. (2006b)]**        
* 이 Bayesian 관점을 확장       
* 새로운 단어들이 어떻게 처음 만들어지고 그리고 그것들이 실행 텍스트에서 어떻게 사용되는지를 설명함       
* **언급한 기술:** 2단계 언어 모델링 (제4.2절 참조)      
    * 1단계: 생성기 (새로운 어휘소를 생성하는)      
    * 2단계: 어댑터 (여기서 재사용을 지배하는, PyP)        
    * 위에서 말한 것과 같이 이 두개의 단계는 서로 상호작용한다.      
* **기능**   
   * 단어 경계를 유추하는 것 가능해짐    
   * = 비지도 단어 분할을 수행하는 것이 가능해짐      
   * 이유: 텍스트를 설명하는 2단계 모델과 무한한 수의 가능한 어휘소에 양의 확률을 할당할 수 있는 Bayesian 비모수 메트릭(nonparametrics)의 사용 덕분       
* 인지 과정, 특히 아동 언어 습득을 설명하고 모델링함으로써 더 많은 동기를 부여받음.       

**[Goldwater et al. (2009)]**      
* transcribe된 유아 지향 음성을 분할하기 위한 Unigram과 Bigram Dirichlet Processs(DP)를 요약하여 오래된 비베이지안 접근 방식보다 우월함을 보여준다.      


**[Mochihashi et al. (2009)]**     
* 아이디어를 bigram DPs에서 ∞-gram nested/hierarchical PYPs로 확장하여 **영어 필기 텍스트에 대한 단어 분할**을 개선한다. 



**[Elsner et al. (2013)]**     
* 일련의 세그먼트를 관찰된 실현으로 변환하는 **음성 프로세스**를 추가로 모델링한다.      


-----



## 5.4 Related task: Unsupervised Chinese Word Segmentation
Word segmentation for languages without whitespace delimiters such as Chinese, Japanese and
Vietnamese (Shao et al., 2018) is an important area
of research and can be notoriously tricky.
In Chinese word segmentation (CWS), there is
growing interest in exploring unsupervised word
segmentation involving neural language models.
Traditionally, popular unsupervised approaches
take on two primary paths: 1) discriminative models and 2) generative models. Discriminative models rely on goodness measures for candidate segmentation. These statistical measures incude Mutual Information (MI), normalized Variation of
Branching Entropy (nVBE) and Minimum Description Length (MDL), etc., see §6.3. Generative
models focus on designing statistical models to
find the optimal segmentation of the highest generative probability. These models include Hidden
Markov Model (HMM), Hierarchical Dirichlet Process (HDP), Nested Pitman-Yor Process (NPY),
etc., see §5.3. It is trivial to extend discriminative
approaches by replacing 𝑛-gram language model
with neural language models. For generative approaches, previous work has shown that constructing a neural language model with a context encoder
and a segment decoder achieves competitive performance to its statistical counterparts (Sun and Deng,
2018, see previous subsection §5.2.2).


















