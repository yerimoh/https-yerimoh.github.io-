---
title: "Symbolic Chain-of-Thought Distillation: Small Models Can Also “Think” Step-by-Step 정리" 
date:   2023-08-15
excerpt: "Symbolic Chain-of-Thought Distillation: Small Models Can Also “Think” Step-by-Step"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---

   **[원 논문]**     
[Symbolic Chain-of-Thought Distillation: Small Models Can Also “Think” Step-by-Step
](https://aclanthology.org/2023.acl-long.150.pdf)


-----



# **ABSTRACT**

Abstract
Chain-of-thought prompting (e.g., “Let’s think
step-by-step") primes large language models
to verbalize rationalization for their predictions. While chain-of-thought can lead to dramatic performance gains, benefits appear to
emerge only for sufficiently large models (beyond 50B parameters). We show that ordersof-magnitude smaller models (125M—1.3B
parameters) can still benefit from chain-ofthought prompting. To achieve this, we introduce Symbolic Chain-of-Thought Distillation
(SCoTD), a method to train a smaller student
model on rationalizations sampled from a significantly larger teacher model. Experiments
across several commonsense benchmarks show
that: 1) SCoTD enhances the performance
of the student model in both supervised and
few-shot settings, and especially for challenge
sets; 2) sampling many reasoning chains per
instance from the teacher is paramount; and
3) after distillation, student chain-of-thoughts
are judged by humans as comparable to the
teacher, despite orders of magnitude fewer parameters. We test several hypotheses regarding
what properties of chain-of-thought samples
are important, e.g., diversity vs. teacher likelihood vs. open-endedness. We release our
corpus of chain-of-thought samples and code.


---



# 1 Introduction
Empirical scaling laws suggest that the accuracy
of Large Language Models (LLMs) on benchmark
tasks can be improved by increasing model size and
pre-training data volume (Hoffmann et al., 2022).
Beyond these training-time improvements, however, an inference-time strategy dubbed “chain-ofthought" (CoT) prompting,1
i.e., eliciting verbalizations of predictive processes via key-phrases like
“Let’s think step-by-step" (Kojima et al., 2022), can
similarly improve performance, e.g., Suzgun et al.
(2022) demonstrate additional performance gains
on a hard subset of the BigBench tasks (BIG-bench
collaboration, 2022) using chain-of-thought.



However, chain-of-thought prompting has only
been shown to be beneficial for models of sufficient scale (e.g., with more than 60B parameters
(Wei et al., 2022b)). In this work, we study whether
small language models can be “taught" the capacity
for chain-of-thought reasoning by larger language
models. We adopt a simple strategy, which we call
Symbolic Chain-of-thought Distillation (SCoTD):
first, we sample chain-of-thought rationales from
large language model given (unlabeled) input instances from a dataset; then, we train a smaller
language model to predict the sampled rationale
and sampled label. This process follows the “symbolic knowledge distillation” paradigm as in West
et al. (2022), wherein corpora are sampled from a
larger language model to serve as training data for
a smaller one.



We find that through SCoTD, smaller language
models learn to self-rationalize and perform significantly better on 3 commonsense QA tasks compared to learning without rationalizations. This result holds for both supervised and few-shot settings,
and across student models of varying scales (125M–
1.3B parameters). Performance gains are especially pronounced when applying distilled chain-ofthought models to difficult scenarios like: contrast
sets (Gardner et al., 2020) (§3.4; SCoTD significantly outperforms supervised learning on labels)
and fully held-out tasks (§3.5; few-shot SCoTD
significantly outperforms in-context learning).



Key to the success of this process is sampling
a relatively large number of rationales per example from the teacher model (e.g., 30 rationales/example) (Figure 2). This is different from
many prior practices that train with one rationale
per example (Camburu et al., 2018; Li et al., 2022a).
In ablation studies, we investigate several competing hypotheses for what are the most important
factors within the corpus: we filter the corpus to
CoTs that are assigned high probability by GPT-3
vs. filtering to CoTs that are diverse vs. filtering to
CoTs that explain more open-ended input instances.
While diversity and high probability are reasonable
filters that on average perform well, the “null hypothesis” of random downsampling performs well,
suggesting that the sheer volume of the rationales
is also a key contributing factor



We will release code and the corpus of sampled
chain-of-thoughts at https://github.com/
allenai/cot_distillation












