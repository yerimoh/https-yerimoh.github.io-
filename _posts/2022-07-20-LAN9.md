---
title: "An Empirical Study of Tokenization Strategies for Various Korean NLP Tasks 정리"
date:   2022-07-20
excerpt: "An Empirical Study of Tokenization Strategies for Various Korean NLP Tasks"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---


원 논문: [An Empirical Study of Tokenization Strategies for Various Korean NLP Tasks](https://arxiv.org/pdf/2010.02534.pdf)


코드:  [깃 ](https://github.com/kakaobrain/kortok)

# Abstract
일반적으로 토큰화는 대부분의 텍스트 처리 작업에서 첫 번째 단계이다.  
토큰은 텍스트의 컨텍스트 정보를 내장하는 atomic 단위 역할을 하므로 **토큰을 정의하는 방법은 모델의 성능에 결정적인 역할**을 한다.    

**[문제]**      
Byte Pair Encoding(BPE)은 단순성과 보편성 때문에 사실상의 표준 토큰화 방법으로 간주되어 왔지만, BPE가 모든 언어와 작업에서 가장 잘 작동하는지 여부는 여전히 불분명하다.      

**[본 논문]**      
본 논문에서는 주요 연구 질문인 "한국 NLP 작업에 가장 적합한 토큰화 전략은 무엇인가?"에 답하기 위해 몇 가지 토큰화 전략을 테스트한다.       
실험 결과는 BPE를 따르는 형태론적 분할의 하이브리드 접근 방식이 KorNLI, KorSTS, NSMC 및 PAWS-X와 같은 영어 기계 번역 및 자연어 이해 작업에서 가장 잘 작동한다는 것을 보여준다.     
KorQuAD의 경우, SQUAD의 한국어 확장이 가장 효과적인 것으로 나타났다.


-----
------


# 1 Introduction
토큰화는 대부분의 텍스트 처리 작업의 첫 번째 단계이다.     
다양한 NLP 작업에 대한 최상의 토큰화 방법을 찾기 위해 엄청난 학문적 노력이 이루어졌다.      

**[BPE]**     
* Sennrich et al.(2016a)에 의해 재도입된 이후 지난 몇 년간 BPE(Byte Pair Encoding) (Gage, 1994)는 사실상의 표준 토큰화 기법으로 간주되어 왔다.     
* BPE가 기계 번역 작업에서 매우 효과적인 것으로 판명     
* BPE가 **데이터 기반 통계 알고리즘**이기 때문에 **언어와 독립적**이라는 점이 BPE의 장점을 극대화.     


❌ 그러나 BPE가 **모든 언어에서 가장 잘 작동하는지 여부**는 여전히 명확하지 않다.     


**[본 논문]**    
* 본 논문에서 우리는 **형태학적으로** 영어보다 훨씬 **풍부**한 언어인 **한국어에 대한 다양한 토큰화 전략**을 연구한다.     
* 한국어 대 영어/영어 대 한국어 기계 번역 작업 및 자연어 이해(NLU) 작업(MRC), 자연어 추론(NLI), 의미론적 텍스트 유사성(STS), 감정 분석 및 paraphrase 식별에 가장 적합한 토큰화 전략이 무엇인지 경험적으로 검토한다.                 
* 특히 BPE와  linguistically motivated segmentation이 **얼마나 상호 보완적**인지에 관심을 둔다.    

---
----

# 2 Background

## 2.1 MeCab-ko: A Korean MorphologicalAnalyzer


**[MeCab(Kudo, 2006)]**        
* 의미: 조건부 무작위 필드(Conditional Random Fields; CRF)를 기반으로 하는 오픈 소스 형태학적 분석기이다.      
* 원래 일본어를 위해 설계됨 ➡ 일반용도 제공하므로 다른 언어에도 적용할 수 있다.     

**[MeCab-ko1]**         
* MeCab의 한국어 확장판          
* **확장 이유:** 형태나 구문 면에서 일본어와 한국어가 매우 유사          
* **train방법:** MeCab-ko는 MeCab을 사용하여  Sejong Corpus(Kang and Kim, 2001)를 이용하여 모델을 훈련시킴     
* **결과:** 2013년 출시된 이후 높은 정확도와 사용성이 좋아 많은 한국 NLP 작업에 널리 사용되고 있다.     
예를 들어, WAT(Workshop on Asian Translation)는 2015년부터 한국어 기계번역 결과를 평가하기 위한 공식 세분화 도구로 채택하고 있다(Nakazawa et al., 2015, 2016, 2017, 2018, 2019).     



---


## 2.2 Byte Pair Encoding
[BPE(Byte Pair Encoding)](https://wikidocs.net/22592)는 텍스트에서 가장 빈번한 바이트 쌍을 사용하지 않는 단일 바이트로 반복적으로 대체하는 간단한 데이터 압축 기법이다(Gage, 1994)     

Sennrich et al.(2016b)이 신경 기계 번역 모델에 성공적으로 적용한 이후, 그것은 언어 전반에 걸친 표준 토큰화 방법으로 간주되어 왔다. (한국어에서도 가장 지배적으로 사용된다)   





---
----

# 3 Related Work

**[tokenization strategies]**    
* 기계 번역을 위한 토큰화 기술에 대한 광범위한 연구가 있었다.    
* 여러 논문은 언어적으로 정보를 얻은 세분화와 BPE 또는 유니그램 언어 모델링과 같은 **데이터 중심 방법**의 혼합이 영어가 아닌 언어에 가장 잘 수행된다고 주장했다.       
* anerjee와 Bhattacharyya (2018): 힌디어와 벵골어 번역에서 기성 형태학적 세그먼트(segmenter)와 BPE를 영어에 대항하여 결합했다.       
* Tawfik et al. (2019): 아랍어에 대한 통계적 분할 방법과 함께 언어 동기 분할 모델의 재교육 버전을 사용했다.      
* Pinnis et al(2017): 영어-라트비아어 번역을 위해 BPE에 대한 언어 지침을 채택했다.    
* Park et al., (2019a): 우리와 가깝지만, 그들의 주된 초점은 신경 기계의 전처리 기술에 있다.     


**[NLU]**     
* 토큰화 전략 자체보다는 parallel corpus filtering과 같은 번역. 기계 번역에 대한 토큰화 연구와 비교하여 **NLU 작업에 대한 연구는 덜 주목을 받았다**.          
* Bostrom과 Durrett(2020): BPE와 유니그램 언어 모델링으로 사전 훈련된 BERT(Devlin et al., 2019)의 미세 조정 작업 성능을 비교          
* Moon과 Okazaki(2020): 한국어를 위한 새로운 인코딩 방법을 제안하고 몇 개의 한국어 NLU 데이터 세트를 사용하여 어휘 압축에서 그 효율성을 보여주었다.     


---
---

# 4 Tokenization Strategies
본 논문은 가장 작은 단위에서 가장 큰 단위로 배열된 다양한 한국 토큰화 전략을 소개한다.     
표 1에 나타난 바와 같이 각각 다른 토큰화 결과를 유도한다.    
![image](https://user-images.githubusercontent.com/76824611/188309388-9582b651-3b7b-4811-9a6e-ff3c61e7c59b.png)



## 4.1 Consonant and Vowel (CV)
In Hangul, the standard Korean writing system,
consonants and vowels, called Jamo in Korean, corresponding to Latin letters are assembled to form
a syllable character. For example, a Hangul consonant ㅎ /h/ (U+314E) is combined with a vowel ㅏ
/a/ (U+314F) to make a syllable character 하 /ha/
(U+558). Readers who are not familiar with such
a mechanism can think of Jamo and syllables as
atoms and molecules respectively. As a molecule
H2O can be decomposed into two H atoms and an
O atom, a syllable 하 /ha/ can be decomposed into
its constituent consonant ㅎ /h/ and vowel ㅏ /a/.
The first syllable 나 /na/ of the raw text in Table 1
is tokenized into ㄴ /n/ and ㅏ /a/, and the second
syllable 랑 /lang/ is tokenized into ㄹ /l/, ㅏ /a/,
and ㅇ /ng/, and so on. A whitespace is replaced
by a special symbol

## 4.2 Syllable
We can tokenize a sentence at the syllable level. A
whitespace is replaced by the special symbol ?.

## 4.3 Morpheme
MeCab-ko provides a convenient tokenization option in the command line interface3
. For example,
it returns A, B, and C given an input text AB C,
where A-C represent morphemes. Note that the
original space between AB and C is missing in the
output token list. Accordingly, it is NOT possible to recover the original text from the tokenized
result.
This can be problematic in some tasks that require us to restore the input text such as machine
translation whose target language is Korean, or
machine reading comprehension where we are expected to suggest a certain phrase in the given text
as the answer.
For this reason, we insert a special token ?
(U+2B51) to the original whitespace position. As
a result, in the above example, the tokenized sequence looks like A, B, ?, and D.

## 4.4 Subword
We learn and apply BPE using the SentencePiece
(Kudo and Richardson, 2018) library. It prepends
‘ ’ (U+2581) to every word to mark the original
whitespace, then tokenizes text into subword pieces.
As seen in Table 1, 나랑 쇼핑하자. can be split
into 나랑, 쇼, 핑하, 자, and . (period).
4.5 Morpheme-aware Subword
Motivated by the combined methods of dataand linguistically-driven approaches (Banerjee and
Bhattacharyya, 2018; Park et al., 2019a; Pinnis
et al., 2017; Tawfik et al., 2019), we apply MeCabko and BPE in sequence to make morpheme-aware
subwords. According to this strategy, since BPE
is applied after the original text is split into morphemes, tokens spanning multiple morphemes (e.g.,
핑하 in the Section 4.4) are not generated. Instead,
the BPE algorithm further segments morphemes
into frequent pieces.

## 4.6 Word
We can simply split text by whitespaces. Note that
punctuation marks are split into separate tokens.
Check that 나랑 쇼핑하자. is tokenized into 나랑,
쇼핑하자 and . (period) in Table 1.

[T@]
Table 2: BLEU scores of Korean to English (Ko-En)
and English to Korean (En-Ko) translation models with
different BPE training data. Note that the English sentences are tokenized using a 32K BPE model trained on
the English Wiki.
5 Experiments
5.1 Korean to/from English Machine
Translation
5.1.1 Dataset
To date, there have yet been few open source benchmark datasets for Korean-English machine translation, not to mention that Korean is not in the
language list of WMT4 or IWSLT5
. Park et al.
(2019a) used OpenSubtitles (Lison and Tiedemann,
2016), a collection of crowd-sourced movie subtitles across 65 different languages, for English to
Korean translation, but they are too noisy to serve
as a translation benchmark dataset.6
Recently, a Korean-English parallel corpus was
publicly released by AI Hub7
, which was gathered
from various sources such as news, government
web sites, legal documents, etc. We download
the news data, which amount to 800K sentence
pairs, and randomly split them into 784K (train),
8K (dev), and 8K (test).
5.1.2 BPE Modeling
Prior to training, we do simple preliminary experiments to decide which dataset to use for learning
BPE.
There are two choices: AI Hub news training
data and open source large text such as Wiki. AI
Hub training data is relatively small in size (130
MB), but can be optimal as its lexical distribution
will be close to that of the test data, considering
both of them are from the same source. On the
other hand, Wiki is larger, but it is not news per
se, so can be not as appropriate as AI Hub data for
BPE modeling.
To investigate this, first we train a 32K Korean BPE model (A) using SentencePiece with
the Korean sentences in the AI Hub training
data. Then we download the latest Wikipedia Korean8
/English9 dumps, and extract plain texts using
WikiExtractor 10. Next, we make 32K BPE models
for Korean (B) and English (C) with them. Finally,
we train Korean to English (Ko-En) and English to
Korean (En-Ko) translation models on the AI Hub
training data with the two different Korean BPE
models (A, B). The training details are explained
in Section 5.1.3. For comparison, we use the same
English BPE model (C) for both.
The results are shown in Table 2. For Ko-En
translation, the Wiki-based BPE model performs
better in both dev and test sets by 2-3 points. For
En-Ko translation, there is no practical difference in
performance between the Wiki and AI Hub-based
models. It is also worth considering the BPE models are used for NLU tasks as well as machine
translation. All things taken together, we opt for

the Wiki-based BPE model.
5.1.3 Training
We test the tokenization strategies in Section 4
with various vocabulary sizes on the AI Hub news
dataset.
We use the Transformer (Vaswani et al., 2017),
the state-of-the-art model for neural machine translation. We mostly follow the base model configuration: 6 blocks of 512-2048 units with 8 attention heads. We run all of our experiments using
FAIRSEQ 11 (Ott et al., 2019), a PyTorch based
deep learning library for sequence to sequence models.
Each model is trained using a Tesla V100 GPU
with batch size 128, dropout rate 0.3, label smoothing 0.1, and the Adam (Kingma and Ba, 2015)
optimizer. We set the learning rate to 5e-4 with the
inverse square-root schedule. We train all models
for 50 epochs and save the checkpoint files at every
epoch.
5.1.4 Results
After all training stages are finished, we evaluate the saved checkpoint files of each model on
the dev set to find the best one, which is subsequently used for the final test. In Table 3 we report
BLEU scores on both the dev and test sets using the
Moses12 multi-bleu.perl script. Following
WAT 2019 (Nakazawa et al., 2019), Moses tokenizer and MeCab-ko are used for tokenizing the
evaluation data.
For both Ko-En and En-Ko, overall, the Subword models (35.64-39.22) and the Syllable models
(38.45-39.30) are superior to the Morpheme models (31.59-37.37) or the Word models (7.04-18.42)
in performance. It is highly likely to come from
the lower OOV rates of the Subword models (0.07-
0.12) and the Syllable models (0.06) compared to
those of the Morpheme models (1.40-7.51) and the
Word models (26.20). While BPE tends to split rare
words into subword pieces, MeCab-ko is ignorant
of statistics so it splits words into morphemes by
linguistic knowledge instead. That the Morpheme
and Word models generate many OOVs suggests
Korean has so large types of morphemes or word
forms that even 64K vocabulary is not enough to
cover them all.
CV models are tiny in vocabulary size (166) so
they show the lowest OOV rate (0.02). However,
their performance is not as good as the Syllable
or Subword models. We speculate this is because
a single consonant or vowel must bear too much
contextual information in the CV models.
Morpheme-aware Subword 32K models achieve
the best BLEU scores. Each Subword model, as
shown in Table 4, contains 6-37% of tokens spanning morpheme boundaries in the test set, which
implies that subword segmentation by BPE is not
optimal and morpheme boundaries are meaningful
in tokenization.
To sum up, morpheme-aware subword tokenization that makes the best use of linguistic knowledge
and statistical information is the best for Korean
machine translation.
5.2 Korean Natural Language
Understanding Tasks
Large pre-trained language models have proven
their effectiveness in many downstream tasks (Peters et al., 2018; Devlin et al., 2019; Liu et al.,
2019). We pre-train BERT (Devlin et al., 2019)
models with various tokenization strategies, and
fine-tune them on five different Korean NLU tasks.
5.2.1 Machine Reading Comprehension:
KorQuAD 1.0 Dataset
The KorQuAD 1.0 dataset (Lim et al., 2019) is a
Korean adaptation of SQuAD 1.0 (Rajpurkar et al.,
2016), a popular reading comprehension dataset.
KorQuAD 1.0 consists of 10,645 passages and
their paired 66,181 questions (60,407 for training
+ 5,774 for development13). Like SQuAD 1.0, KorQuAD 1.0 involves answering a question given a
passage. The answer must be a phrase within the
passage.
5.2.2 Natural Language Inference: KorNLI
Dataset
The KorNLI Dataset (Ham et al., 2020) is a Korean NLI dataset sourced from three different NLI
datasets: SNLI (Bowman et al., 2015), MNLI
(Williams et al., 2018), and XNLI (Conneau et al.,
2018).
It is composed of 950,354 sentence pairs:
942,854 for training, 2,490 for development, and
5,010 for test. A model receives a pair of
sentences—a premise and a hypothesis—and classifies their relationship into one out of three categories: entailment, contradiction, and neutral.
5.2.3 Semantic Textual Similarity: KorSTS
Dataset
The KorSTS Dataset (Ham et al., 2020) is a Korean STS dataset translated from the STS-B dataset
(Cer et al., 2017). It comprises 8,628 sentence
pairs—5,749 for training, 1,500 for development,
and 1,379 for test. The task assesses the gradations
of semantic similarity between two sentences with
a scale from 0 to 5.
5.2.4 Sentiment Analysis: NSMC Dataset
NSMC14 is a movie review dataset scraped from
Naver Movies™. It consists of 200K samples of
which 150K are the training set and the rest 50K
are the test set. Each sample is labeled with 0
(negative) or 1 (positive). We hold out 10 percent
of the training data for development.
5.2.5 Paraphrase Identification: PAWS-X
Dataset
The PAWS-X dataset (Yang et al., 2019) is a challenging paraphrase identification dataset in six languages including Korean. The Korean portion
amounts to 53,338 sentence pairs (49,410 for training, 1,965 for development, and 1,972 for test).
Like the NSMC dataset, each sentence pair is annotated with either 0 (negative) or 1 (positive).
For each tokenization strategy, we pre-train a
BERT-Base model on a large corpus and fine-tune
it on the training sets of the five NLU tasks independently.
Pre-training. Because the Korean Wiki corpus
is not enough in volume, 640 MB, for the pre
training purpose, we additionally download the
recent dump of Namuwiki15, a Korean Wiki, and
extract plain texts using Namu Wiki Extractor16
.
On the resulting Namuwiki corpus (5.5 GB) along
with the Wiki corpus (640 MB), pre-training is
performed with a Cloud TPU v3-8 for 1M steps
using the official BERT training code17, which is
based on TensorFlow. We set the training hyperparameters of all models as follows: batch size
= 1024, max sequence length = 128, optimizer =
AdamW (Loshchilov and Hutter, 2019), learning
rate = 5e-5, warm-up steps = 10K.
Fine-tuning. After converting each of the pretrained models in TensorFlow into PyTorch, we
fine-tune it using HuggingFace Transformers18
(Wolf et al., 2019). The hyper-parameters for each
task are shown in Table 5.
5.2.6 Results
In Table 6 we report the evaluation results of the
various models on the dev and test sets. Since
KorQuAD lacks the test set, we report the results
on the dev set only.
As for KorQuAD, Subword 64K models achieve
the highest Exact Match (EM) and F1 scores. The
scores in the Subword and Morpheme models increase monotonically as the vocabulary size grows.
On the other hand, the 32K models outperform the
others in the Morpheme-aware Subword models;
no clear correlation is found between performance
and vocabulary sizes in them.
For all the other four tasks, Morpheme-aware
Subword 64K models show the best scores. One
noteworthy phenomenon is that the scores tend to
increase as the vocabulary size grows across the
tokenization groups. This is discordant with the machine translation results in Section 5.1.4, where a
larger vocabulary size does not guarantee better performance for the Subword and Morpheme-aware
Subword models.
6 Discussion
We further examine which factors with respect to
tokenization affect the Ko-En and En-Ko translation performance.
6.1 Token Length
Because tokenization involves splitting a text into
shorter segments, we find it important to figure out
how much information each segment bears. To this
end, based on the assumption that the longer a text
is, the more information it is likely to have, we
plot the BLEU scores by the average number of
syllables per Korean token in the translation test
sets in Figure 1.
The BLEU scores of the subword models—
Syllable, Morpheme, Subword, and Morphemeaware Subword—are mostly higher than those of
the CV models, which are plotted as dotted lines. In
particular, the Syllable, Subword, and Morphemeaware Subword models between 1.00 and 1.50
show the best scores both in Ko-En and in En-Ko.
When a token has more than 1.5 syllables on average, the scores begin to decrease, and the Word
models which has more than 2.5 syllables in a token performs the worst (7.07 for Ko-En and 18.42
for En-Ko). Note that they are not in the figures
due to space constraints.
6.2 Linguistic Awareness
Obviously token length is not the only key factor in tokenization strategies. Let us compare
the Morpheme-aware Subword 16K models (green
markers) and Subword 8K models (red markers)
in the shaded regions in Figure 1. Although they
have the same average token length around 1.4,
the Morpheme-aware Subword models outperform
the Subword models. We believe this is evidence
to support that linguistic awareness is another important factor in Korean tokenization strategies for
machine translation.
6.3 Under-trained Tokens
In section 5.1.4, we pointed out high OOV rates
are highly likely to degrade the performance of
Morpheme models. It is also worth noting that in
Figure 1 as most of the orange markers denoting
Morpheme models are below the dotted lines.
OOVs are the tokens that appear only in the test
set. They are an extreme case of under-trained
tokens—test set’s tokens that appear in the training
set for the limited number of times. Figure 2 shows
how much under-trained tokens account for in each
model, ranging from n = 1 to n = 100, where n
is the frequency of the under-trained tokens in the
training set. Clearly, the curve of the Morpheme
32K model is far above that of the others, indicating
that it suffers from the problem of under-trained
tokens the most.
7 Conclusion
We explored various Korean tokenization strategies on machine translation and five NLU tasks.
In machine translation Morpheme-aware Subword
models with a vocabulary size worked best for both
Korean to English and English to Korean settings.
By contrast, there was no single best tokenization
strategy for the NLU tasks. Instead, Subword 64K
models showed the best performance on KorQuAD,
whereas Morpheme-aware Subword 64K models
turned out to be optimal for the other KorNLI, KorSTS, NSMC, and PAWS-X tasks.
