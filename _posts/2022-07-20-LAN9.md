---
title: "An Empirical Study of Tokenization Strategies for Various Korean NLP Tasks 정리"
date:   2022-07-20
excerpt: "An Empirical Study of Tokenization Strategies for Various Korean NLP Tasks"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---


원 논문: [An Empirical Study of Tokenization Strategies for Various Korean NLP Tasks](https://arxiv.org/pdf/2010.02534.pdf)


Abstract
Typically, tokenization is the very first step in
most text processing works. As a token serves
as an atomic unit that embeds the contextual information of text, how to define a token plays
a decisive role in the performance of a model.
Even though Byte Pair Encoding (BPE) has
been considered the de facto standard tokenization method due to its simplicity and universality, it still remains unclear whether BPE works
best across all languages and tasks. In this paper, we test several tokenization strategies in
order to answer our primary research question,
that is, “What is the best tokenization strategy
for Korean NLP tasks?”
Experimental results demonstrate that a hybrid
approach of morphological segmentation followed by BPE works best in Korean to/from
English machine translation and natural language understanding tasks such as KorNLI,
KorSTS, NSMC, and PAWS-X. As an exception, for KorQuAD, the Korean extension of
SQuAD, BPE segmentation turns out to be the
most effective.
Our code and pre-trained models are publicly available at https://github.com/
kakaobrain/kortok.
1 Introduction
Tokenization is the very first step in most text processing works. Not surprisingly, tremendous academic efforts have been made to find the best tokenization method for various NLP tasks. For the
past few years, Byte Pair Encoding (BPE) (Gage,
1994) has been considered the de facto standard
tokenization technique since it was reintroduced by
Sennrich et al. (2016a). Besides the fact that BPE
turns out to be very effective in the machine translation task, another important reason BPE has gained
