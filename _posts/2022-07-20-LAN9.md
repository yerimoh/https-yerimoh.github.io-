---
title: "An Empirical Study of Tokenization Strategies for Various Korean NLP Tasks 정리"
date:   2022-07-20
excerpt: "An Empirical Study of Tokenization Strategies for Various Korean NLP Tasks"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---


원 논문: [An Empirical Study of Tokenization Strategies for Various Korean NLP Tasks](https://arxiv.org/pdf/2010.02534.pdf)


코드:  [깃 ](https://github.com/kakaobrain/kortok)

# Abstract
일반적으로 토큰화는 대부분의 텍스트 처리 작업에서 첫 번째 단계이다.  
토큰은 텍스트의 컨텍스트 정보를 내장하는 atomic 단위 역할을 하므로 **토큰을 정의하는 방법은 모델의 성능에 결정적인 역할**을 한다.    

**[문제]**      
Byte Pair Encoding(BPE)은 단순성과 보편성 때문에 사실상의 표준 토큰화 방법으로 간주되어 왔지만, BPE가 모든 언어와 작업에서 가장 잘 작동하는지 여부는 여전히 불분명하다.      

**[본 논문]**      
본 논문에서는 주요 연구 질문인 "한국 NLP 작업에 가장 적합한 토큰화 전략은 무엇인가?"에 답하기 위해 몇 가지 토큰화 전략을 테스트한다.       
실험 결과는 BPE를 따르는 형태론적 분할의 하이브리드 접근 방식이 KorNLI, KorSTS, NSMC 및 PAWS-X와 같은 영어 기계 번역 및 자연어 이해 작업에서 가장 잘 작동한다는 것을 보여준다.     
KorQuAD의 경우, SQUAD의 한국어 확장이 가장 효과적인 것으로 나타났다.


-----
------


# 1 Introduction
토큰화는 대부분의 텍스트 처리 작업의 첫 번째 단계이다.     
다양한 NLP 작업에 대한 최상의 토큰화 방법을 찾기 위해 엄청난 학문적 노력이 이루어졌다.      

**[BPE]**     
* Sennrich et al.(2016a)에 의해 재도입된 이후 지난 몇 년간 BPE(Byte Pair Encoding) (Gage, 1994)는 사실상의 표준 토큰화 기법으로 간주되어 왔다.     
* BPE가 기계 번역 작업에서 매우 효과적인 것으로 판명     
* BPE가 **데이터 기반 통계 알고리즘**이기 때문에 **언어와 독립적**이라는 점이 BPE의 장점을 극대화.     


❌ 그러나 BPE가 **모든 언어에서 가장 잘 작동하는지 여부**는 여전히 명확하지 않다.     


**[본 논문]**    
* 본 논문에서 우리는 **형태학적으로** 영어보다 훨씬 **풍부**한 언어인 **한국어에 대한 다양한 토큰화 전략**을 연구한다.     
* 한국어 대 영어/영어 대 한국어 기계 번역 작업 및 자연어 이해(NLU) 작업(MRC), 자연어 추론(NLI), 의미론적 텍스트 유사성(STS), 감정 분석 및 paraphrase 식별에 가장 적합한 토큰화 전략이 무엇인지 경험적으로 검토한다.                 
* 특히 BPE와  linguistically motivated segmentation이 **얼마나 상호 보완적**인지에 관심을 둔다.    

---
----

# 2 Background

## 2.1 MeCab-ko: A Korean MorphologicalAnalyzer


**[MeCab(Kudo, 2006)]**        
* 의미: 조건부 무작위 필드(Conditional Random Fields; CRF)를 기반으로 하는 오픈 소스 형태학적 분석기이다.      
* 원래 일본어를 위해 설계됨 ➡ 일반용도 제공하므로 다른 언어에도 적용할 수 있다.     

**[MeCab-ko1]**         
* MeCab의 한국어 확장판          
* **확장 이유:** 형태나 구문 면에서 일본어와 한국어가 매우 유사          
* **train방법:** MeCab-ko는 MeCab을 사용하여  Sejong Corpus(Kang and Kim, 2001)를 이용하여 모델을 훈련시킴     
* **결과:** 2013년 출시된 이후 높은 정확도와 사용성이 좋아 많은 한국 NLP 작업에 널리 사용되고 있다.     
예를 들어, WAT(Workshop on Asian Translation)는 2015년부터 한국어 기계번역 결과를 평가하기 위한 공식 세분화 도구로 채택하고 있다(Nakazawa et al., 2015, 2016, 2017, 2018, 2019).     



---


## 2.2 Byte Pair Encoding
[BPE(Byte Pair Encoding)](https://wikidocs.net/22592)는 텍스트에서 가장 빈번한 바이트 쌍을 사용하지 않는 단일 바이트로 반복적으로 대체하는 간단한 데이터 압축 기법이다(Gage, 1994)     

Sennrich et al.(2016b)이 신경 기계 번역 모델에 성공적으로 적용한 이후, 그것은 언어 전반에 걸친 표준 토큰화 방법으로 간주되어 왔다. (한국어에서도 가장 지배적으로 사용된다)   





---
----

# 3 Related Work

**[tokenization strategies]**    
* 기계 번역을 위한 토큰화 기술에 대한 광범위한 연구가 있었다.    
* 여러 논문은 언어적으로 정보를 얻은 세분화와 BPE 또는 유니그램 언어 모델링과 같은 **데이터 중심 방법**의 혼합이 영어가 아닌 언어에 가장 잘 수행된다고 주장했다.       
* anerjee와 Bhattacharyya (2018): 힌디어와 벵골어 번역에서 기성 형태학적 세그먼트(segmenter)와 BPE를 영어에 대항하여 결합했다.       
* Tawfik et al. (2019): 아랍어에 대한 통계적 분할 방법과 함께 언어 동기 분할 모델의 재교육 버전을 사용했다.      
* Pinnis et al(2017): 영어-라트비아어 번역을 위해 BPE에 대한 언어 지침을 채택했다.    
* Park et al., (2019a): 우리와 가깝지만, 그들의 주된 초점은 신경 기계의 전처리 기술에 있다.     


**[NLU]**     
* 토큰화 전략 자체보다는 parallel corpus filtering과 같은 번역. 기계 번역에 대한 토큰화 연구와 비교하여 **NLU 작업에 대한 연구는 덜 주목을 받았다**.          
* Bostrom과 Durrett(2020): BPE와 유니그램 언어 모델링으로 사전 훈련된 BERT(Devlin et al., 2019)의 미세 조정 작업 성능을 비교          
* Moon과 Okazaki(2020): 한국어를 위한 새로운 인코딩 방법을 제안하고 몇 개의 한국어 NLU 데이터 세트를 사용하여 어휘 압축에서 그 효율성을 보여주었다.     


---
---

# 4 Tokenization Strategies
본 논문은 가장 작은 단위에서 가장 큰 단위로 배열된 다양한 한국 토큰화 전략을 소개한다.     
표 1에 나타난 바와 같이 각각 다른 토큰화 결과를 유도한다.    
![image](https://user-images.githubusercontent.com/76824611/188309388-9582b651-3b7b-4811-9a6e-ff3c61e7c59b.png)

-----

## 4.1 Consonant and Vowel (CV)
한글에서는 한글의 표준 문자 체계인 자음과 모음, 즉 라틴 문자에 해당하는 **자모(Jamo)가 조립**되어 음절 문자를 형성한다.     

예를 들어, 한글 자음 ```ㅎ/h/(U+314E)```는 모음 ```ㅏ/a/(U+314F)```와 결합하여 음절 문자  하/ha/(U+558)를 만든다.    

다른 예를 들어 설명하면,      
이런 메커니즘에 익숙하지 않은 독자들은 자모와 음절을 각각 원자와 분자로 생각할 수 있다.      
분자 $$H_2O$$는 두 개의 H 원자와 O 원자로 분해될 수 있기 때문에, 음절 하/ha/는 구성 자음 ```ㅎ/h/```와 모음 ```ㅏ/a/```로 분해될 수 있다.      

![image](https://user-images.githubusercontent.com/76824611/188309867-0d3fcf99-c2f5-4bc3-b959-f0b32a139a39.png)

-----

## 4.2 Syllable
Syllable은 음절로,       
우리는 음절 수준에서 문장을 토큰화할 수 있다.       
공백은 특수 기호로 대체된다.


---

## 4.3 Morpheme
Morpheme는 형태소이다.     

**[문제]**    
MeCab-ko는 명령행 인터페이스3에서 편리한 토큰화 옵션을 제공한다.      
예를 들어, ```AB C```가 형태소를 나타내는 입력 텍스트 ABC를 A, B, C로 반환한다.      
출력 토큰 목록에서 ```AB```와 ```C```사이의 원래 공간(띄어쓰기)이 누락됨 ➡ 따라서 토큰화된 결과에서 원본 텍스트를 복구할 수 없다.     
이는 목표 언어가 한국어인 기계번역이나 주어진 텍스트의 특정 문구를 답으로 제안할 것으로 예상되는 기계독해와 같이 입력 텍스트를 복원해야 하는 일부 작업에서 문제가 될 수 있다.    


**[해결]**      
이러한 이유로, 우리는 원래의 공백 위치에 특수 토큰 ```* (U+2B51)```을 삽입한다.     
결과: 토큰화된 시퀀스는 ```A/B/*/C```

----

## 4.4 Subword
SentencePiece(Kudo and Richardson, 2018) 라이브러리를 사용하여 BPE를 배우고 적용한다.     
모든 단어에 ```_```(U+2581)을 추가하여 **원래 공백을 표시**한 다음,      
텍스트를 subword pieces로 토큰화한다.     

ex)      
```나랑 쇼핑하자``` can be split into ```_나랑/_쇼/핑하/자/.```  



---


## 4.5 Morpheme-aware Subword
데이터 방법 및 언어 중심 접근 방식(Banerjee 및 Bhattacharyya, 2018; Park et al., 2019a; Pinnis et al., 2017; Tawfik et al., 2019)의 결합된 방법을 토대로 만들어짐.      

형태소 인식 하위 단어를 만들기 위해 MeCabko와 BPE를 순차적으로 적용한다.    

**[장점]**    
이 전략에 따르면, BPE는 원본 텍스트를 형태소로 분할한 후에 적용되므로,    
여러 형태소에 걸친 토큰(예: 섹션 4.4의 ```/핑하/```)이 생성되지 않는다.      

대신, BPE 알고리즘은 형태소를 빈번한 조각으로 더 세분화한다.



ex)      
```나랑 쇼핑하자``` can be split into ```_나/_랑/*/_쇼/핑/_하/_자/ .```  

---

## 4.6 Word
우리는 단순히 텍스트를 공백으로 나눌 수 있다.       
문장 부호는 별도의 토큰으로 분할된다.     


ex)    
```나랑 쇼핑하자``` is tokenized into ```나랑/쇼핑하자/.```

[T@] 표 2: BPE 교육 데이터가 다른 한국어 대 영어(Ko-En) 및 영어 대 한국어(En-Ko) 번역 모델의 BLEU 점수 영어 문장은 영어 위키에서 훈련된 32K BPE 모델을 사용하여 토큰화된다는 점에 유의하십시오.




----
----

# 5 Experiments
## 5.1 Korean to/from English Machine Translation
### 5.1.1 Dataset
현재까지 한국어가 WMT4나 IWSLT5의 언어 목록에 없다.     
또한 한-영 기계 번역에 대한 오픈 소스 벤치마크 데이터 세트가 거의 없었다.     

그래서 Park et al.(2019a)는 65개의 다른 언어에 걸친 크라우드 소싱 영화 자막 모음인 OpenSubtitles(Lison and Tiemann, 2016)를 사용했다.       
➡ 그러나 이는 번역 벤치마크 데이터셋 역할을 하기에는 너무 o noisy하다.    


이에 최근 뉴스, 정부 웹사이트, 법률 문서 등 다양한 출처에서 수집한 한-영 병렬 말뭉치가 [AI Hub](
7http://www.aihub.or.kr/aidata/87)에 의해 공개적으로 공개되었다.     
우리는 80만 개의 문장 쌍에 달하는 뉴스 데이터를 784K(train), 8K(dev), 8K(test)로 무작위로 나눈다.



### 5.1.2 BPE Modeling
trian에 앞서 간단한 예비 실험을 수행하여 BPE 학습에 사용할 데이터 세트를 결정한다.     

데이터 세트는 2가지 종류가 있다.     
* AI Hub training data     
   * 장점: 어휘 분포가 테스트 데이터와 비슷해 최적화할 수 있다.     
   * 단점: 크기가 상대적으로 작음(130MB)  
* Wiki      
   * 크기가 큼       
   * 뉴스 그 자체는 아님 ➡ BPE 모델링을 위한 AI 허브 데이터만큼 적절하지 않을 수 있다.       

**[두 데이터세트 비교]**        
1) AI 허브 훈련 데이터의 한국어 문장과 함께 문장 피스를 사용하여 32K 한국어 BPE 모델(A)을 훈련시킨다.     
2) 최신 위키백과 Wikipedia [Korean](8https://dumps.wikimedia.org/kowiki)/[English](https://dumps.wikimedia.org/enwiki)를 다운로드하고 Wiki Extractor 10을 사용하여 일반 텍스트를 추출한다.        
3) 한국어(B)와 영어(C)를 위한 32K BPE 모델을 만든다.     
4) 우리는 두 가지 다른 한국 BPE 모델(A, B)을 사용하여 AI 허브 훈련 데이터에 대한 한국어 대 영어(Ko-En) 및 영어 대 한국어(En-Ko) 번역 모델을 훈련시킨다.(훈련 세부사항은 섹션 5.1.3에 설명되어 있음)           
5) 비교를 위해 두 가지 모두에 동일한 영어 BPE 모델(C)을 사용함       



**[결과]**    
![image](https://user-images.githubusercontent.com/76824611/188315437-ab34451b-6e5d-4994-8de3-d0faeb07c8ad.png)    
* Ko-En 번역의 경우 Wiki 기반 BPE 모델은 개발 및 테스트 세트 모두에서 2-3점 더 우수한 성능을 발휘함    
* En-Ko 번역의 경우, Wiki와 AI Hub 기반 모델 간에 실질적인 성능 차이가 없음          
* 또한 BPE 모델이 기계 번역뿐만 아니라 NLU 작업에도 사용된다는 점을 고려할 가치가 있다.      
* 모든 것을 종합하면, 우리는 Wiki-based BPE model을 선택한다



### 5.1.3 Training
We test the tokenization strategies in Section 4
with various vocabulary sizes on the AI Hub news
dataset.
We use the Transformer (Vaswani et al., 2017),
the state-of-the-art model for neural machine translation. We mostly follow the base model configuration: 6 blocks of 512-2048 units with 8 attention heads. We run all of our experiments using
FAIRSEQ 11 (Ott et al., 2019), a PyTorch based
deep learning library for sequence to sequence models.
Each model is trained using a Tesla V100 GPU
with batch size 128, dropout rate 0.3, label smoothing 0.1, and the Adam (Kingma and Ba, 2015)
optimizer. We set the learning rate to 5e-4 with the
inverse square-root schedule. We train all models
for 50 epochs and save the checkpoint files at every
epoch.
5.1.4 Results
After all training stages are finished, we evaluate the saved checkpoint files of each model on
the dev set to find the best one, which is subsequently used for the final test. In Table 3 we report
BLEU scores on both the dev and test sets using the
Moses12 multi-bleu.perl script. Following
WAT 2019 (Nakazawa et al., 2019), Moses tokenizer and MeCab-ko are used for tokenizing the
evaluation data.
For both Ko-En and En-Ko, overall, the Subword models (35.64-39.22) and the Syllable models
(38.45-39.30) are superior to the Morpheme models (31.59-37.37) or the Word models (7.04-18.42)
in performance. It is highly likely to come from
the lower OOV rates of the Subword models (0.07-
0.12) and the Syllable models (0.06) compared to
those of the Morpheme models (1.40-7.51) and the
Word models (26.20). While BPE tends to split rare
words into subword pieces, MeCab-ko is ignorant
of statistics so it splits words into morphemes by
linguistic knowledge instead. That the Morpheme
and Word models generate many OOVs suggests
Korean has so large types of morphemes or word
forms that even 64K vocabulary is not enough to
cover them all.
CV models are tiny in vocabulary size (166) so
they show the lowest OOV rate (0.02). However,
their performance is not as good as the Syllable
or Subword models. We speculate this is because
a single consonant or vowel must bear too much
contextual information in the CV models.
Morpheme-aware Subword 32K models achieve
the best BLEU scores. Each Subword model, as
shown in Table 4, contains 6-37% of tokens spanning morpheme boundaries in the test set, which
implies that subword segmentation by BPE is not
optimal and morpheme boundaries are meaningful
in tokenization.
To sum up, morpheme-aware subword tokenization that makes the best use of linguistic knowledge
and statistical information is the best for Korean
machine translation.
5.2 Korean Natural Language
Understanding Tasks
Large pre-trained language models have proven
their effectiveness in many downstream tasks (Peters et al., 2018; Devlin et al., 2019; Liu et al.,
2019). We pre-train BERT (Devlin et al., 2019)
models with various tokenization strategies, and
fine-tune them on five different Korean NLU tasks.
5.2.1 Machine Reading Comprehension:
KorQuAD 1.0 Dataset
The KorQuAD 1.0 dataset (Lim et al., 2019) is a
Korean adaptation of SQuAD 1.0 (Rajpurkar et al.,
2016), a popular reading comprehension dataset.
KorQuAD 1.0 consists of 10,645 passages and
their paired 66,181 questions (60,407 for training
+ 5,774 for development13). Like SQuAD 1.0, KorQuAD 1.0 involves answering a question given a
passage. The answer must be a phrase within the
passage.
5.2.2 Natural Language Inference: KorNLI
Dataset
The KorNLI Dataset (Ham et al., 2020) is a Korean NLI dataset sourced from three different NLI
datasets: SNLI (Bowman et al., 2015), MNLI
(Williams et al., 2018), and XNLI (Conneau et al.,
2018).
It is composed of 950,354 sentence pairs:
942,854 for training, 2,490 for development, and
5,010 for test. A model receives a pair of
sentences—a premise and a hypothesis—and classifies their relationship into one out of three categories: entailment, contradiction, and neutral.
5.2.3 Semantic Textual Similarity: KorSTS
Dataset
The KorSTS Dataset (Ham et al., 2020) is a Korean STS dataset translated from the STS-B dataset
(Cer et al., 2017). It comprises 8,628 sentence
pairs—5,749 for training, 1,500 for development,
and 1,379 for test. The task assesses the gradations
of semantic similarity between two sentences with
a scale from 0 to 5.
5.2.4 Sentiment Analysis: NSMC Dataset
NSMC14 is a movie review dataset scraped from
Naver Movies™. It consists of 200K samples of
which 150K are the training set and the rest 50K
are the test set. Each sample is labeled with 0
(negative) or 1 (positive). We hold out 10 percent
of the training data for development.
5.2.5 Paraphrase Identification: PAWS-X
Dataset
The PAWS-X dataset (Yang et al., 2019) is a challenging paraphrase identification dataset in six languages including Korean. The Korean portion
amounts to 53,338 sentence pairs (49,410 for training, 1,965 for development, and 1,972 for test).
Like the NSMC dataset, each sentence pair is annotated with either 0 (negative) or 1 (positive).
For each tokenization strategy, we pre-train a
BERT-Base model on a large corpus and fine-tune
it on the training sets of the five NLU tasks independently.
Pre-training. Because the Korean Wiki corpus
is not enough in volume, 640 MB, for the pre
training purpose, we additionally download the
recent dump of Namuwiki15, a Korean Wiki, and
extract plain texts using Namu Wiki Extractor16
.
On the resulting Namuwiki corpus (5.5 GB) along
with the Wiki corpus (640 MB), pre-training is
performed with a Cloud TPU v3-8 for 1M steps
using the official BERT training code17, which is
based on TensorFlow. We set the training hyperparameters of all models as follows: batch size
= 1024, max sequence length = 128, optimizer =
AdamW (Loshchilov and Hutter, 2019), learning
rate = 5e-5, warm-up steps = 10K.
Fine-tuning. After converting each of the pretrained models in TensorFlow into PyTorch, we
fine-tune it using HuggingFace Transformers18
(Wolf et al., 2019). The hyper-parameters for each
task are shown in Table 5.
5.2.6 Results
In Table 6 we report the evaluation results of the
various models on the dev and test sets. Since
KorQuAD lacks the test set, we report the results
on the dev set only.
As for KorQuAD, Subword 64K models achieve
the highest Exact Match (EM) and F1 scores. The
scores in the Subword and Morpheme models increase monotonically as the vocabulary size grows.
On the other hand, the 32K models outperform the
others in the Morpheme-aware Subword models;
no clear correlation is found between performance
and vocabulary sizes in them.
For all the other four tasks, Morpheme-aware
Subword 64K models show the best scores. One
noteworthy phenomenon is that the scores tend to
increase as the vocabulary size grows across the
tokenization groups. This is discordant with the machine translation results in Section 5.1.4, where a
larger vocabulary size does not guarantee better performance for the Subword and Morpheme-aware
Subword models.
6 Discussion
We further examine which factors with respect to
tokenization affect the Ko-En and En-Ko translation performance.
6.1 Token Length
Because tokenization involves splitting a text into
shorter segments, we find it important to figure out
how much information each segment bears. To this
end, based on the assumption that the longer a text
is, the more information it is likely to have, we
plot the BLEU scores by the average number of
syllables per Korean token in the translation test
sets in Figure 1.
The BLEU scores of the subword models—
Syllable, Morpheme, Subword, and Morphemeaware Subword—are mostly higher than those of
the CV models, which are plotted as dotted lines. In
particular, the Syllable, Subword, and Morphemeaware Subword models between 1.00 and 1.50
show the best scores both in Ko-En and in En-Ko.
When a token has more than 1.5 syllables on average, the scores begin to decrease, and the Word
models which has more than 2.5 syllables in a token performs the worst (7.07 for Ko-En and 18.42
for En-Ko). Note that they are not in the figures
due to space constraints.
6.2 Linguistic Awareness
Obviously token length is not the only key factor in tokenization strategies. Let us compare
the Morpheme-aware Subword 16K models (green
markers) and Subword 8K models (red markers)
in the shaded regions in Figure 1. Although they
have the same average token length around 1.4,
the Morpheme-aware Subword models outperform
the Subword models. We believe this is evidence
to support that linguistic awareness is another important factor in Korean tokenization strategies for
machine translation.
6.3 Under-trained Tokens
In section 5.1.4, we pointed out high OOV rates
are highly likely to degrade the performance of
Morpheme models. It is also worth noting that in
Figure 1 as most of the orange markers denoting
Morpheme models are below the dotted lines.
OOVs are the tokens that appear only in the test
set. They are an extreme case of under-trained
tokens—test set’s tokens that appear in the training
set for the limited number of times. Figure 2 shows
how much under-trained tokens account for in each
model, ranging from n = 1 to n = 100, where n
is the frequency of the under-trained tokens in the
training set. Clearly, the curve of the Morpheme
32K model is far above that of the others, indicating
that it suffers from the problem of under-trained
tokens the most.
7 Conclusion
We explored various Korean tokenization strategies on machine translation and five NLU tasks.
In machine translation Morpheme-aware Subword
models with a vocabulary size worked best for both
Korean to English and English to Korean settings.
By contrast, there was no single best tokenization
strategy for the NLU tasks. Instead, Subword 64K
models showed the best performance on KorQuAD,
whereas Morpheme-aware Subword 64K models
turned out to be optimal for the other KorNLI, KorSTS, NSMC, and PAWS-X tasks.
