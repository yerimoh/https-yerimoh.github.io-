---
title: "An Empirical Study of Tokenization Strategies for Various Korean NLP Tasks 정리"
date:   2022-07-20
excerpt: "An Empirical Study of Tokenization Strategies for Various Korean NLP Tasks"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---


# 목차
  * [comments: true](#comments--true)
- [Abstract](#abstract)
- [1 Introduction](#1-introduction)
- [2 Background](#2-background)
  * [2.1 MeCab-ko: A Korean MorphologicalAnalyzer](#21-mecab-ko--a-korean-morphologicalanalyzer)
  * [2.2 Byte Pair Encoding](#22-byte-pair-encoding)
- [3 Related Work](#3-related-work)
- [4 Tokenization Strategies](#4-tokenization-strategies)
  * [4.1 Consonant and Vowel (CV)](#41-consonant-and-vowel--cv-)
  * [4.2 Syllable](#42-syllable)
  * [4.3 Morpheme](#43-morpheme)
  * [4.4 Subword](#44-subword)
  * [4.5 Morpheme-aware Subword](#45-morpheme-aware-subword)
  * [4.6 Word](#46-word)
- [5 Experiments](#5-experiments)
  * [5.1 Korean to/from English Machine Translation](#51-korean-to-from-english-machine-translation)
    + [5.1.1 Dataset](#511-dataset)
    + [5.1.2 BPE Modeling](#512-bpe-modeling)
    + [5.1.3 Training](#513-training)
    + [5.1.4 Results](#514-results)
  * [5.2 Korean Natural Language Understanding Tasks](#52-korean-natural-language-understanding-tasks)
    + [5.2.1 Machine Reading Comprehension: KorQuAD 1.0 Dataset](#521-machine-reading-comprehension--korquad-10-dataset)
    + [5.2.2 Natural Language Inference: KorNLI Dataset](#522-natural-language-inference--kornli-dataset)
    + [5.2.3 Semantic Textual Similarity: KorSTS Dataset](#523-semantic-textual-similarity--korsts-dataset)
    + [5.2.4 Sentiment Analysis: NSMC Dataset](#524-sentiment-analysis--nsmc-dataset)
    + [5.2.5 Paraphrase Identification: PAWS-X Dataset](#525-paraphrase-identification--paws-x-dataset)
    + [5.2.6 Results](#526-results)
- [6 Discussion](#6-discussion)
  * [6.1 Token Length](#61-token-length)
  * [6.2 Linguistic Awareness](#62-linguistic-awareness)
  * [6.3 Under-trained Tokens](#63-under-trained-tokens)
- [7 Conclusion](#7-conclusion)



---

원 논문: [An Empirical Study of Tokenization Strategies for Various Korean NLP Tasks](https://arxiv.org/pdf/2010.02534.pdf)


코드:  [깃](https://github.com/kakaobrain/kortok)



----


# Abstract
일반적으로 토큰화는 대부분의 텍스트 처리 작업에서 첫 번째 단계이다.  
토큰은 텍스트의 컨텍스트 정보를 내장하는 atomic 단위 역할을 하므로 **토큰을 정의하는 방법은 모델의 성능에 결정적인 역할**을 한다.    

**[문제]**      
Byte Pair Encoding(BPE)은 단순성과 보편성 때문에 사실상의 표준 토큰화 방법으로 간주되어 왔지만, BPE가 모든 언어와 작업에서 가장 잘 작동하는지 여부는 여전히 불분명하다.      

**[본 논문]**      
본 논문에서는 주요 연구 질문인 "한국 NLP 작업에 가장 적합한 토큰화 전략은 무엇인가?"에 답하기 위해 몇 가지 토큰화 전략을 테스트한다.       
실험 결과는 BPE를 따르는 형태론적 분할의 하이브리드 접근 방식이 KorNLI, KorSTS, NSMC 및 PAWS-X와 같은 영어 기계 번역 및 자연어 이해 작업에서 가장 잘 작동한다는 것을 보여준다.     
KorQuAD의 경우, SQUAD의 한국어 확장이 가장 효과적인 것으로 나타났다.


-----
------


# 1 Introduction
토큰화는 대부분의 텍스트 처리 작업의 첫 번째 단계이다.     
다양한 NLP 작업에 대한 최상의 토큰화 방법을 찾기 위해 엄청난 학문적 노력이 이루어졌다.      

**[BPE]**     
* Sennrich et al.(2016a)에 의해 재도입된 이후 지난 몇 년간 BPE(Byte Pair Encoding) (Gage, 1994)는 사실상의 표준 토큰화 기법으로 간주되어 왔다.     
* BPE가 기계 번역 작업에서 매우 효과적인 것으로 판명     
* BPE가 **데이터 기반 통계 알고리즘**이기 때문에 **언어와 독립적**이라는 점이 BPE의 장점을 극대화.     


❌ 그러나 BPE가 **모든 언어에서 가장 잘 작동하는지 여부**는 여전히 명확하지 않다.     


**[본 논문]**    
* 본 논문에서 우리는 **형태학적으로** 영어보다 훨씬 **풍부**한 언어인 **한국어에 대한 다양한 토큰화 전략**을 연구한다.     
* 한국어 대 영어/영어 대 한국어 기계 번역 작업 및 자연어 이해(NLU) 작업(MRC), 자연어 추론(NLI), 의미론적 텍스트 유사성(STS), 감정 분석 및 paraphrase 식별에 가장 적합한 토큰화 전략이 무엇인지 경험적으로 검토한다.                 
* 특히 BPE와  linguistically motivated segmentation이 **얼마나 상호 보완적**인지에 관심을 둔다.    

---
----

# 2 Background

## 2.1 MeCab-ko: A Korean MorphologicalAnalyzer


**[MeCab(Kudo, 2006)]**        
* 의미: 조건부 무작위 필드(Conditional Random Fields; CRF)를 기반으로 하는 오픈 소스 형태학적 분석기이다.      
* 원래 일본어를 위해 설계됨 ➡ 일반용도 제공하므로 다른 언어에도 적용할 수 있다.     

**[MeCab-ko1]**         
* MeCab의 한국어 확장판          
* **확장 이유:** 형태나 구문 면에서 일본어와 한국어가 매우 유사          
* **train방법:** MeCab-ko는 MeCab을 사용하여  Sejong Corpus(Kang and Kim, 2001)를 이용하여 모델을 훈련시킴     
* **결과:** 2013년 출시된 이후 높은 정확도와 사용성이 좋아 많은 한국 NLP 작업에 널리 사용되고 있다.     
예를 들어, WAT(Workshop on Asian Translation)는 2015년부터 한국어 기계번역 결과를 평가하기 위한 공식 세분화 도구로 채택하고 있다(Nakazawa et al., 2015, 2016, 2017, 2018, 2019).     



---


## 2.2 Byte Pair Encoding
[BPE(Byte Pair Encoding)](https://wikidocs.net/22592)는 텍스트에서 가장 빈번한 바이트 쌍을 사용하지 않는 단일 바이트로 반복적으로 대체하는 간단한 데이터 압축 기법이다(Gage, 1994)     

Sennrich et al.(2016b)이 신경 기계 번역 모델에 성공적으로 적용한 이후, 그것은 언어 전반에 걸친 표준 토큰화 방법으로 간주되어 왔다. (한국어에서도 가장 지배적으로 사용된다)   





---
----

# 3 Related Work

**[tokenization strategies]**    
* 기계 번역을 위한 토큰화 기술에 대한 광범위한 연구가 있었다.    
* 여러 논문은 언어적으로 정보를 얻은 세분화와 BPE 또는 유니그램 언어 모델링과 같은 **데이터 중심 방법**의 혼합이 영어가 아닌 언어에 가장 잘 수행된다고 주장했다.       
* anerjee와 Bhattacharyya (2018): 힌디어와 벵골어 번역에서 기성 형태학적 세그먼트(segmenter)와 BPE를 영어에 대항하여 결합했다.       
* Tawfik et al. (2019): 아랍어에 대한 통계적 분할 방법과 함께 언어 동기 분할 모델의 재교육 버전을 사용했다.      
* Pinnis et al(2017): 영어-라트비아어 번역을 위해 BPE에 대한 언어 지침을 채택했다.    
* Park et al., (2019a): 우리와 가깝지만, 그들의 주된 초점은 신경 기계의 전처리 기술에 있다.     


**[NLU]**     
* 토큰화 전략 자체보다는 parallel corpus filtering과 같은 번역. 기계 번역에 대한 토큰화 연구와 비교하여 **NLU 작업에 대한 연구는 덜 주목을 받았다**.          
* Bostrom과 Durrett(2020): BPE와 유니그램 언어 모델링으로 사전 훈련된 BERT(Devlin et al., 2019)의 미세 조정 작업 성능을 비교          
* Moon과 Okazaki(2020): 한국어를 위한 새로운 인코딩 방법을 제안하고 몇 개의 한국어 NLU 데이터 세트를 사용하여 어휘 압축에서 그 효율성을 보여주었다.     


---
---

# 4 Tokenization Strategies
본 논문은 가장 작은 단위에서 가장 큰 단위로 배열된 다양한 한국 토큰화 전략을 소개한다.     
표 1에 나타난 바와 같이 각각 다른 토큰화 결과를 유도한다.    
![image](https://user-images.githubusercontent.com/76824611/188309388-9582b651-3b7b-4811-9a6e-ff3c61e7c59b.png)

-----

## 4.1 Consonant and Vowel (CV)
한글에서는 한글의 표준 문자 체계인 자음과 모음, 즉 라틴 문자에 해당하는 **자모(Jamo)가 조립**되어 음절 문자를 형성한다.     

예를 들어, 한글 자음 ```ㅎ/h/(U+314E)```는 모음 ```ㅏ/a/(U+314F)```와 결합하여 음절 문자  하/ha/(U+558)를 만든다.    

다른 예를 들어 설명하면,      
이런 메커니즘에 익숙하지 않은 독자들은 자모와 음절을 각각 원자와 분자로 생각할 수 있다.      
분자 $$H_2O$$는 두 개의 H 원자와 O 원자로 분해될 수 있기 때문에, 음절 하/ha/는 구성 자음 ```ㅎ/h/```와 모음 ```ㅏ/a/```로 분해될 수 있다.      

![image](https://user-images.githubusercontent.com/76824611/188309867-0d3fcf99-c2f5-4bc3-b959-f0b32a139a39.png)

-----

## 4.2 Syllable
Syllable은 음절로,       
우리는 음절 수준에서 문장을 토큰화할 수 있다.       
공백은 특수 기호로 대체된다.


---

## 4.3 Morpheme
Morpheme는 형태소이다.     

**[문제]**    
MeCab-ko는 명령행 인터페이스3에서 편리한 토큰화 옵션을 제공한다.      
예를 들어, ```AB C```가 형태소를 나타내는 입력 텍스트 ABC를 A, B, C로 반환한다.      
출력 토큰 목록에서 ```AB```와 ```C```사이의 원래 공간(띄어쓰기)이 누락됨 ➡ 따라서 토큰화된 결과에서 원본 텍스트를 복구할 수 없다.     
이는 목표 언어가 한국어인 기계번역이나 주어진 텍스트의 특정 문구를 답으로 제안할 것으로 예상되는 기계독해와 같이 입력 텍스트를 복원해야 하는 일부 작업에서 문제가 될 수 있다.    


**[해결]**      
이러한 이유로, 우리는 원래의 공백 위치에 특수 토큰 ```* (U+2B51)```을 삽입한다.     
결과: 토큰화된 시퀀스는 ```A/B/*/C```

----

## 4.4 Subword
SentencePiece(Kudo and Richardson, 2018) 라이브러리를 사용하여 BPE를 배우고 적용한다.     
모든 단어에 ```_```(U+2581)을 추가하여 **원래 공백을 표시**한 다음,      
텍스트를 subword pieces로 토큰화한다.     

ex)      
```나랑 쇼핑하자``` can be split into ```_나랑/_쇼/핑하/자/.```  



---


## 4.5 Morpheme-aware Subword
데이터 방법 및 언어 중심 접근 방식(Banerjee 및 Bhattacharyya, 2018; Park et al., 2019a; Pinnis et al., 2017; Tawfik et al., 2019)의 결합된 방법을 토대로 만들어짐.      

형태소 인식 하위 단어를 만들기 위해 MeCabko와 BPE를 순차적으로 적용한다.    

**[장점]**    
이 전략에 따르면, BPE는 원본 텍스트를 형태소로 분할한 후에 적용되므로,    
여러 형태소에 걸친 토큰(예: 섹션 4.4의 ```/핑하/```)이 생성되지 않는다.      

대신, BPE 알고리즘은 형태소를 빈번한 조각으로 더 세분화한다.



ex)      
```나랑 쇼핑하자``` can be split into ```_나/_랑/*/_쇼/핑/_하/_자/ .```  

---

## 4.6 Word
우리는 단순히 텍스트를 공백으로 나눌 수 있다.       
문장 부호는 별도의 토큰으로 분할된다.     


ex)    
```나랑 쇼핑하자``` is tokenized into ```나랑/쇼핑하자/.```

[T@] 표 2: BPE 교육 데이터가 다른 한국어 대 영어(Ko-En) 및 영어 대 한국어(En-Ko) 번역 모델의 BLEU 점수 영어 문장은 영어 위키에서 훈련된 32K BPE 모델을 사용하여 토큰화된다는 점에 유의하십시오.




----
----

# 5 Experiments
## 5.1 Korean to/from English Machine Translation
### 5.1.1 Dataset
현재까지 한국어가 WMT4나 IWSLT5의 언어 목록에 없다.     
또한 한-영 기계 번역에 대한 오픈 소스 벤치마크 데이터 세트가 거의 없었다.     

그래서 Park et al.(2019a)는 65개의 다른 언어에 걸친 크라우드 소싱 영화 자막 모음인 OpenSubtitles(Lison and Tiemann, 2016)를 사용했다.       
➡ 그러나 이는 번역 벤치마크 데이터셋 역할을 하기에는 너무 o noisy하다.    


이에 최근 뉴스, 정부 웹사이트, 법률 문서 등 다양한 출처에서 수집한 한-영 병렬 말뭉치가 [AI Hub](
7http://www.aihub.or.kr/aidata/87)에 의해 공개적으로 공개되었다.     
우리는 80만 개의 문장 쌍에 달하는 뉴스 데이터를 784K(train), 8K(dev), 8K(test)로 무작위로 나눈다.



### 5.1.2 BPE Modeling
trian에 앞서 간단한 예비 실험을 수행하여 BPE 학습에 사용할 데이터 세트를 결정한다.     

데이터 세트는 2가지 종류가 있다.     
* AI Hub training data     
   * 장점: 어휘 분포가 테스트 데이터와 비슷해 최적화할 수 있다.     
   * 단점: 크기가 상대적으로 작음(130MB)  
* Wiki      
   * 크기가 큼       
   * 뉴스 그 자체는 아님 ➡ BPE 모델링을 위한 AI 허브 데이터만큼 적절하지 않을 수 있다.       

**[두 데이터세트 비교]**        
1) AI 허브 훈련 데이터의 한국어 문장과 함께 문장 피스를 사용하여 32K 한국어 BPE 모델(A)을 훈련시킨다.     
2) 최신 위키백과 Wikipedia [Korean](8https://dumps.wikimedia.org/kowiki)/[English](https://dumps.wikimedia.org/enwiki)를 다운로드하고 Wiki Extractor 10을 사용하여 일반 텍스트를 추출한다.        
3) 한국어(B)와 영어(C)를 위한 32K BPE 모델을 만든다.     
4) 우리는 두 가지 다른 한국 BPE 모델(A, B)을 사용하여 AI 허브 훈련 데이터에 대한 한국어 대 영어(Ko-En) 및 영어 대 한국어(En-Ko) 번역 모델을 훈련시킨다.(훈련 세부사항은 섹션 5.1.3에 설명되어 있음)           
5) 비교를 위해 두 가지 모두에 동일한 영어 BPE 모델(C)을 사용함       



**[결과]**    
![image](https://user-images.githubusercontent.com/76824611/188315437-ab34451b-6e5d-4994-8de3-d0faeb07c8ad.png)    
* Ko-En 번역의 경우 Wiki 기반 BPE 모델은 개발 및 테스트 세트 모두에서 2-3점 더 우수한 성능을 발휘함    
* En-Ko 번역의 경우, Wiki와 AI Hub 기반 모델 간에 실질적인 성능 차이가 없음          
* 또한 BPE 모델이 기계 번역뿐만 아니라 NLU 작업에도 사용된다는 점을 고려할 가치가 있다.      
* 모든 것을 종합하면, 우리는 Wiki-based BPE model을 선택한다



### 5.1.3 Training
우리는 AI Hub news dataset에서 다양한 어휘 크기를 사용하여 Section 4의 토큰화 전략을 테스트한다.      
**실험모델:** neural machine 번역을 위해 Transformer(논문과 같은 구성(base model)을 사용)         




### 5.1.4 Results
모든 trian 단계가 완료된 후,     
우리는 dev set에 있는 각 모델의 저장된 체크포인트 파일을 평가하여 **최적의 체크포인트 파일**을 찾고, 이후 최종 test에 사용한다.     

**[BLEU score]**
![image](https://user-images.githubusercontent.com/76824611/188316058-32c8c251-6587-44a8-be45-56a5ef5cc52e.png)

**[결과 해석]**
언어 지식과 통계 정보를 최대한 활용하는 morpheme-aware subword tokenization이 한국어 기계번역에는 최고다.

---

## 5.2 Korean Natural Language Understanding Tasks
대규모  pre-trained 언어 모델은 많은 downstream tasks에서 그 효과를 입증했다(Peters et al., 2018; Devlin et al., 2019; Liu et al., 2019).    
다양한 토큰화 전략으로 BERT(Devlin et al., 2019) 모델을 pre-train하고 5가지 다른 한국 NLU 작업에서 미세 조정한다.      






### 5.2.1 Machine Reading Comprehension: KorQuAD 1.0 Dataset
The KorQuAD 1.0 dataset (Lim et al., 2019) 는 SQuAD 1.0 (Rajpurkar et al.,2016), 2016년 인기 판독 데이터 세트이다.      
KorQuAD 1.0은 10,645개의 구절과 66,181개의 쌍으로 구성된 질문(훈련용 60,407개)으로 구성된다.        
답은 phrase within the passage이어야 한다.


### 5.2.2 Natural Language Inference: KorNLI Dataset
KorNLI Dataset(Ham et al., 2020)는 SNLI(Bowman et al., 2015), MNLI(Williams et al., 2018) 및 XNLI(Conneau et al., 2018)의 세 가지 다른 NLI 데이터 세트에서 소스된 한국의 NLI 데이터 세트이다.      
모델은 한 쌍의 문장(전제와 가설)을 받고 그들의 관계를 세 가지 범주 중 하나로 분류한다: entailment, contradiction, and neutral.   


### 5.2.3 Semantic Textual Similarity: KorSTS Dataset   
STS-B 데이터 세트를 번역한 한국의 STS 데이터 세트이다(Cer et al., 2017).     
train용 5,749문장, dev용 1,500문장, test용 1,379문장 등 8,628문장으로 구성되어 있다.     
이 작업은 0에서 5까지의 척도로 두 문장 사이의 의미적 유사성의 기울기를 평가한다.    





### 5.2.4 Sentiment Analysis: NSMC Dataset
NSMC14는 네이버 Movies™에서 스크랩한 영화 리뷰 데이터 세트임.     
이 샘플은 200K개의 샘플로 구성되며, 그 중 150K는 교육 세트이고 나머지 50K는 테스트 세트임     
각 표본에는 0(negative) 또는 1(positive)으로 레이블이 지정됨      

### 5.2.5 Paraphrase Identification: PAWS-X Dataset
한국어를 포함한 6개 언어로 된 어려운 패러프레이즈 식별 데이터 세트이다.     
한국어 부분은 5만3338개 문장 쌍(train 4만9410개, dev 1,965개, test 1,972개)이 있다.      
NSMC 데이터 세트와 마찬가지로 각 문장 쌍은 0(negative) 또는 1(positive)으로 주석을 달았다.    
각 토큰화 전략에 대해 대규모 말뭉치에서 BERT-Base 모델을 사전 교육하고 5개의 NLU 작업의 교육 세트에서 독립적으로 fine-tuning한다.     



### 5.2.6 Results
![image](https://user-images.githubusercontent.com/76824611/188317880-4debb452-0d45-4223-b694-9fc44693c0fa.png)
* 표 6에서 우리는 dev 및 test dataset에 대한 다양한 모델의 평가 결과를 보고한다.     
* KorQuAD test set이 없어 dev set에만 결과를 보고한다.   
* KorQuAD의 경우, 서브워드 64K 모델은 Exact Match(EM)와 F1 점수가 가장 높다.     
* 하위 단어 및 형태소 모델의 점수는 어휘 크기가 커질수록 단조롭게 증가한다.      
* 반면에 32K 모델은, Morpheme-aware Subword 모델에서 다른 모델보다 성능이 우수하며, 성능과 어휘 크기 사이에 명확한 상관관계가 발견되지 않는다.      
* 다른 네 가지 작업에 대해, Morpheme-aware Subword 64K 모델은 최고의 점수를 보여준다.      
* 주목할 만한 한 가지 현상은 토큰화 그룹에서 어휘 크기가 커짐에 따라 점수가 증가하는 경향이 있다는 것이다.     
* 이것은 5.1.4절의 기계 번역 결과와 일치하지 않는데, 여기서 더 큰 **어휘 크기가 하위 단어 및 형태소 인식 하위 단어 모델에 대한 더 나은 성능을 보장하지 않는다.**     


----
----


# 6 Discussion
토큰화와 관련하여 어떤 요소가 Ko-En 및 En-Ko 번역 성능에 영향을 미치는지 추가로 조사한다.

![image](https://user-images.githubusercontent.com/76824611/188318103-f80000bc-81f2-4421-9473-b451de930b2c.png)

---


## 6.1 Token Length
토큰화는 텍스트를 더 짧은 세그먼트로 분할하는 것을 포함하기 때문에,    
우리는 **각 세그먼트가 얼마나 많은 정보를 가지고 있는지 파악하는 것**이 중요하다는 것을 알게 되었다.       


이를 위해 텍스트가 길수록 정보가 많을 것으로 가정하여 위의 그림의 번역 test 세트에서 BLEU 점수를     
**한국어 토큰당 평균 음절 수**로 표시하였다.     

* subword 인 음절, 형태소, 하위 단어 및 형태소 인식 하위 단어의 BLEU 점수는 대부분 점선으로 표시된 CV 모델의 BLEU 점수보다 높다.      
* 특히 1.00~1.50 사이의 Syllable, Subword, and Morphemeaware Subword models은 Ko-En과 En-Ko 모두에서 가장 높은 점수를 보여준다.      
* 토큰이 **평균 1.5음절 이상이면 점수가 떨어지기 시작하고**,     
토큰에 2.5음절 이상 있는 워드 모델은 최악의 성능을 발휘한다(Ko-En and in En-Ko). (공간제약으로 그림엔 표시X)     



---

## 6.2 Linguistic Awareness
* 토큰 길이가 토큰화 전략의 유일한 핵심 요소는 분명하게 아니다.    
* 위 그림의 음영 영역에서  Morpheme-aware Subword models(녹색 마커)과 Subword 8K models(빨간색 마커)을 비교해보자.     
    * 약 1.4의 평균 토큰 길이를 가지고 있지만, Morpheme-aware Subword은 Subword 8K models보다 성능이 우수하다.     
    * 우리는 이것이 **기계번역을 위한 한국어 토큰화 전략의 또 다른 중요한 요소**라는 것을 뒷받침하는 증거라고 믿는다.     

---


## 6.3 Under-trained Tokens
섹션 5.1.4에서, 우리는 높은 OOV 비율이 Morpheme 모델의 성능을 저하시킬 가능성이 높다고 지적했다.     

위 그림에서 Morpheme 모델을 나타내는 대부분의 주황색 마커가 점선 아래에 있다는 점도 주목할 필요가 있다.    

OOV는 테스트 세트에만 나타나는 토큰이다.    
즉, 제한된 횟수 동안 교육 세트에 나타나는 테스트 세트의 토큰인 훈련되지 않은 토큰의 극단적인 경우이다.      

![image](https://user-images.githubusercontent.com/76824611/188318453-db90f632-9016-4442-bcd0-f3b0c93beea7.png)

위의 그그림 2에선 n = 1부터 n = 100까지 각 모델에서 훈련 부족 토큰이 차지하는 양을 보여준다.      
여기서 n은 훈련 세트에서 훈련 부족 토큰의 빈도다.    

분명하게, **Morpheme 32K 모델의 곡선은 다른 모델의 곡선을 훨씬 능가**하여, **under-trained tokens 문제**를 가장 크게 겪고 있다는 것을 보여준다.    

---

# 7 Conclusion   
machine translation과 5가지 NLU 과제에 대한 다양한 한국어 토큰화 전략을 탐구했다.       

**[machine translation]**     
* 기계 번역에서 어휘 크기를 가진 **Morpheme-aware Subword models**은 한국어에서 영어로, 영어와 한국어로 설정 모두에 **가장 잘 작동**했다.       

**[NLU]**     
* 반면, **NLU 작업에 대한 단일 최상의 토큰화 전략은 없었다**.      
* 대신 **Subword 64K model**s은 **KorQuAD에서 최고의 성능**을 보였다.      
* 반면 **Morpheme-aware Subword 64K models**은 **다른** KorNLI, KorSTS, NSMC 및 PAWS-X **작업**에 **최적**의 것으로 나타났다.        
