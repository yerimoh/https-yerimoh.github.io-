---
title: "An Empirical Study of Tokenization Strategies for Various Korean NLP Tasks 정리"
date:   2022-07-20
excerpt: "An Empirical Study of Tokenization Strategies for Various Korean NLP Tasks"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---


원 논문: [An Empirical Study of Tokenization Strategies for Various Korean NLP Tasks](https://arxiv.org/pdf/2010.02534.pdf)


# Abstract
Typically, tokenization is the very first step in
most text processing works. As a token serves
as an atomic unit that embeds the contextual information of text, how to define a token plays
a decisive role in the performance of a model.
Even though Byte Pair Encoding (BPE) has
been considered the de facto standard tokenization method due to its simplicity and universality, it still remains unclear whether BPE works
best across all languages and tasks. In this paper, we test several tokenization strategies in
order to answer our primary research question,
that is, “What is the best tokenization strategy
for Korean NLP tasks?”
Experimental results demonstrate that a hybrid
approach of morphological segmentation followed by BPE works best in Korean to/from
English machine translation and natural language understanding tasks such as KorNLI,
KorSTS, NSMC, and PAWS-X. As an exception, for KorQuAD, the Korean extension of
SQuAD, BPE segmentation turns out to be the
most effective.
Our code and pre-trained models are publicly available at https://github.com/
kakaobrain/kortok.



# 1 Introduction
Tokenization is the very first step in most text processing works. Not surprisingly, tremendous academic efforts have been made to find the best tokenization method for various NLP tasks. For the
past few years, Byte Pair Encoding (BPE) (Gage,
1994) has been considered the de facto standard
tokenization technique since it was reintroduced by
Sennrich et al. (2016a). Besides the fact that BPE
turns out to be very effective in the machine translation task, another important reason BPE has gained
such popularity is that BPE is a data-driven statistical algorithm so it is independent of language.
However, it is still not clear whether BPE works
best across all languages, irrespective of tasks.
In this paper we study various tokenization strategies for Korean, a language which is morphologically by far richer than English. Concretely, we
empirically examine what is the best tokenization
strategy for Korean to English / English to Korean machine translation tasks, and natural language understanding (NLU) tasks—machine reading comprehension (MRC), natural language inference (NLI), semantic textual similarity (STS),
sentiment analysis, and paraphrase identification.
We are particularly interested in how complementary BPE and linguistically motivated segmentation
are.
2 Background
2.1 MeCab-ko: A Korean Morphological
Analyzer
MeCab (Kudo, 2006) is an open-source morphological analyzer based on Conditional Random Fields
(CRFs). It is originally designed for Japanese, but
also serves generic purposes so it can be applied to
other languages. MeCab-ko1
, a Korean extension
of MeCab, started from the idea that MeCab can
be easily extended to the Korean language due to
the close similarity between Japanese and Korean
in terms of morphology or syntax.
MeCab-ko trained its model on the Sejong Corpus (Kang and Kim, 2001), arguably the largest
Korean corpus morphologically annotated by many
experts, using MeCab. Ever since released in 2013,
MeCab-ko has been widely used for many Korean
NLP tasks due to its high accuracy and good usability. For example, the Workshop on Asian Transla
tion (WAT) has adopted it as the official segmentation tool for evaluating Korean machine translation
results since 2015. (Nakazawa et al., 2015, 2016,
2017, 2018, 2019).
2.2 Byte Pair Encoding
Byte Pair Encoding (BPE) is a simple data compression technique that iteratively replaces the most
frequent pair of bytes in text with a single, unused
byte (Gage, 1994). Since Sennrich et al. (2016b)
successfully applied it to neural machine translation models, it has been regarded as the standard
tokenization method across languages.
Korean is not an exception; Park et al. (2019b)
applied BPE to the Korean text in the Korean to
Japanese task of WAT 2019 and ranked first. In addition, most recent Korean neural language models
(e.g., KoBERT2
) used BPE to tokenize the training
text.
3 Related Work
There have been extensive studies about tokenization techniques for machine translation. Several
papers claimed that a hybrid of linguistically informed segmentation and a data-driven method
like BPE or unigram language modeling performs
the best for non-English languages. Banerjee and
Bhattacharyya (2018) combined an off-the-shelf
morphological segmenter and BPE in Hindi and
Bengali translations against English. Tawfik et al.
(2019) used a retrained version of linguistically
motivated segmentation model along with statistical segmentation methods for Arabic. Pinnis
et al. (2017) adopted linguistic guidance to BPE
for English-Latvian translation. Particularly (Park
et al., 2019a) is close to ours, but their main focus
is on preprocessing techniques for neural machine

translation like parallel corpus filtering rather than
on tokenization strategies per se.
Compared with the tokenization studies for machine translation, those for NLU tasks have gained
less attention. Among them is Bostrom and Durrett
(2020), which compared the fine-tuning task performance of BERT (Devlin et al., 2019) pre-trained
with BPE and unigram language modeling. Moon
and Okazaki (2020) proposed a novel encoding
method for Korean and showed its efficiency in
vocabulary compression with a few Korean NLU
datasets.
4 Tokenization Strategies
We introduce assorted Korean tokenization strategies arranged from the smallest to the largest unit.
Each of them induces different tokenization results,
as illustrated in Table 1.
4.1 Consonant and Vowel (CV)
In Hangul, the standard Korean writing system,
consonants and vowels, called Jamo in Korean, corresponding to Latin letters are assembled to form
a syllable character. For example, a Hangul consonant ㅎ /h/ (U+314E) is combined with a vowel ㅏ
/a/ (U+314F) to make a syllable character 하 /ha/
(U+558). Readers who are not familiar with such
a mechanism can think of Jamo and syllables as
atoms and molecules respectively. As a molecule
H2O can be decomposed into two H atoms and an
O atom, a syllable 하 /ha/ can be decomposed into
its constituent consonant ㅎ /h/ and vowel ㅏ /a/.
The first syllable 나 /na/ of the raw text in Table 1
is tokenized into ㄴ /n/ and ㅏ /a/, and the second
syllable 랑 /lang/ is tokenized into ㄹ /l/, ㅏ /a/,
and ㅇ /ng/, and so on. A whitespace is replaced
by a special symbol

4.2 Syllable
We can tokenize a sentence at the syllable level. A
whitespace is replaced by the special symbol ?.
4.3 Morpheme
MeCab-ko provides a convenient tokenization option in the command line interface3
. For example,
it returns A, B, and C given an input text AB C,
where A-C represent morphemes. Note that the
original space between AB and C is missing in the
output token list. Accordingly, it is NOT possible to recover the original text from the tokenized
result.
This can be problematic in some tasks that require us to restore the input text such as machine
translation whose target language is Korean, or
machine reading comprehension where we are expected to suggest a certain phrase in the given text
as the answer.
For this reason, we insert a special token ?
(U+2B51) to the original whitespace position. As
a result, in the above example, the tokenized sequence looks like A, B, ?, and D.
4.4 Subword
We learn and apply BPE using the SentencePiece
(Kudo and Richardson, 2018) library. It prepends
‘ ’ (U+2581) to every word to mark the original
whitespace, then tokenizes text into subword pieces.
As seen in Table 1, 나랑 쇼핑하자. can be split
into 나랑, 쇼, 핑하, 자, and . (period).
4.5 Morpheme-aware Subword
Motivated by the combined methods of dataand linguistically-driven approaches (Banerjee and
Bhattacharyya, 2018; Park et al., 2019a; Pinnis
et al., 2017; Tawfik et al., 2019), we apply MeCabko and BPE in sequence to make morpheme-aware
subwords. According to this strategy, since BPE
is applied after the original text is split into morphemes, tokens spanning multiple morphemes (e.g.,
핑하 in the Section 4.4) are not generated. Instead,
the BPE algorithm further segments morphemes
into frequent pieces.
4.6 Word
We can simply split text by whitespaces. Note that
punctuation marks are split into separate tokens.
Check that 나랑 쇼핑하자. is tokenized into 나랑,
쇼핑하자 and . (period) in Table 1.

[T@]
Table 2: BLEU scores of Korean to English (Ko-En)
and English to Korean (En-Ko) translation models with
different BPE training data. Note that the English sentences are tokenized using a 32K BPE model trained on
the English Wiki.
5 Experiments
5.1 Korean to/from English Machine
Translation
5.1.1 Dataset
To date, there have yet been few open source benchmark datasets for Korean-English machine translation, not to mention that Korean is not in the
language list of WMT4 or IWSLT5
. Park et al.
(2019a) used OpenSubtitles (Lison and Tiedemann,
2016), a collection of crowd-sourced movie subtitles across 65 different languages, for English to
Korean translation, but they are too noisy to serve
as a translation benchmark dataset.6
Recently, a Korean-English parallel corpus was
publicly released by AI Hub7
, which was gathered
from various sources such as news, government
web sites, legal documents, etc. We download
the news data, which amount to 800K sentence
pairs, and randomly split them into 784K (train),
8K (dev), and 8K (test).
5.1.2 BPE Modeling
Prior to training, we do simple preliminary experiments to decide which dataset to use for learning
BPE.
There are two choices: AI Hub news training
data and open source large text such as Wiki. AI
Hub training data is relatively small in size (130
MB), but can be optimal as its lexical distribution
will be close to that of the test data, considering
both of them are from the same source. On the
other hand, Wiki is larger, but it is not news per
se, so can be not as appropriate as AI Hub data for
BPE modeling.
To investigate this, first we train a 32K Korean BPE model (A) using SentencePiece with
the Korean sentences in the AI Hub training
data. Then we download the latest Wikipedia Korean8
/English9 dumps, and extract plain texts using
WikiExtractor 10. Next, we make 32K BPE models
for Korean (B) and English (C) with them. Finally,
we train Korean to English (Ko-En) and English to
Korean (En-Ko) translation models on the AI Hub
training data with the two different Korean BPE
models (A, B). The training details are explained
in Section 5.1.3. For comparison, we use the same
English BPE model (C) for both.
The results are shown in Table 2. For Ko-En
translation, the Wiki-based BPE model performs
better in both dev and test sets by 2-3 points. For
En-Ko translation, there is no practical difference in
performance between the Wiki and AI Hub-based
models. It is also worth considering the BPE models are used for NLU tasks as well as machine
translation. All things taken together, we opt for

the Wiki-based BPE model.
5.1.3 Training
We test the tokenization strategies in Section 4
with various vocabulary sizes on the AI Hub news
dataset.
We use the Transformer (Vaswani et al., 2017),
the state-of-the-art model for neural machine translation. We mostly follow the base model configuration: 6 blocks of 512-2048 units with 8 attention heads. We run all of our experiments using
FAIRSEQ 11 (Ott et al., 2019), a PyTorch based
deep learning library for sequence to sequence models.
Each model is trained using a Tesla V100 GPU
with batch size 128, dropout rate 0.3, label smoothing 0.1, and the Adam (Kingma and Ba, 2015)
optimizer. We set the learning rate to 5e-4 with the
inverse square-root schedule. We train all models
for 50 epochs and save the checkpoint files at every
epoch.
5.1.4 Results
After all training stages are finished, we evaluate the saved checkpoint files of each model on
the dev set to find the best one, which is subsequently used for the final test. In Table 3 we report
BLEU scores on both the dev and test sets using the
Moses12 multi-bleu.perl script. Following
WAT 2019 (Nakazawa et al., 2019), Moses tokenizer and MeCab-ko are used for tokenizing the
evaluation data.
For both Ko-En and En-Ko, overall, the Subword models (35.64-39.22) and the Syllable models
(38.45-39.30) are superior to the Morpheme models (31.59-37.37) or the Word models (7.04-18.42)
in performance. It is highly likely to come from
the lower OOV rates of the Subword models (0.07-
0.12) and the Syllable models (0.06) compared to
those of the Morpheme models (1.40-7.51) and the
Word models (26.20). While BPE tends to split rare
words into subword pieces, MeCab-ko is ignorant
of statistics so it splits words into morphemes by
linguistic knowledge instead. That the Morpheme
and Word models generate many OOVs suggests
Korean has so large types of morphemes or word
forms that even 64K vocabulary is not enough to
cover them all.
CV models are tiny in vocabulary size (166) so
they show the lowest OOV rate (0.02). However,
their performance is not as good as the Syllable
or Subword models. We speculate this is because
a single consonant or vowel must bear too much
contextual information in the CV models.
Morpheme-aware Subword 32K models achieve
the best BLEU scores. Each Subword model, as
shown in Table 4, contains 6-37% of tokens spanning morpheme boundaries in the test set, which
implies that subword segmentation by BPE is not
optimal and morpheme boundaries are meaningful
in tokenization.
To sum up, morpheme-aware subword tokenization that makes the best use of linguistic knowledge
and statistical information is the best for Korean
machine translation.
5.2 Korean Natural Language
Understanding Tasks
Large pre-trained language models have proven
their effectiveness in many downstream tasks (Peters et al., 2018; Devlin et al., 2019; Liu et al.,
2019). We pre-train BERT (Devlin et al., 2019)
models with various tokenization strategies, and
fine-tune them on five different Korean NLU tasks.
5.2.1 Machine Reading Comprehension:
KorQuAD 1.0 Dataset
The KorQuAD 1.0 dataset (Lim et al., 2019) is a
Korean adaptation of SQuAD 1.0 (Rajpurkar et al.,
2016), a popular reading comprehension dataset.
KorQuAD 1.0 consists of 10,645 passages and
their paired 66,181 questions (60,407 for training
+ 5,774 for development13). Like SQuAD 1.0, KorQuAD 1.0 involves answering a question given a
passage. The answer must be a phrase within the
passage.
5.2.2 Natural Language Inference: KorNLI
Dataset
The KorNLI Dataset (Ham et al., 2020) is a Korean NLI dataset sourced from three different NLI
datasets: SNLI (Bowman et al., 2015), MNLI
(Williams et al., 2018), and XNLI (Conneau et al.,
2018).
It is composed of 950,354 sentence pairs:
942,854 for training, 2,490 for development, and
5,010 for test. A model receives a pair of
sentences—a premise and a hypothesis—and classifies their relationship into one out of three categories: entailment, contradiction, and neutral.
5.2.3 Semantic Textual Similarity: KorSTS
Dataset
The KorSTS Dataset (Ham et al., 2020) is a Korean STS dataset translated from the STS-B dataset
(Cer et al., 2017). It comprises 8,628 sentence
pairs—5,749 for training, 1,500 for development,
and 1,379 for test. The task assesses the gradations
of semantic similarity between two sentences with
a scale from 0 to 5.
5.2.4 Sentiment Analysis: NSMC Dataset
NSMC14 is a movie review dataset scraped from
Naver Movies™. It consists of 200K samples of
which 150K are the training set and the rest 50K
are the test set. Each sample is labeled with 0
(negative) or 1 (positive). We hold out 10 percent
of the training data for development.
5.2.5 Paraphrase Identification: PAWS-X
Dataset
The PAWS-X dataset (Yang et al., 2019) is a challenging paraphrase identification dataset in six languages including Korean. The Korean portion
amounts to 53,338 sentence pairs (49,410 for training, 1,965 for development, and 1,972 for test).
Like the NSMC dataset, each sentence pair is annotated with either 0 (negative) or 1 (positive).
For each tokenization strategy, we pre-train a
BERT-Base model on a large corpus and fine-tune
it on the training sets of the five NLU tasks independently.
Pre-training. Because the Korean Wiki corpus
is not enough in volume, 640 MB, for the pre
training purpose, we additionally download the
recent dump of Namuwiki15, a Korean Wiki, and
extract plain texts using Namu Wiki Extractor16
.
On the resulting Namuwiki corpus (5.5 GB) along
with the Wiki corpus (640 MB), pre-training is
performed with a Cloud TPU v3-8 for 1M steps
using the official BERT training code17, which is
based on TensorFlow. We set the training hyperparameters of all models as follows: batch size
= 1024, max sequence length = 128, optimizer =
AdamW (Loshchilov and Hutter, 2019), learning
rate = 5e-5, warm-up steps = 10K.
Fine-tuning. After converting each of the pretrained models in TensorFlow into PyTorch, we
fine-tune it using HuggingFace Transformers18
(Wolf et al., 2019). The hyper-parameters for each
task are shown in Table 5.
5.2.6 Results
In Table 6 we report the evaluation results of the
various models on the dev and test sets. Since
KorQuAD lacks the test set, we report the results
on the dev set only.
As for KorQuAD, Subword 64K models achieve
the highest Exact Match (EM) and F1 scores. The
scores in the Subword and Morpheme models increase monotonically as the vocabulary size grows.
On the other hand, the 32K models outperform the
others in the Morpheme-aware Subword models;
no clear correlation is found between performance
and vocabulary sizes in them.
For all the other four tasks, Morpheme-aware
Subword 64K models show the best scores. One
noteworthy phenomenon is that the scores tend to
increase as the vocabulary size grows across the
tokenization groups. This is discordant with the machine translation results in Section 5.1.4, where a
larger vocabulary size does not guarantee better performance for the Subword and Morpheme-aware
Subword models.
6 Discussion
We further examine which factors with respect to
tokenization affect the Ko-En and En-Ko translation performance.
6.1 Token Length
Because tokenization involves splitting a text into
shorter segments, we find it important to figure out
how much information each segment bears. To this
end, based on the assumption that the longer a text
is, the more information it is likely to have, we
plot the BLEU scores by the average number of
syllables per Korean token in the translation test
sets in Figure 1.
The BLEU scores of the subword models—
Syllable, Morpheme, Subword, and Morphemeaware Subword—are mostly higher than those of
the CV models, which are plotted as dotted lines. In
particular, the Syllable, Subword, and Morphemeaware Subword models between 1.00 and 1.50
show the best scores both in Ko-En and in En-Ko.
When a token has more than 1.5 syllables on average, the scores begin to decrease, and the Word
models which has more than 2.5 syllables in a token performs the worst (7.07 for Ko-En and 18.42
for En-Ko). Note that they are not in the figures
due to space constraints.
6.2 Linguistic Awareness
Obviously token length is not the only key factor in tokenization strategies. Let us compare
the Morpheme-aware Subword 16K models (green
markers) and Subword 8K models (red markers)
in the shaded regions in Figure 1. Although they
have the same average token length around 1.4,
the Morpheme-aware Subword models outperform
the Subword models. We believe this is evidence
to support that linguistic awareness is another important factor in Korean tokenization strategies for
machine translation.
6.3 Under-trained Tokens
In section 5.1.4, we pointed out high OOV rates
are highly likely to degrade the performance of
Morpheme models. It is also worth noting that in
Figure 1 as most of the orange markers denoting
Morpheme models are below the dotted lines.
OOVs are the tokens that appear only in the test
set. They are an extreme case of under-trained
tokens—test set’s tokens that appear in the training
set for the limited number of times. Figure 2 shows
how much under-trained tokens account for in each
model, ranging from n = 1 to n = 100, where n
is the frequency of the under-trained tokens in the
training set. Clearly, the curve of the Morpheme
32K model is far above that of the others, indicating
that it suffers from the problem of under-trained
tokens the most.
7 Conclusion
We explored various Korean tokenization strategies on machine translation and five NLU tasks.
In machine translation Morpheme-aware Subword
models with a vocabulary size worked best for both
Korean to English and English to Korean settings.
By contrast, there was no single best tokenization
strategy for the NLU tasks. Instead, Subword 64K
models showed the best performance on KorQuAD,
whereas Morpheme-aware Subword 64K models
turned out to be optimal for the other KorNLI, KorSTS, NSMC, and PAWS-X tasks.
