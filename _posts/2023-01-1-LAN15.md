---
title: "Effective Approaches to Attention-based Neural Machine Translation 정리"
date:   2023-01-1
excerpt: "Effective Approaches to Attention-based Neural Machine Translation paper review"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---

# 목차


# Abstract

**[attention-based NMT탐구 배경]**    
최근 번역 중에 원본 문장의 일부에 선택적으로 초점을 맞추어 neural machine translation(NMT)을 개선하기 위해 **attentional mechanism**이 사용되고 있다.     
그러나 attention-based NMT에 대한 유용한 아키텍처를 탐구하는 작업은 거의 없었다.       

**[논문의 목적]**      
이 논문은 **두 가지** 단순하고 효과적인 **attentional mechanism** 클래스를 조사한다.       
* 1) **_global_ attention:** 모든  source words(번역하고자 하는 언어)에 항상 주의를 기울이는 접근방식     
* 2) **_local_ attention:** 한 번에 source words의 하위 집합만 보는 접근 방식     

본 논문은 영어와 독일어 사이의 WMT 번역 작업에 대한 **두 가지 접근 방식의 효과를 입증**한다.       

**_local_ attention**를 사용하면 [dropout](https://yerimoh.github.io/DL17/#%EB%93%9C%EB%A1%AD%EC%95%84%EC%9B%83%EC%97%90-%EC%9D%98%ED%95%9C-%EA%B3%BC%EC%A0%81%ED%95%A9-%EC%96%B5%EC%A0%9C)과 같은 이미 알려진 기술을 통합한 non-attentional systems에 비해 **+ 5.0 BLEU score**의 상당한 이점을 얻을 수 있다.     

서로 다른 attention architectures를 사용하는 앙상블 모델은 25.9 BLEU 포인트가 나왔다.     
➡ WMT'15 영어-독일어 번역 과제에서, NMT와 n-gram reranker를 이용한 기존 보다 1.0 BLEU 포인트 향상된 새로운 state-of-the-art score를 산출한다. 



All our code and models are publicly available at [this link](t
http://nlp.stanford.edu/projects/nmt)


-----


# Introduction
Neural Machine Translation (NMT)은 영어에서 프랑스어 및 영어에서 독일어와 같은 대규모 번역 작업에서 state-of-the-art 성능을 달성했다.       

**[NMT의 특징]**   
* **최소한의 도메인 지식을 필요**로 하고 **개념적으로 간단**함    
* NMT는 **end-to-end 방식**으로 훈련됨     
* 매우 **긴 단어 시퀀스까지 잘 일반화**할 수 있는 능력을 가진 **대규모 신경망**임      
➡ 이것은 모델이 표준 MT의 경우처럼 **거대한 phrase table과 언어 모델을 명시적으로 저장할 필요가 없다**는 것을 의미한다.         
* 따라서 NMT는 메모리 공간이 작음      
* NMT 디코더를 구현하는 것은 표준 MT의 매우 복잡한 디코더와는 달리 쉽다      
 


**[NMT 모델의 예시]**    
Luong et al.(2015)의 모델을 예시로 들음     
1️⃣ Figure 1의 파란색 네모와 같이 문장 끝 기호(```<eos>```)에 도달할 때까지 모든 source words를 읽는다.           
2️⃣ 그런 다음 그림의 빨간색 네모 부분과 같이 한 번에 하나의 대상 단어(eg X, Y, Z in Figure 1)를 방출하기 시작한다.        

![image](https://user-images.githubusercontent.com/76824611/214797384-3b6ecb3f-58e4-46dc-aeaf-07659078c378.png)

Figure 1: Neural machine translation     
 source sequence _A B C D_ (번역하고자 하는 단어)를 target sequence _X, Y, Z_ (번역 대상이 되는 단어)로 변환하기 위한 stacking recurrent architecture. 여기서 ```<eos>```는 문장의 끝을 나타낸다.        


이와 병행하여, "주의"라는 개념은 최근 신경망 훈련에서 인기를 얻고 있으며, 모델이 서로 다른 양식 간의 정렬, 예를 들어 동적 제어 문제(Mnih et al., 2014)에서 이미지 객체와 에이전트 동작 간의 정렬, 음성 인식 작업(?)에서 음성 프레임과 텍스트 간의 정렬 또는 시각적 f 사이의 정렬을 학습할 수 있다그림의 특징 및 이미지 캡션 생성 작업의 텍스트 설명(Xu et al., 2015). NMT의 맥락에서, Bahdanau et al.(2015)은 단어를 공동으로 번역하고 정렬하기 위해 이러한 주의 메커니즘을 성공적으로 적용했다. 우리가 아는 한, NMT에 대한 주의 기반 아키텍처의 사용을 탐구하는 다른 연구는 없었다.
In parallel, the concept of “attention” has gained popularity recently in training neural networks, allowing models to learn alignments between different modalities, e.g., between image objects and agent actions in the dynamic control problem (Mnih et al., 2014), between speech frames and text in the speech recognition task (?), or between visual features of a picture and its text description in the image caption generation task (Xu et al., 2015). In the context of NMT, Bahdanau et al. (2015) has successfully applied such attentional mechanism to jointly translate and align words. To the best of our knowledge, there has not been any other work exploring the use of attention-based architectures for NMT.

In this work, we design, with simplicity and ef- fectiveness in mind, two novel types of attentionbased models: a global approach in which all source words are attended and a local one whereby only a subset of source words are considered at a time. The former approach resembles the model of (Bahdanau et al., 2015) but is simpler architecturally. The latter can be viewed as an interesting blend between the hard and soft attention models proposed in (Xu et al., 2015): it is computationally less expensive than the global model or the soft attention; at the same time, unlike the hard attention, the local attention is differentiable almost everywhere, making it easier to implement and train.2 Besides, we also examine various alignment functions for our attention-based models

Experimentally, we demonstrate that both of our approaches are effective in the WMT translation tasks between English and German in both directions. Our attentional models yield a boost of up to 5.0 BLEU over non-attentional systems which already incorporate known techniques such as dropout. For English to German translation, we achieve new state-of-the-art (SOTA) results for both WMT’14 and WMT’15, outperforming previous SOTA systems, backed by NMT models and n-gram LM rerankers, by more than 1.0 BLEU. We conduct extensive analysis to evaluate our models in terms of learning, the ability to handle long sentences, choices of attentional architectures, alignment quality, and translation outputs.
  
  
