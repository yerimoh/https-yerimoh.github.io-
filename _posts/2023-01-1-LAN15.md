---
title: "Effective Approaches to Attention-based Neural Machine Translation 정리"
date:   2023-01-1
excerpt: "Effective Approaches to Attention-based Neural Machine Translation paper review"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---

# 목차


# Abstract

**[attention-based NMT탐구 배경]**    
최근 번역 중에 원본 문장의 일부에 선택적으로 초점을 맞추어 neural machine translation(NMT)을 개선하기 위해 **attentional mechanism**이 사용되고 있다.     
그러나 attention-based NMT에 대한 유용한 아키텍처를 탐구하는 작업은 거의 없었다.       

**[논문의 목적]**      
이 논문은 **두 가지** 단순하고 효과적인 **attentional mechanism** 클래스를 조사한다.       
* 1) **_global_ attention:** 모든  source words(번역하고자 하는 언어)에 항상 주의를 기울이는 접근방식     
* 2) **_local_ attention:** 한 번에 source words의 하위 집합만 보는 접근 방식     

본 논문은 영어와 독일어 사이의 WMT 번역 작업에 대한 **두 가지 접근 방식의 효과를 입증**한다.       

**_local_ attention**를 사용하면 [dropout](https://yerimoh.github.io/DL17/#%EB%93%9C%EB%A1%AD%EC%95%84%EC%9B%83%EC%97%90-%EC%9D%98%ED%95%9C-%EA%B3%BC%EC%A0%81%ED%95%A9-%EC%96%B5%EC%A0%9C)과 같은 이미 알려진 기술을 통합한 non-attentional systems에 비해 **+ 5.0 BLEU score**의 상당한 이점을 얻을 수 있다.     

서로 다른 attention architectures를 사용하는 앙상블 모델은 25.9 BLEU 포인트가 나왔다.     
➡ WMT'15 영어-독일어 번역 과제에서, NMT와 n-gram reranker를 이용한 기존 보다 1.0 BLEU 포인트 향상된 새로운 state-of-the-art score를 산출한다. 



All our code and models are publicly available at [this link](t
http://nlp.stanford.edu/projects/nmt)


-----
---

# 1. Introduction

## 선행 연구
Neural Machine Translation (NMT)은 영어에서 프랑스어 및 영어에서 독일어와 같은 대규모 번역 작업에서 state-of-the-art 성능을 달성했다.       

**[NMT의 특징]**   
* **최소한의 도메인 지식을 필요**로 하고 **개념적으로 간단**함    
* NMT는 **end-to-end 방식**으로 훈련됨     
* 매우 **긴 단어 시퀀스까지 잘 일반화**할 수 있는 능력을 가진 **대규모 신경망**임      
➡ 이것은 모델이 표준 MT의 경우처럼 **거대한 phrase table과 언어 모델을 명시적으로 저장할 필요가 없다**는 것을 의미한다.         
* 따라서 NMT는 메모리 공간이 작음      
* NMT 디코더를 구현하는 것은 표준 MT의 매우 복잡한 디코더와는 달리 쉽다      
 


**[NMT 모델의 예시]**    
Luong et al.(2015)의 모델을 예시로 들음     
1️⃣ Figure 1의 파란색 네모와 같이 문장 끝 기호(```<eos>```)에 도달할 때까지 모든 source words를 읽는다.           
2️⃣ 그런 다음 그림의 빨간색 네모 부분과 같이 한 번에 하나의 대상 단어(eg X, Y, Z in Figure 1)를 방출하기 시작한다.        

![image](https://user-images.githubusercontent.com/76824611/214797384-3b6ecb3f-58e4-46dc-aeaf-07659078c378.png)

Figure 1: Neural machine translation     
source sequence _A B C D_ (번역하고자 하는 단어)를 target sequence _X, Y, Z_ (번역 대상이 되는 단어)로 변환하기 위한 stacking recurrent architecture. 여기서 ```<eos>```는 문장의 끝을 나타낸다.        


이와 병행하여, "attention"라는 개념은 최근 신경망 훈련에서 인기를 얻고 있으며,    
모델이 **서로 다른 modalities 간의 alignments**을 학습할 수 있다.      
예를 들어,  
* 동적 제어 문제에서 이미지 객체와 에이전트 동작 간의 alignments    
* 음성 인식 작업에서 음성 프레임과 텍스트 간의 alignments     
* 시각적 features및 이미지 캡션 생성 작업의 텍스트 설명 사이의 alignments을 학습할 수 있다   

NMT의 맥락에서, **단어를 공동으로 번역하고 alignments**하기 위해 이러한 attention을 성공적으로 적용했다.       
우리가 아는 한, 위에서 언급한 연구 이외의 NMT에 대한 **attention 기반 아키텍처의 사용을 탐구**하는 다른 연구는 없었다.


## 본 논문의 목적

1️⃣ 본 연구에서는 **단순성과 효율성**을 염두에 두고, **두 가지 새로운 유형의 attention based 모델(_global_, _local_)** 방식을 설계한다.     
* **_global_ 접근 방식**: [Bahdanau et al., 2015의 모델](https://arxiv.org/abs/1409.0473)과 유사하지만 **구조적으로 더 단순**하다.      
* **_local_ 접근 방식**: [Xu et al., 2015](https://arxiv.org/abs/1502.03044)에서 제안된 _hard_ 및 _soft_ attention 모델 간의 흥미로운 혼합으로 볼 수 있다.  
   * _global_ model이나 _soft_ attention보다 **계산적으로 비용이 적게** 든다.            
   * _hard_ attention과 달리 _local_ attention은 거의 모든 곳에서 차별화되어 **구현 및 훈련이 용이**하다    


2️⃣ 또한, 우리는 attention-based models에 대한 **다양한 alignment 기능도 조사**한다.     



3️⃣ 우리는 실험을 통해 우리의 **두 가지 접근 방식이** 영어와 독일어 사이의 **WMT 번역 작업에서 효과적이라는 것**을 보여준다.     
* 우리의 attentional models은 [dropout](https://yerimoh.github.io/DL17/#%EB%93%9C%EB%A1%AD%EC%95%84%EC%9B%83%EC%97%90-%EC%9D%98%ED%95%9C-%EA%B3%BC%EC%A0%81%ED%95%A9-%EC%96%B5%EC%A0%9C)과 같은 이미 알려진 기술을 통합한 **non-attentional systems에 비해 최대 5.0 BLEU의 증가**를 보여준다.         
* 영어에서 독일어로의 번역의 경우, 우리는 WMT'14와 WMT'15 모두에 대한 새로운 최첨단(SOTA) 결과를 달성하여 **NMT 모델과 n-gram LM rerankers가 지원하는 이전 SOTA 시스템을 1.0 BLEU 이상 능가**한다.     


4️⃣ learning, 긴 문장 처리 능력, attentional architectures 선택, alignment 품질 및 번역 outputs에서 모델을 평가하기 위해 **광범위한 분석**을 수행한다. 



------
-----

# 2. Neural Machine Translation

A neural machine translation system is a neural
network that directly models the conditional probability p(y|x) of translating a source sentence,
x1, . . . , xn, to a target sentence, y1, . . . , ym.
3 A
basic form of NMT consists of two components:
(a) an encoder which computes a representation s
for each source sentence and (b) a decoder which
generates one target word at a time and hence decomposes the conditional probability as:

log p(y|x) = Xm
j=1
log p (yj |y<j , s)

(1) 
  A natural choice to model such a decomposition in the decoder is to use a recurrent neural network (RNN) architecture, which most of the recent NMT work
such as (Kalchbrenner and Blunsom, 2013;
Sutskever et al., 2014; Cho et al., 2014;
Bahdanau et al., 2015; Luong et al., 2015;
Jean et al., 2015) have in common. They, however, differ in terms of which RNN architectures
are used for the decoder and how the encoder
computes the source sentence representation s.


Kalchbrenner and Blunsom (2013) used an
RNN with the standard hidden unit for the
decoder and a convolutional neural network for
encoding the source sentence representation. On
the other hand, both Sutskever et al. (2014) and
Luong et al. (2015) stacked multiple layers of an
RNN with a Long Short-Term Memory (LSTM)
hidden unit for both the encoder and the decoder.
Cho et al. (2014), Bahdanau et al. (2015), and
Jean et al. (2015) all adopted a different version of
the RNN with an LSTM-inspired hidden unit, the
gated recurrent unit (GRU), for both components


In more detail, one can parameterize the probability of decoding each word yj as:
p (yj |y<j , s) = softmax (g (hj )) (2)


with g being the transformation function that outputs a vocabulary-sized vector.5 Here, hj is the
RNN hidden unit, abstractly computed as:

hj = f(hj−1, s), (3)

where f computes the current hidden state
given the previous hidden state and can be
either a vanilla RNN unit, a GRU, or an LSTM
unit. In (Kalchbrenner and Blunsom, 2013;
Sutskever et al., 2014; Cho et al., 2014;
Luong et al., 2015), the source representation s is only used once to initialize the
decoder hidden state. On the other hand, in
(Bahdanau et al., 2015; Jean et al., 2015) and
this work, s, in fact, implies a set of source
hidden states which are consulted throughout the
entire course of the translation process. Such an
approach is referred to as an attention mechanism,
which we will discuss next



In this work, following (Sutskever et al., 2014;
Luong et al., 2015), we use the stacking LSTM
architecture for our NMT systems, as illustrated
in Figure 1. We use the LSTM unit defined in
(Zaremba et al., 2015). Our training objective is
formulated as follows:

Jt =
X
(x,y)∈D
− log p(y|x) (4)

with D being our parallel training corpus.



