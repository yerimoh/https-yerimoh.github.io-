---
title: "Effective Approaches to Attention-based Neural Machine Translation 정리"
date:   2023-01-1
excerpt: "Effective Approaches to Attention-based Neural Machine Translation paper review"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---

# 목차


본 포스트는 [Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/pdf/1508.04025.pdf)을 구현하기 위해 상세히 정리해 둔 것으로,   
논문에 나와있는 내용을 추가 지식과 함께 쉽게 풀어쓰려고 노력하여 정리해두었습니다 :)      

# Before Read: attention
이 논문을 읽기 전, 현 논문에서 가장 많이 언급되는 attention의 개념을 알아둬야 이 논문을 읽기 편하다.    

attention이란 translation task에서 중요한 정보에 더 집중을 할 수 있게 만든 method로, 토큰(각 단어)간의 의미적인 관계를 파악할 수 있게한다.     
특히. **decoder의 각 토큰 예측 시, encoder의 모든 혹은 일부 토큰에 집중하여 task를 수행**하게 하는 것이다.       

이 개념은 3개의 중요 개념을 중심으로 이해할 수 있다.      
* **Query**: 검색어, 즉 번역 대상이 되는 단어이다.    
* **Key**: Query값을 기준으로 output이 될 수 있는 후보들이다.(즉 찾는 대상이다)    
* **Value**: 각각의 key가 갖고있는 실제 값이다.   

  
예를 들어서 아래와 같은 도형들이 존재한다고 가정해보자.    
여기서 검색대상인 Query와 가장 유사도가 높은(여기서는 비슷한 모양인) key 2의 value가 가장 높다는 것을 파악가능하다.     
![image](https://user-images.githubusercontent.com/76824611/214841597-d5091eb0-c8c5-4bfb-9ce3-b0bae7e90220.png)

**attietion은 이렇게 가장 높은 유사도를 반영**해 주는 것이다.     
 
그럼 위의 예시들을 통해 **attention value**를 구하는 과정을 생각해보자.    
 
1️⃣ 먼저 전체 key 중 Query에 대해 각 Key들이 지니는 비중인 유사도를 계산하기위해 임의의 백터 $$<a_1, a_2, a_3>$$가 있다고 가정해보자.      

2️⃣ **$$a_n$$ * Value**를 계산한다.(dot product attention)      
![image](https://user-images.githubusercontent.com/76824611/214841640-20db6bf1-789e-4012-8212-5e5e08bbfe89.png)


3️⃣ 그리고 **각각의 값을 더한다**.     
더한 값은 아래와 같은데,  아래식에 따르면 유사도가 가장 높은 Key2의 값이 가장 많이 반영이 된다는 것을 알 수 있다.         
![image](https://user-images.githubusercontent.com/76824611/214841651-67635f23-5647-49c5-bbd9-a908b1f774e8.png)



즉 위 계산은 결국 **Query와 관련이 깊은 Key의 Value 위주로 모든 값을 가져오는 것**이다.           
 


---


# Abstract

**[attention-based NMT탐구 배경]**    
최근 번역 중에 원본 문장의 일부에 선택적으로 초점을 맞추어 neural machine translation(NMT)을 개선하기 위해 **attentional mechanism**이 사용되고 있다.     
그러나 attention-based NMT에 대한 유용한 아키텍처를 탐구하는 작업은 거의 없었다.       

**[논문의 목적]**      
이 논문은 **두 가지** 단순하고 효과적인 **attentional mechanism** 클래스를 조사한다.       
* 1) **_global_ attention:** 모든  source words(번역하고자 하는 언어)에 항상 주의를 기울이는 접근방식     
* 2) **_local_ attention:** 한 번에 source words의 하위 집합만 보는 접근 방식     

본 논문은 영어와 독일어 사이의 WMT 번역 작업에 대한 **두 가지 접근 방식의 효과를 입증**한다.       

**_local_ attention**를 사용하면 [dropout](https://yerimoh.github.io/DL17/#%EB%93%9C%EB%A1%AD%EC%95%84%EC%9B%83%EC%97%90-%EC%9D%98%ED%95%9C-%EA%B3%BC%EC%A0%81%ED%95%A9-%EC%96%B5%EC%A0%9C)과 같은 이미 알려진 기술을 통합한 non-attentional systems에 비해 **+ 5.0 BLEU score**의 상당한 이점을 얻을 수 있다.     

서로 다른 attention architectures를 사용하는 앙상블 모델은 25.9 BLEU 포인트가 나왔다.     
➡ WMT'15 영어-독일어 번역 과제에서, NMT와 n-gram reranker를 이용한 기존 보다 1.0 BLEU 포인트 향상된 새로운 state-of-the-art score를 산출한다. 



All our code and models are publicly available at [this link](t
http://nlp.stanford.edu/projects/nmt)


-----
---

# 1. Introduction

## 선행 연구
Neural Machine Translation (NMT)은 영어에서 프랑스어 및 영어에서 독일어와 같은 대규모 번역 작업에서 state-of-the-art 성능을 달성했다.       

**[NMT의 특징]**   
* **최소한의 도메인 지식을 필요**로 하고 **개념적으로 간단**함    
* NMT는 **end-to-end 방식**으로 훈련됨     
* 매우 **긴 단어 시퀀스까지 잘 일반화**할 수 있는 능력을 가진 **대규모 신경망**임      
➡ 이것은 모델이 표준 MT의 경우처럼 **거대한 phrase table과 언어 모델을 명시적으로 저장할 필요가 없다**는 것을 의미한다.         
* 따라서 NMT는 메모리 공간이 작음      
* NMT 디코더를 구현하는 것은 표준 MT의 매우 복잡한 디코더와는 달리 쉽다      
 


**[NMT 모델의 예시]**    
Luong et al.(2015)의 모델을 예시로 들음     
1️⃣ Figure 1의 파란색 네모와 같이 문장 끝 기호(```<eos>```)에 도달할 때까지 모든 source words를 읽는다.           
2️⃣ 그런 다음 그림의 빨간색 네모 부분과 같이 한 번에 하나의 대상 단어(eg X, Y, Z in Figure 1)를 방출하기 시작한다.        

![image](https://user-images.githubusercontent.com/76824611/214797384-3b6ecb3f-58e4-46dc-aeaf-07659078c378.png)

Figure 1: Neural machine translation     
source sequence _A B C D_ (번역하고자 하는 단어)를 target sequence _X, Y, Z_ (번역 대상이 되는 단어)로 변환하기 위한 stacking recurrent architecture. 여기서 ```<eos>```는 문장의 끝을 나타낸다.     
그리고 여기서 각 네모(셀)은 RNN, CNN, LSTM등 으로 구성되어있다.


이와 병행하여, "attention"라는 개념은 최근 신경망 훈련에서 인기를 얻고 있으며,    
모델이 **서로 다른 modalities 간의 [alignments](https://dos-tacos.github.io/concept/alignment-in-rnn/)**을 학습할 수 있다.      
예를 들어,  
* 동적 제어 문제에서 이미지 객체와 에이전트 동작 간의 alignments    
* 음성 인식 작업에서 음성 프레임과 텍스트 간의 alignments     
* 시각적 features및 이미지 캡션 생성 작업의 텍스트 설명 사이의 alignments을 학습할 수 있다   

NMT의 맥락에서, **단어를 공동으로 번역하고 alignments**하기 위해 이러한 attention을 성공적으로 적용했다.       
우리가 아는 한, 위에서 언급한 연구 이외의 NMT에 대한 **attention 기반 아키텍처의 사용을 탐구**하는 다른 연구는 없었다.


## 본 논문의 목적

1️⃣ 본 연구에서는 **단순성과 효율성**을 염두에 두고, **두 가지 새로운 유형의 attention based 모델(_global_, _local_)** 방식을 설계한다.     
* **_global_ 접근 방식**: [Bahdanau et al., 2015의 모델](https://arxiv.org/abs/1409.0473)과 유사하지만 **구조적으로 더 단순**하다.      
* **_local_ 접근 방식**: [Xu et al., 2015](https://arxiv.org/abs/1502.03044)에서 제안된 _hard_ 및 _soft_ attention 모델 간의 흥미로운 혼합으로 볼 수 있다.  
   * _global_ model이나 _soft_ attention보다 **계산적으로 비용이 적게** 든다.            
   * _hard_ attention과 달리 _local_ attention은 거의 모든 곳에서 차별화되어 **구현 및 훈련이 용이**하다    


2️⃣ 또한, 우리는 attention-based models에 대한 **다양한 alignment 기능도 조사**한다.     



3️⃣ 우리는 실험을 통해 우리의 **두 가지 접근 방식이** 영어와 독일어 사이의 **WMT 번역 작업에서 효과적이라는 것**을 보여준다.     
* 우리의 attentional models은 [dropout](https://yerimoh.github.io/DL17/#%EB%93%9C%EB%A1%AD%EC%95%84%EC%9B%83%EC%97%90-%EC%9D%98%ED%95%9C-%EA%B3%BC%EC%A0%81%ED%95%A9-%EC%96%B5%EC%A0%9C)과 같은 이미 알려진 기술을 통합한 **non-attentional systems에 비해 최대 5.0 BLEU의 증가**를 보여준다.         
* 영어에서 독일어로의 번역의 경우, 우리는 WMT'14와 WMT'15 모두에 대한 새로운 최첨단(SOTA) 결과를 달성하여 **NMT 모델과 n-gram LM rerankers가 지원하는 이전 SOTA 시스템을 1.0 BLEU 이상 능가**한다.     


4️⃣ learning, 긴 문장 처리 능력, attentional architectures 선택, alignment 품질 및 번역 outputs에서 모델을 평가하기 위해 **광범위한 분석**을 수행한다. 



------
-----

# 2. Neural Machine Translation
neural machine translation(NMT) system은 문장 $$x_1, ., x_n$$을 문장 $$y_1, . ., y_m$$으로 **번역하는 조건부 확률 p(y|x)를 직접 모델링하는 신경망**이다.     

NMT의 기본 형태는 **Encoder-Decoder**형태이다.    
* **Encoder**: 각 source sentence(번역하고자 하는 문장)에 대한 표현을 계산을 통한 정보 추출, 이는 RNN, CNN 등으로 구성할 수 있으며 타겟 생성 방식에 따라서도 여러 방법론이 존재함.      
* **Decoder**: 추출한 정보를 바탕으로 target 문장을 생성(번역)         


![image](https://user-images.githubusercontent.com/76824611/214825162-f0bb19b3-805f-4dea-b932-cb0612a650ff.png)


한 번에 하나의 target 단어를 생성하므로 조건부 확률을 다음과 같다:        


(1)
$$log p(y|x) =$$ $$\displaystyle\sum_{j=1}^{m}{log p(y_j |y<j , s)}$$ 


* **y**: target sentence    
* **x**: source sentence    
* **s**: (= source) 번역에서 사용되는 원 문장의 정보    
* **j**: Decoder의 토큰 시점      
* 식의 의미    
   * source sentence가 주어졌을 때, target y가 생성될 확률은,   
     Decoder의 토큰 시점 j까지의 정보들을 합하여 최종적으로 출력이 될 토큰의 확률을 구하는 것이다.    

디코더에서 이러한 분해를 모델링하려면 대부분 recurrent neural network([RNN](https://yerimoh.github.io/DL16/)) 아키텍처를 사용하는데, 이 아키텍처는 (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015; Luong et al., 2015)과 같은 최근 NMT 대부분의 작업에서 공통적으로 수행된다. 그러나 디코더에 사용되는 RNN 아키텍처와 인코더가 소스 문장 표현을 계산하는 방법에 대해서는 다르다.
A natural choice to model such a decomposition in the decoder is to use a  (RNN) architecture, which most of the recent NMT work such as (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Jean et al., 2015) have in common. They, however, differ in terms of which RNN architectures are used for the decoder and how the encoder computes the source sentence representation s.

Kalchbrenner and Blunsom (2013) used an RNN with the standard hidden unit for the decoder and a convolutional neural network for encoding the source sentence representation. On the other hand, both Sutskever et al. (2014) and Luong et al. (2015) stacked multiple layers of an RNN with a Long Short-Term Memory (LSTM) hidden unit for both the encoder and the decoder. Cho et al. (2014), Bahdanau et al. (2015), and Jean et al. (2015) all adopted a different version of the RNN with an LSTM-inspired hidden unit, the gated recurrent unit (GRU), for both components

In more detail, one can parameterize the probability of decoding each word yj as: p (yj |y<j , s) = softmax (g (hj )) (2)

with g being the transformation function that outputs a vocabulary-sized vector.5 Here, hj is the RNN hidden unit, abstractly computed as:

hj = f(hj−1, s), (3)

where f computes the current hidden state given the previous hidden state and can be either a vanilla RNN unit, a GRU, or an LSTM unit. In (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Luong et al., 2015), the source representation s is only used once to initialize the decoder hidden state. On the other hand, in (Bahdanau et al., 2015; Jean et al., 2015) and this work, s, in fact, implies a set of source hidden states which are consulted throughout the entire course of the translation process. Such an approach is referred to as an attention mechanism, which we will discuss next

In this work, following (Sutskever et al., 2014; Luong et al., 2015), we use the stacking LSTM architecture for our NMT systems, as illustrated in Figure 1. We use the LSTM unit defined in (Zaremba et al., 2015). Our training objective is formulated as follows:

Jt = X (x,y)∈D − log p(y|x) (4)

with D being our parallel training corpus.





