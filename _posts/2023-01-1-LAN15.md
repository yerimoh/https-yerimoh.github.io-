---
title: "Effective Approaches to Attention-based Neural Machine Translation 정리"
date:   2023-01-1
excerpt: "Effective Approaches to Attention-based Neural Machine Translation paper review"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---

# 목차


본 포스트는 [Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/pdf/1508.04025.pdf)을 구현하기 위해 상세히 정리해 둔 것으로,   
논문에 나와있는 내용을 추가 지식과 함께 쉽게 풀어쓰려고 노력하여 정리해두었습니다 :)      

# Before Read: attention
이 논문을 읽기 전, 현 논문에서 가장 많이 언급되는 attention의 개념을 알아둬야 이 논문을 읽기 편하다.    

attention이란 translation task에서 중요한 정보에 더 집중을 할 수 있게 만든 method로, 토큰(각 단어)간의 의미적인 관계를 파악할 수 있게한다.     
특히. **decoder의 각 토큰 예측 시, encoder의 모든 혹은 일부 토큰에 집중하여 task를 수행**하게 하는 것이다.       

이 개념은 3개의 중요 개념을 중심으로 이해할 수 있다.      
* **Query**: 검색어, 즉 번역 대상이 되는 단어이다.    
* **Key**: Query값을 기준으로 output이 될 수 있는 후보들이다.(즉 찾는 대상이다)    
* **Value**: 각각의 key가 갖고있는 실제 값이다.   

  
예를 들어서 아래와 같은 도형들이 존재한다고 가정해보자.    
여기서 검색대상인 Query와 가장 유사도가 높은(여기서는 비슷한 모양인) key 2의 value가 가장 높다는 것을 파악가능하다.     
![image](https://user-images.githubusercontent.com/76824611/214841597-d5091eb0-c8c5-4bfb-9ce3-b0bae7e90220.png)

**attietion은 이렇게 가장 높은 유사도를 반영**해 주는 것이다.     
 
그럼 위의 예시들을 통해 **attention value**를 구하는 과정을 생각해보자.    
 
1️⃣ 먼저 전체 key 중 Query에 대해 각 Key들이 지니는 비중인 유사도를 계산하기위해 임의의 백터 $$<a_1, a_2, a_3>$$가 있다고 가정해보자.      

2️⃣ **$$a_n$$ * Value**를 계산한다.(dot product attention)      
![image](https://user-images.githubusercontent.com/76824611/214841640-20db6bf1-789e-4012-8212-5e5e08bbfe89.png)


3️⃣ 그리고 **각각의 값을 더한다**.     
더한 값은 아래와 같은데,  아래식에 따르면 유사도가 가장 높은 Key2의 값이 가장 많이 반영이 된다는 것을 알 수 있다.         
![image](https://user-images.githubusercontent.com/76824611/214841651-67635f23-5647-49c5-bbd9-a908b1f774e8.png)



즉 위 계산은 결국 **Query와 관련이 깊은 Key의 Value 위주로 모든 값을 가져오는 것**이다.           
 


---


# Abstract

**[attention-based NMT탐구 배경]**    
최근 번역 중에 원본 문장의 일부에 선택적으로 초점을 맞추어 neural machine translation(NMT)을 개선하기 위해 **attentional mechanism**이 사용되고 있다.     
그러나 attention-based NMT에 대한 유용한 아키텍처를 탐구하는 작업은 거의 없었다.       

**[논문의 목적]**      
이 논문은 **두 가지** 단순하고 효과적인 **attentional mechanism** 클래스를 조사한다.       
* 1) **_global_ attention:** 모든  source words(번역하고자 하는 언어)에 항상 주의를 기울이는 접근방식     
* 2) **_local_ attention:** 한 번에 source words의 하위 집합만 보는 접근 방식     

본 논문은 영어와 독일어 사이의 WMT 번역 작업에 대한 **두 가지 접근 방식의 효과를 입증**한다.       

**_local_ attention**를 사용하면 [dropout](https://yerimoh.github.io/DL17/#%EB%93%9C%EB%A1%AD%EC%95%84%EC%9B%83%EC%97%90-%EC%9D%98%ED%95%9C-%EA%B3%BC%EC%A0%81%ED%95%A9-%EC%96%B5%EC%A0%9C)과 같은 이미 알려진 기술을 통합한 non-attentional systems에 비해 **+ 5.0 BLEU score**의 상당한 이점을 얻을 수 있다.     

서로 다른 attention architectures를 사용하는 앙상블 모델은 25.9 BLEU 포인트가 나왔다.     
➡ WMT'15 영어-독일어 번역 과제에서, NMT와 n-gram reranker를 이용한 기존 보다 1.0 BLEU 포인트 향상된 새로운 state-of-the-art score를 산출한다. 



All our code and models are publicly available at [this link](t
http://nlp.stanford.edu/projects/nmt)


-----
---

# 1. Introduction

## 선행 연구
Neural Machine Translation (NMT)은 영어에서 프랑스어 및 영어에서 독일어와 같은 대규모 번역 작업에서 state-of-the-art 성능을 달성했다.       

**[NMT의 특징]**   
* **최소한의 도메인 지식을 필요**로 하고 **개념적으로 간단**함    
* NMT는 **end-to-end 방식**으로 훈련됨     
* 매우 **긴 단어 시퀀스까지 잘 일반화**할 수 있는 능력을 가진 **대규모 신경망**임      
➡ 이것은 모델이 표준 MT의 경우처럼 **거대한 phrase table과 언어 모델을 명시적으로 저장할 필요가 없다**는 것을 의미한다.         
* 따라서 NMT는 메모리 공간이 작음      
* NMT 디코더를 구현하는 것은 표준 MT의 매우 복잡한 디코더와는 달리 쉽다      
 


**[NMT 모델의 예시]**    
Luong et al.(2015)의 모델을 예시로 들음     
1️⃣ Figure 1의 파란색 네모와 같이 문장 끝 기호(```<eos>```)에 도달할 때까지 모든 source words를 읽는다.           
2️⃣ 그런 다음 그림의 빨간색 네모 부분과 같이 한 번에 하나의 대상 단어(eg X, Y, Z in Figure 1)를 방출하기 시작한다.        

![image](https://user-images.githubusercontent.com/76824611/214797384-3b6ecb3f-58e4-46dc-aeaf-07659078c378.png)

Figure 1: Neural machine translation     
source sequence _A B C D_ (번역하고자 하는 단어)를 target sequence _X, Y, Z_ (번역 대상이 되는 단어)로 변환하기 위한 stacking recurrent architecture. 여기서 ```<eos>```는 문장의 끝을 나타낸다.     
그리고 여기서 각 네모(셀)은 RNN, CNN, LSTM등 으로 구성되어있다.


이와 병행하여, "attention"라는 개념은 최근 신경망 훈련에서 인기를 얻고 있으며,    
모델이 **서로 다른 modalities 간의 [alignments](https://dos-tacos.github.io/concept/alignment-in-rnn/)**을 학습할 수 있다.      
예를 들어,  
* 동적 제어 문제에서 이미지 객체와 에이전트 동작 간의 alignments    
* 음성 인식 작업에서 음성 프레임과 텍스트 간의 alignments     
* 시각적 features및 이미지 캡션 생성 작업의 텍스트 설명 사이의 alignments을 학습할 수 있다   

NMT의 맥락에서, **단어를 공동으로 번역하고 alignments**하기 위해 이러한 attention을 성공적으로 적용했다.       
우리가 아는 한, 위에서 언급한 연구 이외의 NMT에 대한 **attention 기반 아키텍처의 사용을 탐구**하는 다른 연구는 없었다.


## 본 논문의 목적

1️⃣ 본 연구에서는 **단순성과 효율성**을 염두에 두고, **두 가지 새로운 유형의 attention based 모델(_global_, _local_)** 방식을 설계한다.     
* **_global_ 접근 방식**: [Bahdanau et al., 2015의 모델](https://arxiv.org/abs/1409.0473)과 유사하지만 **구조적으로 더 단순**하다.      
* **_local_ 접근 방식**: [Xu et al., 2015](https://arxiv.org/abs/1502.03044)에서 제안된 _hard_ 및 _soft_ attention 모델 간의 흥미로운 혼합으로 볼 수 있다.  
   * _global_ model이나 _soft_ attention보다 **계산적으로 비용이 적게** 든다.            
   * _hard_ attention과 달리 _local_ attention은 거의 모든 곳에서 차별화되어 **구현 및 훈련이 용이**하다    


2️⃣ 또한, 우리는 attention-based models에 대한 **다양한 alignment 기능도 조사**한다.     



3️⃣ 우리는 실험을 통해 우리의 **두 가지 접근 방식이** 영어와 독일어 사이의 **WMT 번역 작업에서 효과적이라는 것**을 보여준다.     
* 우리의 attentional models은 [dropout](https://yerimoh.github.io/DL17/#%EB%93%9C%EB%A1%AD%EC%95%84%EC%9B%83%EC%97%90-%EC%9D%98%ED%95%9C-%EA%B3%BC%EC%A0%81%ED%95%A9-%EC%96%B5%EC%A0%9C)과 같은 이미 알려진 기술을 통합한 **non-attentional systems에 비해 최대 5.0 BLEU의 증가**를 보여준다.         
* 영어에서 독일어로의 번역의 경우, 우리는 WMT'14와 WMT'15 모두에 대한 새로운 최첨단(SOTA) 결과를 달성하여 **NMT 모델과 n-gram LM rerankers가 지원하는 이전 SOTA 시스템을 1.0 BLEU 이상 능가**한다.     


4️⃣ learning, 긴 문장 처리 능력, attentional architectures 선택, alignment 품질 및 번역 outputs에서 모델을 평가하기 위해 **광범위한 분석**을 수행한다. 



------
-----

# 2. Neural Machine Translation
neural machine translation(NMT) system은 문장 $$x_1, ., x_n$$을 문장 $$y_1, . ., y_m$$으로 **번역하는 조건부 확률 p(y|x)를 직접 모델링하는 신경망**이다.     

NMT의 기본 형태는 **Encoder-Decoder**형태이다.    
* **Encoder**: 각 source sentence(번역하고자 하는 문장)에 대한 표현을 계산을 통한 정보 추출, 이는 RNN, CNN 등으로 구성할 수 있으며 타겟 생성 방식에 따라서도 여러 방법론이 존재함.      
* **Decoder**: 추출한 정보를 바탕으로 target 문장을 생성(번역)         

그런데 우리가 아는 **가장 일반적인 NMT의 모델**은 아래와 같다.    

![image](https://user-images.githubusercontent.com/76824611/214825162-f0bb19b3-805f-4dea-b932-cb0612a650ff.png)


한 번에 하나의 target 단어를 생성하므로 조건부 확률을 다음과 같다:        


(1)
$$log p(y|x) =$$ $$\displaystyle\sum_{j=1}^{m}{log p(y_j |y<j , s)}$$ 


* **y**: target sentence (번역해서 사용하고자하는 문장)    
* **x**: source sentence (번역하고자하는 대상)    
* **s**: (= source) 번역에서 사용되는 원 문장의 정보    
* **j**: Decoder의 토큰 시점      
* 식의 의미    
   * source sentence가 주어졌을 때, target y가 생성될 확률은,   
     Decoder의 토큰 시점 j까지의 정보들을 합하여 최종적으로 출력이 될 토큰의 확률을 구하는 것이다.    



디코더에서 이러한 분해를 모델링하려면 대부분 **recurrent neural network([RNN](https://yerimoh.github.io/DL16/)) 아키텍처**를 사용하는데, 이 아키텍처는 (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015; Luong et al., 2015)과 같은 **최근 NMT 대부분의 작업에서 공통적으로 수행**된다.     

이렇게 같은 방식은 사용하는 위의 각 논문들은 아래 **두가지 방법에서 차이**가 있다.      
**1)** Decoder에 사용되는 RNN 아키텍처의 종류    
**2)** 인코더가 소스 문장 표현(**s**)을 계산하는 방법        


더 자세히 말하면,    
**각 단어 $$y_j$$의 디코딩 확률**을 아래와 같이 매개변수화할 수 있다.         

(2)    
$$p(y_j \| y<j, s) = softmax (g(h_j))$$ 

* **$$g$$**: vocabulary-sized 벡터를 출력하는 변환 함수이다.     
* **$$h_j$$**: RNN hidden unit이며, 추상적으로 다음과 같이 계산된다:

(3)     
$$$h_j = f(h_{j-1}, s)$$ 

* **$$f$$**: 이전 hidden state를 기반으로, 주어진 현재 hidden state를 계산한다. 이는 바닐라 RNN 단위, GRU 또는 LSTM 단위가 될 수 있다.      


**[source representation s 의 사용 횟수]**      
* Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Luong et al., 2015 에서 source representation s 은 **decoder hidden state를 초기화하는 데 한 번만 사용**된다.      
* Bahdanau et al., 2015; Jean et al., 2015과 **본 논문**는 **번역 과정의 전 과정**에 걸쳐 사용된다.    
➡ 이러한 접근 방식을 **attention mechanism**이라고 하며, 이에 대해서는 다음 챕터에서 설명하겠다.    


본 연구에서는 (Sutskever et al., 2014; Luong et al., 2015)에 이어 Figure 1에 나와 있는 것처럼 NMT 시스템에 대한 stacking LSTM 아키텍처를 사용한다.      
우리는 ([Zaremba et al., 2015](https://arxiv.org/abs/1409.2329))에 정의된 LSTM 단위를 사용한다. trainig 목표는 다음과 같이 공식화된다:        

(4)    
$$J_t =$$ $$\displaystyle\sum_{(x,y)∈D}{-log p(y\|x)}$$ 
 
* **D**: 우리의 병렬 training corpus이다.         

----




# 3.  Attention-based Models


우리의 다양한 주의 기반 모델은 **_global_** 과 **_local_** 로 크게 분류된다.      
두 attention은 앞서 말했다시피 "attention"이 모든 source positions에 배치되는지, 아니면 소수의 source positions에만 배치되는지의 관점에서 다르다.      
우리는 이 두 가지 모델 유형을 Figure 2와 Figure 3에 각각 설명할 예정이다.    


Common to these two types of models is the fact that at each time step t in the decoding phase, both approaches first take as input the hidden state ht at the top layer of a stacking LSTM. The goal is then to derive a context vector ct that captures relevant source-side information to help predict the current target word yt . While these models differ in how the context vector ct is derived, they share the same subsequent steps


**[두 attention의 공통점]**    
* **Decoding 단계의 입력:** Decoding 단계에서 각 단계 $$t$$에서 먼저 **stacking LSTM의 최상위 계층에서** hidden state **$$h_t$$를 입력으로 받아들인다**는 사실이다.       
* **공통 목표:** 위와 같이 입력을 받은 다음, 현재 target word $$y_t$$를 예측하는 데 도움이 되는 source-side information를 캡처하는 context vector **$$c_t$$를 도출하는 것이 목표**이다.    
* 이 모델들은 context vector **$$c_t$$가 도출되는 방법이 다르지만** **동일한 후속 단계를 공유**한다


구체적으로, 대상 숨겨진 상태 $$h_t$$와 source-side information를 캡처하는 context vector $$c_t$$를 고려할 때,    
우리는 다음과 같이 **attentional hidden state를 생성하기 위한 두 벡터의 정보를 결합**하기 위해 간단한 연결 레이어를 사용한다:

(5)       
$$h˜_t = tanh(W_c[c_t;h_t])$$     

주의 벡터 **$$h˜_t$$는** 다음과 같이 공식화된 **예측 분포를 생성**하기 위해 **소프트맥스 층을 통해 공급**된다:   

(6)    
$$p(y_t |y<t, x) = softmax(W_sh˜_t)$$    

이제 각 모델 유형이 source-side information context vector $$c_t$$를 계산하는 방법을 자세히 설명한다.


---

## Global Attention

_global_ attentional model의 아이디어는 context vector $$c_t$$를 도출할 때 인코더의 모든 숨겨진 상태를 고려하는 것이다. 이 모델 유형에서 크기가 소스 측의 시간 스텝 수와 같은 가변 길이 정렬 벡터는 현재 대상 숨겨진 상태 ht를 각 소스 숨겨진 상태 hhs와 비교하여 도출된다:

The idea of a global attentional model is to consider all the hidden states of the encoder when deriving the context vector ct . In this model type, a variable-length alignment vector at , whose size equals the number of time steps on the source side, is derived by comparing the current target hidden state ht with each source hidden state h¯s:


at(s) = align(ht
,h¯s) (7)
=
exp
score(ht
,h¯s)

P
s
′ exp
score(ht
,h¯s
′)
