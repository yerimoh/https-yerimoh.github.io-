---
title: "Effective Approaches to Attention-based Neural Machine Translation 정리"
date:   2023-01-1
excerpt: "Effective Approaches to Attention-based Neural Machine Translation paper review"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---

# 목차


# Abstract



**[attention-based NMT탐구 배경]**    
최근 번역 중에 원본 문장의 일부에 선택적으로 초점을 맞추어 neural machine translation(NMT)을 개선하기 위해 **attentional mechanism**이 사용되고 있다.     
그러나 attention-based NMT에 대한 유용한 아키텍처를 탐구하는 작업은 거의 없었다.       

**[논문의 목적]**      
이 논문은 **두 가지** 단순하고 효과적인 **attentional mechanism** 클래스를 조사한다.       
* 1) **_global_ attention:** 모든  source words에 항상 주의를 기울이는 접근방식     
* 2) **_local_ attention:** 한 번에 source words의 하위 집합만 보는 접근 방식     

본 논문은 영어와 독일어 사이의 WMT 번역 작업에 대한 **두 가지 접근 방식의 효과를 입증**한다.       

**_local_ attention**를 사용하면 [dropout](https://yerimoh.github.io/DL17/#%EB%93%9C%EB%A1%AD%EC%95%84%EC%9B%83%EC%97%90-%EC%9D%98%ED%95%9C-%EA%B3%BC%EC%A0%81%ED%95%A9-%EC%96%B5%EC%A0%9C)과 같은 이미 알려진 기술을 통합한 non-attentional systems에 비해 **+ 5.0 BLEU score**의 상당한 이점을 얻을 수 있다.     

서로 다른 attention architectures를 사용하는 앙상블 모델은 25.9 BLEU 포인트가 나왔다.     
➡ WMT'15 영어-독일어 번역 과제에서, NMT와 n-gram reranker를 이용한 기존 보다 1.0 BLEU 포인트 향상된 새로운 state-of-the-art score를 산출한다. 



All our code and models are publicly available at [this link](t
http://nlp.stanford.edu/projects/nmt)


-----


# Introduction
Neural Machine Translation (NMT) achieved
state-of-the-art performances in large-scale translation tasks such as from English to French
(Luong et al., 2015) and English to German
(Jean et al., 2015). NMT is appealing since it requires minimal domain knowledge and is conceptually simple. The model by Luong et al. (2015)
reads through all the source words until the end-ofsentence symbol <eos> is reached. It then starts emitting one target word at a time, as illustrated in
Figure 1. NMT is often a large neural network that
is trained in an end-to-end fashion and has the ability to generalize well to very long word sequences.
This means the model does not have to explicitly
store gigantic phrase tables and language models
as in the case of standard MT; hence, NMT has
a small memory footprint. Lastly, implementing
NMT decoders is easy unlike the highly intricate
decoders in standard MT (Koehn et al., 2003).
  
  
  In parallel, the concept of “attention” has
gained popularity recently in training neural networks, allowing models to learn alignments between different modalities, e.g., between image
objects and agent actions in the dynamic control problem (Mnih et al., 2014), between speech
frames and text in the speech recognition task (?), or between visual features of a picture and
its text description in the image caption generation task (Xu et al., 2015). In the context of
NMT, Bahdanau et al. (2015) has successfully applied such attentional mechanism to jointly translate and align words. To the best of our knowledge, there has not been any other work exploring
the use of attention-based architectures for NMT.
  
  
  In this work, we design, with simplicity and ef-
fectiveness in mind, two novel types of attentionbased models: a global approach in which all
source words are attended and a local one whereby
only a subset of source words are considered at a
time. The former approach resembles the model
of (Bahdanau et al., 2015) but is simpler architecturally. The latter can be viewed as an interesting
blend between the hard and soft attention models
proposed in (Xu et al., 2015): it is computationally less expensive than the global model or the
soft attention; at the same time, unlike the hard attention, the local attention is differentiable almost
everywhere, making it easier to implement and
train.2 Besides, we also examine various alignment functions for our attention-based models

  Experimentally, we demonstrate that both of
our approaches are effective in the WMT translation tasks between English and German in both
directions. Our attentional models yield a boost
of up to 5.0 BLEU over non-attentional systems
which already incorporate known techniques such
as dropout. For English to German translation,
we achieve new state-of-the-art (SOTA) results
for both WMT’14 and WMT’15, outperforming
previous SOTA systems, backed by NMT models and n-gram LM rerankers, by more than 1.0
BLEU. We conduct extensive analysis to evaluate
our models in terms of learning, the ability to handle long sentences, choices of attentional architectures, alignment quality, and translation outputs.
  
  
