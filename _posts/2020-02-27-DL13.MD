---
title: "[013] Deep learning 2:"
date:   2020-02-27
excerpt: "자연어처리의 의미 Natural Language Processing (NLP)"
category: [Deep Learning]
layout: post
tag:
- Deep Learning
order: 0

comments: true
---

# 자연어와 단어의 분산 표현

# 자연어처리란 Natural Language Processing (NLP)
**자연어**: 한국어와 영어 등 우리가 평소에 쓰는 말     
**자연어 처리**: 컴퓨터가 우리 말을 알아듣게(이해하게)만드는 것 으로 **‘단어의 의미’를 이해**시키는 게 중요      
단어의 의미를 잘 파악하는 법      
* **1)** 시소러스를 활용한 기법     
* **2)** 통계 기반 기법     
* **3)** 추론 기반 기법(word2vec)     

여기에서는 1) 과 2)만 보고 [다음 포스트에서 3)](https://yerimoh.github.io/DL14/)을 보겠다!    


----


# 1) 시소러스
국어 사전과 같이 **유의어, 정의** 등을 통해 **‘단어의 의미’** 를 나타내는 방법       
'상위와 하위’ 혹은 ‘전체와 부분’ 등, 더 **세세한 관계**까지 정의해 둘 수도 있음      
**[방법]**       
**1)** 모든 단어에 대한 **유의어 집합**을 만듦    
**2)** 단어들의 관계를 **그래프**로 표현 ➡ 단어 사이의 **연결을 정의** 가능       
**3)** 이 ‘단어 네트워크’를 이용하여 컴퓨터에게 **단어 사이의 관계**를 가르칠 수 있음.

**[WordNet]**    
자연어 처리 분야에서 가장 유명

⛔ **문제점**    
* 시대 변화에 대응하기 어렵다.     
* 사람을 쓰는 비용은 크다.      
* 단어의 미묘한 차이를 표현할 수 없다.      

밑 두 기법은 대량 텍스트 데이터에서 ‘단어의 의미’ 자동 추출     



-----
----


# 2) 통계 기반 기법
**말뭉치: corpus(대량의 텍스트 데이터)**             
* ~~맹목적으로 수집된 텍스트~~ 데이터가 아닌 자연어 처리 **연구**나 애플리케이션을 **염두에 두고 수집**된 텍스트 데이터      
* 그 안에 담긴 문장들은 사람이 쓴 글      
* 자연어에 대한 사람의 ‘지식’이 충분히 담겨 있음

**목표**: 이처럼 사람의 지식으로 가득한 말뭉치에서 **자동**으로, 그리고 **효율적**으로 그 **핵심을 추출**하는 것     

---

## 말뭉치 전처리 구현
**전처리**     
**1)** 텍스트 데이터를 **단어**로 분할       
**2)** 그 분할된 단어들을 단어 **ID 목록**으로 변환하는 일      
  
  
**1️⃣ 텍스트를 단어 단위로 분할**    
ID로 변환시키기 위해 변환시킬 단어들을 쪼개준다     
쪼개기 전 대소문자 구분을 없애고 마침표가 다른 단어에 포함되지 않게 띄어준다     

<details>
<summary>👀 코드 보기</summary>
<div markdown="1">
  
```python
text = 'You say goodbye and I say hello.'

# 모든 문자를 소문자로 변환
text = text.lower() 

# 마침표도 문자에서 띄어줌
text = text.replace('.', ' .') 
# 'you say goodbye and i say hello .'

# 공백을 기준으로 분할
words = text.split(' ') 
## words ['you', 'say', 'goodbye', 'and', 'i', 'say', 'hello', '.']
```
  
</div>
</details>
  
이제 단어들을 추출했으니 ID를 추출해보자  

 
**2️⃣ 단어에 ID를 부여**   
ID의 리스트로 이용할 수 있도록 한 번 더 손질     
컴퓨터는 단어 자체로 이해를 못하기 때문에 단어마다 ID를 구분해 주는 것이다.    
* 파이썬의 **딕셔너리**를 이용하여 **단어 ID와 단어를 짝지어주는 대응표**를 작성        
* ```word _to _id```: 단어 ➡ ID 로의 변환      
   * **key**: 단어      
   * **value**:  ID    
* ```id _to _word```: ID  ➡ 단어 로의 변환   
   * **key**: ID     
   * **value**:  단어    

<details>
<summary>👀 코드, 코드 설명 보기</summary>
<div markdown="1">

```python
## 처리 문장 words ['you', 'say', 'goodbye', 'and', 'i', 'say', 'hello', '.']
#단어-> ID
word _to _id = {} 
#ID-> 단어
id _to _word = {} 
 
# 반복된 단어(say)처리
for word in words:
    if word not in word_to_id:
        new_id = len(word_to_id) 
        word_to_id[word] = new_id 
        id_to_word[new_id] = word
```

**[코드 설명]**       
단어 단위로 분할된 words의 각 원소를 처음부터 하나씩 봄    
단어가 ```word_to_id```에 들어 있지X   
➡ ```word_to_ id```와 ```id_to_word``` 각각에 새로운 ID와 단어를 추가     
* 추가 시점의 딕셔너리 길이가 새로운 단어의 ID로 설정되기 때문에 단어 ID는 0, 1, 2, … 식으로 증가      


```python
# ID와 단어의 대응표가 만들어 짐
id_to_word 
## {0:'you', 1:'say', 2:'goodbye', 3:'and', 4:'i', 5:'hello', 6:'.'} 
word_to_id
## {'you':0, 'say':1, 'goodbye':2, 'and':3, 'i':4, 'hello':5, '.':6}
```
  
</div>
</details>



**3️⃣ ‘단어 목록’을 ‘단어 ID 목록’으로 변경**       
파이썬의 **[내포 comprehension 표기](https://yerimoh.github.io//Algo2/#%EB%A6%AC%EC%8A%A4%ED%8A%B8-%EC%BB%B4%ED%94%84%EB%A6%AC%ED%97%A8%EC%85%98list-comprehension)**를 사용     
* **1)** 단어 목록에서 단어 ID 목록으로 변환      
* **2)** 다시 넘파이 배열로 변환      

➕**내포**       
리스트나 딕셔너리 등의 **반복문 처리를 간단하게** 쓰기 위한 기법.     
**(ex)**    
xs = [1, 2, 3, 4]라는 리스트의 각 원소를 2배하여 새로운 리스트를 만들고 싶다면 [x* 2 for x in xs] 처럼 쓰면 됨       


<details>
<summary>👀 코드, 코드 설명 보기</summary>
<div markdown="1">
  
```python
import numpy as np 
corpus = [word_to_id[w] for w in words] 
corpus = np.array(corpus) 
corpus array([0, 1, 2, 3, 4, 1, 5, 6])
```
  
</div>
</details>


이것으로 말뭉치를 이용하기 위한 사전 준비 완료


**4️⃣ preprocess ( )라는 함수로 구현**      
위의 1,2,3의 과정을 나타내는 함수를 구현   

<details>
<summary>👀 코드 보기</summary>
<div markdown="1">
  
```python
def preprocess(text):
  # 1️⃣
  text = text.lower() 
  text = text.replace('.', ' .') 
  words = text.split(' ')

  # 2️⃣
  word_to_id = {} 
  id_to_word = {} 
  for word in words:
    if word not in word_to_id:
       new_id = len(word_to_id) 
      word_to_id[word] = new_id 
      id_to_word[new_id] = word

# 3️⃣
corpus = np.array([word_to_id[w] for w in words])

return corpus, word_to_id, id_to_word
```
  
</div>
</details>


그럼 이제 말뭉치 전처리를 다음과 같이 수행가능 하다  
```python
text = 'You say goodbye and I say hello.'
corpus, word _to _id, id _to _word = preprocess(text)
```

----


## 단어의 분산 표현
색 표현을 고유 이름 VS [RGB (Red/Green/Blue)](https://yerimoh.github.io/C1/#rgb-%EC%83%89%EA%B3%B5%EA%B0%84)                    
* RGB 같은 벡터 표현이 색을 더 정확하게 명시

**그럼 단어도 색을 RGB처럼 표현하는 것처럼 벡터로 표현이 가능할까?**     
* ‘단어의 의미’를 정확하게 파악할 수 있는 벡터 **표현이 가능하다**        

**[백터로 표현하는 법]**

**1) ‘단어의 의미’ 추출**      
= 단어의 분산 표현 distributional representation        
단어를 **고정 길이의 밀집벡터 dense vector** 로 표현.      
* 밀집벡터: 대부분의 원소가 0이 아닌 실수인 벡터.         

**2) 분포 가설(distributional hypothesis) 사용**        
‘단어의 의미는 주변 단어에 의해 형성된다’를 나타내기 위한 기법        
* 단어 자체에는 의미가 없고, 그 단어가 사용된 **‘맥락 context ’**이 의미를 형성한다는 것     
   * **맥락**: 특정 단어를 중심에 둔 그 주변 단어    
   * **맥락의 크기(윈도우 크기 window size)**: 주변 단어를 몇 개나 포함할지 
   ![image](https://user-images.githubusercontent.com/76824611/129548326-ce2fc64f-78dc-4f9b-9257-7998d1c469ea.png)
   * 윈도우 크기 = 2
   
   
**3) 동시발생 행렬 사용**     
**분포 가설**에 기초해 **단어를 벡터로 나타내는 방법**          
**통계 기반 statistical based**: **주변 단어**를 **‘세어 보는’** 방법       
* 어떤 단어에 주목했을 때, 그 주변에 어떤 단어가 몇 번이나 등장하는지를 세어 집계하는 방법    


1️⃣ **말뭉치 전처리**


<details>
<summary>👀 코드 보기</summary>
<div markdown="1">
  
```python
# 말뭉치와 preprocess ( ) 함수를 사용
import sys
sys.path.append('..')
import numpy as np 
from common.util import preprocess

text = 'You say goodbye and I say hello.'
corpus, word _to _id, id _to _word = preprocess(text)

print(corpus)
# [0 1 2 3 4 1 5 6]

print(id _to _word) 
# {0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}
```
➡ 단어 수: 총 7개
  
</div>
</details>



2️⃣ 각 단어의 맥락에 해당하는 단어의 빈도를 세기
: 윈도우 크기는 1, 단어 ID가 0인 “you”부터 시작
: 단어 “you”의 맥락을 세어본다.
 
-> 단어 “you”의 맥락은 “say”라는 단어 하나뿐.
-> 이를 표로 정리
 
-> “you”를 [0, 1, 0, 0, 0, 0, 0 ]이라는 벡터로 표현가능



3) 동시발생 행렬 (co-occurrence matrix)
: 계속해서 ID가 1인 “say”…에 대해서도 같은 작업 수행 
: 이 표가 행렬의 형태를 띤다는 뜻에서
        
       [구현]
         
          C = np.array([ 
[0, 1, 0, 0, 0, 0, 0], 
[1, 0, 1, 0, 1, 1, 0],
[0, 1, 0, 1, 0, 0, 0], 
[0, 0, 1, 0, 1, 0, 0], 
[0, 1, 0, 1, 0, 0, 0], 
[0, 1, 0, 0, 0, 0, 1], 
[0, 0, 0, 0, 0, 1, 0], ], dtype=np.int32)


4) 각 단어의 벡터를 얻음
: 동시발생 행렬을 사용.
-> 단어를 벡터로 나타낼 수 있음.

print(C[0]) # ID 가 0 인 단어의 벡터 표현
# [0 1 0 0 0 0 0]
print(C[4]) # ID 가 4 인 단어의 벡터 표현
# [0 1 0 1 0 0 0]
print(C[word _to _id['goodbye']]) 
# "goodbye" 의 벡터 표현
# [0 1 0 1 0 0 0]


+) 말뭉치로부터 동시발생 행렬을 만들어주는 함수를 구현

# 인수들은 차례로 단어 ID의 리스트, 어휘 수, 윈도우 크기
def create _co _matrix(corpus, vocab _size, window _size=1):
corpus _size = len(corpus) 
# 먼저 co _matrix를 0으로 채워진 2차원 배열로 초기화
co _matrix = np.zeros((vocab _size, vocab _size), dtype=np.int32)

# 말뭉치의 모든 단어에 대하여 윈도우에 포함된 주변 단어 셈
for idx, word _id in enumerate(corpus):
for i in range(1, window _size + 1):
left _idx = idx - i 
right _idx = idx + i

# 말뭉치의 왼쪽 끝, 오른쪽 끝 경계를 벗어나지 않는지 확인
if left _idx >= 0:
left _word _id = corpus[left _idx] 
co _matrix[word_id, left_word _id] += 1

if right _idx < corpus _size:
right _word _id = corpus[right _idx] 
co_matrix[word_id, right_word_id] += 1

return co _matrix


- 벡터 간 유사도
: 계속해서 벡터 사이의 유사도를 측정하는 방법
: 벡터 사이의 유사도를 측정하는 방법은 다양.

: 코사인 유사도(cosine similarity)
단어 벡터의 유사도를 나타낼 때 자주 이용
두 벡터 x = (x 1 , x 2 , x 3 , …, x n )과
        y = (y 1 , y 2 , y 3 , …, y n )이 있다면, 
코사인 유사도는 다음 식으로 정의
 .
-> 분자_ 벡터의 내적
   분모_ 각 벡터의 노름(norm) 
노름은 벡터의 크기를 나타낸 것
여기에서는 ‘L2 노름’을 계산
-> 벡터를 정규화하고 내적을 구하는 것입니다.
-> 코사인 유사도를 직관적으로 풀면 
‘두 벡터가 가리키는 방향이 얼마나 비슷한가’
 두벡터의 방향이 완전히 같다면 코사인 유사도가 1
 완전히 반대라면 -1이 됩니다.

    [구현]

    def cos _similarity(x, y):
# x 의 정규화
nx = x / np.sqrt(np.sum(x**2)) 
# y 의 정규화
ny = y / np.sqrt(np.sum(y**2)) 

return np.dot(nx, ny)

[PROBLEM] 
‘0으로 나누기 divide by zero ’ 오류
[S] (+eps) eps(엡실론 epsilon)의 인수를 받아 더함
인수의 값을 지정하지 않으면 기본값으로 1e-8 (=0.00000001 )

def cos _similarity(x, y, eps=1e-8):
nx = x / (np.sqrt(np.sum(x ** 2)) + eps) 
ny = y / (np.sqrt(np.sum(y ** 2)) + eps) 

return np.dot(nx, ny)

+ 작은 값으로 1e-8을 사용_ 이 정도 작은 값이면 일반적으로 부동소수점 계산 시 ‘반올림’되어 다른 값에 ‘흡수.
 앞의 구현에서는 이 값이 벡터의 노름에 ‘흡수’되기 때문에 대부분의 경우 eps를 더한다고 해서 최종 계산 결과에는 영향X

[단어 벡터의 유사도 구현] 
:“you”와 “i (=I )”의 유사도를 구하는 코드

import sys
sys.path.append('..') 
from common.util import preprocess, create _co _matrix, cos _similarity

      text = 'You say goodbye and I say hello.'
corpus, word _to _id, id _to _word = preprocess(text) 
vocab _size = len(word _to _id) 
C = create _co _matrix(corpus, vocab _size)

c0 = C[word _to _id['you']] # "you" 의 단어 벡터
c1 = C[word _to _id['i']] # "i" 의 단어 벡터

print(cos _similarity(c0, c1)) # 0.7071067691154799


     -> 유사도 앵간 높음
- 유사 단어의 랭킹 표시
 
[구현]

def most_similar(query, word_to_id, id_to_word, word_matrix, top=5):

# ❶ 검색어의 단어 벡터를 꺼낸다 
if query not in word _to _id:
print('%s( 을 ) 를 찾을 수 없습니다 .' % query) 
return

print('\n[query] ' + query) 
query_id = word_to_id[query] 
query_vec = word_matrix[query_id]

# ❷ 코사인 유사도 계산
# 검색어의 단어 벡터와 다른 모든 단어 벡터와의 코사인 유
사도를 각각 구한다.
vocab _size = len(id _to _word) 
similarity = np.zeros(vocab _size) 
for i in range(vocab _size):
similarity[i] = cos_similarity(word _matrix[i], query_vec)

# ❸ 코사인 유사도를 기준으로 내림차순으로 출력
count = 0 
for i in (-1 * similarity).argsort():
if id _to _word[i] == query:
continue 
print(' %s: %s' % (id _to _word[i], similarity[i]))

count += 1 

if count >= top:
return
.
❸(1) similarity 배열에 담긴 원소의 인덱스를 내림차순으로 
정렬 
(2) 상위 원소들을 출력.

-> 이때 배열 인덱스의 정렬을 바꾸는데 사용한 argsort ( ) 
메서드는 넘파이 배열의 원소를 오름차순으로 정렬
(단, 반환값은 배열의 인덱스). 

>>> x = np.array([100, -20, 2]) 
# 작은 순서
>>> x.argsort() array([1, 2, 0])
# 큰 순서
>>> (-x).argsort() array([0, 2, 1])


[함수 사용]
:“you”를 검색어로 지정해 유사한 단어들을 출력 

import sys
sys.path.append('..') from common.util import preprocess, 
create_co_matrix, most _similar

text = 'You say goodbye and I say hello.'
corpus, word _to _id, id _to _word = preprocess(text) 
vocab _size = len(word _to _id) 
C = create _co _matrix(corpus, vocab _size)

most _similar('you', word _to _id, id _to _word, C, top=5)
결과

[query] you goodbye: 0.7071067691154799
i: 0.7071067691154799
hello: 0.7071067691154799
say: 0.0
and: 0.0



- 통계기반 기법 개선

상호정보량
동시발생 행렬의 원소_ 두 단어가 동시에 발생한 횟수 
[PROBLEM] ‘발생’ 횟수라는 것은 좋은 특징X
[SOLVE] 점별 상호정보량 (Pointwise Mutual Information)(PMI). 
: 확률 변수 x와 y에 대해 다음 식으로 정의
 
-> P (x )는 x가 일어날 확률
P (y )는 y가 일어날 확률
P (x, y )는 x와 y가 동시에 일어날 확률
-> 이 PMI 값이 높을수록 관련성이 높다는 의미
-> (적용) P (x )는 단어 x가 말뭉치에 등장할 확률
     
-> C는 동시발생 행렬
 C (x, y )는 단어 x와 y가 동시발생하는 횟수
C (x )와 C (y )는 각각 단어 x와 y의 등장 횟수
말뭉치에 포함된 단어 수를 N

[EX]말뭉치의 단어 수(N )를 10,000이라 하고,
 “the”와 “car”와 “drive”가 각 1,000번, 20번, 10번 등장
 “the”와 “car”의 동시발생 수는 10회, 
“car” 와 “drive”의 동시발생 수는 5회
-> 동시발생 횟수 관점_ “car”는 “drive”보다 “the”와 관
련이 깊다
   PMI 관점_ PMI를 이용하면 “car”는 “the”보다 “drive”
와의 관련성이 강해짐. 
 -> 단어가 단독으로 출현하는 횟수가 고려 되었기 때문.

[PROBLEM]
바로 두 단어의 동시발생 횟수가 0이면 log_2 0 = -∞
[SOLVE] 
양의 상호정보량 Positive PMI (PPMI)사용
 
-> PMI가 음수일 때는 0으로 취급.

[구현]
동시발생 행렬을 PPMI 행렬로 변환하는 함수.

def ppmi(C, verbose=False, eps=1e-8):
M = np.zeros _like(C, dtype=np.float32) 
N = np.sum(C) 
S = np.sum(C, axis=0)
total = C.shape[0] * C.shape[1] 
cnt = 0

for i in range(C.shape[0]):
for j in range(C.shape[1]):
pmi = np.log2(C[i, j] * N / (S[j]*S[i]) + eps) 
M[i, j] = max(0, pmi)

if verbose:
cnt += 1  
if cnt % (total//100) == 0:
print('%.1f%% 완료 ' % (100*cnt/total)) 
return M

 -> 인수 C는 동시발생 행렬
verbose는 진행상황 출력 여부를 결정하는 플래그
큰 말뭉치를 다룰 때 verbose=True로 설정하면 중간중간 진
행 상황을 알려줌 
-> 이 코드는 동시발생 행렬에 대해서만 PPMI 행렬을 구할 수 
있도록 하고자 단순화해 구현. 
-> 단어 x와 y가 동시에 발생하는 횟수를 C (x, y )라 했을 때,
  (근삿값 구함)구현
[구현]
: 동시발생 행렬을 PPMI 행렬로 변환해보죠. 

import sys
sys.path.append('..')
import numpy as np from common.util import preprocess, 
create _co _matrix, cos _similarity, ppmi

text = 'You say goodbye and I say hello.'
corpus, word _to _id, id _to _word = preprocess(text) 
vocab _size = len(word _to _id) 
C = create _co _matrix(corpus, vocab _size) 
W = ppmi(C)

# 유효 자릿수를 세 자리로 표시
np.set _printoptions(precision=3) 
print(' 동시발생 행렬 ') 
print(C) 
print('-'*50) 
print('PPMI') 
print(W)

  결과
동시발생 행렬
[[0 1 0 0 0 0 0]
 [1 0 1 0 1 1 0]
 [0 1 0 1 0 0 0]
 [0 0 1 0 1 0 0]
 [0 1 0 1 0 0 0]
 [0 1 0 0 0 0 1]
 [0 0 0 0 0 1 0]] 
--------------------------------------------------
PPMI 
[[ 0. 1.807 0. 0. 0. 0. 0. ]
 [ 1.807 0. 0.807 0. 0.807 0.807 0. ]
 [ 0. 0.807 0. 1.807 0. 0. 0. ]
 [ 0. 0. 1.807 0. 1.807 0. 0. ]
 [ 0. 0.807 0. 1.807 0. 0. 0. ]
 [ 0. 0.807 0. 0. 0. 0. 2.807]
 [ 0. 0. 0. 0. 0. 2.807 0. ]]


[PROBLEM]
1) 말뭉치의 어휘 수가 증가함에 따라 각 단어 벡터의 차원 수
도 증가 
EX) 말뭉치의 어휘 수가 10만 = 그 벡터의 차원 수 10만.
-> 비현실적
2) 원소 대부분이 0 
-> 벡터의 원소 대부 분이 중요하지 않음
-> 각 원소의 ‘중요도’가 낮다
-> 이런 벡터는 노이즈에 약하고 견고하지 못함 

[SOLVE] 벡터의 차원 감소
차원 감소(dimensionality reduction)
벡터의 차원을 줄이는 방법
핵심_‘중요한 정보’는 최대한 유지하면서 줄임
[EX]
데이터의 분포를 고려해 중요한 ‘축’을 찾는 일을 수행.
 
-> 2차원 데이터를 1차원으로 표현하기 위해 중요한 축(데이터
를 넓게 분포시키는 축)을 찾는다.
-> 왼쪽은 데이터점들을 2차원 좌표에 표시 모습. 
-> 오른쪽은 새로운 축을 도입해 똑같은 데이터를 좌표축 하나
만으로 표시
(새로운 축을 찾을 때는 데이터가 넓게 분포되도록 고려필요).
 -> 각 데이터점의 값은 새로운 축으로 사영된 값으로 변함.
   중요 가장 적합한 축을 찾아내는 일로, 1차원 값만으로도 데이터의 본질적인 차이를 구별할 수 있어야 함

+ 희소행렬(sparse matrix), 희소벡터(sparse vector)
원소 대부분이 0인 행렬 또는 벡터
차원 감소의 핵심_ 희소벡터에서 중요한 축을 찾아내 더 적은 
차원으로 다시 표현
-> 차원 감소의 결과_ 원래의 희소벡터는 원소 대부분이 0이 아
닌 값으로 구성된 ‘밀집벡터’로 변환.
-> 이 조밀한 벡터_ 우리가 원하는 단어의 분산 표현.

[SOLVE 1] 특잇값분해 Singular Value Decomposition (SVD)
: 임의의 행렬을 세 행렬의 곱으로 분해
X = USV^T
- 임의의 행렬 X를 U, S, V라는 세 행렬의 곱으로 분해
- U와 V는 직교행렬(orthogonal matrix) 
 그 열벡터는 서로 직교. 
 - S는 대각행렬(diagonal matrix)_대각성분 외는 다 0인행렬
  
- 행렬의 ‘흰 부분’=원소가 0
- U 직교행렬_어떠한 공간의 축(기저)을 형성.
   지금 우리의 맥락에서는 이 U 행렬을 ‘단어 공간’
- S 대각 행렬_ 그 성분에 ‘특잇값 singular value ’이 큰 순
서로 나열됨. 
특잇값: 쉽게 말해 ‘해당 축’의 중요도.
-> 중요도가 낮은 원소(특잇값이 작은 원소)를 깎아내는 방법.
 

- 행렬 S에서 특잇값이 작다면 중요도가 낮다는 뜻
-> 행렬 U에서 여분의 열벡터를 깎아내 원래의 행렬 근사가능

<단어의 PPMI 행렬에 적용>
행렬 X의 각 행: 해당 단어 ID의 단어 벡터가 저장
행렬 U: 그 단어 벡터가 차원 감소된 벡터로 표현되는 것
-> 문헌 [20] 참고
[구현]
SVD는 넘파이의 linalg 모듈의 svd메서드로 실행 가능 
-> “linalg”는 선형대수 linear algebra 의 약어

import sys
sys.path.append('..')
import numpy as np 
import matplotlib.pyplot as plt
 from common.util import preprocess, create _co _matrix, ppmi

text = 'You say goodbye and I say hello.'
corpus, word _to _id, id _to _word = preprocess(text) 
vocab _size = len(id _to _word) 
C = create _co _matrix(corpus, vocab _size, window _size=1) 
W = ppmi(C)

# SVD 
U, S, V = np.linalg.svd(W)

-> SVD에 의해 변환된 밀집벡터 표현은 변수 U에 저장 

[실제 구현] 
: 단어 ID가 0인 단어 벡터를 보겠습니다.

print(C[0]) # 동시발생 행렬
# [0 1 0 0 0 0 0]

print(W[0]) # PPMI 행렬
# [ 0. 1.807 0. 0. 0. 0. 0. ]

print(U[0]) # SVD 
#[ 3.409e-01 -1.110e-16 -1.205e-01 -4.441e-16 0.000e+00 -9.323e-01 2.226e-16]

->  희소벡터인 W[0 ]가 SVD에 의해서 밀집벡터 U[0 ]로 변함
-> 그리고 이 밀집벡터의 차원을 감소시키려면, 
EX) 2차원 벡터로 줄이려면 처음의 두원소를 꺼내면 됩니다.

print(U[0, :2]) 
# [ 3.409e-01 -1.110e-16]

#각 단어를 2차원 벡터로 표현한 후 그래프로 그리기.
for word, word _id in word _to _id.items():
# plt.annotate (word, x,y) 메서드는 2차원 그래프상에서 
# 좌표 (x, y ) 지점에 word에 담긴 텍스트를 그림
plt.annotate(word, (U[word _id, 0], U[word _id, 1]))

plt.scatter(U[:,0], U[:,1], alpha=0.5) 
plt.show()
. 

-> 동시발생 행렬에 SVD를 적용한 후, 각 단어를 2차원 벡터로 
변환해 그린 그래프(“i”와 “goodbye”가 겹쳐 있음)
 

[WARNING] 행렬의 크기가 N이면 SVD 계산은 O(N 3 ). 
계산량이 N의 3 제곱에 비례해 늘어남.
-> 이는 현실적으로 감당하기 어려운 수준이므로 Truncated SVD  
같은 더 빠른 기법을 이용.
Truncated SVD는 특잇값이 작은 것은 버리는 truncated 방식으로 
성능 향상을 꾀함. 
사이킷런 scikit-learn 라이브러리의 Truncated SVD를 이용.
PTB 데이터셋
> 펜 트리뱅크 Penn Treebank (PTB)
: 지금까지 사용한 것 보다 큰 말뭉치
 말뭉치(텍스트 파일)의 예
  
-> PTB 말뭉치에서는 한 문장이 하나의 줄로 저장
-> 각 문장을 연결한 ‘하나의 큰 시계열 데이터’로 취급
-> 이때 각 문장 끝에 <eos>라는 특수 문자를 삽입
(“eos”는 “end of sentence”의 약어).
-> 이 책에서는 문장의 구분을 고려x,
 여러 문장을 연결한 ‘하나의 큰 시계열 데이터’로 간주
 즉, 문장단위로 안보고 단어단위로 봄
  -> 이 코드는 dataset/ptb.py 파일에 담겨 있음

[구현]

import sys
sys.path.append('..') 
from dataset import ptb

# ptb.load _data ( )는 데이터를 읽어들임
#  인수로 ‘train’, ‘test’,‘valid’ 중 하나 지정 가능
# 차례대로 ‘훈련용’, ‘테스트용’, ‘검증용’ 데이터
corpus, word _to _id, id _to _word = ptb.load _data('train')

print(' 말뭉치 크기 :', len(corpus)) 
print('corpus[:30]:', corpus[:30]) 
print() 
print('id _to _word[0]:', id _to _word[0]) 
print('id _to _word[1]:', id _to _word[1]) 
print('id _to _word[2]:', id _to _word[2]) 
print() 
print("word _to _id['car']:", word _to _id['car']) 
print("word _to _id['happy']:", word _to _id['happy']) 
print("word _to _id['lexus']:", word _to _id['lexus'])


[결과] 

corpus size: 929589
corpus[:30]: [ 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22
23 24 25 26 27 28 29]

id _to _word[0]: aer 
id _to _word[1]: banknote 
id _to _word[2]: berlitz

word _to _id['car']: 3856
word _to _id['happy']: 4428
word _to _id['lexus']: 7426





[PTB 데이터셋 평가]
: PTB 데이터셋에 통계 기반 기법을 적용.
-> 큰 행렬에 SVD를 적용해야 하므로 고속 SVD를 이용
sklearn 모듈을 설치 필요
물론 간단한 SVD (np.linalg.svd ( ))도 사용 가능
But,시간, 메모리 비효율적


import sys
sys.path.append('..')
import numpy as np 
from common.util import most_similar, create_co_matrix, ppmi 
from dataset import ptb

window _size = 2
wordvec _size = 100

corpus, word _to _id, id _to _word = ptb.load _data('train') 
vocab _size = len(word _to _id) 
print(' 동시발생 수 계산 ...') 
C = create _co _matrix(corpus, vocab _size, window _size) 
print('PPMI 계산 ...') 
W = ppmi(C, verbose=True)

print('SVD 계산 ...') 
#SVD 수행하는 데 sklearn의 randomized _svd () 메서드 이용
try:
# truncated SVD ( 빠르다 !) 
from sklearn.utils.extmath import randomized _svd 
U, S, V = randomized _svd(W, n _components=wordvec 
_size, n _iter=5, random _state=None)

except ImportError:
# SVD ( 느리다 ) 
U, S, V = np.linalg.svd(W)

word _vecs = U[:, :wordvec _size]

querys = ['you', 'year', 'car', 'toyota'] 
for query in querys:
most _similar(query, word _to _id, id _to _word, 
word _vecs, top=5)

+ sklearn의 randomized_svd ( ) 메서드
: 무작위 수를 사용한 Truncated SVD 
-> 특잇값이 큰 것들만 계산하여 기본적인 SVD보다 훨씬 빠름.  
-> Truncated SVD는 무작위 수를 사용하므로 결과가 매번 다름.
[결과]
    
  - “you”라는 검색어에서는 인칭대명사인 “i”와 “we”가 상
위를 차지
    -> 영어 문장에서 관용적으로 자주 같이 나오는 단어들이기 
때문 
- “year”의 연관어로는 “month”와 “quarter”
  “car”의 연관어로는 “auto”와 “vehicle” 등
-  “toyota”와 관련된 단어 “nissan”, “honda”, “lexus” 
등 자동차 제조업체나 브랜드가 뽑힌 것도 확인
-> 이처럼 단어의 의미 혹은 문법적인 관점에서 비슷한 단
어들이 가까운 벡터로 나타남.
 우리의 직관과 비슷한 결과. 

















































