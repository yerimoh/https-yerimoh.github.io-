---
title: "[32] CS2241N: Lecture 2 Word Vectors 2 and Word Window Classification 정리"
date:   2019-12-29
excerpt: "Lecture 2 | Word Vectors 2 and Word Window Classification 요약"  
category: [Deep Learning]
layout: post
tag:
- Deep Learning
order: 0

comments: true
---


# 목차
- [**INTRO**](#--intro--)
- [**Word2vec2**](#--word2vec2--)
  * [negative sampling](#negative-sampling)
  * [aside](#aside)
  * [co-occurrence matrix](#co-occurrence-matrix)
- [**GLOVE**](#--glove--)
  * [evaluate word vectors](#evaluate-word-vectors)
    + [Intrinsic](#intrinsic)
    + [Extrinsic](#extrinsic)
- [**Word senses and word sense ambiguity**](#--word-senses-and-word-sense-ambiguity--)





-----

# **INTRO**

**[Review: Main idea of word2vec]**   
* [word2vec 강의 정리 보러가기](https://yerimoh.github.io/DLCS31/)       
* random word vectors로 시작한다.     
* 단어에 대한 단어 벡터와 문맥벡터 사이의 내적의 관점에서 아래 확률분포 식으로 이를 수행        
![image](https://user-images.githubusercontent.com/76824611/179308881-20a77774-79f5-44ce-9eac-4e172297e873.png)
![image](https://user-images.githubusercontent.com/76824611/179224320-f4af78e2-281e-4ef9-92be-a9e25e80aaa9.png)
* **학습**      
   * 벡터를 업데이트하여 주변 단어 예측을 더 잘하게 한다.        
   * 이러한 학습을 통해 벡터는 고차원에서 유사성이 높은 단어들과 가까이 있고 의미있는 방향을 갖게된다.       


<details>
<summary>👀 강의 슬라이드 보기</summary>
<div markdown="1">
  
![image](https://user-images.githubusercontent.com/76824611/178133425-7ff3b02a-118f-451a-8636-cded1f948e5c.png)

  
</div>
</details>

------

# **Word2vec2**    
Word2vec2는 Word2vec의 단점을 보완한 것으로 크게 2가지 보완점을 갖고 나온 것이다.    
* negative sampling    
* [embedding](https://yerimoh.github.io/DL15/#1%EF%B8%8F%E2%83%A3-embedding-%EA%B3%84%EC%B8%B5-%EB%8F%84%EC%9E%85)     

그런데 강의에는 이 두가지 보완점 중 negative sampling만 설명한다고 되어있어 내가 따로 정리해둔 embedding 포스팅 링크를 추거해뒀다.     
궁금하면 읽어보면 된다.     


**[Word2vec parameters의 특징]**     
* Word2vec의 유일한 매개변수(parameters)는 단어벡터이다.          
* Word2vec는 “Bag of words” model 이다.      
    * Bag of words: 단어의 순서와 위치를 고려하지 않는다.      
    * 즉, 문맥안에 있는 단어들은 모두 같은 취급을 받는다는 것이다.            
* 각 단어에 대해 외부단어 벡터(outside), 중앙단어 벡터가 있다.     
![image](https://user-images.githubusercontent.com/76824611/179228387-1b9f4383-004e-4a42-95a0-448ecff23ace.png)



**[Word2vec parameters의 계산과정]**        
* **1)** 특정 외부 단어가 중심 단어와 함께 발생할 가능성에 대한 점수를 얻기 위해 target 벡터와 context 벡터를 내적을 취한다.   
* **2)** 이 정수를 확률로 변환하기 위해서 softmax를 사용한다.     
![image](https://user-images.githubusercontent.com/76824611/179228402-5b4df729-d505-4d4d-a28f-d519af32a110.png)





**[Word2vec in space]**     
* 학습 결과 비슷한 단어끼리 고차원 공간에 가깝게 배치되어있다.     
* 이 고차원 공간은 우리가 아는 2차원과 다르게 생겼다.       
![image](https://user-images.githubusercontent.com/76824611/179245067-928be223-a29b-45b4-81e1-11d09e5588e7.png)



**[Optimization: (Stochastic) Gradient Descent]**     
* **Gradient Descent**: 모든 벡터를 한꺼번에 업데이트   
  * 느림       
  * 우리가 앞에서 배웠던 $$J(θ)$$가 GD임               
* **Stochastic Gradient Descent**: 말뭉치를 배치단위로 쪼개 배치단위로 업데이트    
  * 빠름      
  * 이걸 대부분 많이 씀      
* **Optimization 과정**       
  * 1) 무작위(0에 가까운 숫자들)로 단어벡터의 가중치 초기화(randon initial value)        
  * 2) 지난시간에 정의한 손실함수 $$J(θ)$$로 손실을 계산한 다음 손실값이 낮은 곳으로 기울기 하강        
* 이 최적화의 아이디어는 현재 위치에서 현개 기울기를 계산한 다음 음의 기울기 방향으로 **작은 걸음**을 내딛음      
* **step size(learning step)**: 작은 걸음의 보폭        
  * 이 사이즈가 너무 크면 왔다갔다함     
  * 이 사이즈가 너무 작으면 학습이 오래걸림      
  * 그러므로 적절함 사이즈로 조정하는것이 중요함      
* 위와 같이 최적화하여 손실값이 낮은(minimum) 좋은 단어 벡터를 만드는것이 목표     
![image](https://user-images.githubusercontent.com/76824611/179247632-1e48b880-f72e-4ff7-b15a-34a5c1f5914e.png)




**[Word2vec algorithm family: More details]**     
* 왜 **2개**의 벡터인가?(왜 더 많은 벡터를 보지 않고 2개의 벡터만 보고 계산에 포함시키는가?)     
   * 우리가 보기로한 벡터들은 벡터끼리 내적을 수행해야하는데 2개를 넘어가면 **계산**이 너무 **복잡**해져서 오히려 더 비효율적이다.     
   * 특히 분모를 계산하는게 너무 cost가 높다.     
* **Word2vec의 모델**     
   * Word2vec는 2개의 모델을 아우르는 표현이다.      
   * **1) [Skip-grams (SG)](https://yerimoh.github.io/DL14/#skip-gram-%EB%AA%A8%EB%8D%B8)**: target단어로 context단어 예측(성능이 좋아서 더 많이 씀)(이 강의에서 지금까지 배운 것)     
   * **2) [Continuous Bag of Words (CBOW)](https://yerimoh.github.io/DL14/#cbow-%EB%AA%A8%EB%8D%B8)**: context단어로 target단어 예측            
    



<details>
<summary>👀 강의 슬라이드 보기</summary>
<div markdown="1">
  
![image](https://user-images.githubusercontent.com/76824611/179227583-15fc6e6a-2fda-467d-b469-28e3cc130a75.png)
![image](https://user-images.githubusercontent.com/76824611/179245132-c72cabbc-eed4-4349-8554-46841879b7e2.png)
![image](https://user-images.githubusercontent.com/76824611/179249318-5861b727-f8b9-470e-8a5b-d6afed0ed40f.png)
![image](https://user-images.githubusercontent.com/76824611/179249359-6af99138-237d-49e7-8ec2-f7e1b80894d7.png)

  
</div>
</details>


-----


## negative sampling
Word2vec를 더 최적화하기 위한 방법 중 하나이다.      
이 강의에선 skip-gram의 negative sampling (SGNS)위주로 설명하겠다.        


**[기존 skip-gram의 문제]**       
* 아래 skip-gram의 식을 보면 분모가 모든 단어의 내적을 수행해야하기 때문에 계산이 너무 비싸다.      
* 그러므로 모든 내적의 평균을 내는 ```softmax```대신 ```negative sampling```을 수행해야한다.    
![image](https://user-images.githubusercontent.com/76824611/179309792-e3710d21-b389-45fc-a410-6bf714975d8a.png)

<details>
<summary>📜 more about Softmax classifier 보기</summary>
<div markdown="1">
  
![image](https://user-images.githubusercontent.com/76824611/179307890-24828139-7cc1-4293-8f02-89618def92a5.png)
  
</div>
</details>  



**[negative sampling: $$maximize$$]**     
* 여러 "noise" 쌍(random 단어와 짝을 이루는 center 단어) 대비 true 쌍(center 단어와 context창의 단어)에 대한 이항 로지스틱 회귀 분석 훈련
* Overall objective function($$maximize$$)         
   * $$log𝜎(u_{o}^{T} v_{c})$$: 정답 쌍의 내적을 통해 두 단어가 동시에 발생할 확률을 최대화      
   * $$ \sum_{i=1}^{k} E_{j~P(w)}[log𝜎(-u_{j}^{T} v_{c}] $$: 정답이 아닌 오답값 중에서 랜덤으로 선택하여 오답값을 학습시켜 단어의 확률을 최소화, 즉 소수의 무작위의 단어에 대해 계산한다.(무작위 단어를 샘플링하는 비율은 확률에 따라 다르다)         
![image](https://user-images.githubusercontent.com/76824611/179262441-47905dd3-b358-4471-bf6d-cbe0c635a155.png)
* 여기서 softmax 대신 **sigmoid(𝜎)** 를 사용한다      
   * sigmoid = logisitc 함수 이다.        
   * 이걸 통해서 중심단어와 무작위로 고른 단어 사이의 내적을 구한다.    
   * 이 sigmoid를 통하면 이 내적값이 크면 사실상 1, 아니면 0이 된다.    
   * 이해가 안간다면? [더 알아보기](https://yerimoh.github.io/DL15/#%EB%84%A4%EA%B1%B0%ED%8B%B0%EB%B8%8C-%EC%83%98%ED%94%8C%EB%A7%81)  
![image](https://user-images.githubusercontent.com/76824611/179262929-1a22dcce-5b2a-446d-8b20-07a75ace8a51.png)

<details>
<summary>📜 more about binary logistic regression 보기</summary>
<div markdown="1">
  
 
![image](https://user-images.githubusercontent.com/76824611/179308149-8752a9c2-382d-4955-81f4-0a2bae73e569.png)
  ![image](https://user-images.githubusercontent.com/76824611/179308178-04ffbf75-6710-4f42-a4d1-b13472b21588.png)
![image](https://user-images.githubusercontent.com/76824611/179308196-d712f638-28c5-4cc5-89ae-c52fdbcdcbc8.png)
![image](https://user-images.githubusercontent.com/76824611/179308214-c9d6a242-b586-4fc3-8091-c03849be5819.png)

  
</div>
</details>  


**[negative sampling: $$minimize$$]**     
* 이 강의에서 내주는 과제를 위해 위의 식을 조금 더 분석해봤다.       
   * 이 식은 음의 로그 가능성을 최소화 하는 것으로 볼 수 있다.    
   * 즉 위의 식에다 -를 붙이고 이를 최소화 하는 것이다.       
   * 각 식에 대한 설명은 위와 같다.(대신 거기에 음을 취한다는 것만 다르다)        
   * $$k$$:negative samples (using word probabilities)      
![image](https://user-images.githubusercontent.com/76824611/179264744-a8a232f6-d5d4-4b2d-8324-c2521c14fc40.png)
* 실제 **외부 단어**가 나타날 **확률** **최대화**, 중심 단어 주변에 임의 단어가 나타날 확률 최소화     
➡ 이를 통해 주변 단어에만 국한된 학습이 아닌 좀 더 전체를 고려한 학습이 된다.     
* $$P(w)= U(w)^{3/4}/Z$$의 확률로 부정적인 예를 샘플링한다.     
    * 정답이 아닌 샘플(negative sample)을 무작위로 샘플링 할 때 단어들의 출변 빈도로 샘플링할 단어를 뽑는 것이 원칙이다.    
그런데 말뭉치 출현 빈도가 낮은 정답이 아닌 샘플은 선택될 확률이 낮다.      
그러므로 기본 확률($$U(w)$$)에 $$3/4$$을 제곱하여 **낮은 빈도의 단어가 선택될 확률을 높이는 것**이다.    
    * 이해가 안간다면? [더 알아보기](https://yerimoh.github.io/DL15/#%EB%84%A4%EA%B1%B0%ED%8B%B0%EB%B8%8C-%EC%83%98%ED%94%8C%EB%A7%81%EC%9D%98-%EC%83%98%ED%94%8C%EB%A7%81-%EA%B8%B0%EB%B2%95)     





**[negative sampling의 장점]**    
* softmax보다 효율적   
  * softmax는 내적이 많은데 모든 단어를 모두 내적하려면 계산비용, 시간이 상당하다.      
  * 반면, negative sampling은 모든 것을 계산하는 것이 아닌 정답값과 정답이 아닌 값을 각각 이진(정답이다, 정답이 아니다)분류하여 계산하기 때문에 확률을 구하는 softmax보다 효율적이다.         
  ![image](https://user-images.githubusercontent.com/76824611/179268880-eab2ddcc-19d2-4f7c-9718-ac25f0dc7a58.png)  
* context 범위에 없는 단어를 negative sampling하여 context에만 국한된 학습에서 벗어날 수 있다.        



<details>
<summary>👀 강의 슬라이드 보기</summary>
<div markdown="1">
  
![image](https://user-images.githubusercontent.com/76824611/179269649-c93edcaf-ecf1-4346-87cc-b4fed3ca7e40.png)
![image](https://user-images.githubusercontent.com/76824611/179269686-024c9965-91d1-4893-9066-5033375ac064.png)
![image](https://user-images.githubusercontent.com/76824611/179269728-fe22b264-dc83-45d8-8dfb-9fabb62293d0.png)

  
</div>
</details>


------


## aside 
word2vec에서 추가 지식(TMI)을 쌓아보자!


**[Stochastic gradients with negative sampling]**      
* 우리는 SGD에 대해 각 창에서 그레이디언트를 반복적으로 측정한다.    
* 그런데 매 step마다 업데이트 되는 벡터는 매우매우 희소하다.      
![image](https://user-images.githubusercontent.com/76824611/179284710-2b93fb2c-cff2-421f-a7a7-f311687629c5.png)


**[벡터의 표현]**      
* 현재는 계속 단어를 열벡터로 표현했지만, 사실상 직접 컴퓨터로 계산한때는 행(row)벡터로 표현된다.    
* 행으로 표현하면 연속으로 계산이 가능하여 메모리를 효율적으로 사용 가능하기 떄문이다.         
![image](https://user-images.githubusercontent.com/76824611/179309741-dd60c069-e534-433b-b1fb-f1a17dd73108.png)


<details>
<summary>📜 nore about neural word vectors 보기</summary>
<div markdown="1">
 
![image](https://user-images.githubusercontent.com/76824611/179308031-625cb572-7a0b-446c-a008-54431fa610d3.png)
  
  
</div>
</details>  


<details>
<summary>👀 강의 슬라이드 보기</summary>
<div markdown="1">
  
![image](https://user-images.githubusercontent.com/76824611/179290962-79e16661-7f54-4ef3-961a-afaf8b610152.png)
![image](https://user-images.githubusercontent.com/76824611/179290976-d2cbfcf0-71fa-454f-9f19-81938e6c79f1.png)
![image](https://user-images.githubusercontent.com/76824611/179291005-ca7138a6-8bd1-4ef8-9305-d382e4d42416.png)

  
</div>
</details>


-----

## co-occurrence matrix
그렇다면 왜 단어 벡터를 표현할 때 동시발생 행렬(co-occurrence matrix)을 **사용하지 않는가**?      

동시발생행렬(co-occurrence matrix)은 통계기반 기법이다.    

* **통계기반 기법**      
   * 한단어를 말뭉치 전체를 이용하여 표현     
   * 말뭉치가 조금 수정되도 모든 단어를 업데이트 해야함    
   * 비효율적       
   * [PMI](https://yerimoh.github.io/DL13/#%EC%83%81%ED%98%B8%EC%A0%95%EB%B3%B4%EB%9F%89-pmi),**SVD**등을 통해서 차원 축소를 통해 필요없는 단어 표현 정보를 없애면서 성능 개선    
* **추론기반 기법**      
   * 한 단어를 표현할 때 말뭉치 전체가 아닌 몇개의 단어로 추론하여 단어 표현    
   * 말뭉치가 조금 수정되어도 이에 영향을 받는 몇몇 단어 벡터만 수정하면 됨     
   * 비교적 효율적       



**[동시발생행렬(co-occurrence matrix) 예시]**     
* 각 표의 칸의 숫자의 의미는 그 단어가 나오는 횟수이다.    
* Window length 1 (more common: 5–10)    
* Symmetric (irrelevant whether left or right context)     
* Example corpus:    
   * I like deep learning  
   * I like NLP   
   * I enjoy flying     

![image](https://user-images.githubusercontent.com/76824611/179309696-fa8b8643-8a0e-4a50-8488-7c50663ab07a.png)


**[동시발생행렬의 문제점]**      
* **단순히 개수를 센 벡터의 문제**      
   * 벡터가 말뭉치 전체로 표현되기 때문에 어휘에 따라 크기가 커진다.       
   * 매우 높은 dimensional: 많은 저장소 필요      
   * 후속 분류 모델은 희소성 문제가 있다 → 모델은 덜 견고하다.        
   * 차원이 낮아 모델이 결고하지 못하다.           
* **해결책**       
   * 중요한 정보의 "대부분"을 소수의 고정된 정보에 저장 차원: **밀집 벡터**    
   * 이렇게 기존 벡터를 밀집 벡터를 벡터를 통해 표현하면 word2vec와 비슷해진다.(Usually 25–1000 dimensions)      



**[해결책: SVD]**    
* 위 문제의 해결 방법으로 차원이 낮은 단순 벡터를 밀집 벡터로 만드는 것이다.      
* 원래 x를 아래와 같이 3개로 나눠 표현한다.      
  * $$𝛴$$는 높은 수(중요한 수, 빈도가 높은 수)로 정렬되어있어 더 높은 밀집도의 벡터를 얻고싶으면 이 행렬의 아래(파란색)부터 지워 중요한 정보만 나타내게하면 된다.     
  * 노란색 부분이 없어지며 무시된다.       
* 이해가 안된다면? [더 알아보기](https://yerimoh.github.io/DL13.3/) ➡ 같은 LSA와 같은 맥락이다.        
![image](https://user-images.githubusercontent.com/76824611/179289682-6faa51bb-ded2-46a5-a769-018f025b3c67.png)


**[SVD의 문제 해결]**    
* **문제점**    
  * 위와 같은 SVD를 통해 중요한 단어(빈도가 높은 단어)위주로 표현하면  function words (the, he, has)가 많이 추출된다.    
  * 이는 의미와 연관이 없는 경우가 많으므로 이러한 단어들이 문제가 된다.       
* **해결책: Hacks to X**     
  * 위와 같이 너무 빈도가 높은 단어들을 지워준다.     
      * log the frequencies    
      * min(X, t), with t ≈ 100   
      * Ignore the function words     
   * 카운트 대신 Pearson 상관 관계를 사용한 다음 음수 값을 0으로 설정한다.     
* **문제 해결 결과**    
   * 단어의 상관관계를 보면 잘 학습되었다는 것을 알 수 있다.    
   ![image](https://user-images.githubusercontent.com/76824611/179290742-62e87293-756c-4d19-84df-d7080f31e850.png)


<details>
<summary>👀 강의 슬라이드 보기</summary>
<div markdown="1">
  
![image](https://user-images.githubusercontent.com/76824611/179291123-29729b0e-8c3a-48eb-8857-24d3976baf2d.png)
![image](https://user-images.githubusercontent.com/76824611/179291140-4c412fae-9f0d-4aac-8a95-359236eded05.png)
![image](https://user-images.githubusercontent.com/76824611/179291150-d6ee7567-10ef-4579-a4a0-563d0e7cfdf3.png)
![image](https://user-images.githubusercontent.com/76824611/179291170-061688cf-87f4-4147-8bbd-1da057ed69b2.png)
![image](https://user-images.githubusercontent.com/76824611/179291187-3332e6d5-bce4-4407-98cf-b3a75a70618b.png)
![image](https://user-images.githubusercontent.com/76824611/179291220-3d63515b-b667-41b0-b6b0-60bc09e5e823.png)

  
</div>
</details>

-------
-----

# **GLOVE**     

**[Towards GloVe: Count based vs. direct prediction]**     
* **Count based**    
  * 빠른 학습  
  * 통계의 효율적 사용       
  * 주로 단어 유사성을 포착하는 데 사용됨     
  * 주어진 말뭉치에 대한 단어 개수의 불균형      
  * ex) ```LSA```, ```HAL```, ```COALS```, ```Hellinger-PCA```      
* **direct prediction**     
  * 말뭉치 크기를 Scales     
  * 통계의 비효율적 사용     
  * 다른 작업에서 향상된 성능 생성    
  * 단어의 유사성을 넘은 복잡한 패턴 포착가능     
  * ex) ```Skip-gram```/```CBOW```, ```NNLM```, ```HLBL```, ```RNN```     


**[Encoding meaning components in vector differences]**    
* 유추문제(meaning components)를 풀기 위해서는 동시발생확률의 비율로 표기해야한다.    
  * 'ice'와 연관있는 'solid', 'water'의 동시 발생 비율은 'large'      
  * 'ice'와 연관없는 'gas', ' random'의 동시 발생 비율은 'samll'    
  ![image](https://user-images.githubusercontent.com/76824611/179300317-0e7dab6f-90bc-44b3-874d-9ac0be2c5524.png)
* 이를 수치화해보면,     
  * 비슷한 수치끼리는 1에 가까운 수가 나온다는것 을 알 수 있다 
  ![image](https://user-images.githubusercontent.com/76824611/179300586-5eb6e7d9-1a1f-4b0b-b9dd-dcd46fe4a471.png)
* **로그쌍선형모델(Log-bilinear model)**       
  * 이러한 동시발생확률의 비율을 선형 의미구성요소로 포착하는 방법    
  * $$w_i$$, $$w_j$$: 각각의 선형 쌍      
  * $$w_a - w_b$$: 두 벡터의 차이    
  ![image](https://user-images.githubusercontent.com/76824611/179300916-e9035b97-0ab3-4cd1-8ff1-913b021d10d0.png)




**[Combining the best of both worlds GloVe]**     
* $$f(X_{ij})$$: 높은 빈도의 단어 수 제한    
* $$b$$: 편향         
* $$j$$: 동시발생행렬에서 count작업 최적화 기능         
![image](https://user-images.githubusercontent.com/76824611/179302510-2da2bc9e-84ae-45ff-8816-350302576266.png)
* **특징**     
   * Fast training    
   * Scalable to huge corpora    
   * Good performance even with small corpus and small vectors
   ![image](https://user-images.githubusercontent.com/76824611/179302639-da747452-29e9-4106-a03f-643ec06b7c0e.png)
* **결과**    
![image](https://user-images.githubusercontent.com/76824611/179302750-0e4220ad-a6df-4e79-b42e-f5cc4a7796ee.png)


-----

## evaluate word vectors    
그럼 이 단어 벡터들이 잘 설정되었는지 평가하는 방법들을 알아보겠다.     
대표적으로 2가지 방법이 있다.     
* **Intrinsic:** 내적      
* **Extrinsic:** 외적     


----


### Intrinsic

**[특징]**
* 특정/중간 하위 작업에서의 평가    
* 계산 속도 향상   
* 시스템을 이해하는 데 도움이 됨    
* 실제 작업에 대한 상관관계가 설정되지 않은 경우 실제로 도움이 되는지 명확하지 않음      


**[방법: Word Vector Analogies]**         
* **평가 방법**: 단어 벡터를 추가 후 코사인 거리가 직관적인 의미론적 및 구문적 유사 질문을 얼마나 잘 포착하는지로 평가    
* ex) ```man:woman :: king:?``` ➡ Queen이란 답을 도출해야함    
![image](https://user-images.githubusercontent.com/76824611/179303950-c97413ab-880c-46d1-9495-6bc25b46b1ae.png)
![image](https://user-images.githubusercontent.com/76824611/179303989-96a55a58-a5bd-4ad5-acb2-f731396c838e.png)
* **문제**: 만약 정보가 거기에 있지만 선형적이지 않다면 문제가 된다.       

<details>
<summary>📜 Glove 평가 보기</summary>
<div markdown="1">
![image](https://user-images.githubusercontent.com/76824611/179304122-84b57381-3f39-4757-9f16-5dbd20f0d2e9.png)
![image](https://user-images.githubusercontent.com/76824611/179304137-ce6cbfdd-6b66-4750-9636-bee50dc73a11.png)
![image](https://user-images.githubusercontent.com/76824611/179304162-69c4c45e-08d7-468e-a50a-c82b6196ef2b.png)
![image](https://user-images.githubusercontent.com/76824611/179304181-6ea59d4d-f145-4cd1-989c-13060e507b7d.png)
![image](https://user-images.githubusercontent.com/76824611/179304236-4a43c063-8ce2-4b75-a822-2345a24d5aab.png)

  
</div>
</details>  

<details>
<summary>📜 추가 Intrinsic 보기</summary>
<div markdown="1">
![image](https://user-images.githubusercontent.com/76824611/179304332-89b6c646-a646-42e0-959b-744d8bf6bdb7.png)
![image](https://user-images.githubusercontent.com/76824611/179304349-dd0b0ccc-726a-4b68-9d3b-1d3356dd2277.png)
![image](https://user-images.githubusercontent.com/76824611/179304388-61ed39bf-65e5-4d79-a228-d6961530d220.png)

  
</div>
</details>  


-----


### Extrinsic
* 실제 작업에 대한 평가       
* 정확성을 계산하는 데 오랜 시간이 걸릴 수 있음   
* 하위 시스템이 문제인지 상호 작용인지 또는 다른 하위 시스템인지 불분명함    
* 정확히 하나의 서브시스템을 다른 서브시스템으로 교체하면 정확도가 향상됨 » Winning!     


**[방법: named entity recognition]**    
* 개인, 조직 또는 위치에 대한 참조 식별: Chris Manning은 Palo Alto에 산다   
![image](https://user-images.githubusercontent.com/76824611/179304578-5dde2b13-93f3-439d-95ec-0346b796f753.png)




-----
-----


# **Word senses and word sense ambiguity**     
**단어 의미의 모호성 문제**: 한 단어가 여러개의 의미를 갖을 수 있다.       
* 특히 흔한단어에서 많이 나타남    
* 특히 존재한지 오래된 단어에서 많이 나타남      

**[Example: pike]**    
* 이 단어는 아래의 모든 의미를 갖고있음    
  * A sharp point or staff
  * A type of elongated fish
  * A railroad line or system
  * A type of road
  * The future (coming down the pike)
  * A type of body position (as in diving)
  * To kill or pierce with a pike
  * To make one’s way (pike along)
  * In Australian English, pike means to pull out from doing something: I reckon he could have climbed that cliff, but he piked!   

즉 위의 간어의 의미에 대해 같은 벡터가 아닌 각 의미마다 다른 벡터를 갖는것이 목표이다. 

**[SOLUTION 1: Improving Word Representations Via Global Context And Multiple Word Prototypes (Huang et al. 2012)]**     
* 단어를 학습시키고 군집마다 다르게 위치된 같은 단어는 따로 벡터를 부여하고 따로학습시킴       
* 다른 클러스터에 있는 같은 단어는 다른단어 취급하는 것      
* 하지만 말뭉치마다 편차가 크게 학습된다는 단점이 있음    
![image](https://user-images.githubusercontent.com/76824611/179307513-f6d93138-5881-4a14-b3ad-5ad2683adf9a.png)



**[SOLUTION 2: Linear Algebraic Structure of Word Senses, with Applications to Polysemy]**    
* 다른 단어는 따로 벡터를 만들어 학습시키고 아래와 같이 이에 대한 평균을 냄     
![image](https://user-images.githubusercontent.com/76824611/179307724-e0e18fb9-a9cd-4494-a052-393f2ec7205a.png)
* 하지만 이러한 평균은 더욱 큰 모호성을 갖고올 수 있다는 단점이 있음    
* 그럼에도 아래와 같이 좋은 결과를 내긴 함    
![image](https://user-images.githubusercontent.com/76824611/179307759-8c72e140-6b9e-4649-9c17-d7e89bebc5b8.png)



