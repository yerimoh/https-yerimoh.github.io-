---
title: "[32] CS2241N: Lecture 2 Word Vectors 2 and Word Window Classification 정리"
date:   2019-12-29
excerpt: "Lecture 1 | Word Vectors 요약"  
category: [Deep Learning]
layout: post
tag:
- Deep Learning
order: 0

comments: true
---


# 목차




-----

# **INTRO**

**[Review: Main idea of word2vec]**   
* [word2vec 강의 정리 보러가기](https://yerimoh.github.io/DLCS31/)       
* random word vectors로 시작한다.     
* 단어에 대한 단어 벡터와 문맥벡터 사이의 내적의 관점에서 아래 확률분포 식으로 이를 수행        
![image](https://user-images.githubusercontent.com/76824611/179224062-0951a2bf-6a66-4fe8-b2c9-c6941dc56b88.png)
![image](https://user-images.githubusercontent.com/76824611/179224320-f4af78e2-281e-4ef9-92be-a9e25e80aaa9.png)
* **학습**      
   * 벡터를 업데이트하여 주변 단어 예측을 더 잘하게 한다.        
   * 이러한 학습을 통해 벡터는 고차원에서 유사성이 높은 단어들과 가까이 있고 의미있는 방향을 갖게된다.       


<details>
<summary>👀 강의 슬라이드 보기</summary>
<div markdown="1">
  
![image](https://user-images.githubusercontent.com/76824611/178133425-7ff3b02a-118f-451a-8636-cded1f948e5c.png)

  
</div>
</details>

------

# **Word2vec2**    
Word2vec2는 Word2vec의 단점을 보완한 것으로 크게 2가지 보완점을 갖고 나온 것이다.    
* negative sampling    
* [embedding](https://yerimoh.github.io/DL15/#1%EF%B8%8F%E2%83%A3-embedding-%EA%B3%84%EC%B8%B5-%EB%8F%84%EC%9E%85)     

그런데 강의에는 이 두가지 보완점 중 negative sampling만 설명한다고 되어있어 내가 따로 정리해둔 embedding 포스팅 링크를 추거해뒀다.     
궁금하면 읽어보면 된다.     


**[Word2vec parameters의 특징]**     
* Word2vec의 유일한 매개변수(parameters)는 단어벡터이다.          
* Word2vec는 “Bag of words” model 이다.      
    * Bag of words: 단어의 순서와 위치를 고려하지 않는다.      
    * 즉, 문맥안에 있는 단어들은 모두 같은 취급을 받는다는 것이다.            
* 각 단어에 대해 외부단어 벡터(outside), 중앙단어 벡터가 있다.     
![image](https://user-images.githubusercontent.com/76824611/179228387-1b9f4383-004e-4a42-95a0-448ecff23ace.png)



**[Word2vec parameters의 계산과정]**        
**1)** 특정 외부 단어가 중심 단어와 함께 발생할 가능성에 대한 점수를 얻기 위해 target 벡터와 context 벡터를 내적을 취한다.   
**2)** 이 정수를 확률로 변환하기 위해서 softmax를 사용한다.     
![image](https://user-images.githubusercontent.com/76824611/179228402-5b4df729-d505-4d4d-a28f-d519af32a110.png)





**[Word2vec in space]**     
* 학습 결과 비슷한 단어끼리 고차원 공간에 가깝게 배치되어있다.     
* 이 고차원 공간은 우리가 아는 2차원과 다르게 생겼다.       
![image](https://user-images.githubusercontent.com/76824611/179245067-928be223-a29b-45b4-81e1-11d09e5588e7.png)



**[Optimization: (Stochastic) Gradient Descent]**     
* **Gradient Descent**: 모든 벡터를 한꺼번에 업데이트   
  * 느림       
  * 우리가 앞에서 배웠던 $$J(θ)$$가 GD임               
* **Stochastic Gradient Descent**: 말뭉치를 배치단위로 쪼개 배치단위로 업데이트    
  * 빠름      
  * 이걸 대부분 많이 씀      
* **Optimization 과정**       
  * 1) 무작위(0에 가까운 숫자들)로 단어벡터의 가중치 초기화(randon initial value)        
  * 2) 지난시간에 정의한 손실함수 $$J(θ)$$로 손실을 계산한 다음 손실값이 낮은 곳으로 기울기 하강        
* 이 최적화의 아이디어는 현재 위치에서 현개 기울기를 계산한 다음 음의 기울기 방향으로 **작은 걸음**을 내딛음      
* **step size(learning step)**: 작은 걸음의 보폭        
  * 이 사이즈가 너무 크면 왔다갔다함     
  * 이 사이즈가 너무 작으면 학습이 오래걸림      
  * 그러므로 적절함 사이즈로 조정하는것이 중요함      
* 위와 같이 최적화하여 손실값이 낮은(minimum) 좋은 단어 벡터를 만드는것이 목표     
![image](https://user-images.githubusercontent.com/76824611/179247632-1e48b880-f72e-4ff7-b15a-34a5c1f5914e.png)




**[Word2vec algorithm family: More details]**     
* 왜 **2개**의 벡터인가?(왜 더 많은 벡터를 보지 않고 2개의 벡터만 보고 계산에 포함시키는가?)     
   * 우리가 보기로한 벡터들은 벡터끼리 내적을 수행해야하는데 2개를 넘어가면 **계산**이 너무 **복잡**해져서 오히려 더 비효율적이다.     
   * 특히 분모를 계산하는게 너무 cost가 높다.     
* **Word2vec의 모델**     
   * Word2vec는 2개의 모델을 아우르는 표현이다.      
   * **1) [Skip-grams (SG)](https://yerimoh.github.io/DL14/#skip-gram-%EB%AA%A8%EB%8D%B8)**: target단어로 context단어 예측(성능이 좋아서 더 많이 씀)(이 강의에서 지금까지 배운 것)     
   * **2) [Continuous Bag of Words (CBOW)](https://yerimoh.github.io/DL14/#cbow-%EB%AA%A8%EB%8D%B8)**: context단어로 target단어 예측            
    



<details>
<summary>👀 강의 슬라이드 보기</summary>
<div markdown="1">
  
![image](https://user-images.githubusercontent.com/76824611/179227583-15fc6e6a-2fda-467d-b469-28e3cc130a75.png)
![image](https://user-images.githubusercontent.com/76824611/179245132-c72cabbc-eed4-4349-8554-46841879b7e2.png)
![image](https://user-images.githubusercontent.com/76824611/179249318-5861b727-f8b9-470e-8a5b-d6afed0ed40f.png)
![image](https://user-images.githubusercontent.com/76824611/179249359-6af99138-237d-49e7-8ec2-f7e1b80894d7.png)

  
</div>
</details>


-----


## negative sampling
Word2vec를 더 최적화하기 위한 방법 중 하나이다.      
이 강의에선 skip-gram의 negative sampling (SGNS)위주로 설명하겠다.        


**[기존 skip-gram의 문제]**       
* 아래 skip-gram의 식을 보면 분모가 모든 단어의 내적을 수행해야하기 때문에 계산이 너무 비싸다.      
* 그러므로 모든 내적의 평균을 내는 ```softmax```대신 ```negative sampling```을 수행해야한다.    
![image](https://user-images.githubusercontent.com/76824611/179256915-a89a9ecf-85e3-4a44-8aea-8de45aae9188.png)




**[negative sampling: $$maximize$$]**     
* 여러 "noise" 쌍(random 단어와 짝을 이루는 center 단어) 대비 true 쌍(center 단어와 context창의 단어)에 대한 이항 로지스틱 회귀 분석 훈련
* Overall objective function($$maximize$$)         
   * $$log𝜎(u_{o}^{T} v_{c})$$: 정답 쌍의 내적을 통해 두 단어가 동시에 발생할 확률을 최대화      
   * $$ \sum_{i=1}^{k} E_{j~P(w)}[log𝜎(-u_{j}^{T} v_{c}] $$: 정답이 아닌 오답값 중에서 랜덤으로 선택하여 오답값을 학습시켜 단어의 확률을 최소화, 즉 소수의 무작위의 단어에 대해 계산한다.(무작위 단어를 샘플링하는 비율은 확률에 따라 다르다)         
![image](https://user-images.githubusercontent.com/76824611/179262441-47905dd3-b358-4471-bf6d-cbe0c635a155.png)
* 여기서 softmax 대신 **sigmoid(𝜎)** 를 사용한다      
   * sigmoid = logisitc 함수 이다.        
   * 이걸 통해서 중심단어와 무작위로 고른 단어 사이의 내적을 구한다.    
   * 이 sigmoid를 통하면 이 내적값이 크면 사실상 1, 아니면 0이 된다.    
   * 이해가 안간다면? [더 알아보기](https://yerimoh.github.io/DL15/#%EB%84%A4%EA%B1%B0%ED%8B%B0%EB%B8%8C-%EC%83%98%ED%94%8C%EB%A7%81)  
![image](https://user-images.githubusercontent.com/76824611/179262929-1a22dcce-5b2a-446d-8b20-07a75ace8a51.png)


**[negative sampling: $$minimize$$]**     
* 이 강의에서 내주는 과제를 위해 위의 식을 조금 더 분석해봤다.       
   * 이 식은 음의 로그 가능성을 최소화 하는 것으로 볼 수 있다.    
   * 즉 위의 식에다 -를 붙이고 이를 최소화 하는 것이다.       
   * 각 식에 대한 설명은 위와 같다.(대신 거기에 음을 취한다는 것만 다르다)        
   * $$k$$:negative samples (using word probabilities)      
![image](https://user-images.githubusercontent.com/76824611/179264744-a8a232f6-d5d4-4b2d-8324-c2521c14fc40.png)
* 실제 **외부 단어**가 나타날 **확률** **최대화**, 중심 단어 주변에 임의 단어가 나타날 확률 최소화     
➡ 이를 통해 주변 단어에만 국한된 학습이 아닌 좀 더 전체를 고려한 학습이 된다.     
* $$P(w)= U(w)^{3/4}/Z$$의 확률로 부정적인 예를 샘플링한다.     
    * 정답이 아닌 샘플(negative sample)을 무작위로 샘플링 할 때 단어들의 출변 빈도로 샘플링할 단어를 뽑는 것이 원칙이다.    
그런데 말뭉치 출현 빈도가 낮은 정답이 아닌 샘플은 선택될 확률이 낮다.      
그러므로 기본 확률($$U(w)$$)에 $$3/4$$을 제곱하여 **낮은 빈도의 단어가 선택될 확률을 높이는 것**이다.    
    * 이해가 안간다면? [더 알아보기](https://yerimoh.github.io/DL15/#%EB%84%A4%EA%B1%B0%ED%8B%B0%EB%B8%8C-%EC%83%98%ED%94%8C%EB%A7%81%EC%9D%98-%EC%83%98%ED%94%8C%EB%A7%81-%EA%B8%B0%EB%B2%95)     





**[negative sampling의 장점]**    
* softmax보다 효율적   
  * softmax는 내적이 많은데 모든 단어를 모두 내적하려면 계산비용, 시간이 상당하다.      
  * 반면, negative sampling은 모든 것을 계산하는 것이 아닌 정답값과 정답이 아닌 값을 각각 이진(정답이다, 정답이 아니다)분류하여 계산하기 때문에 확률을 구하는 softmax보다 효율적이다.         
  ![image](https://user-images.githubusercontent.com/76824611/179268880-eab2ddcc-19d2-4f7c-9718-ac25f0dc7a58.png)  
* context 범위에 없는 단어를 negative sampling하여 context에만 국한된 학습에서 벗어날 수 있다.        



<details>
<summary>👀 강의 슬라이드 보기</summary>
<div markdown="1">
  
![image](https://user-images.githubusercontent.com/76824611/179269649-c93edcaf-ecf1-4346-87cc-b4fed3ca7e40.png)
![image](https://user-images.githubusercontent.com/76824611/179269686-024c9965-91d1-4893-9066-5033375ac064.png)
![image](https://user-images.githubusercontent.com/76824611/179269728-fe22b264-dc83-45d8-8dfb-9fabb62293d0.png)

  
</div>
</details>


------


## aside 
word2vec에서 추가 지식(TMI)을 쌓아보자!


**[Stochastic gradients with negative sampling]**      
* 우리는 SGD에 대해 각 창에서 그레이디언트를 반복적으로 측정한다.    
* 그런데 매 step마다 업데이트 되는 벡터는 매우매우 희소하다.      
![image](https://user-images.githubusercontent.com/76824611/179284710-2b93fb2c-cff2-421f-a7a7-f311687629c5.png)


**[벡터의 표현]**      
* 현재는 계속 단어를 열벡터로 표현했지만, 사실상 직접 컴퓨터로 계산한때는 행(row)벡터로 표현된다.    
* 행으로 표현하면 연속으로 계산이 가능하여 메모리를 효율적으로 사용 가능하기 떄문이다.         
![image](https://user-images.githubusercontent.com/76824611/179285123-8239bb5a-e461-4687-a2d0-d877dba706bd.png)


<details>
<summary>👀 강의 슬라이드 보기</summary>
<div markdown="1">
  
![image](https://user-images.githubusercontent.com/76824611/179290962-79e16661-7f54-4ef3-961a-afaf8b610152.png)
![image](https://user-images.githubusercontent.com/76824611/179290976-d2cbfcf0-71fa-454f-9f19-81938e6c79f1.png)
![image](https://user-images.githubusercontent.com/76824611/179291005-ca7138a6-8bd1-4ef8-9305-d382e4d42416.png)

  
</div>
</details>


-----

## co-occurrence matrix
그렇다면 왜 단어 벡터를 표현할 때 동시발생 행렬(co-occurrence matrix)을 **사용하지 않는가**?      

동시발생행렬(co-occurrence matrix)은 통계기반 기법이다.    

* **통계기반 기법**      
   * 한단어를 말뭉치 전체를 이용하여 표현     
   * 말뭉치가 조금 수정되도 모든 단어를 업데이트 해야함    
   * 비효율적       
   * [PMI](https://yerimoh.github.io/DL13/#%EC%83%81%ED%98%B8%EC%A0%95%EB%B3%B4%EB%9F%89-pmi),**SVD**등을 통해서 차원 축소를 통해 필요없는 단어 표현 정보를 없애면서 성능 개선    
* **추론기반 기법**      
   * 한 단어를 표현할 때 말뭉치 전체가 아닌 몇개의 단어로 추론하여 단어 표현    
   * 말뭉치가 조금 수정되어도 이에 영향을 받는 몇몇 단어 벡터만 수정하면 됨     
   * 비교적 효율적       



**[동시발생행렬(co-occurrence matrix) 예시]**     
* 각 표의 칸의 숫자의 의미는 그 단어가 나오는 횟수이다.    
* Window length 1 (more common: 5–10)    
* Symmetric (irrelevant whether left or right context)     
* Example corpus:    
   * I like deep learning  
   * I like NLP   
   * I enjoy flying     

![image](https://user-images.githubusercontent.com/76824611/179286625-3079e9b8-5953-4975-9fb3-679b63369194.png)


**[동시발생행렬의 문제점]**      
* **단순히 개수를 센 벡터의 문제**      
   * 벡터가 말뭉치 전체로 표현되기 때문에 어휘에 따라 크기가 커진다.       
   * 매우 높은 dimensional: 많은 저장소 필요      
   * 후속 분류 모델은 희소성 문제가 있다 → 모델은 덜 견고하다.        
   * 차원이 낮아 모델이 결고하지 못하다.           
* **해결책**       
   * 중요한 정보의 "대부분"을 소수의 고정된 정보에 저장 차원: **밀집 벡터**    
   * 이렇게 기존 벡터를 밀집 벡터를 벡터를 통해 표현하면 word2vec와 비슷해진다.(Usually 25–1000 dimensions)      



**[해결책: SVD]**    
* 위 문제의 해결 방법으로 차원이 낮은 단순 벡터를 밀집 벡터로 만드는 것이다.      
* 원래 x를 아래와 같이 3개로 나눠 표현한다.      
  * $$𝛴$$는 높은 수(중요한 수, 빈도가 높은 수)로 정렬되어있어 더 높은 밀집도의 벡터를 얻고싶으면 이 행렬의 아래(파란색)부터 지워 중요한 정보만 나타내게하면 된다.     
  * 노란색 부분이 없어지며 무시된다.       
* 이해가 안된다면? [더 알아보기](https://yerimoh.github.io/DL13.3/) ➡ 같은 LSA와 같은 맥락이다.        
![image](https://user-images.githubusercontent.com/76824611/179289682-6faa51bb-ded2-46a5-a769-018f025b3c67.png)


**[SVD의 문제 해결]**    
* **문제점**    
  * 위와 같은 SVD를 통해 중요한 단어(빈도가 높은 단어)위주로 표현하면  function words (the, he, has)가 많이 추출된다.    
  * 이는 의미와 연관이 없는 경우가 많으므로 이러한 단어들이 문제가 된다.       
* **해결책: Hacks to X**     
  * 위와 같이 너무 빈도가 높은 단어들을 지워준다.     
      * log the frequencies    
      * min(X, t), with t ≈ 100   
      * Ignore the function words     
   * 카운트 대신 Pearson 상관 관계를 사용한 다음 음수 값을 0으로 설정한다.     
* **문제 해결 결과**    
   * 단어의 상관관계를 보면 잘 학습되었다는 것을 알 수 있다.    
   ![image](https://user-images.githubusercontent.com/76824611/179290742-62e87293-756c-4d19-84df-d7080f31e850.png)


<details>
<summary>👀 강의 슬라이드 보기</summary>
<div markdown="1">
  
![image](https://user-images.githubusercontent.com/76824611/179291123-29729b0e-8c3a-48eb-8857-24d3976baf2d.png)
![image](https://user-images.githubusercontent.com/76824611/179291140-4c412fae-9f0d-4aac-8a95-359236eded05.png)
![image](https://user-images.githubusercontent.com/76824611/179291150-d6ee7567-10ef-4579-a4a0-563d0e7cfdf3.png)
![image](https://user-images.githubusercontent.com/76824611/179291170-061688cf-87f4-4147-8bbd-1da057ed69b2.png)
![image](https://user-images.githubusercontent.com/76824611/179291187-3332e6d5-bce4-4407-98cf-b3a75a70618b.png)
![image](https://user-images.githubusercontent.com/76824611/179291220-3d63515b-b667-41b0-b6b0-60bc09e5e823.png)

  
</div>
</details>

-------
-----

# **GLOVE**     












