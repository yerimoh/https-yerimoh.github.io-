---
title: "[23] CS231N: Lecture 3 Regularization and Optimization Regularization"
date:   2020-02-10
excerpt: "Lecture 3 | Stochastic Gradient Descent, Momentum, AdaGrad, Adam, Learning rate schedules 요약"
category: [Deep Learning]
layout: post
tag:
- Deep Learning
order: 0

comments: true
---


# 목차
CS231N: Lecture 3 강의를 빠짐 없이 정리하였고, 어려운 개념에대한 보충 설명까지 알기 쉽게 추가해 두었습니다.   



# 손실함수
손실함수는 **"모델이 얼마나 잘 예측하지 못하는가"** 를 계산해주는 함수이다.     
* 즉, 우리 분류기가 얼마나 좋은지를 이야기해 주는 것이다.     
* 이것에 대해 알고리즘을 작성해서, 자동으로 어떤 W(모델의 가중치)가 최선인지 결정하도록 하는 것이다.     
* 우리는 모든 **특정 W(모델의 가중치)에 대해서 이 나쁨의 정도를 정량화**할 방법이 필요한데, 손실 함수는 W를 받아서, 점수를 보고 얼마나 정량적으로 나쁜지를 말해준다.          
* * 일단 우리가 이 손실 함수 아이디어를 갖게 되면, 이것으로 우리는 주어진 어떤 W에 대해 얼마나 좋은지 나쁜지에 대해 정량화 할 수 있다     

먼저 아래의 예를 보면,  
3개의 예제(입역 이미지)와 3개의 클래스(정답)가 있는 것을 알 수 있다.       
또한 나타나있는 값들은 모델이 그 클래스라고 예측한 값이다.       
![image](https://user-images.githubusercontent.com/76824611/169459704-a4f9d47d-f91e-49f3-93d1-2a1570ec9658.png)
그럼 이를 통해서 모델을 평가해보면,      
* **고양이:** 잘 예측 못함 (고양이 클래스(3.2)보다 자동차 클래스(5.1) 점수가 가장 높음)       
* **자동차:** 잘 예측함 (자동차 클래스(4.9) 점수가 가장 높음)      
* **개구리:** 심각하게 예측 못함 (개구리클래스의 점수(-3.1)가 가장 낮음)        

그런데 이렇게 각각을 개별로 보는 것은 모델을 평가하기에 한계가 있다.     

그러므로 이 이미지 분류를 위해 나쁨의 정도(손실)을 정량화할 기본적인 손실함수를 볼 것이다.


----

## 손실함수 공식   
그럼 위에서 언급한 손실함수를 정형화 해보겠다.    

**[데이터 셋]**        
먼저 아래와 같은 데이터 셋 주어진다고 있다고 해보자.    
![image](https://user-images.githubusercontent.com/76824611/169463981-79473e59-977a-4add-9e65-5a014b335b55.png)  
* x와 y로 이루어진 학습 데이터 셋을 가지고 있다고 가정함    
* **N:** 예제의 개수         
* **x:** 알고리즘으로의 입력(이미지 분류에서는 이미지의 픽셀 값)       
* **y:** 알고리즘이 예측하고 싶은 값(진짜 정답)(= 레이블, 타겟)     
CIFAR 10의 경우 정답 레이블이 10개 이므로 y는 0~9(혹은1~10)의 정수값을 갖는다.            


**[손실함수]**      
데이터 셋에 대한 손실은 예제에 다한 손실의 합으로 다음 공식과 같다.     
![image](https://user-images.githubusercontent.com/76824611/169467031-90e0c1a3-c57c-45b0-a39a-645ca80a0f54.png)
* **f():** 예측 함수  
   *  예제 x와 가중치 매트릭스 W를 받아서 y에 대한 예측 수행     
* **$$L_{i}$$**: 손실 함수       
   * 예측함수 f로부터 나오는 **예측 점수**와 **레이블 y의 참인 타겟을** 받아들여 모델의 손실 계산     
   * 학습 예제에 대해 그 예측이 얼마나 나쁜지에 대한 정량적인 값을 주는 함수임        
   * 계산 방법은 손실함수마다 다름
* **$$L$$**: 최종 손실값    
   * 우리의 데이터 셋 N개 예제에 걸쳐 전체 데이타 셋을 합친 것에 대한 이 손실들의 평균임.      



**[평가]**   
이 함수는 아주 일반적인 공식으로 Image classification 외에도 다양하게 확장 가능함.    
딥러닝 알고리즘의 목적은  X와 Y가 존재하고, 여러분이 만들 파라미터 W에 W가 얼마나 좋은지를 정량화하는 손실 함수를 만드는 것임      
W의 공간을 탐색하면서 의트레이닝 데이터의 Loss를 최소화하는 어떤 W를 찾게 될 것임.    


----


## Multiclass SVM loss   
구체적인 손실 함수의 첫번째 예로, 이미지 분류로 작업하기 좋은 손실 함수임.    
이 손실함수의 기본 아이디어는 손실 $$L_i$$가 **$$y_i가$$ 참인 카테고리만 빼고** 모든 카테고리 **y에 대해서** 각 예제 하나하나에 대해 **손실 합**을 구하는 것이다.(이해 안되도 나중에 자세히 설명 할 것이다)

**[SVM Loss 작동 방식]**    
"True인 카테고리" 를 제외한 "나머지 카테고리 Y"의 합을 구할 것이다.(맞지 않는 카테고리를 전부 합치는 것)      
**1)** **정답 점수와 오답 점수 비교**      
올바른 카테고리의 스코어와 올바르지 않은 카테고리의 스코어를 비교한다.      

**2)** **정답 점수+ margin점수가 더 큰 경우**   
만약 정답 카테고리 점수가 틀린 카테고리 점수보다 크다면(정답일 확률이 더 높으면), Loss = 0      
또한 마진이 있는 경우, 정답+ margin(이 예제에선 1)차이가 오답보다 크면(일정 오차 허용), Loss = 0     
위의 두 경우 모두 True인 스코어가 다른 false 카테고리보다 훨씬 더 크다는 것을 의미           


**2)** **오답점수가 더 큰 경우**            
**(현재 카테고리의 점수 + 1) - (정답의 점수)** 를 오차값(Loss)에 더해줌   


**3)** **이렇게 모든 카테고리를 돈다**     
정답을 제외한 모든 카테고리에 1,2,3을 적용한다.        
미지 내 정답이 아닌 카테고리의 모든 값들을 합치면 그 값이 바로 한 이미지의 최종 Loss가 된다.     

**4)** **평균**     
각 Loss의 평균을 구한다.     



<details>
<summary>📜 margin값인 + 1 을 사용하는 이유</summary>
<div markdown="1">

만약 현재 카테고리의 점수 그 자체를 비교한다면, 2등의 점수와 1등의 점수가 굉장히 **근사**하다 하더라도,     
**오차값은 무조건 0**, 즉 최고로 좋은 상태가 되어 있을 것이다.     
이는, 비록 정답은 맞혔다 하더라도, 운 좋게 맞췄다고 볼 수 있으므로 성능이 좋지 않다.       
그렇기 때문에, 더욱 확실히 정답을 맞히는 것을 목표로, +1을 더해주는 것이다.    
 
</div>
</details>  



위 과정을 반영한 수식을 보여주면,    
기본형은 위의 기본 손실함수와 같다.    
위의 기본형에 손실함수를 max()로구체화 한것이다
![image](https://user-images.githubusercontent.com/76824611/169474340-09dbf0fe-ead2-48f5-9d7f-cb2fe6aa4e14.png)


또한 이 수식을 시각화하면,  
![image](https://user-images.githubusercontent.com/76824611/169476669-416c7244-2852-4296-a5b8-f6899c5ac508.png)
이 그래프는 hinge(경첩)(오른쪽 그림)과 비슷하다고 해서 hinge loss라고도 불린다.    
* **x축**: $$S_{Y_{i}}$$로 실제 정답 클래스의 스코어      
   * **S:** 분류기의 출력으로 나온 예측된 스코어      
   가령 1이 고양이고 2가 개면 $$S_{1}$$은 고양이 스코어 $$S_{2}$$는 개 스코어       
   * **$$Y_{i}$$**:  이미지의 실제 정답 카테고리(정답)      
   * **$$S_{Y_{i}}$$**: 트레이닝 셋의 i번째 이미지의 정답 클래스의 스코어      
* **y축**: Loss      
➡ 정답 카테고리의 점수가 올라갈수록 Loss가 선형적으로 줄어드는 것 확인 가능     
* **Safety margin**: 그래프에 표시된 1, 이 로스는 0이 된 이후에도 Safety margin 을 넘어설 때 까지 더 줄어듦   
➡ Loss가 0이 됐다는 건 클래스를 잘 분류했다는 뜻임       
 








 












 

**[목표]**       
       
* 한계 그리고 나서 우리는 모든 가능한 더블유의 공간 전체를 탐색하고 실제로 뭐가 정확한 값인지 즉, 가장 덜 나쁜 W를 찾을 효율적인 절차를 찾아야 하죠. 이 절차가 최적화 과정이고 여기에 대해 더 얘기할 겁니다.







# SVM Loss
오차 함수 중 하나이다.       
오차를 나타내는 값으로 이 값이 낮을수록 좋다.      

**오차(손실)함수란?**: 얼마나 정답에 오차가 있는가를 나타내 주는 함수 [더 알아보기](https://yerimoh.github.io/DL3/#%EC%86%90%EC%8B%A4-%ED%95%A8%EC%88%98loss-function)      

 
 
즉, SVM의 식은 아래와 같다.   
![image](https://user-images.githubusercontent.com/76824611/167994974-ddef4e12-14f5-4240-a815-ef6324e805a4.png)

 




