---
title: "[23] CS231N: Lecture 3 Regularization and Optimization Regularization"
date:   2020-02-10
excerpt: "Lecture 3 | Stochastic Gradient Descent, Momentum, AdaGrad, Adam, Learning rate schedules 요약"
category: [Deep Learning]
layout: post
tag:
- Deep Learning
order: 0

comments: true
---


# 목차


# SVM Loss
오차 함수 중 하나이다.       
오차를 나타내는 값으로 이 값이 낮을수록 좋다.      

**오차(손실)함수란?**: 얼마나 정답에 오차가 있는가를 나타내 주는 함수 [더 알아보기](https://yerimoh.github.io/DL3/#%EC%86%90%EC%8B%A4-%ED%95%A8%EC%88%98loss-function)      

**[SVM Loss 작동 방식]**    
**1)** 카테고리를 본다. 만약 그 카테고리가 정답이라면 무시한다.              
**2)** 정답 카테고리가 아닐 때, **(현재 카테고리의 점수 + 1) < (정답의 점수)** 이라면, 무시한다.       
**3)** 그렇지 않다면, **(현재 카테고리의 점수 + 1) - (정답의 점수)** 를 오차값에 더해준다.   
**4)** 이렇게 모든 카테고리를 돌았을 때, 나온 최종 오차값을 구한다.          


➕ **현재 카테고리의 점수 + 1 값을 사용하는 이유**       
: 만약 현재 카테고리의 점수 그 자체를 비교한다면, 2등의 점수와 1등의 점수가 굉장히 **근사**하다 하더라도,     
**오차값은 무조건 0**, 즉 최고로 좋은 상태가 되어 있을 것이다.     
이는, 비록 정답은 맞혔다 하더라도, 운 좋게 맞췄다고 볼 수 있으므로 성능이 좋지 않다.       
그렇기 때문에, 더욱 확실히 정답을 맞히는 것을 목표로, +1을 더해주는 것이다.    
 
 
즉, SVM의 식은 아래와 같다.   
![image](https://user-images.githubusercontent.com/76824611/167994974-ddef4e12-14f5-4240-a815-ef6324e805a4.png)

 



