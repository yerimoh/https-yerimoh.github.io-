---
title: "Do Language Models Understand Measurements? 정리"
date:   2023-02-13
excerpt: "Do Language Models Understand Measurements?"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---

# Abstract
**[배경]**     
최근 pre-trained language models (PLMs)의 **성공은 숫자를 이해**하고 **작업**하는 능력에 대한 관심을 자극했다.     

**[문제]**     
그러나 <span style="background-color:#FFE6E6">**측정에 대한 수치 추론**은 그 중요성에도 불구하고 공식적으로 연구되지 않았다</span>.    

**[논문의 목적]**       
본 논문은 아래와 같은 내용으로 구성되어있다.     
* **PLMs가 측정에 대한 추론에 필요한 능력이 부족**하다는 것을 보여준다.       
* **measurement-rich 말뭉치에 대해 train된 언어 모델**이, **understanding measurements**에 **더 나은 성능**을 보인다는 것을 발견했다.     
* 본 논문은 **숫자와 단위를 더 잘 구별**하기 위한 **간단한 임베딩 전략**을 제안한다.   
➡ 이는 probing tasks의 상당한 개선으로 이어진다.   



# 1 Introduction
[기존 연구]     
pre-trained language models (PLMs)의 성공은 **상식을 이해하는 능력**에 대한 더 많은 연구로 이어졌다.    
이러한 맥락에서 numerical reasoning over text (NRoT)은 숫자 또는 단어 형태의 숫자를 해석하고 작업하는 NLP 모델의 능력이다(Spithourakis and Riedel, 2018). 수치(Wallace et al., 2019), 스칼라 크기 비교(Zhang et al., 2020), 수치 사실(Lin et al., 2020) 및 수학 단어 문제(Wu et al., 2021)에 대한 질문에 답하기 위한 NRoT 테스트 PLM에 대한 최근 연구.
The success of pre-trained language models (PLMs) has led to more research on their ability to understand commonsense. In this context, numerical reasoning over text (NRoT) is a NLP model’s ability to interpret and work with numbers in either digit or word form (Spithourakis and Riedel, 2018). Recent studies on NRoT test PLMs to answer questions on numeracy (Wallace et al., 2019), scalar magnitude comparison (Zhang et al., 2020), numerical facts (Lin et al., 2020), and math word problems (Wu et al., 2021).


**[기존 연구의 한계]**     
이러한 노력에도 불구하고 기존의 paper들은 **숫자가 나타나는 형태에 대한 분석이 부족**하다.    
특히, 우리는 맥락에서 숫자가 **measurement으로 나타나는 경우에만 초점을 맞추는 경향**이 있다.   
* **위 문제점의 예**        
대부분의 과학 기사에서 measurement은 **맥락의 필수적인 부분**이다.(적적한 의미를 담기 위해선)   
아래 두 문장을 보자    
![image](https://user-images.githubusercontent.com/76824611/220443999-fd3d274d-c83b-48d2-80b9-067b155f6675.png)   
두 문장은 측정단위(unit of measurement,UoM)인 g과 mg을 제외하고는 같은 단어이지만,    
두 번째 문장은 UoM 때문에 incorrect하다.    


**[본 논문의 개선]**    
* 본 논문에서는 PLMs의 **measuring skill**를 조사한다.     
즉, measurement **시스템을 이해**하고 measurement에 대한 **수치 추론을 수행하는 능력**을 조사한다.     
* 세 가지 측정 기술 테스트(measuring skill tests, MSTs)를 설계한다.     
➡ 얼마나 많은 measuring 기술을 습득할 수 있는지 연구한다.       
   * UNIT CONVERSION(단위 변환): 측정 시스템에 대한 이해 필요        
   * REFERENCE RANGE DETECTION(기준 범위 검출): 생체 의학 실체의 정상 범위 이해 필요       
   * MEASUREMENT COMPARISON(측정 비교): 측정 시스템과 NRoT에 대한 지식을 결합할 수 있는 능력 필요로 한다. 표 1은 각 측정 기술 테스트의 예를 보여줍니다.
In this work, we examine the measuring skill of PLMs: the ability to understand the system of measurement and perform numerical reasoning over measurements. We design three measuring skill tests (MSTs) and study how many measuring skills can be acquired. Specifically, UNIT CONVERSION, REFERENCE RANGE DETECTION, and MEASUREMENT COMPARISON require understanding of the system of measurement, the normal range of the biomedical entity, and the ability to combine knowledge about the system of measurement and NRoT, respectively. Table 1 shows an example of each of the measuring skill tests.


MST 결과는 모델들이 측정값 목록에서 가장 큰(또는 가장 작은) 값을 찾고 측정값을 다른 단위로 변환하는 데 어려움을 겪는 반면 다른 테스트에서는 잘 수행되었음을 보여주었다. 다른 PLM에 비해 BioBERT(Lee et al., 2020)는 UNIT CONVERSION 및 REFERENCE RANGE Detection에서 우수한 성능을 보여 측정이 풍부한 텍스트를 사용한 사전 훈련이 모델이 측정 시스템을 이해하는 데 도움이 된다는 것을 의미한다. 마지막으로, 우리는 맥락에서 숫자, 단위 및 다른 단어를 구별하는 기술의 부족이 일부 MST에서 모델을 실패하게 한다고 추측한다. 이를 완화하기 위해 입력 텍스트에서 숫자의 위치와 척도에 대한 정보를 모델에 제공하는 척도 임베딩을 도입한다. 우리는 스케일 임베딩이 모든 PLM의 MST 성능을 크게 향상시킨다는 것을 보여준다.
MST results showed that the models struggled to find the largest (or smallest) value on the list of measurements and convert the measurement to another unit, while they performed well on other tests. Compared to other PLMs, BioBERT (Lee et al., 2020) showed superior performance on UNIT CONVERSION and REFERENCE RANGE DETECTION, which implies that pre-training with measurementrich text helps the model understand the system of measurement. Finally, we speculate that the lack of skills to distinguish numbers, units, and other words in the context makes the models fail in some MSTs. To mitigate this, we introduce scale embedding, which provides the model with the information regarding the position and scale of the numbers in the input text. We show that scale embedding significantly improves the MST performance of all PLMs.

# 2 Measuring Skill Test
In this section, we describe three MSTs to carefully
study the ability of PLMs to understand the system
of measurement and perform numerical reasoning
over the measurements.

## 2.1 Unit Conversion
This task requires the model to decide whether
the two measurements represent the same quantity.
For example, the model might correctly predict
[MASK] in a sentence, such as "3.5g and 3500mg
are [MASK] value" to be filled with same if it under
stands the conversion of units correctly. In general,
it is a convention to combine the unit (e.g., liter,
meter) and its prefix (e.g., kilo, milli) to represent
the numerical value of the measurement within a
range [10−3, 103). Therefore, various unit prefixes
can appear in a single passage, even if the units
are the same. To handle this, UNIT CONVERSION
is essential for complex reasoning over measurements. To succeed in UNIT CONVERSION, we expect the model to handle the unit and numerical
value jointly, based on an understanding of the system of measurement


## 2.2 Reference Range Detection
Given a biomedical entity and measurement, this
task requires a model to predict whether the measurement falls within the reference range. Knowledge of the biomedical entity plays a crucial role
in understanding measurements, since the unit is
determined by the biomedical entity. For example,
we measure the hemoglobin level in g/dL. In addition to understanding UoMs, PLMs must rely on
domain knowledge embedded in their parameters
to solve this task, as context alone does not provide
sufficient clues as to what the reference range is for
the given biomedical entity

## 2.3 Measurement Comparison
Given two measurements (or a series of n measurements), the task is to predict the correct relationship between them. We created the synthetic dataset following other well-known NRoT
tasks. Here, we consider three numerical reasoning tasks: COMPARISON (Talmor et al., 2020),
ARGMIN/MAX (Wallace et al., 2019), and SORTING (Pal and Baral, 2021), all requiring the model
to compare numbers. Note that each measurement
in this task can have a different unit prefix. For
example, the sample "1.59mg is [MASK] than 3.8g"
containing two different units "mg" and "g" appears
in the COMPARISON dataset. This task assesses the
model’s ability to combine an understanding of
measurements and numerical reasoning skills.

