---
title: "[2] CS231N: Lecture 1 Introduction"
date:   2020-02-17
excerpt: "Lecture 1 | Introduction to Convolutional Neural Networks for Visual Recognition 요약"
category: [Deep Learning]
layout: post
tag:
- Deep Learning
order: 0

comments: true
---


# 목차
- [컴퓨터 비전(Computer Vision)이란?](#컴퓨터-비전--computer-vision-이란-)
- [컴퓨터 비전의 역사](#컴퓨터-비전의-역사)
  * [생물학적 비전: 진화의 빅뱅](#생물학적-비전--진화의-빅뱅)
  * [공학적 비전: 카메라의 역사](#공학적-비전--카메라의-역사)
- [컴퓨터 비전의 발전](#컴퓨터-비전의-발전)
  * [Block World](#block-world)
  * [David Marr의 책](#david-marr의-책)
  * [Recognition via Parts](#recognition-via-parts)
  * [Recognition via Edge Detection](#recognition-via-edge-detection)
  * [앞선 연구들의 한계](#앞선-연구들의-한계)
  * [영상분할(Image Segmentation)](#영상분할-image-segmentation-)
    + [Jitendra Malik, Jianbo Shi의 연구](#jitendra-malik--jianbo-shi-의-연구)
  * [얼굴인식](#얼굴인식)
    + [실시간 얼굴인식에 성공](#실시간-얼굴인식에-성공)
  * [객체 인식(SIFT feature)](#객체-인식--sift-feature-)
    + [SIFT feature](#sift-feature)
    + [Spatial Pyramid Matching](#spatial-pyramid-matching)
    + [연구의의](#연구의의)
  * [ImageNet 프로젝트](#imagenet-프로젝트)
- [CNN](#cnn)




----


CS231n은 컴퓨터 비전에 관한 수업입니다.


# **컴퓨터 비전(Computer Vision)이란?**
의의: 시각데이터에 대한 연구이다.     

최근 몇 년간 엄청나게 많은 시각 데이터가 쏟아져 나오고 있다.  
이런 수많은 데이터는 전 세계 각처에 퍼져있는 무수한 센서로부터 비롯된다. 
요즘은 스마트폰이 없는 분들을 더 찾기 힘들다.
여러분의 스마트폰에는 한두 개의 카메라가 내장되어 있다.
이런 카메라와 같은 센서들이 전 세계 각지에서 매일매일 데이터를 쏟아내고 있어 이미지 데이터가 기하급수적으로 증가하고 있다.     


CISCO에서 수행한 2015 ~ 2017년도까지의 한 통계자료가  이 사실을 아주 적나라하게 보여준다.     
통계에 따르면 인터넷 트래픽 중 80%의 지분은 바로 비디오 데이터다.      
➡️ 통계는 인터넷의 데이터 대부분이 시각 데이터라는 사실을 보여준다.   
➡️ 시각 데이터들을 잘 활용할 수 있는 알고리즘을 잘 개발하는 것이 무엇보다 중요해졌다.     

**[문제]**    
이런 시각데이터는 해석하기 상당히 까다로움   
* 시각 데이터를 암흑물질(dark matter)이라고 함.     
   * 암흑물질: 우주 대부분의 질량을 차지하고 있는 물질         
   *  우리는 여러 가지 간접적인 측정실험을 통해서 암흑물질의 "존재" 까지는 알 수 있지만      
   ➡️  시각 데이터가 대부분이지만    
   *  암흑물질을 직접 "관측" 불가     
   ➡️ 사실상 이들을 이해하고 해석하는 일은 상당히 어려움     



컴퓨터 비전은 여러 학문이 베이스가 되어있음         
➡️ 광학, 이미지 구성, 이미지의 물리학적 형성 등을 이해하려면 물리학적인 현상들을 이해할 필요가 있기 때문        
* 생물학이나 심리학: 동물의 뇌가 어떤 방식으로 시각정보를 물리적으로 "보고 처리하는지"를 이해하기 위해 필요       
* 컴퓨터 과학, 수학, 공학 등: 컴퓨터 비전 알고리즘을 구현할 컴퓨터 시스템을 실제로 구축할 때 필요한 분야         


-----
-----


# **컴퓨터 비전의 역사**


## 생물학적 비전: 진화의 빅뱅
비전의 역사는 아주 오래전으로 돌아감.   
정확하게는 눈(eyes)이 없던 5억 4천만 년 전   


그 시대의 삶은 어땠을까?   
  

**[눈이 없던 시절]**  
지구 대부분은 물이었고 바다를 부유하는 일부 생물들만 존재함.      
이들의 삶은 단조로웠습니다. 그들은 많이 움직이지 않았고 눈(eyes) 같은 건 존재하지 않았음   
먹이가 주변에 있으면 잡아먹고 없으면 그저 둥둥 떠 있는 것이 다였음     


**[진화의 빅뱅]**    
* 5억 4천만 년 전에 "진화의 빅뱅"이라는 아주 놀라운 사건이 벌어짐.     
* 진화의 빅뱅: 화석을 연구하면서 천만 년이라는 아주 짧은 시기 동안에 생물의 종이 폭발적으로 늘어남    
얼마 없던 종의 수가 수십만이 된 것임.     

**비젼(시각)의 탄생**           
* 이러한 진회의 빅뱅의 이유로 앤드류 파커 (Andrew Parker)가 이 연구에서 가장 설득력 있는 이론 중 하나를 제안함          
* 그는 약 5억 4천만 년 전 최초의 눈(eyes)이 생겨났다는 것을 발견함          
* 눈(eyes)이 폭발적인 종 분화의 시기를 시킴    
* 생물들은 갑자기 볼 수 있게 됨 
➡️ 삶이 훨씬 더 능동적이게 됨.     
* 일부 포식자들은 먹이를 찾아다니고 먹이들은 포식자로부터 달아나야만 함 
➡️ 진화적 군비경쟁을 촉발시켰고 생물들은 하나의 종으로 살아남으려면 빠르게 진화해야만 했음    

**그결과,**           
비전은 거의 모든 동물, 특히 지능을 가진 동물들의 가장 큰 감각 체계로 발전함      
우리 인간은 대뇌 피질의 50%가량의 뉴런이 시각처리에 관여.        
➡️ 비전은 가장 큰 감각체계이며 우리의 삶에 많은 것들을 가능하게 해줌        
➡️ 비전은 특히 지능을 가진 동물들에게 정말 중요    


---

## 공학적 비전: 카메라의 역사
오늘날 우리가 알고 있는 초창기의 카메라는 1600년대 르네상스 시대의 카메라인 Obscura임.      
* Obscura: 핀홀 카메라 이론을 기반으로 한 카메라(생물학적으로 발전한 초기의 눈과 상당히 유사)임    
* 이러한 카메라는 발전하면서 오늘날 보편적으로 쓰이게 되었음   
![image](https://user-images.githubusercontent.com/76824611/162037258-16fa66b3-2666-46fe-990e-e17ad6e451ad.png)   


**[Hubel과 Wiesel의 연구]**        
생물학자들은 1950/60년대 전기생리학을 이용하여  비전의 매커니즘을 연구 ➡️ 컴퓨터 비전에도 영감을 준 연구임     
* **연구주제**: "포유류의 시각적 처리 메커니즘은 무엇일까?"     
* **연구방법**: 고양이의 뇌를 연구(시각 처리 매커니즘만 보면 고양이와 인간은 비슷)      
   * 1) 그들의 고양이 두뇌 뒷면("일차 시각 피질"이 있는 곳)에 전극 몇 개를 꽂음.    
   * 2) 어떤 자극을 줘야 일차 시각 피질의 뉴런들이 격렬하게 반응하는지 관찰함.    
* **연구성과**: 일차 시각 피질에는 다양한 종류의 세포가 있다는 것을 발견함.   
   * **경계(edges)가 움직이면 이에 반응하는 세포들** 발견     
   ➡️ 아주 단순하지만 중요한 세포임       
* **결과분석**: **시각 처리가 처음에는 단순한 구조로 시작**되며 그 정보가 실제 세상을 제대로 인지할 수 있을 때까지 통로를 거치면서 **점점 복잡해진**다는 것임.    
![image](https://user-images.githubusercontent.com/76824611/162046650-5b244ed5-d619-4146-9bb0-caa33f543e78.png)


----
----

# **컴퓨터 비전의 발전**
60년대 초반에 시작    

## Block World    
컴퓨터 비전 분야에서의 최초의 박사 학위 논문임     
* **연구목표**: 우리 눈에 보이는 세상을 인식하고 그 모양을 재구성하는 일(대부분의 시각 체계를 구현하는 것)      
* **연구방법**: 이 연구에서는 우리 눈에 보이는 사물들을 기하학적 모양으로 단순화시킴         
* **연구의의**: 컴퓨터 비전 연구의 시작을 알림 ➡️ 현재 전 세계 수천 명의 연구자들이 아직도 비전의 가장 근본적인 문제들을 연구하고 있음  
![image](https://user-images.githubusercontent.com/76824611/162046902-22f187b3-8cdd-47ef-bdd7-440240fd486c.png)


컴퓨터 비전은 아직 숙제가 많지만, 인공지능 분야에서 가장 중요하고 빠르게 성장하는 분야 중 하나임




## David Marr의 책 
* **책 내용**: 책은 그가 비전을 무엇이라 생각하는지, 그리고 어떤 방향으로 컴퓨터 비전이 나아가야 하는지, 그리고 컴퓨터가 비전을 인식하게 하기 위해 어떤 방향으로 알고리즘을 개발해야 하는지를 다룬 책이었음.    
* **핵심 주제**: 우리가 눈으로 받아들인 "이미지"를 "최종적인 full 3D 표현"으로 만드는 방법    
   * 1 단계) "Primal Sketch": 이 과정은 주로 경계(edges), 막대(bars), 끝(ends), 가상의 선(virtual lines), 커브(curves), 경계(boundaries)가 표현되는 과정 -> 신경과학자들에게 영감을 받음    
   ➡️ 시각처리의 초기 단계는 경계와 같은 단순한 구조와 아주 밀접한 관계 존재      
   * 2단계) "2.5-D sketch": 시각 장면을 구성하는 표면(surfaces) 정보, 깊이 정보, 레이어, 불연속 점과 같은 것들을 종합    
   * 3단계) 앞선 단계의 모든 것을 한데 모아서 surface and volumetric primives의 형태의 계층적으로 조직화된 최종적인 3D 모델을 만들어 냄    
* **책의 의의**: 이런 방식은 "비전이 무엇인가"라는 것에 대한 아주 "이상적인" 사고과정이었음. 그리고 이런 방식의 사고방식은 실제로 수십 년간 컴퓨터 비전 분야를 지배했으며 학생들이 컴퓨터 비전을 처음 입문하고 나서 "어떻게 시각정보를 분석할 수 있을까"라는 질문을 던졌을 때 직관적인 생각해 볼 수 있는 방법이었습니다.   
![image](https://user-images.githubusercontent.com/76824611/162049238-e2297ca0-224d-4a9a-a76e-02576a3f3f67.png)


## Recognition via Parts
70년대의 중요한 연구     
* **연구목표**: 우리는 어떻게 해야 장난감 같은 단순한 블록 세계를 뛰어넘어서 실제 세계를 인식하고 표현할 수 있을까?"라는 질문을 하기 시작함.     
* **실험제약**: 70 년대엔 사용할 수 있는 데이터가 거의 없었음, 컴퓨터도 정말 느림(심지어 PC가 보급되기도 전임) 
➡️ 이 상황에서 컴퓨터 과학자들은 어떻게해야 대상을 인식하고 표현할 수 있을지를 고민하기 시작함    
* **실험 아이디어**: "모든 객체는 단순한 기하학적 형태로 표현할 수 있다"      
   * 아이디어 1: "generalized cylinder"  
   * 아이디어 2: "pictorial structure"  
* **실험 방법**  
   * 가령 사람은 원통 모양을 조합해서 만들 수 있음 (왼쪽 그림)   
   * 또는 "주요 부위"와 "관절"로 표현할 수도 있을 것입니다. (오른쪽 그림)   
   ![image](https://user-images.githubusercontent.com/76824611/162050382-022fc151-d6ab-488d-bfa0-632c39eef075.png)
   * 두 방법 모두 단순한 모양과 기하학적인 구성을 이용해서 복잡한 객체를 단순화시키는 방법임    
* **연구의의**: 이러한 연구들은 수년간 다른 연구에 상당히 많은 영향을 줌     


## Recognition via Edge Detection
80년대 또 다른 사례     
* **연구목표**: 어떻게 하면 단순한 구조로 실제 세계를 재구성/인식할 수 있을지 고민함     
* **연구방법**: 면도기를 인식하기 위해서 면도기를 선(lines)과 경계(edges) 그리고 직선(straight lines) 그리고 이들의 조합을 이용해서 구성함     
![image](https://user-images.githubusercontent.com/76824611/162052046-f107ec74-00cc-46bc-9608-ef5b26aadd9a.png)

  
## 앞선 연구들의 한계    
60/70/80년대에는 컴퓨터 비전으로 어떤 일을 할 수 있을까 고민한 시대였음    
하지만 너무 어려운 문제였음    

지금까지 제가 보여 드린 연구들이 모두 아주 대담했고 큰 야망을 가진 시도였지만,     
**그들은 단순한 수준(toy example)에 불과**했습니다.     
* 현실 세계에서 잘 동작할지를 생각해보면 많이 진보하지 못함    
* AI의 발전속도가 더뎌진  AI winter시기 도래함    

➡️ 영상 분할로 해결   


------


## 영상분할(Image Segmentation)

### Jitendra Malik, Jianbo Shi의 연구
위의 AI winter시기를 해결   
* **연구목적**: 이 연구는 영상분할 문제를 해결     
* **연구방법**: 그래프 이론 도입     
  * 객체인식이 너무 어렵다면 우선 객체 분할(segmentation)을 먼저 하자    
  * 객체분할: 이미지의 각 픽셀을 의미 있는 방향으로 군집화하는 방법임    
  * 픽셀을 모아놔도 사람을 정확히 인식할 수 없을지도 모르지만,   
  적어도 배경인 픽셀과 사람이 속해 있을지도 모르는 픽셀을 가려낼 수는 있었음      
  ![image](https://user-images.githubusercontent.com/76824611/162054135-4885d7c2-1657-40b7-b098-af19ae995334.png)


## 얼굴인식
컴퓨터 비전 중 유난히 발전 속도가 빨랐던 분야          

대략 1999/2000년대에는 "기계학습", 특히나 "통계적 기계학습" 이라는 방법이 점차 탄력을 얻기 시작함    
➡️ "Support Vector Machine", "Boosting", "Graphical models" 그리고 초기 "Neural Network" 등이 이 시기에 개발      

### 실시간 얼굴인식에 성공 
얼굴인식에 그중 가장 큰 기여를 한 연구    
* **연구성과**: Paul Viola와 Michael Jones이 실시간 얼굴인식에 성공함        
* **연구의의**: 느린 컴퓨터 자원에도 불구하고 실시간과 가깝게(near-real-time) 인식할 수 있는 얼굴인식 개발     
* **연구활용**: 논문발표 5년이 지난 2006년에 Fujifilm은 실시간 얼굴인식을 지원하는 최초의 디지털카메라를 선보임 
![image](https://user-images.githubusercontent.com/76824611/162058180-efc9f1e5-a220-43bf-ba7a-57e6930e1ff5.png)

----


## 객체 인식(SIFT feature) 
자! 이제 다시 "어떻게 객체를 잘 인식할 것인가?" 라는 질문으로 다시 한번 돌아가 보자       
90년대 후반부터 2010년도까지의 시대를 풍미했던 알고리즘은, "특징기반 객체인식 알고리즘" 임.   

### SIFT feature       
* **연구목적**: 전체 객체를 특징틍 찾아 이를 기반으로 인식하는 것.     
* **연구과정**: 
   * 1) 아래 정지 표지판이 있습니다. 이 정지 표지판들을 서로 매칭하기는 상당히 어렵다(카메라 앵글이 변할 수 있고, 겹치거나 화각이 변하고 빛도 변하고 객체 자체도 얼마든지 변할 수 있기 때문)     
   하지만 그들은 **객체의 특징 중 일부**는 다양한 변화에 조금 더 강인하고 불변하다는 점을 발견함.     
   그리하여 객체인식은 객체에서 이와 같은 중요한 특징들을 찾아내고 그 특징들을 다른 객체에 매칭시키는 과제로 변함         
   * 2) 정지표지판 이미지에서 일부 SIFT 특징들을 추출하고    
   * 3) 또 다른 정지 표지판에서도 특징을 추출하여 이를 식별하고 매칭       
* **연구의의**: 객체 특징 인식이 이미지 전체를 매칭하는 일보다 훨씬 쉬운 일임     
   * 이미지에 존재하는 "특징"을 사용하게 되면서 컴퓨터 비전은 또 한 번의 도약을 할 수 있었음
   * 그리고 장면 전체를 인식하기에 이르렀음      
![image](https://user-images.githubusercontent.com/76824611/162112453-544e0e7d-5aa5-4cc7-b283-14c8a6c4b722.png)    

### Spatial Pyramid Matching     
* **연구 아이디어**: 기본 아이디어는 우리가 특징들을 잘 뽑아낼 수만 있다면 그 특징들이 일종의 "단서"(풍경인지, 부엌인지, 또는 고속도로인지)를 제공해 줄 수 있음        
* **연구방법**    
  * 1) 이미지 내의 여러 부분과 여러 해상도에서 추출한 다수의 특징을 하나의 특징 기술자로 표현      
  * 2) 표현한 특징 기술자에 Support Vector Algorithm을 적용        
* **연구의의**: 이런 방식의 연구들은 사람 인식에도 탄력을 줌(여러 특징들을 잘 조합해 보자는 경향으로 변함)       
![image](https://user-images.githubusercontent.com/76824611/162113655-35025386-c780-4d0c-ade5-50ef80ab43e4.png)     

### 연구의의
2000년대 초에 일궈낸 것 중 하나는 바로 컴**퓨터 비전이 앞으로 풀어야 할 문제가 무엇인지의 정의**를 어느 정도 내렸다는 것임.  

물론 해결해야 할 다양한 문제가 있겠지만, "객체인식" 또한 아주 중요한 문제였음. 



-----


## ImageNet 프로젝트
그 무렵, Princeton과 Stanford에 있던 그룹에서 더 어려운 질문을 던졌음.   
우리는 이 세상의 모든 객체들을 인식할 준비가 되었는가?    
* **연구목적**       
  * 이 세상의 모든 것들을 인식하고 싶다는 것      
  * Graphical Model, SVM, AdaBoost 같은 기계학습 알고리즘들이 트레이닝 과정에서 Overfitting 하는 문제 방지   
    * 문제원인: 시각 데이터가 너무 복잡함(모델의 입력은 복잡한 고차원 데이터였고, 이로 인해 모델을 fit하려면 더 많은 파라미터가 필요함)              
    * 학습 데이터가 부족하면 Overfiting이 훨씬 더 빠르게 발생했고 일반화 능력이 떨어짐       
* **연구방법**: 구할 수 있는 모든 이미지를 담은 가장 큰 데이터셋을 만듦  
* **연구성과**: ImageNet은 대략 15만 장에 달하는 이미지와 22만 가지의 클래스 카테고리를 보유하게 됨     
ImageNet 덕분에 객체인식은 다른 국면으로 접어듦.   
* **연구한계**: 하지만 ImageNet을 Benchmark에 어떻게 활용할지 명확하지 않음     
* **한계극복**: 그래서 ImageNet 팀은 2009년부터 이미지 분류 문제를 푸는 국제 규모의 대회(ILSVRC)를 주최  

----


# **CNN**
비록 컴퓨터 비전이 아직 객체인식의 모든 문제를 풀지는 못했지만, 진전이 있었다는 것은 사실임.      
하지만 실생활에 적용하기에는 턱없이 부족했던 낮은 오류율이 인간의 수준으로 오기까지는 불과 몇 년뿐이 걸리지 않았음.       
특히, 2012년부터 눈에띄는 오류 감소를 보였는데 이는 CNN(convolutional neural network)의 발견 덕분이다.     
![image](https://user-images.githubusercontent.com/76824611/162115352-86358608-ae07-41a3-91c4-732e505de534.png)


우리가 한 학기 동안 배울 내용이 바로 Convolutional neural network에 관한 것이다.   

앞으로 CNN이 무엇인지, 어떤 법칙이 있는지, 어떤 선례가 있는지, 이 모델의 최근 동향은 어떠한지를 살펴볼 것입니다.   



이 수업에서 중점적으로 다룰 문제는 Image Classification임.    
* **Image Classification**      
  *  1) 알고리즘이 이미지 한 장을 봄    
  *  2) 그리고 몇 개의 **고정된 카테고리 안에서** 정답 하나를 고름     

이 방법이 다소 한정적이거나 인위적으로 보일 수도 있지만 사실 매우 일반적인 방법임     
➡️ Image Classification는 다양한 환경에 적용 가능    



그리고 object detection과 image captioning도 배워 볼 것임     
* **object detection**    
  * classification과 조금 다름    
  * 이 이미지가 고양이다, 개다, 말이다 이렇게 하는 실제로 어디에 있는지 네모박스를 그릴 수 있어야 함      
  * 네모박스를 객체의 위치에 정확히 그려 넣어야 함      
* **image captioning** 
  * 이미지가 입력으로 주어지면 이미지를 묘사하는 적절한 문장을 생성해야 함      
  * 이 문제가 어렵고 복잡해 보이고 Image classification과도 별로 관련이 없어 보일 수 있지만 image classification 기술을 이런 문제들에서 충분히 재사용가능    

그럼 이제 위의 내용을 중점으로 강의 요약을 해보겠다!


