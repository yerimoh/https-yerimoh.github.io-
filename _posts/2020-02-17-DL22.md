---
title: "[2] CS231N: Lecture 1 Introduction"
date:   2020-02-17
excerpt: " "
category: [Deep Learning]
layout: post
tag:
- Deep Learning
order: 0

comments: true
---


# 목차





----


CS231n은 컴퓨터 비전에 관한 수업입니다.


# 컴퓨터 비전(Computer Vision)이란?
의의: 시각데이터에 대한 연구이다.   

최근 몇 년간 엄청나게 많은 시각 데이터가 쏟아져 나오고 있습니다.
이런 수많은 데이터는 전 세계 각처에 퍼져있는 무수한 센서로부터 비롯됩니다.
요즘은 스마트폰이 없는 분들을 더 찾기 힘들죠 
여러분의 스마트폰에는 한두 개의 카메라가 내장되어 있습니다.
이런 카메라와 같은 센서들이 전 세계 각지에서 매일매일 데이터를 쏟아내고 있는 실정이죠


CISCO에서 수행한 2015 ~ 2017년도까지의 한 통계자료가  이 사실을 아주 적나라하게 보여줍니다.
통계에 따르면 인터넷 트래픽 중 80%의 지분은 바로 비디오 데이터입니다.
➡️ 통계는 인터넷의 데이터 대부분이 시각 데이터라는 사실을 보여줍니다.
➡️ 시각 데이터들을 잘 활용할 수 있는 알고리즘을 잘 개발하는 것이 무엇보다 중요해졌습니다.

[문제]
이런 시각데이터는 해석하기 상당히 까다로움
* 시각 데이터를 암흑물질(dark matter)이라고 합니다.
   * 암흑물질: 우주 대부분의 질량을 차지하고 있는 물질     
   *  우리는 여러 가지 간접적인 측정실험을 통해서 암흑물질의 "존재" 까지는 알 수 있지만  
   ➡️  시각 데이터가 대부분이지만    
   *  암흑물질을 직접 "관측" 불가   
   ➡️ 사실상 이들을 이해하고 해석하는 일은 상당히 어려움   



컴퓨터 비전은 여러 학문이 베이스가 되어있음     
➡️ 광학, 이미지 구성, 이미지의 물리학적 형성 등을 이해하려면 물리학적인 현상들을 이해할 필요가 있기 때문     
* 생물학이나 심리학: 동물의 뇌가 어떤 방식으로 시각정보를 물리적으로 "보고 처리하는지"를 이해하기 위해 필요    
* 컴퓨터 과학, 수학, 공학 등: 컴퓨터 비전 알고리즘을 구현할 컴퓨터 시스템을 실제로 구축할 때 필요한 분야       


-----
-----


# 컴퓨터 비전의 역사


## 생물학적 비전: 진화의 빅뱅
비전의 역사는 아주 오래전으로 돌아갑니다.
정확하게는 눈(eyes)이 없던 5억 4천만 년 전이죠.


그 시대의 삶은 어땠을까요?
지구 대부분은 물이었고

[눈이 없던 시절]  
바다를 부유하는 일부 생물들만 존재했습니다.   
이들의 삶은 단조로웠습니다. 그들은 많이 움직이지 않았고 눈(eyes) 같은 건 존재하지 않았습니다.
먹이가 주변에 있으면 잡아먹고 없으면 그저 둥둥 떠 있는 것이 다였습니다.


[진화의 빅뱅]
* 5억 4천만 년 전에 "진화의 빅뱅"이라는 아주 놀라운 사건이 벌어짐.
* 진화의 빅뱅: 화석을 연구하면서 천만 년이라는 아주 짧은 시기 동안에 생물의 종이 폭발적으로 늘어남
얼마 없던 종의 수가 수십만이 된 것입니다.

비젼(시각)의 탄생       
* 이러한 진회의 빅뱅의 이유로 앤드류 파커 (Andrew Parker)가 이 연구에서 가장 설득력 있는 이론 중 하나를 제안함       
* 그는 약 5억 4천만 년 전 최초의 눈(eyes)이 생겨났다는 것을 발견함       
* 눈(eyes)이 폭발적인 종 분화의 시기를 시킴    
* 생물들은 갑자기 볼 수 있게 됨 ➡️ 삶이 훨씬 더 능동적이게 됨.     
* 일부 포식자들은 먹이를 찾아다니고 먹이들은 포식자로부터 달아나야만 함 ➡️ 진화적 군비경쟁을 촉발시켰고 생물들은 하나의 종으로 살아남으려면 빠르게 진화해야만 했음    

그결과,   
비전은 거의 모든 동물, 특히 지능을 가진 동물들의 가장 큰 감각 체계로 발전함    
우리 인간은 대뇌 피질의 50%가량의 뉴런이 시각처리에 관여합니다.    
비전은 가장 큰 감각체계이며 우리의 삶에 많은 것들을 가능하게 해줌     
비전은 특히 지능을 가진 동물들에게 정말 중요


---

## 공학적 비전: 카메라의 역사
오늘날 우리가 알고 있는 초창기의 카메라는 1600년대 르네상스 시대의 카메라인 Obscura입니다.   
* Obscura: 핀홀 카메라 이론을 기반으로 한 카메라(생물학적으로 발전한 초기의 눈과 상당히 유사) 
* 이러한 카메라는 발전하면서 오늘날 보편적으로 쓰이게 되었음   
![image](https://user-images.githubusercontent.com/76824611/162037258-16fa66b3-2666-46fe-990e-e17ad6e451ad.png)   


[Hubel과 Wiesel의 연구]     
생물학자들은 1950/60년대 전기생리학을 이용하여  비전의 매커니즘을 연구 ➡️ 컴퓨터 비전에도 영감을 준 연구임    
* 연구주제: "포유류의 시각적 처리 메커니즘은 무엇일까?"     
* 연구방법: 고양이의 뇌를 연구(시각 처리 매커니즘만 보면 고양이와 인간은 비슷)      
   * 1) 그들의 고양이 두뇌 뒷면("일차 시각 피질"이 있는 곳)에 전극 몇 개를 꽂음.    
   * 2) 어떤 자극을 줘야 일차 시각 피질의 뉴런들이 격렬하게 반응하는지 관찰함.    
* 연구성과: 일차 시각 피질에는 다양한 종류의 세포가 있다는 것을 발견함. 
   * **경계(edges)가 움직이면 이에 반응하는 세포들** 발견     
   ➡️ 아주 단순하지만 중요한 세포임       
* 결과분석: **시각 처리가 처음에는 단순한 구조로 시작**되며 그 정보가 실제 세상을 제대로 인지할 수 있을 때까지 통로를 거치면서 **점점 복잡해진**다는 것임.    
![image](https://user-images.githubusercontent.com/76824611/162046650-5b244ed5-d619-4146-9bb0-caa33f543e78.png)


----

## 컴퓨터 비전의 역사
60년대 초반에 시작    

### Block World는 Larry Roberts의 연구    
컴퓨터 비전 분야에서의 최초의 박사 학위 논문임     
* 연구목표: 우리 눈에 보이는 세상을 인식하고 그 모양을 재구성하는 일(대부분의 시각 체계를 구현하는 것)      
* 연구방법: 이 연구에서는 우리 눈에 보이는 사물들을 기하학적 모양으로 단순화시킴         
* 연구의의: 컴퓨터 비전 연구의 시작을 알림 ➡️ 현재 전 세계 수천 명의 연구자들이 아직도 비전의 가장 근본적인 문제들을 연구하고 있음  
![image](https://user-images.githubusercontent.com/76824611/162046902-22f187b3-8cdd-47ef-bdd7-440240fd486c.png)


컴퓨터 비전은 아직 숙제가 많지만, 인공지능 분야에서 가장 중요하고 빠르게 성장하는 분야 중 하나임




### David Marr의 책 
* 책 내용: 책은 그가 비전을 무엇이라 생각하는지, 그리고 어떤 방향으로 컴퓨터 비전이 나아가야 하는지, 그리고 컴퓨터가 비전을 인식하게 하기 위해 어떤 방향으로 알고리즘을 개발해야 하는지를 다룬 책이었습니다.
* 핵심 주제: 우리가 눈으로 받아들인 "이미지"를 "최종적인 full 3D 표현"으로 만드는 방법    
   * 1 단계) "Primal Sketch": 이 과정은 주로 경계(edges), 막대(bars), 끝(ends), 가상의 선(virtual lines), 커브(curves), 경계(boundaries)가 표현되는 과정 -> 신경과학자들에게 영감을 받음    
   ➡️ 시각처리의 초기 단계는 경계와 같은 단순한 구조와 아주 밀접한 관계 존재      
   * 2단계) "2.5-D sketch": 시각 장면을 구성하는 표면(surfaces) 정보, 깊이 정보, 레이어, 불연속 점과 같은 것들을 종합    
   * 3단계) 앞선 단계의 모든 것을 한데 모아서 surface and volumetric primives의 형태의 계층적으로 조직화된 최종적인 3D 모델을 만들어 냄    
* 책의 의의: 이런 방식은 "비전이 무엇인가"라는 것에 대한 아주 "이상적인" 사고과정이었음. 그리고 이런 방식의 사고방식은 실제로 수십 년간 컴퓨터 비전 분야를 지배했으며 학생들이 컴퓨터 비전을 처음 입문하고 나서 "어떻게 시각정보를 분석할 수 있을까"라는 질문을 던졌을 때 직관적인 생각해 볼 수 있는 방법이었습니다.
![image](https://user-images.githubusercontent.com/76824611/162049238-e2297ca0-224d-4a9a-a76e-02576a3f3f67.png)


### Recognition via Parts
70년대의 중요한 연구     
* 연구목표: 우리는 어떻게 해야 장난감 같은 단순한 블록 세계를 뛰어넘어서 실제 세계를 인식하고 표현할 수 있을까?"라는 질문을 하기 시작함.     
* 실험제약: 70 년대엔 사용할 수 있는 데이터가 거의 없었음, 컴퓨터도 정말 느림(심지어 PC가 보급되기도 전임) 
➡️ 이 상황에서 컴퓨터 과학자들은 어떻게해야 대상을 인식하고 표현할 수 있을지를 고민하기 시작함    
* 실험 아이디어: "모든 객체는 단순한 기하학적 형태로 표현할 수 있다"      
   * 아이디어 1: "generalized cylinder"  
   * 아이디어 2: "pictorial structure"  
* 실험 방법
   * 가령 사람은 원통 모양을 조합해서 만들 수 있음 (왼쪽 그림)   
   * 또는 "주요 부위"와 "관절"로 표현할 수도 있을 것입니다. (오른쪽 그림)   
   ![image](https://user-images.githubusercontent.com/76824611/162050382-022fc151-d6ab-488d-bfa0-632c39eef075.png)
   * 두 방법 모두 단순한 모양과 기하학적인 구성을 이용해서 복잡한 객체를 단순화시키는 방법임    
* 연구의의: 이러한 연구들은 수년간 다른 연구에 상당히 많은 영향을 줌    


## David Lowe
80년대 또 다른 사례    
 어떻게 하면 단순한 구조로
실제 세계를 재구성/인식할 수 있을지 고민했습니다.

160
00:19:33,699 --> 00:19:43,440
David Lowe는 이 연구에서 면도기를 인식하기 위해서 면도기를

161
00:19:43,440 --> 00:19:50,860
선(lines)과 경계(edges) 그리고 직선(straight lines)
그리고 이들의 조합을 이용해서 구성했습니다.

162
00:19:50,860 --> 00:20:10,410
60/70/80년대에는 컴퓨터 비전으로 어떤 일을 할 수 있을까
고민한 시대였습니다. 하지만 너무 어려운 문제였습니다.

163
00:20:10,410 --> 00:20:17,980
지금까지 제가 보여 드린 연구들이 모두 아주 대담했고
큰 야망을 가진 시도였지만

164
00:20:17,980 --> 00:20:24,160
그들은 단순한 수준(toy example)에 불과했습니다.

165
00:20:24,160 --> 00:20:38,019
현실 세계에서 잘 동작할지를 생각해보면
많이 진보하지 못했다고 할 수 있죠. 그래서 컴퓨터 비전 연구자들은

166
00:20:38,019 --> 00:20:43,709
우리가 도대체 무슨 실수를 하고 있을까
고민하다가 한가지 질문을 떠올리게 됩니다.

167
00:20:43,709 --> 00:20:50,200
객체인식이 너무 어렵다면 우선 객체 분할(segmentation)
이 우선이 아니었을까 라고 말이죠

168
00:20:50,200 --> 00:20:58,760
객체분할은 이미지의 각 픽셀을 의미 있는
방향으로 군집화하는 방법입니다.

169
00:20:58,760 --> 00:21:03,880
픽셀을 모아놔도 사람을 정확히
인식할 수 없을지도 모르지만

170
00:21:03,880 --> 00:21:10,140
적어도 배경인 픽셀과 사람이 속해 있을지도
모르는 픽셀을 가려낼 수는 있었습니다.

171
00:21:10,140 --> 00:21:15,339
이를 "영상분할(Image Segmentation)"이라고 합니다.
이 문제를 다룬 아주 중요한 연구가 있었는데

172
00:21:15,339 --> 00:21:21,759
Berkeley 대학의 Jitendra Malik 교수와
그의 제자인 Jianbo Shi의 연구였습니다.

173
00:21:21,760 --> 00:21:29,880
이 연구는 영상분할 문제를 해결하기 위해서
그래프 이론을 도입했습니다.

174
00:21:29,880 --> 00:21:39,600
그리고 컴퓨터 비전에서 유난히
발전 속도가 빨랐던 분야가 있었습니다.

175
00:21:39,610 --> 00:21:45,850
바로 "얼굴인식" 입니다. 인간에게 가장
중요한 부위 중 하나가 바로 얼굴이죠.

176
00:21:45,850 --> 00:21:51,779
어쩌면 얼굴이 가장 중요할 수도 있겠군요.

177
00:21:51,779 --> 00:22:05,220
대략 1999/2000년대에는 "기계학습", 특히나 "통계적 기계학습"
이라는 방법이 점차 탄력을 얻기 시작했습니다.

178
00:22:05,220 --> 00:22:11,620
가령 "Support Vector Machine", "Boosting", "Graphical models"
그리고 초기 "Neural Network" 등이 있었습니다.

179
00:22:11,620 --> 00:22:18,449
그중 가장 큰 기여를 한 연구는 바로

180
00:22:18,449 --> 00:22:24,939
Paul Viola와 Michael Jones이 AdaBoost를
이용해 실시간 얼굴인식에 성공한 것입니다.

181
00:22:24,939 --> 00:22:31,779
이 연구는 당시 아주 대단한 성과였습니다.
연구 당시는 2001년이었고

182
00:22:31,779 --> 00:22:36,730
컴퓨터는 여전히 엄청 느렸습니다.
하지만 그들의 얼굴인식 알고리즘은

183
00:22:36,730 --> 00:22:50,800
실시간과 가깝게(near-real-time) 인식할 수
있었고 논문발표 5년이 지난 -

184
00:22:50,800 --> 00:22:58,960
2006년에 Fujifilm은 실시간 얼굴인식을 지원하는
최초의 디지털카메라를 선보였습니다.

185
00:22:58,960 --> 00:23:05,960
이는 기초 과학 연구의 성과를 실제 응용 제품으로
가장 빠르게 전달한 사례라고 할 수 있습니다.

186
00:23:05,960 --> 00:23:13,920
자! 이제 다시 "어떻게 객체를 잘 인식할 것인가?"
라는 질문으로 다시 한번 돌아가 봅시다.

187
00:23:13,930 --> 00:23:31,300
90년대 후반부터 2010년도까지의 시대를 풍미했던 알고리즘은
"특징기반 객체인식 알고리즘" 이었습니다. 이 시절 나온 -

188
00:23:31,300 --> 00:23:39,670
아주 유명한 알고리즘이 바로 David Lowe의 SIFT
feature입니다. 그의 아이디어는 전체 객체를 -

189
00:23:39,670 --> 00:23:44,860
가령, 여기 정지 표지판이 있습니다.
이 정지 표지판들을 서로 매칭하기는 상당히 어렵죠

190
00:23:44,860 --> 00:23:57,210
카메라 앵글이 변할 수 있고, 겹치거나 화각이 변하고 빛도
변하고 객체 자체도 얼마든지 변할 수 있습니다.

191
00:23:57,210 --> 00:24:15,000
하지만 그들은 객체의 특징 중 일부는 다양한 변화에 조금 더 강인하고
불변하다는 점을 발견했습니다.

192
00:24:15,010 --> 00:24:21,610
그리하여 객체인식은 객체에서 이와 같은
중요한 특징들을 찾아내고

193
00:24:21,610 --> 00:24:28,569
그 특징들을 다른 객체에 매칭시키는 과제가 되었습니다.
이미지 전체를 매칭하는 일보다 훨씬 쉬운 일이었죠.

194
00:24:28,569 --> 00:24:42,060
이 그림은 그 논문에서 가져온 것입니다. 정지표지판
이미지에서 일부 SIFT 특징들을 추출하고

195
00:24:42,060 --> 00:24:49,440
또 다른 정지 표지판에서도 특징을 추출하여
이를 식별하고 매칭합니다.

196
00:24:51,130 --> 00:24:59,330
이미지에 존재하는 "특징"을 사용하게 되면서

197
00:24:59,330 --> 00:25:04,780
컴퓨터 비전은 또 한 번의 도약을 할 수 있었습니다.
그리고 장면 전체를 인식하기에 이르렀습니다.

198
00:25:04,780 --> 00:25:18,620
한 예로, Spatial Pyramid Matching이 있습니다.
기본 아이디어는 우리가 특징들을 잘 뽑아낼 수만 있다면

199
00:25:18,620 --> 00:25:23,750
그 특징들이 일종의 "단서"를 제공해 줄 수 있다는 것이었죠
이미지가 풍경인지, 부엌인지, 또는 고속도로인지 하는 것을 말이죠.

200
00:25:23,750 --> 00:25:37,130
이 연구는 이미지 내의 여러 부분과 여러 해상도에서
추출한 특징을 하나의 특징 기술자로 표현하고

201
00:25:37,130 --> 00:25:44,780
Support Vector Algorithm을 적용합니다.

202
00:25:44,780 --> 00:25:53,930
이런 방식의 연구들은 사람 인식에도 탄력을 주었습니다.

203
00:25:53,930 --> 00:26:02,990
여러 특징들을 잘 조합해 보자는 시도들이었죠
사람인식에 관련된 연구들도 아주 많았습니다.

204
00:26:02,990 --> 00:26:10,490
사람인식과 관련된 연구들은 어떻게 해야 사람의 몸을
현실적으로 모델링할 수 있을지에 관련된 연구였습니다.

205
00:26:10,490 --> 00:26:15,710
그중 하나는 "Histogram Of Gradients"
입니다. , 또 한가지는 -

206
00:26:15,710 --> 00:26:26,770
"Deformable Part Models" 입니다.
그 당시에는 60/70/80년대를 거치고

207
00:26:26,770 --> 00:26:34,160
21세기를 맞이하고 있었고
하나의 변곡점을 마주하게 됩니다.

208
00:26:34,160 --> 00:26:45,680
사진의 품질이 점점 좋아졌습니다. 인터넷과 디지털카메라의 발전은
더더욱 좋은 실험 데이터를 만들어 낼 수 있었습니다.

209
00:26:45,680 --> 00:27:02,840
2000년대 초에 일궈낸 것 중 하나는 바로 컴퓨터 비전이 앞으로
풀어야 할 문제가 무엇인지의 정의를 어느 정도 내렸다는 것입니다.

210
00:27:02,840 --> 00:27:11,120
물론 해결해야 할 다양한 문제가 있겠지만, 이 또한
아주 중요한 문제였습니다. 바로 "객체인식" 입니다.

211
00:27:11,120 --> 00:27:18,950
제가 여태 말씀드린 것들이 객체인식이지만 2000년대 초 -

212
00:27:18,950 --> 00:27:26,600
우리는 Benchmark Dataset를 모으기 시작했습니다.
객체인식 기술의 어디쯤 왔는지 측정해 보기 위해서였죠

213
00:27:26,600 --> 00:27:41,480
그 중 하나는 PASCAL Visual Object Challenge(VOC)
입니다. 이 데이터셋에는 20개의 클래스가 있고

214
00:27:41,480 --> 00:27:57,440
여기 보이는 것들과 같이 기차, 비행기, 사람이 있고 소, 병, 고양이
등도 있는 것으로 기억합니다. 데이터셋은

215
00:27:57,440 --> 00:28:04,280
클래스당 수천 수만 개의 이미지들이 있었으며,
다양한 연구 집단에서

216
00:28:04,280 --> 00:28:11,750
이를 통해 알고리즘의 자신들의 알고리즘을 테스트했고

217
00:28:11,750 --> 00:28:19,870
얼마나 진보했는지를 지켜보았습니다.
여기 2007년부터 2012년도까지의 표가 있습니다.

218
00:28:19,870 --> 00:28:38,680
객체인식 성능은 꾸준히 증가했습니다. 많은 진보가 이루어졌죠

219
00:28:38,680 --> 00:28:53,330
그 무렵, Princeton과 Stanford에 있던 그룹에서 더 어려운
질문을 던졌습니다. 우리는 이 세상의 모든 객체들을

220
00:28:53,330 --> 00:29:00,260
인식할 준비가 되었는가? 였습니다.
이 질문은 한 사실로부터 비롯되었습니다.

221
00:29:00,260 --> 00:29:07,970
거의 대부분의 기계학습 알고리즘에 해당하는 사실이었습니다.

222
00:29:07,970 --> 00:29:20,070
Graphical Model, SVM, AdaBoost 같은 기계학습
알고리즘들이 트레이닝 과정에서 Overfit을 하는 것 같았습니다.

223
00:29:20,070 --> 00:29:25,410
이 문제의 원인 중 하나는 시각 데이터가 너무 복잡하다는 것입니다.

224
00:29:25,410 --> 00:29:37,559
모델의 입력은 복잡한 고차원 데이터였고, 이로 인해
모델을 fit하려면 더 많은 파라미터가 필요했죠

225
00:29:37,559 --> 00:29:44,160
학습 데이터가 부족하면 Overfiting이 훨씬 더 빠르게 발생했고
일반화 능력이 떨어졌습니다.

226
00:29:44,160 --> 00:29:52,440
우리에게는 두 가지 motivation이 있었습니다. 하나는
이 세상의 모든 것들을 인식하고 싶다는 것이고

227
00:29:52,440 --> 00:30:04,620
또 하나는 기계학습의 Overfiting 문제를 극복해보자는 것이었죠

228
00:30:04,620 --> 00:30:17,900
이 동기를 바탕으로 ImageNet 프로젝트를 시작했습니다.
구할 수 있는 모든 이미지를 담은 가장 큰 데이터셋을 만들고 싶었습니다.

229
00:30:17,910 --> 00:30:23,250
이 데이터셋으로 모델을 학습시킬 수 있고 Benchmark도
할 수 있도록 말이죠. 프로젝트는 약 3년 정도 걸렸습니다.

230
00:30:23,250 --> 00:30:37,620
어려운 일도 많았습니다. 우선 인터넷에서 수십억 장의 이미지를
다운받았고 WordNet이라는 Dictionary로 정리했습니다.

231
00:30:37,620 --> 00:30:45,770
WordNet에는 수천 가지의 객체 클래스가 있습니다.

232
00:30:45,770 --> 00:30:52,230
그리고 Clever Crowd Engineering trick을 도입했습니다.
Amazon Mechanical Turk에서 사용하는

233
00:30:52,230 --> 00:31:02,270
이미지의 정렬, 정제, 레이블 등을 제공하는 플랫폼입니다.

234
00:31:02,270 --> 00:31:10,830
그 결과 ImageNet은 대략 15만 장에 달하는 이미지와 22만 가지의
클래스 카테고리를 보유하게 되었습니다.

235
00:31:10,830 --> 00:31:35,759
아마도 당시 AI 분야에서 만든 가장 큰 데이터셋 이었습니다.
ImageNet 덕분에 객체인식은 다른 국면으로 접어들었습니다.

236
00:31:35,759 --> 00:31:41,200
하지만 ImageNet을 Benchmark에 어떻게
활용하는지가 큰 화두였습니다.

237
00:31:41,200 --> 00:31:57,309
그래서 ImageNet 팀은 2009년부터 국제 규모의 대회를
주최했습니다. ILSVRC입니다. 이 대회를 위해서

238
00:31:57,309 --> 00:32:06,190
1000개의 객체에서 140만 개의 test set 이미지를 엄선했습니다.

239
00:32:06,190 --> 00:32:13,629
이 대회의 목적은 이미지 분류 문제를 푸는
알고리즘들을 테스트하기 위함이었습니다.

240
00:32:13,629 --> 00:32:42,909
여기 예제 이미지들이 있습니다. 참가자들은 정답 후보를 총 5가지
고를 수 있습니다. 5개 중에 정답이 있으면 맞춘 것이죠

241
00:32:42,909 --> 00:32:49,720
Image Classification Challenge의 결과입니다.
2010년도부터 2015년도까지의 결과입니다.

242
00:32:49,720 --> 00:33:00,740
x은 연도를, y축은 오류율을 입니다.

243
00:33:00,740 --> 00:33:06,820
좋은 소식으로는 오류율이 점차 감소하고 있습니다.

244
00:33:06,820 --> 00:33:15,369
2012년도의 오류율은 사람보다 낮습니다. 여기에서의 사람은

245
00:33:15,369 --> 00:33:32,470
Stanford의 한 PhD 학생입니다. 이 대회에 참가한 알고리즘처럼
테스트를 수행하며 몇 주를 보내야만 했습니다.

246
00:33:32,470 --> 00:33:43,110
비록 컴퓨터 비전이 아직 객체인식의 모든 문제를
풀지는 못했지만, 진전이 있었다는 것은 사실입니다.

247
00:33:43,110 --> 00:33:56,400
하지만 실생활에 적용하기에는 턱없이 부족했던 낮은 오류율이
인간의 수준으로 오기까지는

248
00:33:56,400 --> 00:34:05,640
불과 몇 년뿐이 걸리지 않았습니다. 그리고 여러분이 이 그래프에서
절대로 놓쳐서 안 되는 특별한 순간이 있습니다.

249
00:34:05,640 --> 00:34:25,649
바로 2012년입니다. 처음 2년 동안은 오류율이 약 25%를 맴돌았습니다.
2012년에는 오류율이 16%로 거의 10%가량 떨어졌고

250
00:34:25,650 --> 00:34:32,969
물론 현재의 오류율이 더 낮지만 2012년도의 감소는 아주 중요합니다.

251
00:34:32,969 --> 00:34:42,569
2012년도에 우승한 알고리즘은
convolutional neural network 모델입니다.

252
00:34:42,570 --> 00:34:49,850
CNN은 그 당시 다른 알고리즘들을 능가하고
ImageNet Challenge에서 우승하였습니다.

253
00:34:49,850 --> 00:34:58,200
우리가 한 학기 동안 배울 내용이 바로
Convolutional neural network에 관한 것입니다.

254
00:34:58,200 --> 00:35:10,370
CNN 모델이 무엇인지 심도 깊게 다룰 것입니다.
CNN을 Deep Learning이라고도 합니다.

255
00:35:10,520 --> 00:35:15,330
Deep Learning이 더 유명한 이름이겠군요

256
00:35:15,330 --> 00:35:26,400
앞으로 CNN이 무엇인지, 어떤 법칙이 있는지, 어떤 선례가 있는지,
이 모델의 최근 동향은 어떠한지를 살펴볼 것입니다. 하지만 이 역사의

257
00:35:26,400 --> 00:35:41,309
시작은 바로 2012년입니다. CNN, Deep learning 모델은
컴퓨터 비전 분야의 진보를 이뤄냄으로써 CNN의 우수성을

258
00:35:41,309 --> 00:35:51,900
입증하였습니다. 자연어 처리나 음성 인식과 같은 다른
관련 분야들도 더불어서 말이죠. 소개는 이쯤 해두고

259
00:35:51,900 --> 00:36:02,500
CS231n 수업 소개를 위해서 나머지 시간은
Justin에게 맡기도록 하겠습니다.

260
00:36:03,000 --> 00:36:08,158
Fei-Fei 교수님 감사합니다.
제가 여기서 이어받겠습니다.

261
00:36:08,189 --> 00:36:14,077
지금부터는 주제를 바꿔서 우리 수업과 관련된
이야기를 해볼까 합니다.

262
00:36:15,436 --> 00:36:22,950
이 수업에서 중점적으로 다룰 문제는
Image Classification입니다.

263
00:36:22,950 --> 00:36:27,037
앞서 ImageNet Challenge 이야기에서
살짝 들어보셨을 것입니다.

264
00:36:27,037 --> 00:36:31,470
Image Classification의 문제 정의를 해보자면
알고리즘이 이미지 한 장을 봅니다.

265
00:36:31,470 --> 00:36:36,443
몇 개의 고정된 카테고리 안에서 정답 하나를 고르는 거죠

266
00:36:36,443 --> 00:36:42,506
이 방법이 다소 한정적이거나 인위적으로 보일 수도
있지만 사실 매우 일반적입니다.

267
00:36:42,506 --> 00:36:49,630
이 문제는 다양한 환경에 적용될 수 있습니다. industry이던
academia이던 말이죠. 다양한 곳에서 적용 가능합니다.

268
00:36:49,630 --> 00:36:58,043
가령 음식, 음식의 칼로리, 미술작품들 등을 인식해야
하는 다양한 제품에 적용할 수 있습니다.

269
00:36:58,043 --> 00:37:08,503
따라서 image classification이라는 간단한 도구가
자체로도 유용할뿐더러 다양한 응용이 될 수도 있습니다.

270
00:37:08,503 --> 00:37:19,660
수업에서는 다양한 문제들을 다룰 것입니다. 하지만 이 문제들 모두
image classification 기반하에 일궈진 것들입니다.

271
00:37:19,660 --> 00:37:24,783
그리고 object detection과 image captioning도
배워 볼 것입니다.

272
00:37:24,783 --> 00:37:28,435
object detection 문제는
classification과 조금 다릅니다.

273
00:37:28,435 --> 00:37:40,351
이 이미지가 고양이다, 개다, 말이다 이렇게 하는 실제로
어디에 있는지 네모박스를 그릴 수 있어야 합니다.

274
00:37:40,351 --> 00:37:44,110
네모박스를 객체의 위치에 정확히 그려 넣어야 합니다.

275
00:37:44,110 --> 00:37:51,475
image captioning 도 배울 것입니다. 이미지가 입력으로 주어지면
이미지를 묘사하는 적절한 문장을 생성해야 합니다.

276
00:37:51,475 --> 00:37:55,599
이 문제가 어렵고 복잡해 보이고 Image classification과도
별로 관련이 없어 보일 수 있지만

277
00:37:55,599 --> 00:38:02,880
image classification 기술을 이런 문제들에서
충분히 재사용할 수 있습니다.

278
00:38:06,482 --> 00:38:14,398
지금까지는 ImageNet Challenge의 맥락에서 말씀드렸습니다만
최근 컴퓨터 비전 분야의 진보를 이끌어낸 주역은 바로

279
00:38:14,398 --> 00:38:20,350
Convolutional neural networks, 즉
CNN입니다. 또는 convnet으로도 불리죠

280
00:38:20,350 --> 00:38:26,827
지난 몇 해 간 ImageNet Challenge의 우승자들을 살펴봅시다.

281
00:38:26,827 --> 00:38:32,631
2011년에서 Lin et al의 알고리즘은 보시면
여전히 계층적(hierarchical)이죠

282
00:38:32,631 --> 00:38:41,211
여러 단계가 있습니다. 특징들을 뽑고, 지역 불변 특징들을 계산하고,
pooling을 거치고 이렇게 여러 단계를 거쳐서

283
00:38:41,211 --> 00:38:46,276
최종적인 특징 기술자를 Linear SVM에 태웁니다.

284
00:38:46,276 --> 00:38:52,583
핵심은 여전히 "계층적" 이라는 점입니다. edges를 뽑고
"불변 특징" 의 개념도 들어있습니다.

285
00:38:52,583 --> 00:38:56,177
그리고 대부분 이러한 직관들은
CNN에도 영향을 미칩니다.

286
00:38:56,177 --> 00:38:59,115
하지만 2012년 가장 획기적인 순간이었습니다.

287
00:38:59,115 --> 00:39:09,225
당시 Toronto에서 Jeff Hinton 교수님의 연구실의
PHD였던 Alex Krizhevsky와 Ilya Sutskever는

288
00:39:09,225 --> 00:39:12,504
7-Layer Convolutional neural network
을 만들었습니다.

289
00:39:12,504 --> 00:39:19,651
AlexNet 또는 Supervision으로도 알려져 있습니다.
AlexNet은 LSVRC'12 에서 아주 좋은 성과를 달성했습니다.

290
00:39:19,651 --> 00:39:24,197
이후 ImageNet의 우승 트로피는
매년 Neural Network의 몫이었습니다.

291
00:39:24,197 --> 00:39:28,096
그리고 이러한 추세로 CNN은 매년 더 깊어져 갔습니다.

292
00:39:28,096 --> 00:39:33,592
AlexNet은 7(8)-Layer Neural Network입니다.
Layer를 세는 방식에 따라 조금 다릅니다.

293
00:39:33,592 --> 00:39:43,172
2015년에 네트워크가 훨씬 더 깊어졌습니다. Google의
GoogleNet 그리고 Oxford의 VGG가 바로 그 주인공이죠.

294
00:39:43,172 --> 00:39:52,373
2015년에는 정말 대박입니다. MSRA의 Residual Network의
Layer 수는 152개에 육박합니다.

295
00:39:52,373 --> 00:39:58,505
이후 Layer 200개까지 쌓으면 성능이 더 좋아진다고는 하지만
아마도 여러분의 GPU 메모리가 감당할 수 없을 것입니다.

296
00:39:58,505 --> 00:40:00,352
나중에 더 다루기로 하죠

297
00:40:00,352 --> 00:40:13,479
오늘 수업에서 알고 가셔야 할 점은 2012년의 CNN의 시대가 도래했고,
이후 CNN을 개선하고 튜닝하려는 많은 시도들이 있었다는 것입니다.

298
00:40:13,479 --> 00:40:19,949
그리고 이 강의 전반에 걸쳐 CNN 모델들이 어떻게
동작하는지는 심도 깊게 살펴볼 것입니다.

299
00:40:22,514 --> 00:40:32,394
하지만 한 가지 명심하셔야 할 점은 CNN이 2012년
ImageNet Challenge에서 빛을 본 것은 사실이지만

300
00:40:32,394 --> 00:40:36,551
CNN이 2012년에 발명된 것은 아닙니다.

301
00:40:36,551 --> 00:40:40,310
사실 CNN은 아주 오래전부터 존재했습니다.

302
00:40:40,310 --> 00:40:53,633
CNN의 기초연구라고 한다면 90년도의 Jan LeCun과
Bell Labs와의 공동 과제를 말씀드릴 수 있습니다.

303
00:40:53,633 --> 00:40:58,829
1998년에 그들은 숫자인식을 위해 CNN을 구축했습니다.

304
00:40:58,829 --> 00:41:07,366
이들은 자필 수표 자동 판독과
우편주소 자동인식에 CNN을 적용하고 싶었습니다.

305
00:41:07,366 --> 00:41:17,237
그들은 이미지를 입력으로 받아서 숫자와 문자를
인식할 수 있는 CNN을 만들었습니다.

306
00:41:17,237 --> 00:41:23,618
CNN의 구조만 보자면 2012년의 AlexNet과 유사합니다.

307
00:41:23,618 --> 00:41:29,080
그림처럼, raw pixel을 입력으로 받아 여러
Convolution Layer Layer를 거치고 Sub-Sampling,

308
00:41:29,080 --> 00:41:31,398
Fully Connected Layer를 거치게 됩니다.

309
00:41:31,398 --> 00:41:34,714
이 모든 건 다음 강의 부터 더 자세히 다루도록 하겠습니다.

310
00:41:34,714 --> 00:41:38,397
하지만, 여러분이 이 두 그림을 보고 있자면
둘이 상당히 비슷해 보일 겁니다.

311
00:41:38,397 --> 00:41:48,420
2012년의 CNN 아키텍쳐들은 서로 비슷비슷했습니다.
90년대의 LeNet 아키텍처를 공유하기 때문입니다.

312
00:41:49,299 --> 00:41:53,377
그럼 이런 질문을 할 수 있겠군요
90년대부터 알고리즘이 있었다면

313
00:41:53,377 --> 00:41:57,454
왜 최근에야 갑자기 유명해진 것일까요?

314
00:41:57,454 --> 00:42:03,277
90년대 이래로 아주 큰 혁신들이 있었습니다.

315
00:42:03,277 --> 00:42:09,217
하나는 바로 계산능력입니다. 무어의 법칙 덕분에
컴퓨터의 계산속도가 매년 빨라졌습니다.

316
00:42:09,217 --> 00:42:18,574
완벽한 척도는 아니지만, CPU의 트랜지스터 개수만 세어봐도
90년대보다 몇십 배 이상 발전했음을 알 수 있죠

317
00:42:18,574 --> 00:42:25,878
또한 graphics processing units의 진보도 한몫했습니다.
GPU는 아주 강력한 병렬처리가 가능한데

318
00:42:25,878 --> 00:42:33,032
계산 집약적인 CNN 모델을 고속으로
처리하는 데 안성맞춤입니다.

319
00:42:33,032 --> 00:42:42,150
단지 더 많은 계산이 가능하다는 것만으로도
연구자들이 더 큰 아키텍쳐를 연구해 볼 수 있었고

320
00:42:42,150 --> 00:42:48,476
경우 따라서는 기존의 고전 알고리즘들의 크기만 키웠음에도
훨씬 더 잘 동작하는 경우도 많았습니다.

321
00:42:48,476 --> 00:42:55,554
연산량의 증가는 딥러닝의 역사에서 아주 중요한 요소입니다.

322
00:42:55,554 --> 00:43:00,559
90년대와 지금은 데이터의 차이도 있었습니다.

323
00:43:00,559 --> 00:43:09,395
CNN 알고리즘이 잘 동작하려면 아주 많은
레이블이 매겨진 이미지가 필요합니다.

324
00:43:09,395 --> 00:43:23,614
90년대에는 레이블이 매겨진 이미지 데이터를 구하기가 쉽지 않았습니다.
아주 크고 다양한 데이터셋을 수집하기가 힘든 시기였습니다.

325
00:43:23,614 --> 00:43:33,176
오늘날은 PASCAL이나 ImageNet 같은 규모가 크고
잘 분류된 레이블들을 가진 데이터셋이 많습니다.

326
00:43:34,228 --> 00:43:38,775
90년대에와 비교하면 사용 가능한 데이터셋이 훨씬 많습니다.

327
00:43:38,775 --> 00:43:43,153
큰 데이터셋들을 잘 활용하면
Higher Capacity Model을 만들 수 있습니다.

328
00:43:43,153 --> 00:43:47,157
그렇게 학습시킨 모델들은 실생활 문제에서도 잘 동작했습니다.

329
00:43:47,157 --> 00:43:56,117
하지만 가장 중요한 것은 CNN이 엄청 좋아 보이고 새로워 보이고
몇 해 전에 갑자기 툭 하고 튀어나온 것처럼 보이지만

330
00:43:56,117 --> 00:43:57,527
그렇지 않다는 것입니다.

331
00:43:57,527 --> 00:44:03,666
CNN스러운 알고리즘들은 이미 아주 오래전부터 있었습니다.

332
00:44:05,015 --> 00:44:12,755
그리고 또 한 가지 중요한 점은 컴퓨터 비전 연구의 목적은
"사람처럼 볼 수 있는" 기계를 만드는 것입니다.

333
00:44:12,755 --> 00:44:16,650
사람들은 시각 체계를 통해 아주 많은 것들을 할 수 있습니다.

334
00:44:16,650 --> 00:44:24,988
여러분은 고양이나 강아지를 찾아서 사각형을
그리는 것 이상의 일들을 할 수 있습니다.

335
00:44:24,988 --> 00:44:27,711
여러분의 시각체계는 컴퓨터 비전보다 훨씬 더 강력합니다.

336
00:44:27,711 --> 00:44:34,047
컴퓨터 비전 분야 이야기를 해드리자면 아직도 우리가 풀어야 할
수많은 도전과제와 미해결 문제가 있습니다.

337
00:44:34,047 --> 00:44:40,220
우리는 더 나은 일을 하고, 더 야심 찬 문제에 도전할 수 있도록
알고리즘을 계속해서 연구해야 합니다.

338
00:44:40,220 --> 00:44:44,043
아직 풀지 못한 문제들의 예를 한번 살펴보겠습니다.
사실 예전부터 연구가 활발히 진행됐습니다.

339
00:44:44,043 --> 00:44:46,923
Semantic Segmentation 즉
Perceptual Grouping 같은 문제들이죠

340
00:44:46,923 --> 00:44:53,866
이미지 전체를 레이블링하는 것 대신
모든 픽셀 하나하나를 이해하는 것입니다.

341
00:44:53,866 --> 00:44:56,846
Semantic Segmentation은 다음에 다시 다루도록 하죠

342
00:44:56,846 --> 00:45:06,127
3D understanding은 실세계를 재구성하는 문제입니다.
제 생각에는 여전히 완벽하게 풀지는 못한 문제이죠

343
00:45:07,498 --> 00:45:10,178
여러분도 엄청나게 많은 것들을 상상해 볼 수 있습니다.

344
00:45:10,178 --> 00:45:11,817
행동 인식의 예를 들어볼까요

345
00:45:11,817 --> 00:45:19,469
가령 어떤 사람이 비디오에서 무언가를 하고 있을 때, 그 행동을 인식할
수 있는 가장 좋은 방법은 무엇일까요? 상당히 도전적인 문제입니다.

346
00:45:19,469 --> 00:45:27,578
그리고 증강현실, 가상현실, 또는 새로운 센서 등을 마주하게 되면

347
00:45:27,578 --> 00:45:32,455
그 자체를 한 분야로 다뤄도 될 만큼 아주 새롭고 흥미롭고
도전적인 문제들을 만나게 될 것입니다.

348
00:45:33,916 --> 00:45:42,228
지금부터 보여 드릴 것은 제가 연구실에서 진행 중인
프로젝트의 일부인데 Visual Genome이라는 데이터셋입니다.

349
00:45:42,228 --> 00:45:47,474
이 프로젝트에서는 실제 세상에서 복잡한 것들을 일부 
포착해 내려고 시도하고 있습니다.

350
00:45:47,474 --> 00:45:57,525
이미지에 박스만 치는 게 아니라 커다란 의미론적 그래프로 
표현하는 것이죠. 이 그래프는 객체를 식별하는 것을 넘어

351
00:45:57,525 --> 00:46:02,590
그 장면에서의 객체 간의 관계, 객체의 성격, 행동 등을 나타낼 수 있습니다.

352
00:46:02,590 --> 00:46:09,527
그리고 이런 방식을 이용한다면 실제 세상을
일부는 포착할 수 있지 않을까 예상합니다.

353
00:46:09,527 --> 00:46:12,889
이런 것들은 우리가 단순하게 Classification만 할 때
활용하지 못했던 것들입니다.

354
00:46:12,889 --> 00:46:15,270
이 프로젝트는 현시점으로서는 표준 접근방식은 아니지만

355
00:46:15,270 --> 00:46:24,840
Image Classification만으로는 포착해 낼 수 없는 훨씬 더 
다양한 일들이 있다는 사실을 알려 드리고 싶었습니다.

356
00:46:28,003 --> 00:46:31,592
그런 관점에서, 정말 재밌는 연구가 하나 있습니다.

357
00:46:31,592 --> 00:46:38,952
Fei-Fei 교수님의 Cal Tech에서의 박사과정 시절의 연구입니다.

358
00:46:38,952 --> 00:46:44,604
연구를 소개해 드리자면 임의의 사람들을 붙잡아서 
이런 사진을 아주 잠시 동안만 보여줬습니다.

359
00:46:44,604 --> 00:46:47,896
사람들에게 아주 짧은 시간 동안만 이미지를 보여준 것입니다.

360
00:46:47,896 --> 00:46:56,473
그런데 사람들은 이미지를 아주 잠깐만 봤음에도 이 같은 
아주 긴 문장을 작성할 수 있었습니다.

361
00:46:56,473 --> 00:47:05,560
주목할만한 결과였습니다. 인간은 이미지를 짧은 시간만
보더라도 이렇게 묘사할 수 있었습니다.

362
00:47:05,560 --> 00:47:10,375
"사람들이 어떤 놀이 또는 싸움을 하고 있고, 두 명씩 짝지어져 있고,
왼쪽 사람은 무언가를 던지고 있고 -

363
00:47:10,375 --> 00:47:14,576
잔디밭인 것 같은 느낌이 드니까 밖인 것 같고.." 등등

364
00:47:14,576 --> 00:47:17,617
사람들이 이미지를 조금만 더 오래 볼 수만 있었다면
어땠을지 상상이 가실 것입니다.

365
00:47:17,617 --> 00:47:22,307
이들이 누구이고 왜 저곳에서 게임을 하는지에 대해서
소설을 한 편 쓸 수 있을지 모르겠습니다.

366
00:47:22,307 --> 00:47:27,187
외부 지식과 경험이 가미된다면 아마 끝도 없을 것입니다.

367
00:47:27,187 --> 00:47:34,663
이미지의 내용을 아주 풍부하고 깊게 이해하는 것은 
이는 컴퓨터 비전 분야가 진정으로 추구하는 방향입니다.

368
00:47:34,663 --> 00:47:44,460
제 생각에는 컴퓨터 비전이라는 분야에는 많은 진보가 
있었지만, 아직 가야 할 길은 멀고도 험난합니다.

369
00:47:44,460 --> 00:47:52,890
다른 예를 하나 더 들어보겠습니다. Andrej Karpathy의
블로그에서 가져온 이미지입니다. 아주 재미있는 이미지입니다.

370
00:47:52,890 --> 00:47:57,696
많은 사람들의 웃음을 자아냈습니다. 제가 보기에도 상당히 
재미있는 이미지입니다. 왜 이 이미지가 웃기죠?

371
00:47:57,696 --> 00:48:04,380
한 남자가 체중계에 서 있습니다. 뭐 보통 사람들이 
체중을 재려고 체중계를 사용하곤 합니다.

372
00:48:04,380 --> 00:48:10,900
그런데 어떤 사람이 뒤에서 몰래 체중계를 밟고 있군요.
우리는 체중계가 어떻게 동작할지 짐작할 수 있습니다.

373
00:48:10,900 --> 00:48:13,867
저 남자는 자신의 "부풀려진" 체중을 보게 될 것이란 것도 알 수 있습니다.

374
00:48:13,867 --> 00:48:16,819
하지만 더 많은 정보가 있습니다. 우리는 저 사람이 
평범한 사람이 아니란 것을 알고 있습니다.

375
00:48:16,819 --> 00:48:24,741
저 사람은 당시의 미국 대통령 Barack Obama입니다. 우리는 
미국의 대통령이라면 존경받는 정치인이어야 한다고 생각합니다.

376
00:48:24,741 --> 00:48:27,045
[웃음]

377
00:48:27,045 --> 00:48:31,304
적어도 동료에게 이런 식의 장난을 치지 말아야 하겠죠

378
00:48:31,304 --> 00:48:34,564
그런데 뒤쪽의 사람들이 이 장면을 보고 웃고 있군요

379
00:48:34,564 --> 00:48:37,912
이를 미루어 우리는 사람들이 이 장면을 어떻게 
받아드리는지 이해할 수 있습니다.

380
00:48:37,912 --> 00:48:42,866
그들도 우리와 같은 생각을 한다는 것을 알 수 있죠

381
00:48:42,866 --> 00:48:45,830
이건 정말 놀라운 것입니다.
이 이미지 한 장에 정말 많은 것들이 있습니다.

382
00:48:45,830 --> 00:48:53,002
컴퓨터 비전 알고리즘이 이런 진정한 깊은 이해를 
하기까지는 아직 갈 길이 멀다고 생각합니다.

383
00:48:53,002 --> 00:49:01,385
이 분야가 큰 진보를 이루긴 했지만 갈 길은 한참 남았습니다.
연구자로서 저에게는 아주 짜릿한 일입니다.

384
00:49:01,385 --> 00:49:06,694
앞으로 더 진보할 수 있는 흥미진진하고 재미있는 문제들이 
우리를 기다리고 있기 때문입니다.

385
00:49:07,913 --> 00:49:13,054
그래서 저는 컴퓨터 비전이 정말 재미있는 분야라는 
것을 여러분이 아셨으면 좋겠습니다.

386
00:49:13,054 --> 00:49:20,043
정말 재밌습니다. 그리고 매우 유용합니다. 
아주 다양한 방법으로 이 세상에 기여할 수 있습니다.

387
00:49:20,043 --> 00:49:28,134
컴퓨터 비전은 의학 진단, 자율주행, 로보틱스 등
어디든 적용할 수 있습니다.

388
00:49:28,134 --> 00:49:33,120
그리고 인간의 지능을 이해하기 위한 여러 핵심 아이디어들을
집대성하는 일종의 실마리가 될지도 모릅니다.

389
00:49:33,120 --> 00:49:37,141
제 생각에는 컴퓨터 비전은 정말 기상천외하고 재밌는 분야입니다.

390
00:49:37,141 --> 00:49:46,234
저는 여러분과 이 수업을 빌어 그런 알고리즘들이 실제로 어떻게 
동작하는지를 심도 깊게 다룰 수 있어서 정말 좋습니다.

391
00:49:46,234 --> 00:49:50,673
여기까지는 컴퓨터 비전의 역사에 대한 
저의 개인적인 견해였습니다.

392
00:49:50,673 --> 00:49:57,055
혹시 궁금한 점이 있으신가요?

393
00:49:57,055 --> 00:50:02,408
그러면 이제는 앞으로의 수업 방향에 대해서 말씀드리겠습니다.

394
00:50:02,408 --> 00:50:06,904
우선 교수진에 대해서 말씀드리겠습니다.
이 수업은 Fei-Fei Li 교수님께서 가르치십니다.

395
00:50:06,904 --> 00:50:11,271
이곳 Stanford의 컴퓨터 과학 교수님이시죠

396
00:50:11,271 --> 00:50:16,852
그리고 제 지도 교수님이시기도 하고
Stanford Vision Lab의 교수님이십니다.

397
00:50:16,852 --> 00:50:22,519
그리고 그 옆에 두 명은 저 Justin Tohnson과
Serena Yeung입니다. 그녀는 여기 앞에 서 있습니다.

398
00:50:22,519 --> 00:50:27,379
우리 둘은 Fei-Fei 교수님의 지도하에
다양한 컴퓨터 비전 과제를 수행하고 있는 PHD 학생입니다.

399
00:50:27,379 --> 00:50:31,920
우리에게 18명의 훌륭한 조교가 있습니다.

400
00:50:31,920 --> 00:50:34,179
대부분은 여기 앞에 앉아 있습니다.

401
00:50:34,179 --> 00:50:40,320
우리가 수업을 잘 진행하고, 모든 것이 잘 돌아갈 수 있도록
도와주는 얼굴 없는 영웅들입니다.

402
00:50:40,320 --> 00:50:42,365
그러니 그들에게 잘해주세요.

403
00:50:42,365 --> 00:50:44,196
[웃음]

404
00:50:44,196 --> 00:50:47,153
말씀드려야 할 점이 있다면, 이 강의는 세 번째 열리는 강의이지만

405
00:50:47,153 --> 00:50:53,050
Andrej Karpathy가 강의를 하지 않는 첫 번째 강의이기도 합니다.

406
00:50:53,050 --> 00:50:58,353
그는 저와 아주 친한 친구입니다. 그 친구는 아직 
안 죽었습니다. 괜찮습니다. 걱정 마세요.

407
00:50:58,353 --> 00:50:59,612
[웃음]

408
00:50:59,612 --> 00:51:11,617
이 수업의 발전과 역사의 대부분은 수년 동안 저와 함께 
일해준 그의 덕분이었습니다.

409
00:51:11,617 --> 00:51:15,398
그 사실은 여러분도 알고 계셨으면 좋겠습니다.

410
00:51:15,398 --> 00:51:22,209
강의에 대해 말해보자면, 조교들과 연락할 수 있는 가장
좋은 방법은 Piazza를 이용하는 것입니다.

411
00:51:22,209 --> 00:51:25,212
당장 가서 가입하세요

412
00:51:25,212 --> 00:51:30,353
Piazza는 이 수업에 대해 의사소통을 할 수 있는
우리가 가장 선호하는 방법입니다.

413
00:51:30,353 --> 00:51:34,313
만약 친구들 앞에서 질문하는 것이 꺼려진다면,

414
00:51:34,313 --> 00:51:40,572
Piazza에 가서 익명으로 질문하세요. private
질문을 올릴 수도 있습니다. 교직원에게 직접 연락하십시오.

415
00:51:40,572 --> 00:51:44,452
기본적으로 여러분이 필요한 것은 그저 Piazza를 경험하는 것입니다.

416
00:51:44,452 --> 00:51:46,445
조교들의 메일링 리스트가 있긴 하지만

417
00:51:46,445 --> 00:51:53,517
Piazza에 올리기 껄끄러운 개인적이거나 비공개적인 것들을 위해서만 쓰고

418
00:51:53,517 --> 00:52:02,125
혹시나 정말 중요한 말 못 할 사항이 있다면, 저나, Serena, Fei-Fei
교수님께 직접 메일을 주셔도 되겠습니다. 이메일로 보내주십시오.

419
00:52:02,125 --> 00:52:06,096
하지만 그 외의 조교와의 커뮤니케이션은 Piazza를
통해서야만 합니다.

420
00:52:06,096 --> 00:52:14,372
올해는 부교재도 있습니다. 필수는 아닙니다. 이 책 없이도 이
강의를 들을 수 있습니다. 이 교재는 스스로 구하셔야 합니다.

421
00:52:14,372 --> 00:52:19,786
개인적으로 아주 기분이 좋습니다. 이 책이 아마도 가장 
처음 출간된 딥러닝 교과서입니다.

422
00:52:19,786 --> 00:52:24,078
올해 초에 출간되었고 E.N. Goodfellow, 
Yoshua Bengio, Aaron Courville이 저자입니다.

423
00:52:24,078 --> 00:52:28,197
슬라이드에 아마존 링크를 추가했습니다.
원한다면 구입하도록 하세요

424
00:52:28,197 --> 00:52:32,943
하지만 온라인에 무료 컨텐츠도 있기 때문에
굳이 살 필요는 없습니다.

425
00:52:32,943 --> 00:52:34,261
다시 한번 말하지만, 이 교재는 완전히 선택 사항입니다.

426
00:52:34,261 --> 00:52:40,614
하지만 아마도 학기 동안 이 책의 일부를 읽으라고 공지할 수도 있습니다.
이 책은 여러분이 추가적인 관점을 얻는 데 많은 도움을 줄 것입니다.

427
00:52:41,697 --> 00:52:48,794
이 수업의 철학은 여러분이 딥러닝에 관한 모든 알고리즘을 
제대로 이해해야 한다는 것입니다.

428
00:52:48,794 --> 00:52:56,097
아주 깊은 수준에서 이해해야 합니다. 어떤 Neural network 모델을
사용했을 때 정확히 어떻게 작동하는지 정확하게 이해해야 합니다.

429
00:52:56,097 --> 00:53:02,314
그리고 그런 아키텍쳐를 선택하면 어떤 영향을 미치는지, 
네트워크가 어떻게 학습되고 테스팅 되는지와 같은 것들을 알아야 합니다.

430
00:53:02,314 --> 00:53:08,757
그리고 여러분은 나만의 CNN을 Python으로 밑바닥부터 구현해야 합니다.

431
00:53:08,757 --> 00:53:16,320
여러분이 전체 foward/backward passes을 직접 구현하다 보면
결국 CNN모델을 완벽히 구현하게 될 것입니다.

432
00:53:16,320 --> 00:53:18,320
저는 이것이 정말 좋다고 생각합니다.

433
00:53:18,320 --> 00:53:25,613
하지만 실제로는 대부분의 사람들이 CNN을 
밑바닥부터 구현하지는 않을 것입니다.

434
00:53:25,613 --> 00:53:31,326
그렇기 때문에 여러분에게도 아주 실용적인 최신 
소프트웨어 도구들 역시 소개해 드릴 것입니다.

435
00:53:31,326 --> 00:53:37,663
Tensor Flow, Torch, PyTorch 와 같은 SOTA 
소프트웨어들도 다룰 것입니다.

436
00:53:37,663 --> 00:53:44,528
아마도 여러분은 그런 툴들을 강의 과제나 프로젝트를
진행하면서 접하실 수 있을 것입니다.

437
00:53:44,528 --> 00:53:49,122
또 한 가지 말씀드릴 점은 이 강의가 아-주 SOTA 하는 것입니다. 
저는 이점이 가장 마음에 듭니다.

438
00:53:49,122 --> 00:53:50,715
이 분야는 아주 빠르게 변합니다.

439
00:53:50,715 --> 00:54:03,749
ImageNet 그래프에도 보았듯이 2012년 이후로 엄청나게 
변하고 있습니다. 제가 여기 있는 동안 일 년 내내 변하고 있죠

440
00:54:03,749 --> 00:54:12,893
어쩌면 우리가 지난해에 다뤘던 수업 내용이 
올해 들어서는 순식간에 사라질 수도 있습니다.

441
00:54:12,893 --> 00:54:16,629
제가 이 과목을 가르칠 때 가장 좋아하는 대목이지요

442
00:54:16,629 --> 00:54:24,041
이 분야는 과학적으로 새롭게 발견된 것들을 모조리 흡수할 수 있고
또 제가 여러분에게 그것들을 알려줄 수 있다는 점이 정말 좋습니다.

443
00:54:24,041 --> 00:54:26,071
재밌는 것들도 있습니다.

444
00:54:26,071 --> 00:54:30,453
심각하지 않은 재미있는 주제에 관해서도 다룰 것입니다.

445
00:54:30,453 --> 00:54:33,122
이미지 캡셔닝과 같은 것인데 매우 재밌습니다.

446
00:54:33,122 --> 00:54:35,349
이미지 캡셔닝은 이미지의 내용을 기술하는 것입니다.

447
00:54:35,349 --> 00:54:39,896
그리고 여기 왼쪽에 보이는 DeepDream과 같은
좀 더 예술적인 것들에 관해서는 다룰 것입니다.

448
00:54:39,896 --> 00:54:44,277
이것은 우리가 Neural Network를 통해
이런 사이키델릭한 이미지를 만들게도 해줍니다.

449
00:54:44,277 --> 00:54:46,877
코스가 끝날쯤이면 어떻게 동작하는지 알 수 있을 겁니다.

450
00:54:46,877 --> 00:54:55,340
오른쪽 그림은 style transfer입니다. 이미지가 주어지면
이미지를 피카소나 반 고흐와 같은 유명화가의 풍으로 바꿔줍니다.

451
00:54:55,340 --> 00:54:59,654
이 또한 코스가 끝날쯤이면
어떻게 동작하는지 알게 되실 것입니다.

452
00:54:59,654 --> 00:55:03,794
학기 동안 세 가지 과제가 있습니다.

453
00:55:03,794 --> 00:55:10,706
첫 번째 과제는 잘하면 일주일 내로 끝낼 수도 있습니다.
그리고 중간고사가 있습니다.

454
00:55:10,706 --> 00:55:17,407
그리고 여러분의 학점에서 가장 큰 비율을 차지하는 것이 바로
최종 코스 프로젝트인데, 3인 1조로 진행할 것이며,

455
00:55:17,407 --> 00:55:20,514
여러분은 모든 사람들의 마음을 날려버릴 만큼
놀라운 프로젝트를 만드시면 됩니다.

456
00:55:20,514 --> 00:55:26,380
제출기한 정책이 있습니다. 도합 7일 정도는 늦을 수 있고 
여러분들의 과제 수행 중 자유롭게 분배할 수 있습니다.

457
00:55:26,380 --> 00:55:34,204
몸이 조금 아프거나, 여행을 가거나, 컨퍼런스에 참가하거나 할 때
유용하게 사용하시면 됩니다.

458
00:55:34,204 --> 00:55:40,880
하지만, 갑자기 학기 말에 와서는 "이번에 컨퍼런스에서
발표를 해야만 해요" 하고 하셔도 소용없습니다

459
00:55:40,880 --> 00:55:42,624
late days를 잘 활용하시기 바랍니다.

460
00:55:42,624 --> 00:55:50,295
하지만 만일 정상참작 할만한 피치 못할 사유가 있다면 
조교들에게 이메일을 보내주시길 바랍니다.

461
00:55:50,295 --> 00:55:54,177
또 한가지 알려 드릴 것은
collaboration policy입니다.

462
00:55:54,177 --> 00:56:00,785
여러분은 Stanford 학생으로서 반드시 윤리규정
(honer code)을 명심해야만 합니다. 아주 중요합니다.

463
00:56:00,785 --> 00:56:03,609
아주 엄격하게 다룰 것입니다.

464
00:56:03,609 --> 00:56:11,037
반드시 윤리규정의 테두리 안에서 협력할 수 있도록
신중하게 행동해 주시기 바랍니다.

465
00:56:12,304 --> 00:56:17,492
Pre-requisites에 대해 말씀드리겠습니다.
Python이 가장 중요할 것 같네요

466
00:56:17,492 --> 00:56:22,339
모든 프로그래밍 과제가 Python으로 진행됩니다.

467
00:56:22,339 --> 00:56:26,066
C나 C++에 익숙해지는 것도
어느 정도는 유용할 것입니다.

468
00:56:26,066 --> 00:56:31,705
이번 코스에서 C나 C++코드를 작성할 일은 
없겠지만 과제를 하다 보면

469
00:56:31,705 --> 00:56:39,879
여러 소프트웨어 패키지의 코드를 살펴볼 것이고, C++ 코드를 알면
패키지들이 어떻게 동작하는지 이해하는데 유용할 것입니다.

470
00:56:39,879 --> 00:56:44,971
그리고 여러분이 미분을 안다고 가정하고 수업을 진행할 것입니다.

471
00:56:44,971 --> 00:56:46,533
또한 일부 선형대수도 안다고 가정할 것입니다.

472
00:56:46,533 --> 00:56:52,072
행렬이 무엇이고, 어떻게 곱하는지 등을 
미리 알아두셔야 합니다.

473
00:56:52,072 --> 00:56:55,691
여기에서 도함수 계산 등을 전부 다 가르칠 순 없습니다.

474
00:56:55,691 --> 00:57:01,238
또한 CS131 또는 CS231a 수준의 컴퓨터 비전 지식이
있다고 가정하고 수업을 진행할 것입니다.

475
00:57:02,367 --> 00:57:05,120
이 과목들을 수강한 적이 있는 분들은 수월할 것입니다.

476
00:57:05,120 --> 00:57:11,550
그렇지 않아도 수업을 듣는 데 큰 지장은 없겠지만 따라잡으려면 좀 
더 노력해야 할 것입니다. 그래도 아마 큰 문제는 없으리라 봅니다.

477
00:57:11,550 --> 00:57:13,704
완전 완전 필수인 prerequisites은 없습니다.

478
00:57:13,704 --> 00:57:20,540
또한 CS229 수준의 기계학습 배경 지식이 있다고 가정합니다.

479
00:57:20,540 --> 00:57:32,416
하지만 기계학습 개념 중에서 정말 중요하다 싶은 것들을 필요할 때 
마다 제가 다시 설명해 드릴 것입니다. 미리 익숙하면 좋겠죠

480
00:57:34,774 --> 00:57:36,950
강의 홈페이지가 있습니다. 가서 확인해 보세요

481
00:57:36,950 --> 00:57:39,742
많은 정보와 링크 강의계획 등 많은 자료가 있습니다.

482
00:57:39,742 --> 00:57:43,656
오늘 다룰 것들은 거의 다 끝낸 것 같군요

483
00:57:43,656 --> 00:57:48,733
그리고 이번 주 목요일부터는 본격적으로
수업을 진행하도록 하겠습니다.
