---
title: "[11] Deep learning 2: 자연어와 단어의 통계기반 기법을 이용한 분산표현"
date:   2020-02-27
excerpt: "LSA(Latent Semantic Analysis): 특잇값분해 Singular Value Decomposition (SVD)"
category: [Deep Learning]
layout: post
tag:
- Deep Learning
order: 0

comments: true
---



# 목차


------

👀 코드 보기 , 🤷‍♀️     
이 두개의 아이콘을 누르시면 코드, 개념 부가 설명을 보실 수 있습니다:)

------


# 토픽 모델링 (Latent Semantic Analysis, LSA)     
BoW에 기반한 [통계기반 분석](https://yerimoh.github.io/DL13/)은 단어의 **빈도** 수를 이용한 통계적 분석이기 때문에 단어의 의미를 고려하지 못한다.      
즉 단어의 토픽을 고려하지 못한다는 것이다.     
그러므로 이를 위해 잠재된(Latent) 의미를 이끌어내는 방법으로 잠재 의미 분석(Latent Semantic Analysis, LSA)이 사용된다.     
이 LSA를 이해하기 위해선 먼저 선형 대수학의 개념인 **특이값분해 Singular Value Decomposition (SVD)** 를 이해해야 한다.     


## 특이값분해 Singular Value Decomposition (SVD)
※ 특이값분해(Singular Value Decomposition, SVD)는 보통 복소수 공간에 대하여 정의하는 것이 일반적이지만, 현 포스트에서는 실수 벡터 공간에 한정했다.    
 




## 차원 감소(dimensionality reduction)
벡터의 차원을 줄이는 방법       
* **핵심**: ‘중요한 정보’는 최대한 유지하면서 줄임      
* [EX] 데이터의 분포를 고려해 중요한 ‘축’을 찾는 일을 수행.     

![image](https://user-images.githubusercontent.com/76824611/129596748-54900ead-195e-496d-8f95-ef8ad1c0c0f5.png)

2차원 데이터를 1차원으로 표현하기 위해 중요한 축(데이터를 넓게 분포시키는 축)을 찾는다.       
* 왼쪽: 데이터점들을 2차원 좌표에 표시 모습.      
* 오른쪽: 새로운 축을 도입해 똑같은 데이터를 좌표축 하나 만으로 표시 (새로운 축을 찾을 때는 데이터가 넓게 분포되도록 고려필요).             

 각 데이터점의 값은 새로운 축으로 사영된 값으로 변함.      
* 중요 가장 적합한 축을 찾아내는 일로, 1차원 값만으로도 데이터의 본질적인 차이를 구별할 수 있어야 함      

➕ **희소행렬(sparse matrix), 희소벡터(sparse vector)**      
원소 대부분이 0인 행렬 또는 벡터    
**차원 감소의 핵심**: 희소벡터에서 **중요한 축**을 찾아내 더 적은 차원으로 다시 표현    
**차원 감소의 목표**: 원래의 희소벡터는 원소 대부분이 0이 아닌 값으로 구성된 **‘밀집벡터’로 변환.**     


**[SOLVE 1] 특잇값분해 Singular Value Decomposition (SVD)**             
임의의 행렬을 세 행렬의 곱으로 분해    
**$$X = USV^T$$**     
* 임의의 행렬 X를 U, S, V라는 세 행렬의 곱으로 분해      
* U와 V는 직교행렬(orthogonal matrix)            
  * 그 열벡터는 서로 직교.     
  * **U 직교행렬**: 어떠한 공간의 축(기저)을 형성. (지금 우리의 맥락에서는 이 U 행렬을 **‘단어 공간’**)        
* S는 대각행렬(diagonal matrix): 대각성분 외는 다 0인 행렬          
  * 그 성분에 ‘특잇값 singular value ’이 큰 순서로 나열됨.    
  * 특잇값: 쉽게 말해 ‘해당 축’의 중요도.    

➡ 중요도가 낮은 원소(특잇값이 작은 원소)를 깎아내는 방법.    
➡ 행렬 S에서 특잇값이 작다면 중요도가 낮다는 뜻      
➡ 행렬 U에서 여분의 열벡터를 깎아내 원래의 행렬 근사가능       
  
![image](https://user-images.githubusercontent.com/76824611/129597819-fe7371d6-b913-4d2d-9d74-855213f89416.png)

![image](https://user-images.githubusercontent.com/76824611/129598258-12039bec-c3a4-4484-9a07-ae0897280585.png)


 



**📜 단어의 PPMI 행렬에 적용**             
* **행렬 X의 각 행**: 해당 단어 ID의 단어 벡터가 저장          
* **행렬 U**: 그 단어 벡터가 차원 감소된 벡터로 표현되는 것              

**[구현]**         
SVD는 넘파이의 linalg 모듈의 svd메서드로 실행 가능         
* “linalg”는 선형대수 linear algebra 의 약어   

<details>
<summary>👀 코드 보기</summary>
<div markdown="1">
  
```python  
import sys
sys.path.append('..')
import numpy as np 
import matplotlib.pyplot as plt
from common.util import preprocess, create_co_matrix, ppmi

text = 'You say goodbye and I say hello.'
corpus, word_to_id, id_to_word = preprocess(text) 
vocab_size = len(id_to_word) 
C = create_co_matrix(corpus, vocab_size, window_size=1) 
W = ppmi(C)

# SVD 
U, S, V = np.linalg.svd(W)
```
  
</div>
</details>
  
SVD에 의해 변환된 밀집벡터 표현은 변수 U에 저장 

**[실제 구현]**         
단어 ID가 0인 단어 벡터를 보겠습니다.
```python  
print(C[0]) # 동시발생 행렬
# [0 1 0 0 0 0 0]

print(W[0]) # PPMI 행렬
# [ 0. 1.807 0. 0. 0. 0. 0. ]

print(U[0]) # SVD 
#[ 3.409e-01 -1.110e-16 -1.205e-01 -4.441e-16 0.000e+00 -9.323e-01 2.226e-16]
```
  
➡ 희소벡터인 W[0]가 SVD에 의해서 밀집벡터 U[0]로 변함           
➡ 그리고 이 밀집벡터의 차원을 감소시키는 방법     
* N차원 벡터로 줄이려면 인덱그 0부터 N개의 원소를 꺼내면 됩니다.
 
```python 
print(U[0, :2]) 
# [ 3.409e-01 -1.110e-16]

#각 단어를 2차원 벡터로 표현한 후 그래프로 그리기.
for word, word_id in word_to_id.items():
  # plt.annotate (word, x,y) 메서드는 2차원 그래프상에서 
  # 좌표 (x, y ) 지점에 word에 담긴 텍스트를 그림
  plt.annotate(word, (U[word_id, 0], U[word_id, 1]))

plt.scatter(U[:,0], U[:,1], alpha=0.5) 
plt.show()
```

-> 동시발생 행렬에 SVD를 적용한 후, 각 단어를 2차원 벡터로 변환해 그린 그래프(“i”와 “goodbye”가 겹쳐 있음)
 

⛔ WARNING       
행렬의 크기가 N이면 SVD 계산은 $$O(N^3)$$      
계산량이 N의 3 제곱에 비례해 늘어남.         
-> 이는 현실적으로 감당하기 어려운 수준       
  
⭕ solution       
**Truncated SVD**: 더 빠른 기법         
* 특잇값이 작은 것은 버리는 truncated 방식으로 성능 향상           
* 사이킷런 scikit-learn 라이브러리의 Truncated SVD를 이용          

📜 **PTB 데이터셋** 
펜 트리뱅크 Penn Treebank (PTB)           
지금까지 사용한 것 보다 큰 말뭉치         
말뭉치(텍스트 파일)의 예        
![image](https://user-images.githubusercontent.com/76824611/129600222-8164b092-0d05-4de9-a779-50d3dd57ad46.png)
   
* PTB 말뭉치에서는 한 문장이 하나의 줄로 저장       
* 각 문장을 연결한 ‘하나의 큰 시계열 데이터’로 취급     
* 이때 각 문장 끝에 <eos>라는 특수 문자를 삽입 (“eos”는 “end of sentence”의 약어).         
   * 지금까지의 구현은 문장의 구분을 고려x      
   * 여러 문장을 연결한 ‘하나의 큰 시계열 데이터’로 간주


**[구현]**     
<details>
<summary>👀 코드 보기</summary>
<div markdown="1">
  
```python  
import sys
sys.path.append('..') 
from dataset import ptb

# ptb.load _data ( )는 데이터를 읽어들임
# 인수로 ‘train’, ‘test’,‘valid’ 중 하나 지정 가능
# 차례대로 ‘훈련용’, ‘테스트용’, ‘검증용’ 데이터
corpus, word _to _id, id _to _word = ptb.load _data('train')

print(' 말뭉치 크기 :', len(corpus)) 
print('corpus[:30]:', corpus[:30]) 
print() 
print('id_to_word[0]:', id_to_word[0]) 
print('id_to_word[1]:', id_to_word[1]) 
print('id_to_word[2]:', id_to_word[2]) 
print() 
print("word_to_id['car']:", word_to_id['car']) 
print("word_to_id['happy']:", word_to_id['happy']) 
print("word_to_id['lexus']:", word_to_id['lexus'])
```

결과
```python  
corpus size: 929589
corpus[:30]: [ 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]

id_to_word[0]: aer 
id_to_word[1]: banknote 
id_to_word[2]: berlitz

word_to_id['car']: 3856
word_to_id['happy']: 4428
word_to_id['lexus']: 7426
```
  
</div>
</details>




**[PTB 데이터셋 평가]**     
PTB 데이터셋에 통계 기반 기법을 적용.      
* 큰 행렬에 SVD를 적용해야 하므로 고속 SVD를 이용 sklearn 모듈을 설치 필요    
* 물론 간단한 SVD (np.linalg.svd ( ))도 사용 가능     
* But,시간, 메모리 비효율적


**[구현]**     
<details>
<summary>👀 코드 보기</summary>
<div markdown="1">
  
```python  
import sys
sys.path.append('..')
import numpy as np 
from common.util import most_similar, create_co_matrix, ppmi 
from dataset import ptb

window _size = 2
wordvec _size = 100

corpus, word_to_id, id_to_word = ptb.load_data('train') 
vocab_size = len(word_to_id) 
print(' 동시발생 수 계산 ...') 
C = create_co_matrix(corpus, vocab _size, window _size) 
print('PPMI 계산 ...') 
W = ppmi(C, verbose=True)

print('SVD 계산 ...') 
#SVD 수행하는 데 sklearn의 randomized _svd () 메서드 이용
try:
  # truncated SVD ( 빠르다 !) 
  from sklearn.utils.extmath import randomized_svd  # 🤷‍♀️sklearn의 randomized_svd ( ) 메서드
  U, S, V = randomized_svd(W, n_components=wordvec_size, n_iter=5, random_state=None)

except ImportError:
  # SVD ( 느리다 ) 
  U, S, V = np.linalg.svd(W)

  word _vecs = U[:, :wordvec _size]

querys = ['you', 'year', 'car', 'toyota'] 
for query in querys:
  most_similar(query, word_to_id, id_to_word, word_vecs, top=5)
```
  
</div>
</details>

<details>
<summary>🤷‍♀️sklearn의 randomized_svd ( ) 메서드</summary>
<div markdown="1">
  
   
무작위 수를 사용한 Truncated SVD     
* 특잇값이 큰 것들만 계산하여 기본적인 SVD보다 훨씬 빠름.       
* Truncated SVD는 무작위 수를 사용하므로 결과가 매번 다름.        
    
   
</div>
</details>
 
  
**[결과 분석]**      
![image](https://user-images.githubusercontent.com/76824611/129601589-5cd6d72f-a8cf-41e1-ab6f-bc32e0f37f9e.png)

“you”라는 검색어에서는 인칭대명사인 “i”와 “we”가 상위를 차지       
* 영어 문장에서 관용적으로 자주 같이 나오는 단어들이기 때문        
  
* “year”의 연관어로는 “month”와 “quarter”      
* “car”의 연관어로는 “auto”와 “vehicle” 등    
* “toyota”와 관련된 단어 “nissan”, “honda”, “lexus” 등 자동차 제조업체나 브랜드가 뽑힌 것도 확인      

➡ 이처럼 단어의 의미 혹은 문법적인 관점에서 비슷한 단어들이 가까운 벡터로 나타남.        
➡ 우리의 직관과 비슷한 결과.      



LSA의 장점과 단점
지금까지 잠재 의미 분석(LSA)에 대해 알아봤습니다. LSA는 쉽고 빠르게 구현이 가능합니다. 하지만 문서에 포함된 단어가 가우시안 분포를 따라야만 LSA를 적용할 수 있습니다. 일반적으로 가우시안 분포를 따르겠지만 모든 문서의 단어가 가우시안 분포를 따르는 것은 아니기 때문에 적용하기가 힘들 때도 있습니다. 또한 문서가 업데이트가 된다면 처음부터 다시 SVD를 적용해줘야 하므로 자원이 많이 소모됩니다.


