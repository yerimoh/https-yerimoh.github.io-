---
title: "[20] [INDEX] CS231N 정리"
date:   2021-03-10
excerpt: "Deep learning starting from the bottom 2"
category: [Deep Learning]
layout: post
tag:
- Deep Learning
order: 0

comments: true
---

본 포스팅은 CS231N을 한국어로 정리한 포스팅입니다   
* [CS231N lecture overview](http://cs231n.stanford.edu/schedule.html)     
* [CS231N lecture vedio](https://www.youtube.com/watch?v=vT1JzLTH4G4)


-----


# [21] Lecture 1: Introduction    

[learn 21](https://yerimoh.github.io/DL22/){: .btn}  
- [컴퓨터 비전(Computer Vision)이란?](#컴퓨터-비전--computer-vision-이란-)
- [컴퓨터 비전의 역사](#컴퓨터-비전의-역사)
- [컴퓨터 비전의 발전](#컴퓨터-비전의-발전)
  * [Block World](#block-world)
  * [David Marr의 책](#david-marr의-책)
  * [Recognition via Parts](#recognition-via-parts)
  * [Recognition via Edge Detection](#recognition-via-edge-detection)
  * [앞선 연구들의 한계](#앞선-연구들의-한계)
  * [영상분할(Image Segmentation)](#영상분할-image-segmentation-)
  * [얼굴인식](#얼굴인식)
  * [객체 인식(SIFT feature)](#객체-인식--sift-feature-)
  * [ImageNet 프로젝트](#imagenet-프로젝트)
- [CNN](#cnn)


-----


# [23] Lecture 3: Regularization and Optimization Regularization 


## Regularization       

[learn 23](https://yerimoh.github.io/DL201/){: .btn}  
- [**손실함수**](#--손실함수--)
  * [손실함수 공식](#손실함수-공식)
- [**Multiclass SVM loss**](#--multiclass-svm-loss--)
  * [SVM Loss 작동 방식](#svm-loss-작동 방식)
  * [코드 구현](#코드-구현)
  * [예제 적용](#예제-적용)
  * [추가 질문](#추가-질문)
  * [개선: Regularization](#개선:-regularization)
    + [Regularization의 종류](#regularization의-종류)
- [**Softmax Classifier**](#--softmax-classifier--)
  * [작동 방식](#작동-방식)
  * [예시 적용](#예시-적용)
  * [추가 질문](#추가-질문-1)
- [**SVM vs Softmax**](#--svm-vs-softmax--)






## Optimization Regularization 

[learn 23](https://yerimoh.github.io/DL202/){: .btn}  
- [**Optimization**](#--optimization--)
  * [임의 탐색: random search](#임의-탐색--random-search)
  * [경사 따라가기](#경사-따라가기)
  * [수치 미분: numerical gradient](#수치-미분--numerical-gradient)
  * [해석적 미분: Analytic gradient](#해석적-미분--analytic-gradient)
    + [코드](#코드)
- [**경사 하강법**](#--경사-하강법--)
  * [SGD](#sgd)
- [**Image Feature**](#--image-feature--)
  * [color histogram](#color-histogram)
  * [Histogram of oriented gradients (HOG)](#histogram-of-oriented-gradients--hog-)
  * [bag of words](#bag-of-words)
  * [Image feature vs ConvNet](#image-feature-vs-convnet)



------


# [24] Lecture 4: Neural Networks and Backpropagation


[learn 24](https://yerimoh.github.io/DL203/){: .btn}  
- [**Intro**](#--intro--)
- [**computational graph**](#--computational-graph--)
- [Backpropagation](#backpropagation)
  * [계산](#계산)
  * [특징: local 계산](#특징--local-계산)
  * [복잡한 예제](#복잡한-예제)
  * [analytic gradient의 간편성](#analytic-gradient의-간편성)
  * [Patterns in backward flow](#patterns-in-backward-flow)
- [**벡터로 확장**](#--벡터로-확장--)
  * [백터화 예제 1](#백터화-예제-1)
  * [백터화 예제 2](#백터화-예제-2)
  * [forward / backward API](#forward---backward-api)
  * [관련 프레임워크](#관련-프레임워크)
- [**Neural Networks**](#--neural-networks--)
  * [실제 뉴런과 차이점](#실제-뉴런과-차이점)
