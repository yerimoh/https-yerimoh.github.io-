---
title: "[20] [INDEX] CS231N 정리"
date:   2021-03-10
excerpt: "Deep learning starting from the bottom 2"
category: [Deep Learning]
layout: post
tag:
- Deep Learning
order: 0

comments: true
---

본 포스팅은 CS231N을 한국어로 정리한 포스팅입니다   
* [CS231N lecture overview](http://cs231n.stanford.edu/schedule.html)     
* [CS231N lecture vedio](https://www.youtube.com/watch?v=vT1JzLTH4G4)


-----


# [21] Lecture 1: Introduction    

[learn 21](https://yerimoh.github.io/DL22/){: .btn}  
- [컴퓨터 비전(Computer Vision)이란?](#컴퓨터-비전--computer-vision-이란-)
- [컴퓨터 비전의 역사](#컴퓨터-비전의-역사)
- [컴퓨터 비전의 발전](#컴퓨터-비전의-발전)
  * [Block World](#block-world)
  * [David Marr의 책](#david-marr의-책)
  * [Recognition via Parts](#recognition-via-parts)
  * [Recognition via Edge Detection](#recognition-via-edge-detection)
  * [앞선 연구들의 한계](#앞선-연구들의-한계)
  * [영상분할(Image Segmentation)](#영상분할-image-segmentation-)
  * [얼굴인식](#얼굴인식)
  * [객체 인식(SIFT feature)](#객체-인식--sift-feature-)
  * [ImageNet 프로젝트](#imagenet-프로젝트)
- [CNN](#cnn)




# [23] Lecture 3: Regularization and Optimization Regularization 


## [23] Lecture 3:(1/2) Regularization       

[learn 21](https://yerimoh.github.io/DL201/){: .btn}  
- [**손실함수**](#--손실함수--)
  * [손실함수 공식](#손실함수-공식)
- [**Multiclass SVM loss**](#--multiclass-svm-loss--)
  * [SVM Loss 작동 방식](#svm-loss-작동 방식)
  * [코드 구현](#코드-구현)
  * [예제 적용](#예제-적용)
  * [추가 질문](#추가-질문)
  * [개선: Regularization](#개선:-regularization)
    + [Regularization의 종류](#regularization의-종류)
- [**Softmax Classifier**](#--softmax-classifier--)
  * [작동 방식](#작동-방식)
  * [예시 적용](#예시-적용)
  * [추가 질문](#추가-질문-1)
- [**SVM vs Softmax**](#--svm-vs-softmax--)
