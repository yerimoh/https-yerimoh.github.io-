---
title: "UXLA: A Robust Unsupervised Data Augmentation Framework for
Zero-Resource Cross-Lingual NLP 정리 "
date:   2021-09-20
excerpt: "UXLA: A Robust Unsupervised Data Augmentation Framework for
Zero-Resource Cross-Lingual NLP" 
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---

# INTRO

**[원논문]**      
* [Generative Adversarial Network](https://aclanthology.org/2021.acl-long.154.pdf)     



 
----


# Abstract

**[모델의 목표]**   

[전이 학습(Transfer learning)](https://yerimoh.github.io/DL12/)으로 인해 NLP는 많은 발전을 거듭했다.

그러나, 전이학습을 위해 필요한 대화 데이터 셋이 드물다.      
(모드 글을 다 쓸 수 있는 것이 아닌 정말 학습에 맞는 글들이 필요하기 때문이다)       
* 대상 언어의 모든 대상 작업에 대한 답이 달린 데이터는 드물다.     
* 특히 리소스가 적은 언어의 경우 더 그렇다.    


그래서! 이 논문에선 위의 문제(cross-lingual transfer 문제)를 해결하기위해, 이 모델을 제안한다.        




**[모델의 특징]**       
* 대상 언어의  training label이 없다고 가정한다. ➡️ 데이터에  training label이 필요 없다.      
* **데이터 확대(data augmentation)**, **비지도 샘플 선택 (unsupervised sample selection)** 을 통해 **자체 훈련(self training)을 수행**한다.           





--------


 INTRO  
pretrained language model의 의 종류 중 하나인 Self-supervised learning(LM)은 다음과 같이 작동한다      
* 1) 
*



이러한 방법은 일반적으로 두 가지 기본 단계를 따르며, 여기서 감독되는 작업별 미세 조정은 대규모 LM 사전 훈련을 따른다(Radford et al., 2019). 그러나 모든 대상 언어의 모든 대상 작업에 대해 레이블링된 데이터를 가져오는 것은 특히 리소스가 적은 언어의 경우 어렵습니다.
최근 사전 학습-마감 패러다임은 제로샷 교차 언어 전송에 사용할 수 있는 효과적인 다중 언어 모델을 훈련하기 위해 다중 언어 설정으로도 확장되었다. 소스 언어의 감독 미세 조정과 결합된 mBERT(Devlin et al.,2019) 및 XLM-R(Conneau et al., 2020)과 같이 공동으로 훈련된 심층 다국어 LM은 대상 언어의 작업 레이블을 사용하지 않고 언어 및 작업 지식을 한 언어에서 다른 언어로 전달하는 데 상당히 성공했다.
여러 언어와의 공동 사전 훈련을 통해 이러한 모델은 교차 언어를 일반화할 수 있다.
효과에도 불구하고 최근 연구(Pires 등, 2019; K 등, 2020)에서도 성공적인 언어 간 전달을 위한 하나의 중요한 제한 요소를 강조했다. 이들 모두는 모델의 교차 언어 일반화 능력이 소스 언어와 대상 언어 간의 구조적 유사성 부족에 의해 제한된다는 데 동의한다. 예를 들어 영어에서 mBERT를 옮기기 위해 K 외(2020)는 힌두어(구조적으로 다른)의 정확도가 약 23.6% 떨어졌다고 보고했으며, 스페인어(구조적으로 유사한)는 9% 떨어졌다(XNLI). (비슷한) 목표 언어가 저자원인 경우 전송 난이도는 더욱 악화된다. 왜냐하면 공동 사전 훈련 단계에서 애초에 이 언어에서 많은 예시를 보지 못했을 수 있기 때문이다. 우리의 실험(33.2)에서 언어 간 NER(XNER)에서, 우리는 Urdu에서 28.3%, XLM-R에서 버마어에서 30.4%의 F1 감소를 보고했는데, 이것은 MBER보다 훨씬 더 큰 다국어 데이터 세트에 대해 훈련되었다.





