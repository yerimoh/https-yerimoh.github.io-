---
title: "UXLA: A Robust Unsupervised Data Augmentation Framework for
Zero-Resource Cross-Lingual NLP 정리 "
date:   2021-09-20
excerpt: "UXLA: A Robust Unsupervised Data Augmentation Framework for
Zero-Resource Cross-Lingual NLP" 
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---

# INTRO

**[원논문]**      
* [Generative Adversarial Network](https://aclanthology.org/2021.acl-long.154.pdf)     



 
----


# Abstract

**[모델의 목표]**   

[전이 학습(Transfer learning)](https://yerimoh.github.io/DL12/)으로 인해 NLP는 많은 발전을 거듭했다.

그러나, 전이학습을 위해 필요한 대화 데이터 셋이 드물다.      
(모드 글을 다 쓸 수 있는 것이 아닌 정말 학습에 맞는 글들이 필요하기 때문이다)       
* 대상 언어의 모든 대상 작업에 대한 답(레이블)이 달린 데이터는 드물다.     
* 특히 리소스가 적은 언어의 경우 더 그렇다.    


그래서! 이 논문에선 위의 문제(cross-lingual transfer 문제)를 해결하기위해, 이 모델을 제안한다.        




**[모델의 특징]**       
* 대상 언어의  training label이 없다고 가정한다. ➡️ 데이터에  training label이 필요 없다.      
* **데이터 확대(data augmentation)**, **비지도 샘플 선택 (unsupervised sample selection)** 을 통해 **자체 훈련(self training)을 수행**한다.           





--------


# UXLA Framework
최근의 언어 간  transfer learning 노력은 거의 전적으로  multi-lingual pretraining과 fine-tuned source model의 zero-shot transfer에 의존해왔다.     
하지만 이러한 최근 동향보다 UXLA는 레이블링이 되지 않은 데이터를 더 잘 활용할 수 있는 방법이다.     
UXLA는 zero-resource cross-lingual task adaptation을 위한 unsupervised data augmenttion framework이다.    

**[UXLA의 개요]**     
![image](https://user-images.githubusercontent.com/76824611/141054415-99599d61-7c59-4ebc-a897-158cc83c8966.png)

전제
**labeled source samples**: $$D_s = (X_s, Y_s)$$ ➡️ 답이 있는 샘플이다    
**Unlabeled target samples**: $$D_t = (X_t)$$ ➡️ 답이 없는 샘플이다(이를 학습시켜 이러한 샘플로도 답이 있는 레이블기능을 하게하는 것이 목표이다)    
* s: training data for a source language        
* t: target language      

UXLA의 augments 방법    
traning 시 various origins at different stages에사 가져온 데이터로 증강한다.

+ Data augmentation는 갖고 있는 데이터셋을 여러 가지 방법으로 [증강]()하여 실질적인 학습 데이터셋의 규모를 키울 수 있는 방법입니다.


1️⃣ (epoch 1)  target language$$(D'\_t)$$ augmentated training samples

