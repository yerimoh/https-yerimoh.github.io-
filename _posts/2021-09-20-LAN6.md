---
title: "UXLA: A Robust Unsupervised Data Augmentation Framework for
Zero-Resource Cross-Lingual NLP 정리 "
date:   2021-09-20
excerpt: "UXLA: A Robust Unsupervised Data Augmentation Framework for
Zero-Resource Cross-Lingual NLP" 
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---

# INTRO

**[원논문]**      
* [Generative Adversarial Network](https://aclanthology.org/2021.acl-long.154.pdf)     



 
----


# Abstract

**[모델의 목표]**   

[전이 학습(Transfer learning)](https://yerimoh.github.io/DL12/)으로 인해 NLP는 많은 발전을 거듭했다.

그러나, 전이학습을 위해 필요한 대화 데이터 셋이 드물다.      
(모드 글을 다 쓸 수 있는 것이 아닌 정말 학습에 맞는 글들이 필요하기 때문이다)       
* 대상 언어의 모든 대상 작업에 대한 답(레이블)이 달린 데이터는 드물다.     
* 특히 리소스가 적은 언어의 경우 더 그렇다.    


그래서! 이 논문에선 위의 문제(cross-lingual transfer 문제)를 해결하기위해, 이 모델을 제안한다.        




**[모델의 특징]**       
* 대상 언어의  training label이 없다고 가정한다. ➡️ 데이터에  training label이 필요 없다.      
* **데이터 확대(data augmentation)**, **비지도 샘플 선택 (unsupervised sample selection)** 을 통해 **자체 훈련(self training)을 수행**한다.           





--------


# UXLA Framework
최근의 언어 간  transfer learning 노력은 거의 전적으로  multi-lingual pretraining과 fine-tuned source model의 zero-shot transfer에 의존해왔다.     
하지만 이러한 최근 동향보다 UXLA는 레이블링이 되지 않은 데이터를 더 잘 활용할 수 있는 방법이다.     
UXLA는 zero-resource cross-lingual task adaptation을 위한 unsupervised data augmenttion framework이다.    

**[UXLA의 개요]**     
![image](https://user-images.githubusercontent.com/76824611/141054415-99599d61-7c59-4ebc-a897-158cc83c8966.png)

전제
**labeled source samples**: $$D_s = (X_s, Y_s)$$ ➡️ 답이 있는 샘플이다    
**Unlabeled target samples**: $$D_t = (X_t)$$ ➡️ 답이 없는 샘플이다(이를 학습시켜 이러한 샘플로도 답이 있는 레이블기능을 하게하는 것이 목표이다)    
* s: training data for a source language        
* t: target language      

UXLA의 augments 방법    
traning 시 various origins at different stages에사 가져온 데이터로 증강한다.

+ Data augmentation는 갖고 있는 데이터셋을 여러 가지 방법으로 [증강]()하여 실질적인 학습 데이터셋의 규모를 키울 수 있는 방법입니다.


1️⃣ (epoch 1)  target language$$(D_t)$$에서 training samples을  original source $$(D_s)$$와 함께 [augmentated]() 한다       
2️⃣ (epoch 2-3) source 및 target examples의 인접 분포에서 생성된 간접 문장을 사용
![image](https://user-images.githubusercontent.com/76824611/141056825-f8d02cb3-e477-4d61-99e0-ee714a86d642.png)
: augmented data에 대해 self-training을 수행하여 이에 해당하는 pseudo-lable을 획득. 
: 모델이 자체 오류를 누적하는 self-training을 피하기 위해, 세 가지 작업 모델을 동시에 훈련한다.   
[세가지 모델 동시에 훈련]   
* 다중 에포치 공동 학습: 잠재적 레이블 노이즈의 데이터 augmentation 및 filtering을 통해 가상 훈련 데이터를 생성
   * 각 에포치 학습: 
       * 1) co-distillation(공동분산): 두 개의 동료 과제 모델이 세 번째 모델을 훈련하기 위한 **"신뢰할 수 있는(reliable)" training examples** 를 선택하는데 사용된다. 
       * 2) coguessing(공동 추정): 그런 다음 pseudo label이 있는 **선택된 샘플**은 다른 두 모델로부터 합의된 내용을 취하여  target task model의 training data에 추가된다.      
       * co-distillation 및  co-guessing 메커니즘은 견고성을 보장: 번역하고자 하는 두 언어가 구조적으로 다르거나,  traget 언어가 학습할 데이터가 없는 상황일 때 견고하게 작동    


---
---

# 알고리즘 
다국어 설정에서 발생할 수 있는 도메인 외부 분포에 대한 UXLA의  훈련 방법의 유사코드. 
* UXLA의 각 작업 모델은 소스 언어 작업(예: 영어 NER)에서 최종 조정된 XLM-R의 인스턴스임      
* µmlm(예: 미세 조정 전)으로 사전 훈련된 마스크된 LM은 선택된 각 예 주위의 주변vicinity 분포 ϑ(μxn|xn, µmlm)를 정의하는 데 사용됨.    


![image](https://user-images.githubusercontent.com/76824611/141064209-7ff8eecc-3700-4af9-8735-e6221fc1138c.png)

---

## 1. Warm-up: Training Task Models
![image](https://user-images.githubusercontent.com/76824611/141075070-a20d964f-7523-48cb-8024-4482354cdf53.png)
![image](https://user-images.githubusercontent.com/76824611/141070942-abb053fd-aa59-4a98-82a7-43cc30885032.png)

처음으로 세개의 XLM-R model의 3개의 인스턴스인 ($$θ^{(1)}$$, $$θ^{(2)}$$, $$θ^{(3)}$$)를 trainning할 것이다.      
* source language (English)의 레이블 데이터에 추가적인 task-specific linear layer를 사용하여 trainning한다   

각 인스턴스는 **동일한 아키텍처**(XLMR large)를 이루고 있지만, **다른 랜덤 시드**로 초기화된다     
 
token-level prediction tasks (e.g., NER)의 경우,    
* **토큰수준 예측작업(token-level representations)**: 분류 계층(classification layer)에 입력됨     
* **문장 수준 작업(sentence-level)**(예: XNLI): **[CLS] 표현**이 분류 계층(classification layer)에 대한 입력으로 사용됨        

### Training with confidence penalty    
목표: 잠재적으로 **유사하지 않고**(dissimilar) **리소스가 부족**( low-resourced)한 target 언어에 대한 self-training을 **안정적**(reliably)으로 사용할 수 있도옥 함     


**유사하지 않고**(dissimilar) **리소스가 부족**( low-resourced) 문제가 있는 상황에서 지나치게 overly confident(과적합된) 모델의 문제점    
* 노이즈가 많은 pseudo 레이블을 생성 가능      
* 훈련이 진행됨에 따라 노이즈가 누적     
* 우리의 distillation methods(잡음이 있는 샘플에서 좋은 샘플을 분리)에 어려움 초래 가능

**[문제 원인]**      
[표준 교차 엔트로피(CE) 손실](https://yerimoh.github.io/DL3/#%EA%B5%90%EC%B0%A8-%EC%97%94%ED%8A%B8%EB%A1%9C%ED%94%BC-%EC%98%A4%EC%B0%A8cross-entropy-error-cee)로 훈련하면, 특히 **클래스 분포가 균형을 이루지 않을 때** overly confident predictions(낮은 엔트로피)을 생성하는 과적합된 모델이 발생할 수 있음. 

**[해결]**    
다음과 같이 CE 손실에 음의 엔트로피 항 -H를 추가하여 이 문제를 해결합니다. 
![image](https://user-images.githubusercontent.com/76824611/141073663-5919adba-b590-41c9-abab-6b1512ee95d0.png)   
* x: 출력 레이어로 이동하는 표현    
* $$y^c$$, $$p^{c}_ {θ}(x)$$: 각각 클래스 c에 대한 정답 레이블 및 모델 예측    

이러한 출력 분포의 regularizer는 **큰 모델을 훈련**하는 데 효과적임.    



---

## 2. Sentence Augmentation
우리의 augmentated sentences은 두 가지 소스에서 가져온다.    
* $$X_t$$: 원본 target language 샘플        
* 소스 및 목표 샘플의 주변 분포(vicinity distribution)에서 생성된 가상(virtual) 샘플
![image](https://user-images.githubusercontent.com/76824611/141075401-90dca424-2d90-40ee-8225-553aa3ca0c20.png)

위의  augmentated sentences은 유용한 언어적 특징을 포착하고 유창한 문법 텍스트를 생성할 수 있다(Hewitt and Manning, 2019).     

$$θ_{mlm}$$    
우리는 **XLM-R 마스크 LM**(Conneauet al., 2020)을 대규모 다국어 말뭉치(100개 언어로 된 2.5TB의 Common-Crawl 데이터)에 대해 훈련된 **주변vicinity 모델 $$θ_{mlm}$$** 으로 사용함.     

주변vicinity 모델은 매개변수가 task 목표에 대해 훈련되지 않은 분리된 사전 훈련된 엔터티임.    
1) 선택한 각 예제에 대한 샘플을 생성하기 위해 먼저 무작위로  input tokens의 P %를 선택함.           
2) 선택한 tokens 중 하나를 연속적으로(한 번에 하나씩) 마스킹하고 XLM R 마스킹된 LM에게 해당 마스킹된 위치의 토큰을 예측하도록 요청.  
![image](https://user-images.githubusercontent.com/76824611/141078626-a76e4ed0-3ff2-406c-867d-d011dfcab213.png)  
3) 특정 마스크의 경우 출력 분포에서 S 후보 단어를 샘플링하고 두 가지 대안 중 하나를 따라 새로운 문장을 생성함니다.    

[bert의 MLM]()과 유사


Our augmentated sentences come from two different sources: the original target language samples Xt, and the virtual samples generated from the vicinity distribution of the source and target samples: ϑ(˜xIt has been shown that contextual LMs pretrained on large-scale datasets capture useful linguistic features and can be used to generate fluent grammatical texts (Hewitt and Manning, 2019). We use XLM-R masked LM (Conneauet al., 2020) as our vicinity model θmlm, which is trained on massive multilingual corpora (2.5 TB of Common-Crawl data in 100 languages). The vicinity model is a disjoint pretrained entity whose parameters are not trained on any task objective. In order to generate samples around each selected example, we first randomly choose P % of the input tokens. Then we successively (one at a time) mask one of the chosen tokens and ask XLMR masked LM to predict a token in that masked position, i.e., compute ϑ(˜xm|x, θmlm) with m being the index of the masked token. For a specific mask, we sample S candidate words from the output distribution, and generate novel sentences by following one of the two alternative approaches.



