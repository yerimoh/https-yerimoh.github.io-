---
title: "FastText: Enriching Word Vectors with Subword Information 정리"
date:   2022-11-10
excerpt: "Enriching Word Vectors with Subword Information"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---


   
# 원 논문
[Enriching Word Vectors with Subword Information](https://arxiv.org/abs/2112.10508)

**[소스코드]**     
* [git adress](https://github.com/zhang-yu-wei/MTP-CLNN)


**[사전 학습]**
* 읽기 전 아래의 포스트들을 읽어야 무슨소린지 알아듣기 편하다..!   
* 

---

# Abstract
**[기존 모델의 문제]**      
* 레이블이 지정되지 않은 큰 말뭉치에 대해 훈련된 Continuous word representations은 많은 자연어 처리 작업에 유용하다.    
이러한 표현을 학습하는 인기 있는 모델은 각 단어에 고유한 벡터를 할당하여 **단어의 형태를 무시**한다.    
이렇게 단어의 형상을 무시하는 문제는 특히 **어휘가 많고 희귀한 단어가 많은 언어**의 경우 더 문제가 된다.    

**[본 논문에서의 해결법]**      
* 본 논문에서는 **skipgram model**을 기반으로 한 새로운 접근법을 제안한다.     
여기서 각 단어는 문자 n-grams의 bag of character로 표현된다.     
벡터 표현은 각 문자 n-grams과 연관되어 있으며, 단어는 이러한 표현의 합으로 표현된다.     

**[skipgram model의 장점]**      
* 빠름      
* 큰 말뭉치에서 모델을 빠르게 훈련 가능      
* 훈련 데이터에 나타나지 않은 단어에 대한 단어 표현을 계산 가능      
* 단어 유사성과 유추 작업 모두에서 9개의 다른 언어로 우리의 단어 표현 평가한(ord similarity and analogy tasks)결과,          
최근 제안된 형태학적 단어 표현과 비교하여 벡터가 이러한 작업에서 최첨단 성능을 달성한다는 것을 보여준다.    


-----
-----

# 1 Introduction
단어의 연속적인 표현(continuous representations)을 배우는 것은 자연어 처리에서 오랜 역사를 가지고 있다.     
이러한 표현은 일반적으로 **동시 발생 통계**([co-occurrence statistics](https://yerimoh.github.io/DL13/#%EB%8B%A8%EC%96%B4%EC%9D%98-%EB%B6%84%EC%82%B0-%ED%91%9C%ED%98%84))를 사용하여 **레이블이 지정되지 않은 대형 말뭉치**에서 파생된다.   
분포 의미론으로 알려진 많은 연구가 이러한 방법의 특성을 연구했다(Turney et al., 2010; Baroni and Lenci, 2010). 신경망 커뮤니티에서 콜로버트와 웨스턴(2008)은 왼쪽의 두 단어와 오른쪽의 두 단어를 기반으로 단어를 예측하여 피드포워드 신경망을 사용하여 단어 임베딩을 학습할 것을 제안했다. 더 최근에, Mikolov et al. (2013b)는 매우 큰 기업에서 단어의 연속적인 표현을 효율적으로 학습하기 위한 간단한 로그 이진 모델을 제안했다.

Learning continuous representations of words has a long history in natural language processing (Rumelhart et al., 1988). These representations are typically derived from large unlabeled corpora using co-occurrence statistics (Deerwester et al., 1990; Schütze, 1992; Lund and Burgess, 1996). A large body of work, known as distributional semantics, has studied the properties of these methods (Turney et al., 2010; Baroni and Lenci, 2010). In the neural network community, Collobert and Weston (2008) proposed to learn word embeddings using a feedforward neural network, by predicting a word based on the two words on the left and two words on the right. More recently, Mikolov et al. (2013b) proposed simple log-bilinear models to learn continuous representations of words on very large corpora efficiently

이러한 기술의 대부분은 매개 변수 공유 없이 고유한 벡터로 어휘의 각 단어를 나타낸다. 특히 터키어나 핀란드어와 같이 형태학적으로 풍부한 언어의 중요한 한계인 단어의 내부 구조를 무시한다. 예를 들어, 프랑스어나 스페인어에서, 대부분의 동사는 40개 이상의 다른 굴절형을 가지고 있는 반면, 핀란드어는 명사에 대해 15개의 경우를 가지고 있다. 이러한 언어에는 훈련 말뭉치에서 드물게 발생하는(또는 전혀 발생하지 않는) 많은 단어 형태가 포함되어 있어 좋은 단어 표현을 학습하기 어렵다. 많은 단어 형성이 규칙을 따르기 때문에 문자 수준 정보를 사용하여 형태학적으로 풍부한 언어에 대한 벡터 표현을 개선할 수 있다.
Most of these techniques represent each word of the vocabulary by a distinct vector, without parameter sharing. In particular, they ignore the internal structure of words, which is an important limitation for morphologically rich languages, such as Turkish or Finnish. For example, in French or Spanish, most verbs have more than forty different inflected forms, while the Finnish language has fifteen cases for nouns. These languages contain many word forms that occur rarely (or not at all) in the training corpus, making it difficult to learn good word representations. Because many word formations follow rules, it is possible to improve vector representations for morphologically rich languages by using character level information.


본 논문에서는 문자 n-그램에 대한 표현을 학습하고 단어를 n-그램 벡터의 합으로 표현할 것을 제안한다. 우리의 주요 기여는 하위 단어 정보를 고려한 연속 스킵그램 모델(Mikolov et al., 2013b)의 확장을 도입하는 것이다. 우리는 서로 다른 형태를 나타내는 9개 언어에서 이 모델을 평가하여 접근 방식의 이점을 보여준다.
In this paper, we propose to learn representations for character n-grams, and to represent words as the sum of the n-gram vectors. Our main contribution is to introduce an extension of the continuous skipgram model (Mikolov et al., 2013b), which takes into account subword information. We evaluate this model on nine languages exhibiting different morphologies, showing the benefit of our approach.

