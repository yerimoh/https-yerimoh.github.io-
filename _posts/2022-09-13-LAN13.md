---
title: "FastText: Bag of Tricks for Efficient Text Classification 정리"
date:   2022-11-10
excerpt: "Bag of Tricks for Efficient Text Classification"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---


# intro
 
      

## 핵심  

## 읽기 위해 필요한 지식
* [word2vec](https://yerimoh.github.io/DL14/): baseline 모델이기 때문에 꼭 알아야 한다.        
* [Embedding](https://yerimoh.github.io/DL15/): word2vec 속도 개선으로 이 포스팅도 꼭 알아야 한다.      

## 원 논문
[Bag of Tricks for Efficient Text Classification](https://arxiv.org/pdf/1607.01759.pdf)


---

# 목차  


---

# Abstract
This paper explores a simple and efficient
baseline for text classification. Our experiments show that our fast text classifier fastText is often on par with deep
learning classifiers in terms of accuracy, and
many orders of magnitude faster for training
and evaluation. We can train fastText on
more than one billion words in less than ten
minutes using a standard multicore CPU, and
classify half a million sentences among 312K
classes in less than a minute.



# 1 Introduction
Text classification is an important task in Natural
Language Processing with many applications, such
as web search, information retrieval, ranking and
document classification (Deerwester et al., 1990;
Pang and Lee, 2008). Recently, models based
on neural networks have become increasingly
popular (Kim, 2014; Zhang and LeCun, 2015;
Conneau et al., 2016). While these models achieve
very good performance in practice, they tend to be
relatively slow both at train and test time, limiting
their use on very large datasets.


Meanwhile, linear classifiers are often considered as strong baselines for text
classification problems (Joachims, 1998;
McCallum and Nigam, 1998; Fan et al., 2008).
Despite their simplicity, they often obtain stateof-the-art performances if the right features are
used (Wang and Manning, 2012). They also
have the potential to scale to very large corpus (Agarwal et al., 2014)

In this work, we explore ways to scale these
baselines to very large corpus with a large output
space, in the context of text classification. Inspired
by the recent work in efficient word representation
learning (Mikolov et al., 2013; Levy et al., 2015),
we show that linear models with a rank constraint
and a fast loss approximation can train on a billion
words within ten minutes, while achieving performance on par with the state-of-the-art. We evaluate the quality of our approach fastText1 on two
different tasks, namely tag prediction and sentiment
analysis.
