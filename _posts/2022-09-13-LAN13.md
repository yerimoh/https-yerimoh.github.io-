---
title: "FastText: Bag of Tricks for Efficient Text Classification 정리"
date:   2022-11-10
excerpt: "Bag of Tricks for Efficient Text Classification"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---


# intro
 
      

## 핵심  

## 읽기 위해 필요한 지식
* [word2vec](https://yerimoh.github.io/DL14/): baseline 모델이기 때문에 꼭 알아야 한다.        
* [Embedding](https://yerimoh.github.io/DL15/): word2vec 속도 개선으로 이 포스팅도 꼭 알아야 한다.      

## 원 논문
[Bag of Tricks for Efficient Text Classification](https://arxiv.org/pdf/1607.01759.pdf)


---

# 목차  


---

# Abstract
이 논문은 text classification를 위한 간단하고 효율적인 baseline을 탐구한다.      
우리의 실험은 [fastText](https://yerimoh.github.io/LAN7/)가 accuracy 측면에서 딥 러닝 classifiers와 동등하지만, 훈련과 평가에선 이 모델이 몇 배나 더 빠르다는 것을 보여준다.      

이 논문은 standard multicore CPU를 사용하여 10분 이내에 10억 개 이상의 단어에 대해서 **빠르게 텍스트를 train**할 수 있고, 312K 클래스 중 50만 개의 문장을 1분 이내에 분류할 수 있다.   

✨ **즉 빠르게 train하며 빠르게 분류가 가능한 모델을 만들어 냈다**는 것이다 ✨ 

---


# 1. Introduction

**[기존 모델들의 문제]**    
* 텍스트 분류는 웹 검색, 정보 검색, 순위 및 문서 분류와 같은 많은 응용 프로그램이 있는 자연어 처리에서 중요한 작업이다.  
또한 이는 14도부터 신경망을 기반으로 한 모델이 점점 더 인기를 끌고 있다.    
➡ 이러한 모델은 실제로 매우 우수한 성능을 달성하지만,     
**훈련과 테스트 시간 모두에서 상대적으로 느린 경향**이 있어 매우 큰 데이터 세트에서의 사용엔 한계가 존재한다.     


**[linear classifiers의 잠재력]**        
* [linear classifiers](https://en.wikipedia.org/wiki/Linear_classifier)는 텍스트 분류 문제의 강력한 baselines으로 간주된다.     
이것은 단순하지만 적절하게 사용하면 stateof-the-art 성능을 얻을 수 있다.              
또한 **매우 큰 말뭉치로 확장할 수 있는 잠재력**도 가지고 있다.       




**[linear classifiers를 모델에 적용]**    
* 본 논문은 텍스트 분류의 맥락에서 이러한 baselines을 **large output space을 가진 매우 큰 말뭉치로 확장**하는 방법을 탐구한다.      
* [Word2vec](https://arxiv.org/abs/1301.3781)의 연구에서 영감을 받아,   
rank 제약과 빠른 loss 근사를 가진 **linear models이 10분 이내에 10억 단어에 대해 훈련할 수 있는 동시에 최첨단 수준의 성능을 달성할 수 있음을 보여준다**.       
* evaluation: 우리는 태그 예측과 감정 분석이라는 두 가지 다른 작업에서 [fastText](https://github.com/facebookresearch/fastText) 접근 방식의 품질을 평가한다.           




----


# 2. Model architecture


**[linear classifiers의 사용]**     
* 문장 분류를 위한 간단하고 효율적인 기준은 문장을 [bag of words](https://en.wikipedia.org/wiki/Bag-of-words_model) (BoW) 으로 표현하고,     
linear classifier(예: logistic regression 또는 [SVM](https://yerimoh.github.io/ML3/)를 훈련하는 것이다.      


**[linear classifiers의 개선]**     
* 그러나 linear classifiers는 **features 및 class 간에 매개 변수를 공유하지 않는다**.      
* 이는 **예제가 거의 없는 large output space에서 일반화를 제한**할 수 있다.      
➡ 이 문제에 대한 일반적인 해결책은 linear classifier를 **low rank matrices로 분해**하거나 **다층 신경망을 사용**하는 것이다.   




**[모델 구성]**   
본 논문의 모델은 아래와 같다.
이는 rank 제약 조건이 있는 simple linear model을 보여준다.     

<img width="323" alt="image" src="https://user-images.githubusercontent.com/76824611/210780968-f9457e60-d22b-4410-889b-36e5ab4f57fe.png">
Figure 1: Model architecture of fastText for a sentence with N ngram features x1, . . . , xN . The features are embedded and averaged to form the hidden variable.

그림 1: Ngram 특징이 있는 문장에 대한 fastText의 모델 아키텍처 x1, . ., xN. 특징들은 숨겨진 변수를 형성하기 위해 내장되고 평균화된다.


1️⃣ 첫 번째 가중치 행렬 A는 단어 위의 look-up table이다.       
2️⃣ 그런 다음 단어 표현은 text representation으로 평균화된다.             
3️⃣ text representation은 linear classifier에 입력된다.      
   text representation은 잠재적으로 재사용될 수 있는 hidden variable입니다.     


<details>
<summary>📜 look-up table이란? </summary>
<div markdown="1">
 
 
Embedding 레이어는 **입력으로 들어온 단어를 분산 표현으로 연결해 주는 역할**을 하는데,     
그것이 **Weight에서 특정 행을 읽어오는 것과 같아** **이 레이어를 룩업 테이블(Lookup Table)**이라고 부르기도 합니다.   
 
이게 무슨 소리냐면, 
임베딩 층의 입력으로 사용하기 위해서 입력 시퀀스의 **각 단어들은 모두 정수 인코딩**이 되어있어야 한다.       

특정 단어와 맵핑되는 정수를 인덱스로 가지는 테이블로부터 **임베딩 벡터 값을 가져오는 룩업 테이블이라고 볼 수 있다.**    
![image](https://user-images.githubusercontent.com/76824611/210789841-ddb0d5c9-212d-4459-a8b5-4b92a17a8c29.png)
![image](https://user-images.githubusercontent.com/76824611/210788684-7cfd8297-99ea-4ac3-8d2f-57077702f69f.png)

이해가 안되다면 이 논문의 기반 논문인 [Word2vec](https://yerimoh.github.io/DL14/)를 제대로 알고오자.   

</div>
</details>


이 아키텍처는 중간 단어가 레이블로 대체되는 [cbow 모델](https://yerimoh.github.io/DL14/#cbow-%EB%AA%A8%EB%8D%B8)과 유사하다.      
[소프트맥스 함수](https://yerimoh.github.io/DL2/#%EC%86%8C%ED%94%84%ED%8A%B8%EB%A7%A5%EC%8A%A4-%ED%95%A8%EC%88%98-softmax-function) $$f$$를 사용하여 사전 정의된 클래스에 대한 확률 분포를 계산한다.       
N개의 문서 집합의 경우, 이것은 클래스에 대한 negative loglikelihood를 최소화한다.     



<img width="354" alt="image" src="https://user-images.githubusercontent.com/76824611/210780823-b4f9953e-cfed-4628-8848-9b67c3aca1fa.png">
* 여기서 $$x_n$$은 n번째 document의 정규화된 bag of features이다.     
* $$y_n the label$$ A와 B의  weight matrices이다.        
* 이 model 은 [stochastic gradient descent](https://yerimoh.github.io/DL5/#%ED%99%95%EB%A5%A0%EC%A0%81-%EA%B2%BD%EC%82%AC-%ED%95%98%EA%B0%95%EB%B2%95-sgd)와 linearly decaying learning rate를 사용하여 여러 CPU에서 비동기식으로 훈련된다.












<details>
<summary>📜 Negative Log-Likelihood (NLL) 보기</summary>
<div markdown="1">

실제 softmax을 쓸 때는 negative log-likelihood(NLL)와 사용된다.     

<img width="224" alt="image" src="https://user-images.githubusercontent.com/76824611/210790935-eb6ad9f7-dc6a-4ab0-8aca-4fc8ec44b626.png">

즉, 비유를 하자면 소프트맥스함수는 행복지수를 찾는 것이라면,      
Negative Log-Likelihood (NLL)는 불행지수를 찾는것이다.     

Negative Log-Likelihood (NLL)은 아래와 같은데,   
input이 0일 때 무한으로 가고, input이 1일 때 0으로 간다.     
![image](https://user-images.githubusercontent.com/76824611/210791528-7e2f0770-0c33-4377-ab4a-3c505efe13e4.png)

즉 정리해보면 아래와 같은 관계를 갖는다.   
![image](https://user-images.githubusercontent.com/76824611/210791835-8d044c5e-e3f7-43c5-8ab8-27ce3200cb5d.png)

loss를 계산할 때 우리는 정답 class에 대한 높은 확률은 낮은 loss로 이어지는 것을 볼 수 있다.           


</div>
</details>  


---


## 2.1 Hierarchical softmax


Hierarchical Softmax은 [negative sampling](https://yerimoh.github.io/DL15/#2--%EB%84%A4%EA%B1%B0%ED%8B%B0%EB%B8%8C-%EC%83%98%ED%94%8C%EB%A7%81%EC%9D%B4%EB%9E%80-%EC%86%90%EC%8B%A4-%ED%95%A8%EC%88%98-%EB%8F%84%EC%9E%85)과 같이 연산이 너무 비대해지는 것을 막기 위해 고안된 방식이다.     

즉 한마디로 요약하자면 Hierarchical Softmax 방법을 사용하면 백터의 내적을 이진 분류로 바꿔 계산량을 줄일 수 있다는 것이다.    
➡ 100만개의 단어를 벡터의 내적으로 구하면 벡터의 내적을 100만번 해야하지만,       
 Hierarchical Softmax 방법을 통한 이진분류로 구하면 $$log_2(100만)$$ 약 19번만 계산하면 되는 것이다.     
 
그렇다면 어떻게 이와 같이 이진분류로 계산하는지 알아보겠다.    


**[예시]**    
이는 모델 구조 자체를 [full binary tree](https://yerimoh.github.io/Algo022/#%EC%9D%B4%EC%A7%84-%ED%8A%B8%EB%A6%AC-%EC%9C%A0%ED%98%95-types-of-binary-trees) 구조로 바꾼 후에 단어들은 [leaf node](https://yerimoh.github.io/Algo022/#%ED%8A%B8%EB%A6%AC%EC%9D%98-%EA%B8%B0%EB%B3%B8-%EA%B0%9C%EB%85%90)에 배치하는 것으로 부터 시작된다.


예를 들어서 설명해보겠다.
아래와 같은 문장(Context)가 있다고 해보자.   

그렇다면 이 문장에서 **'to'** 라는 중심단어를, window size = 1에 해당하는 주변단어 **(want, eat)** 들을 사용하여 학습한다고 가정하자.    

그렇다면 아래와 같은 tree에서 학습이 될 것이다.     
편의를 위해 want와 eat중 **want**로 만 를 예시로 들었다.


 
여기서 최종 확률(출력층의 값)은 root부터 leaf까지 가는 길에 있는 확률을 모두 곱하여 계산된다.     
더 높은 확률의 edge를 선택해나간다.    
그리고 아래에서 보다시피 각 edge의 합은 1이다.(확률)(똑같은 색의 합은 1이다)    
즉 I가 나올 확률을 계산해보면, **$$0.7 x 0.8 x 0.6$$** 이다.    

![image](https://user-images.githubusercontent.com/76824611/210959022-df840eb3-0892-4637-84f8-bf47411e93c6.png)


즉 위의 방법들을 통해 Hierarchical Softmax는 출력층의 값을 softmax 함수로 얻는 것 대신에 binatr tree를 이용하여 얻는 것이다.        


**[일반화]**   
그렇다면 위의 식들을 일반화해보자.    
일반화한 노드는 아래와 같다.     
![image](https://user-images.githubusercontent.com/76824611/210958934-b07bbb8f-6b76-4a12-9c6e-12dd8a26c5a6.png)



When the number of classes is large, computing the
linear classifier is computationally expensive. More
precisely, the computational complexity is O(kh)
where k is the number of classes and h the dimension of the text representation. In order to improve our running time, we use a hierarchical softmax (Goodman, 2001) based on the Huffman coding tree (Mikolov et al., 2013). During training, the
computational complexity drops to O(h log2
(k)).
The hierarchical softmax is also advantageous at
test time when searching for the most likely class.
Each node is associated with a probability that is the
probability of the path from the root to that node. If
the node is at depth l + 1 with parents n1, . . . , nl
, its
probability is





This means that the probability of a node is always
lower than the one of its parent. Exploring the tree
with a depth first search and tracking the maximum
probability among the leaves allows us to discard
any branch associated with a small probability. In
practice, we observe a reduction of the complexity
to O(h log2
(k)) at test time. This approach is further extended to compute the T-top targets at the
cost of O(log(T)), using a binary heap.

----

# 3 Experiments
We evaluate fastText on two different tasks.
First, we compare it to existing text classifers on the
problem of sentiment analysis. Then, we evaluate
its capacity to scale to large output space on a tag
prediction dataset. Note that our model could be implemented with the Vowpal Wabbit library,2 but we
observe in practice, that our tailored implementation
is at least 2-5× faster.





##  N-gram features
Bag of words is invariant to word order but taking
explicitly this order into account is often computationally very expensive. Instead, we use a bag of
n-grams as additional features to capture some partial information about the local word order. This
is very efficient in practice while achieving comparable results to methods that explicitly use the order (Wang and Manning, 2012).
We maintain a fast and memory efficient
mapping of the n-grams by using the hashing
trick (Weinberger et al., 2009) with the same hashing function as in Mikolov et al. (2011) and 10M
bins if we only used bigrams, and 100M otherwise.
