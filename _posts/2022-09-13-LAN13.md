---
title: "FastText: Enriching Word Vectors with Subword Information 정리"
date:   2022-11-10
excerpt: "Enriching Word Vectors with Subword Information"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---


   
# 원 논문
[Enriching Word Vectors with Subword Information](https://arxiv.org/abs/2112.10508)

**[소스코드]**     
* [git adress](https://github.com/zhang-yu-wei/MTP-CLNN)


**[사전 학습]**
* 읽기 전 아래의 포스트들을 읽어야 무슨소린지 알아듣기 편하다..!   
* 

---

# Abstract
**[기존 모델의 문제]**      
* 레이블이 지정되지 않은 큰 말뭉치에 대해 훈련된 Continuous word representations은 많은 자연어 처리 작업에 유용하다.    
이러한 표현을 학습하는 인기 있는 모델은 각 단어에 고유한 벡터를 할당하여 **단어의 형태를 무시**한다.    
이렇게 단어의 형상을 무시하는 문제는 특히 **어휘가 많고 희귀한 단어가 많은 언어**의 경우 더 문제가 된다.    

**[본 논문에서의 해결법]**      
* 본 논문에서는 **skipgram model**을 기반으로 한 새로운 접근법을 제안한다.     
여기서 각 단어는 문자 n-grams의 bag of character로 표현된다.     
벡터 표현은 각 문자 n-grams과 연관되어 있으며, 단어는 이러한 표현의 합으로 표현된다.     

**[skipgram model의 장점]**      
* 빠름      
* 큰 말뭉치에서 모델을 빠르게 훈련 가능      
* 훈련 데이터에 나타나지 않은 단어에 대한 단어 표현을 계산 가능      
* 단어 유사성과 유추 작업 모두에서 9개의 다른 언어로 우리의 단어 표현 평가한(ord similarity and analogy tasks)결과,          
최근 제안된 형태학적 단어 표현과 비교하여 벡터가 이러한 작업에서 최첨단 성능을 달성한다는 것을 보여준다.    


-----
-----

# 1 Introduction
Learning continuous representations of words has a
long history in natural language processing (Rumelhart et al., 1988). These representations are typically derived from large unlabeled corpora using
co-occurrence statistics (Deerwester et al., 1990;
Schütze, 1992; Lund and Burgess, 1996). A large
body of work, known as distributional semantics,
has studied the properties of these methods (Turney et al., 2010; Baroni and Lenci, 2010). In the neural
network community, Collobert and Weston (2008)
proposed to learn word embeddings using a feedforward neural network, by predicting a word based
on the two words on the left and two words on the
right. More recently, Mikolov et al. (2013b) proposed simple log-bilinear models to learn continuous representations of words on very large corpora
efficiently

Most of these techniques represent each word of
the vocabulary by a distinct vector, without parameter sharing. In particular, they ignore the internal
structure of words, which is an important limitation
for morphologically rich languages, such as Turkish or Finnish. For example, in French or Spanish,
most verbs have more than forty different inflected
forms, while the Finnish language has fifteen cases
for nouns. These languages contain many word
forms that occur rarely (or not at all) in the training
corpus, making it difficult to learn good word representations. Because many word formations follow
rules, it is possible to improve vector representations
for morphologically rich languages by using character level information.

In this paper, we propose to learn representations
for character n-grams, and to represent words as the
sum of the n-gram vectors. Our main contribution
is to introduce an extension of the continuous skipgram model (Mikolov et al., 2013b), which takes
into account subword information. We evaluate this
model on nine languages exhibiting different morphologies, showing the benefit of our approach.

