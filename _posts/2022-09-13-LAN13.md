---
title: "FastText: Bag of Tricks for Efficient Text Classification 정리"
date:   2022-11-10
excerpt: "Bag of Tricks for Efficient Text Classification"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---


# intro
 
      

## 핵심  

## 읽기 위해 필요한 지식
* [word2vec](https://yerimoh.github.io/DL14/): baseline 모델이기 때문에 꼭 알아야 한다.        
* [Embedding](https://yerimoh.github.io/DL15/): word2vec 속도 개선으로 이 포스팅도 꼭 알아야 한다.      

## 원 논문
[Bag of Tricks for Efficient Text Classification](https://arxiv.org/pdf/1607.01759.pdf)


---

# 목차  


---

# Abstract
이 논문은 text classification를 위한 간단하고 효율적인 baseline을 탐구한다.      
우리의 실험은 [fastText](https://yerimoh.github.io/LAN7/)가 accuracy 측면에서 딥 러닝 classifiers와 동등하지만, 훈련과 평가에선 이 모델이 몇 배나 더 빠르다는 것을 보여준다.      

이 논문은 standard multicore CPU를 사용하여 10분 이내에 10억 개 이상의 단어에 대해서 **빠르게 텍스트를 train**할 수 있고, 312K 클래스 중 50만 개의 문장을 1분 이내에 분류할 수 있다.   

✨ **즉 빠르게 train하며 빠르게 분류가 가능한 모델을 만들어 냈다**는 것이다 ✨ 

---


# 1. Introduction

**[기존 모델들의 문제]**    
* 텍스트 분류는 웹 검색, 정보 검색, 순위 및 문서 분류와 같은 많은 응용 프로그램이 있는 자연어 처리에서 중요한 작업이다.  
또한 이는 14도부터 신경망을 기반으로 한 모델이 점점 더 인기를 끌고 있다.    
➡ 이러한 모델은 실제로 매우 우수한 성능을 달성하지만,     
**훈련과 테스트 시간 모두에서 상대적으로 느린 경향**이 있어 매우 큰 데이터 세트에서의 사용엔 한계가 존재한다.     


**[linear classifiers의 잠재력]**        
* [linear classifiers](https://en.wikipedia.org/wiki/Linear_classifier)는 텍스트 분류 문제의 강력한 baselines으로 간주된다.     
이것은 단순하지만 적절하게 사용하면 stateof-the-art 성능을 얻을 수 있다.              
또한 **매우 큰 말뭉치로 확장할 수 있는 잠재력**도 가지고 있다.       




**[linear classifiers를 모델에 적용]**    
* 본 논문은 텍스트 분류의 맥락에서 이러한 baselines을 **large output space을 가진 매우 큰 말뭉치로 확장**하는 방법을 탐구한다.      
* [Word2vec](https://arxiv.org/abs/1301.3781)의 연구에서 영감을 받아,   
rank 제약과 빠른 loss 근사를 가진 **linear models이 10분 이내에 10억 단어에 대해 훈련할 수 있는 동시에 최첨단 수준의 성능을 달성할 수 있음을 보여준다**.       
* evaluation: 우리는 태그 예측과 감정 분석이라는 두 가지 다른 작업에서 [fastText](https://github.com/facebookresearch/fastText) 접근 방식의 품질을 평가한다.           




----


# 2. Model architecture
A simple and efficient baseline for sentence
classification is to represent sentences as bag of
words (BoW) and train a linear classifier, e.g., a
logistic regression or an SVM (Joachims, 1998;
Fan et al., 2008). However, linear classifiers do
not share parameters among features and classes.
This possibly limits their generalization in the
context of large output space where some classes
have very few examples. Common solutions
to this problem are to factorize the linear classifier into low rank matrices (Schutze, 1992;
Mikolov et al., 2013) or to use multilayer
neural networks (Collobert and Weston, 2008;
Zhang et al., 2015).


Figure 1 shows a simple linear model with rank
constraint. The first weight matrix A is a look-up
table over the words. The word representations are
then averaged into a text representation, which is in
turn fed to a linear classifier. The text representation is an hidden variable which can be potentially
be reused. This architecture is similar to the cbow
model of Mikolov et al. (2013), where the middle
word is replaced by a label. We use the softmax
function f to compute the probability distribution
over the predefined classes. For a set of N documents, this leads to minimizing the negative loglikelihood over the classes:

<img width="354" alt="image" src="https://user-images.githubusercontent.com/76824611/210780823-b4f9953e-cfed-4628-8848-9b67c3aca1fa.png">

where xn is the normalized bag of features of the nth document, yn the label, A and B the weight matrices. This model is trained asynchronously on multiple CPUs using stochastic gradient descent and a
linearly decaying learning rate.


<img width="323" alt="image" src="https://user-images.githubusercontent.com/76824611/210780968-f9457e60-d22b-4410-889b-36e5ab4f57fe.png">



Figure 1: Model architecture of fastText for a sentence with
N ngram features x1, . . . , xN . The features are embedded and
averaged to form the hidden variable.






