---
title: "[33] CS2241N: Lecture 3 Backprop and Neural Networks 정리"
date:   2019-12-28
excerpt: "Lecture 3 | Lecture 3 Backprop and Neural Networks 요약"  
category: [Deep Learning]
layout: post
tag:
- Deep Learning
order: 0

comments: true
---


# **Deep Learning Classification Task: NER**   
 
**[NER]**    
* Named Entity Recognition의 약자     
* 문맥을 추론하여 해당 자리에 어떠한 종류의 단어가 들어갈지를 정하는 것이다.     
* 미리 정의해 둔 사람, 회사, 장소, 시간, 단위 등에 해당하는 단어(개체명)를 문서에서 인식하여 추출 분류하는 기법이다.    
* 추출된 개체명은 인명(person), 지명(location), 기관명(organization), 시간(time) 등으로 분류된다. 개체명 인식(NER)은 정보 추출을 목적으로 시작되어 자연어 처리, 정보 검색 등에 사용된다.       
* 쉽게 표현하자면 '문자열을 입력으로 받아 단어 별로 해당되는 태그를 내뱉게 하는 multi-class 분류 작업'이라 말할 수 있다.   



예를 들어보자.     
아래의 문장들에서 문맥에 따라 각 분홍색에는,    
* ```PER```에는 사람이 들어가야 자연스럽다.     
* ```LOC```에는 위치가 들어가야 자연스럽다.       
* ```DATE```에는 날짜가 들어가야 자연스럽다. 
* 즉 이와 같이 각 단어에 대해 태그를 표현해주는 것이 NER이다.      
![image](https://user-images.githubusercontent.com/76824611/180602458-33ad2484-759b-4d17-87e5-7ce210685240.png)



**[Simple NER]**    
* 그럼 간단한 NER로 설명을 더 구체적으로 해보자.     
* 이는 binary logistic classifier를 이용한 Window classification이다.     
* **Idea**: 각 단어를 인접 단어의 context window에 분류한다.      
  * window의 단어 word vectors 연결을 기반으로 각 클래스에 대해 중심 단어 {yes/no}를 분류하도록 손으로 레이블링된 데이터에 대한 logistic classifier 교육     
  * 사실, 우리는 보통 multi-class softmax를 사용하지만, 우리는 그것을 단순하게 유지하려고 노력하고 있다.(이진 분류기로!)     
* **과정:**      
   * **1)** 각 단어 모두 아래와 같은 과정을 반복하는데 지금은 “Paris” 차례라고 가정해보자       
   * **2)** 주위에 window를 만든다.       
   * **3)** 5단어에 대해 word2vec이나 Glove를 통해 단어 벡터를 얻는다.      
   * **4)** targrt 단어(“Paris”)가 중간에 오도록 5개의 단어 벡터를 연결하여 긴 벡터를 만들 것이다.           
   * **5)** 그러고 나서 이렇게 만든 벡터를 classifier에 넣으면 classifier는 단어가 위치될 확률을 구할 수 있다.    
   * **6)** 즉 그 결과 단어가 "위치(Paris니까)"일 확률을 말하는 분류기 획득이 가능하다.       
   * **7)** 이러한 과정 모든 단어에 적용하여 각 단어에 대해 평가를 진행한다
   * window 길이가 2인 문장에서 “Paris”를 **+(맞는 자리)/-(틀린 자리)** 위치로 분류:      
   ![image](https://user-images.githubusercontent.com/76824611/180617707-b104c32f-0db8-41b8-b459-943715cda69a.png)


**[NER의 쓰임]**    
* 자연어 처리를 이용한 정보 검색과 요약    
* 질문 답변    
* 지식 base build     
* 기계 번역    
* 즉, 명사를 잘못된 태그로 분류하면 번역 성능이 나빠질 수 있는데 (HOT DOG를 음식이 아닌 동물로 분류하면 해석이 이상해진다.) 이를 방지해 준다.     



**[NER의 계산]**    
* NER은 center word의 위치에 대한 이진 분류(+/-)이므로 아래와 같이 표현할 수 있다.    
* **목적:** 적절한 위치에 있을때 높은 점수를 내는 supervised training을 할 것이다.    
* **계산 과정:**    
   * **1)** $$x(input)$$: $$x$$는 중심 단어인 $$x_{Paris}$$를 중심으로 window size가 2인 벡터이다.        
   * **2)** $$h = f(W**x**+b)$$: 위의 $$x(input)$$을 신경망 레이어에 넣는다.     
      * $$W$$: 신경망에서 학습할 단어의 가중치를 곱함     
      * $$b$$: 단어를 보정해줄 bias를 더함    
      * $$f$$: softmax로 변환   
      * $$h$$: 그 결과 신경망 안의 은닉벡터가 된다.     


아래는 강의에는 언급되었지 않지만 슬라이드에 있어서 관연 정리 내용을 링크 걸어뒀다.     
➕ 추가: [Neural classification](https://yerimoh.github.io/DLCS31/)      
➕ 추가: [cross entropy loss](https://yerimoh.github.io/DL3/#%EA%B5%90%EC%B0%A8-%EC%97%94%ED%8A%B8%EB%A1%9C%ED%94%BC-%EC%98%A4%EC%B0%A8cross-entropy-error-cee)     







<details>
<summary>👀 강의 슬라이드 보기</summary>
<div markdown="1">
  
![image](https://user-images.githubusercontent.com/76824611/180602482-bcf4d530-417b-4d2e-a638-ac2ae8650153.png)
![image](https://user-images.githubusercontent.com/76824611/180602496-3ca42484-bd11-49ca-a72e-48346991066a.png)
![image](https://user-images.githubusercontent.com/76824611/180618475-fcc28a71-ec17-4bd7-a009-269a7d2d2622.png)
![image](https://user-images.githubusercontent.com/76824611/180618484-a0e740d1-3730-4a5a-b17c-102dec9dcd87.png)

  
</div>
</details>













