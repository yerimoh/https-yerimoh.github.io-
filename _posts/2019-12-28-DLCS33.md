---
title: "[33] CS2241N: Lecture 3 Backprop and Neural Networks 정리"
date:   2019-12-28
excerpt: "Lecture 3 | Lecture 3 Backprop and Neural Networks 요약"  
category: [Deep Learning]
layout: post
tag:
- Deep Learning
order: 0

comments: true
---


# **Deep Learning Classification Task: NER**   
 
**[NER]**    
* Named Entity Recognition의 약자     
* 문맥을 추론하여 해당 자리에 어떠한 종류의 단어가 들어갈지를 정하는 것이다.     
* 미리 정의해 둔 사람, 회사, 장소, 시간, 단위 등에 해당하는 단어(개체명)를 문서에서 인식하여 추출 분류하는 기법이다.    
* 추출된 개체명은 인명(person), 지명(location), 기관명(organization), 시간(time) 등으로 분류된다. 개체명 인식(NER)은 정보 추출을 목적으로 시작되어 자연어 처리, 정보 검색 등에 사용된다.       
* 쉽게 표현하자면 '문자열을 입력으로 받아 단어 별로 해당되는 태그를 내뱉게 하는 multi-class 분류 작업'이라 말할 수 있다.   



예를 들어보자.     
아래의 문장들에서 문맥에 따라 각 분홍색에는,    
* ```PER```에는 사람이 들어가야 자연스럽다.     
* ```LOC```에는 위치가 들어가야 자연스럽다.       
* ```DATE```에는 날짜가 들어가야 자연스럽다. 
* 즉 이와 같이 각 단어에 대해 태그를 표현해주는 것이 NER이다.      
![image](https://user-images.githubusercontent.com/76824611/180602458-33ad2484-759b-4d17-87e5-7ce210685240.png)



**[Simple NER]**    
* 그럼 간단한 NER로 설명을 더 구체적으로 해보자.     
* 이는 binary logistic classifier를 이용한 Window classification이다.     
* **Idea**: 각 단어를 인접 단어의 context window에 분류한다.      
  * window의 단어 word vectors 연결을 기반으로 각 클래스에 대해 중심 단어 {yes/no}를 분류하도록 손으로 레이블링된 데이터에 대한 logistic classifier 교육     
  * 사실, 우리는 보통 multi-class softmax를 사용하지만, 우리는 그것을 단순하게 유지하려고 노력하고 있다.(이진 분류기로!)     
* **과정:**      
   * **1)** 각 단어 모두 아래와 같은 과정을 반복하는데 지금은 “Paris” 차례라고 가정해보자       
   * **2)** 주위에 window를 만든다.       
   * **3)** 5단어에 대해 word2vec이나 Glove를 통해 단어 벡터를 얻는다.      
   * **4)** targrt 단어(“Paris”)가 중간에 오도록 5개의 단어 벡터를 연결하여 긴 벡터를 만들 것이다.           
   * **5)** 그러고 나서 이렇게 만든 벡터를 classifier에 넣으면 classifier는 단어가 위치될 확률을 구할 수 있다.    
   * **6)** 즉 그 결과 단어가 "위치(Paris니까)"일 확률을 말하는 분류기 획득이 가능하다.       
   * **7)** 이러한 과정 모든 단어에 적용하여 각 단어에 대해 평가를 진행한다
   * window 길이가 2인 문장에서 “Paris”를 **+(맞는 자리)/-(틀린 자리)** 위치로 분류:      
   ![image](https://user-images.githubusercontent.com/76824611/180617707-b104c32f-0db8-41b8-b459-943715cda69a.png)


**[NER의 쓰임]**    
* 자연어 처리를 이용한 정보 검색과 요약    
* 질문 답변    
* 지식 base build     
* 기계 번역    
* 즉, 명사를 잘못된 태그로 분류하면 번역 성능이 나빠질 수 있는데 (HOT DOG를 음식이 아닌 동물로 분류하면 해석이 이상해진다.) 이를 방지해 준다.     



**[NER의 계산]**    
* NER은 center word의 위치에 대한 이진 분류(+/-)이므로 아래와 같이 표현할 수 있다.    
* **목적:** 적절한 위치에 있을때 높은 점수를 내는 supervised training을 할 것이다.    
* **계산 과정:**    
   * **1)** $$x(input)$$: $$x$$는 중심 단어인 $$x_{Paris}$$를 중심으로 window size가 2인 벡터이다.        
      * 초기 벡터는 임의의 숫자를 사용하여 채점단어가 해당 위치일 확률 변환         
   * **2)** $$h = f(W**x**+b)$$: 위의 $$x(input)$$을 신경망 레이어에 넣는다. 그 결과 밀집 벡터(정보가 추가된 벡터)가 된다.        
      * $$W$$: 신경망에서 학습할 단어의 가중치를 곱함     
      * $$b$$: 단어를 보정해줄 bias를 더함    
      * $$f$$: logistic, tanh, ReLU와 같은 non-linear function 활성화 함수       
      * $$h$$: 그 결과 신경망 안의 은닉벡터가 된다.     
   * **3)** $$s = u^{T}**h**$$: 추가 벡터와 내적한다     
       * $$s$$: 하나의 숫자로 임의의 숫자일 수 있다, 즉 점수(score)이다.    
   * **4)** $$ J_t(𝜃) = 𝜎(s) = $$ $$\frac{1}{1+e^{-**s**}} \right$$: 음수 샘플링할 때 봤던 것과 같이 같은 종류의 logisitc 변환                      
      *  $$ J_t(𝜃)$$: predicted model probability of class      
      *  logisitc 변환: 아래의 그래프와 같이 s의 값에 따라  1이나 0에 근사한 극단적인 값으로 이진 분류됨     
      ![image](https://user-images.githubusercontent.com/76824611/180619417-dd27d84b-ca95-4ff6-ae0b-fc410e963a2a.png)
   * **5)** 출력은 특정 클래스에 단어가 속할 예측 확률    
   * **6)** 이 출력을 진짜 정답과 비교하여 위치 단어일 확률에 대해 window의 각 단어를 분류할 수 있는 위치 분류기가 될 수 있음      ![image](https://user-images.githubusercontent.com/76824611/180619572-d880458e-716e-4c0b-b779-03a82d8d5601.png)



<details>
<summary>📜 Remember: Stochastic Gradient Descent</summary>
<div markdown="1">
  
**[SGD]**    
현재 손실을 기반으로 업데이트    
* 앞의 식에서 진짜면 1    
* 앞의 식에서 진짜가 아니면 0      
 
데이터를 기반으로 신경망을 훈련시키면 매개변수의 기울기를 계산할 수 있어야 함     
➡ 이러면 모델의 가중치를 반복적으로 업데이트하고, 모델을 효율적으로 훈련할 수 있음     
➡ 좋은 가중치는 정확도가 높음      
 
아래 식은 업데이트 방정식,     
* 확률함수를 기반으로 손실을 최소화 하도록 계속 𝜃 값을 업데이트 해나간다.        
![image](https://user-images.githubusercontent.com/76824611/180619888-c591e511-4655-4d73-95a6-35e021e7f90b.png)

$$∇_{𝜃}J(𝜃)$$을 계산하기 위해선,     
* 손으로 계산 할 수 있다     
* 그리고 backpropagation algorithm을 이용하여 계산가능하다 ➡ 이걸 배울 것이다
 
 
</div>
</details>  


아래는 강의에는 언급되었지 않지만 슬라이드에 있어서 관연 정리 내용을 링크 걸어뒀다.     
➕ 추가: [Neural classification](https://yerimoh.github.io/DLCS31/)      
➕ 추가: [cross entropy loss](https://yerimoh.github.io/DL3/#%EA%B5%90%EC%B0%A8-%EC%97%94%ED%8A%B8%EB%A1%9C%ED%94%BC-%EC%98%A4%EC%B0%A8cross-entropy-error-cee)     
➕ 추가: [Non-linearities (i.e., “f ” on previous slide): Why they’re needed](https://yerimoh.github.io/DL201/#%EA%B0%9C%EC%84%A0-regularization)






<details>
<summary>👀 강의 슬라이드 보기</summary>
<div markdown="1">
  
![image](https://user-images.githubusercontent.com/76824611/180602482-bcf4d530-417b-4d2e-a638-ac2ae8650153.png)
![image](https://user-images.githubusercontent.com/76824611/180602496-3ca42484-bd11-49ca-a72e-48346991066a.png)
![image](https://user-images.githubusercontent.com/76824611/180618475-fcc28a71-ec17-4bd7-a009-269a7d2d2622.png)
![image](https://user-images.githubusercontent.com/76824611/180618484-a0e740d1-3730-4a5a-b17c-102dec9dcd87.png)
![image](https://user-images.githubusercontent.com/76824611/180619590-9713b4f1-c154-4d38-af66-9eae254b7fb8.png)
![image](https://user-images.githubusercontent.com/76824611/180619682-79d5c72c-1b45-4728-b185-5ed08d1e0916.png)
![image](https://user-images.githubusercontent.com/76824611/180619970-d3f97978-53f7-4eef-819e-d87cce3cc72b.png)

  
</div>
</details>



----
-----


# Introduction 
들어가기 전에 미분의 의미에 대해 직관적으로 파악해보자.     

## Gradients

**[Basic 의미]**    
* $$f(x) = x^{3}$$와 같은 방정식이 있다고 해보자     
   * 위 방정식의 구성     
   * $$f(x)$$: 1 output      
   * $$x^{3}$$: 1 input    
* **미분(Gradients)**     
   * $$ $\frac{df}{dx} \right$$ $$= 3x^{2}$$: 위의 식을 미분한 것이다  
   * 이 미분값의 의미는 x가 1일 때 입력을 조금만 움직이면 출력이 3배만큼 변한다는 것이다         
   * ex1) At x = 1 it changes about 3 times as much: 1.013 = 1.03    
   * ex2) At x = 4 it changes about 48 times as much: 4.013 = 64.48     


**[복잡한 식에 적용]**    
* 그럼 이번엔 위의 미분의 의미를 되새겨 조금 더 복잡한 식의 의미를 찾아보자      
* $$f(x) = f(x_1, x2, ... , x_n)$$과 같은 방정식이 있다고 해보자.     
   * 위 방정식의 구성     
   * $$f(x)$$: 1 output      
   * $$f(x_1, x2, ... , x_n)$$: n input   
* **미분(Gradients)**     
   * $$ $\frac{∂f}{∂x} \right$$ $$ = left( \frac{∂f}{∂x_1}, {∂f}{∂x_2}, {∂f}{∂x_n} \right)$$: 위의 식을 미분한 것이다    



# Matrix calculus 
# Backpropagation 




<details>
<summary>👀 강의 슬라이드 보기</summary>
<div markdown="1">
  
![image](https://user-images.githubusercontent.com/76824611/180620017-c558efda-fb41-4346-9efb-c4793db461d4.png)1
![image](https://user-images.githubusercontent.com/76824611/180620023-7eeac476-101e-4d59-b269-d242ebc10a65.png)

  
</div>
</details>




