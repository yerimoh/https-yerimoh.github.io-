---
title: "[33] CS2241N: Lecture 3 Backprop and Neural Networks 정리"
date:   2019-12-28
excerpt: "Lecture 3 | Lecture 3 Backprop and Neural Networks 요약"  
category: [Deep Learning]
layout: post
tag:
- Deep Learning
order: 0

comments: true
---


# **Deep Learning Classification Task: NER**   
 
**[NER]**    
* Named Entity Recognition의 약자     
* 문맥을 추론하여 해당 자리에 어떠한 종류의 단어가 들어갈지를 정하는 것이다.     
* 미리 정의해 둔 사람, 회사, 장소, 시간, 단위 등에 해당하는 단어(개체명)를 문서에서 인식하여 추출 분류하는 기법이다.    
* 추출된 개체명은 인명(person), 지명(location), 기관명(organization), 시간(time) 등으로 분류된다. 개체명 인식(NER)은 정보 추출을 목적으로 시작되어 자연어 처리, 정보 검색 등에 사용된다.       
* 쉽게 표현하자면 '문자열을 입력으로 받아 단어 별로 해당되는 태그를 내뱉게 하는 multi-class 분류 작업'이라 말할 수 있다.   



예를 들어보자.     
아래의 문장들에서 문맥에 따라 각 분홍색에는,    
* ```PER```에는 사람이 들어가야 자연스럽다.     
* ```LOC```에는 위치가 들어가야 자연스럽다.       
* ```DATE```에는 날짜가 들어가야 자연스럽다. 
* 즉 이와 같이 각 단어에 대해 태그를 표현해주는 것이 NER이다.      
![image](https://user-images.githubusercontent.com/76824611/180602458-33ad2484-759b-4d17-87e5-7ce210685240.png)


----

**[Simple NER]**    
* 그럼 간단한 NER로 설명을 더 구체적으로 해보자.     
* 이는 binary logistic classifier를 이용한 Window classification이다.     
* **Idea**: 각 단어를 인접 단어의 context window에 분류한다.      
  * window의 단어 word vectors 연결을 기반으로 각 클래스에 대해 중심 단어 {yes/no}를 분류하도록 손으로 레이블링된 데이터에 대한 logistic classifier 교육     
  * 사실, 우리는 보통 multi-class softmax를 사용하지만, 우리는 그것을 단순하게 유지하려고 노력하고 있다.(이진 분류기로!)     
* **과정:**      
  * **1)** 각 단어 모두 아래와 같은 과정을 반복하는데 지금은 “Paris” 차례라고 가정해보자       
  * **2)** 주위에 **window**를 만든다.       
  * **3)** 5단어에 대해 word2vec이나 Glove를 통해 **단어 벡터**를 얻는다.      
  * **4)** targrt 단어(“Paris”)가 중간에 오도록 5개의 **단어 벡터를 연결**하여 긴 벡터를 만들 것이다.           
  * **5)** 그러고 나서 이렇게 만든 벡터를 **classifier에 넣으면** classifier는 단어가 위치될 확률을 구할 수 있다.    
  * **6)** 즉 그 결과 단어가 "위치(Paris니까)"일 확률을 말하는 **분류기 획득**이 가능하다.       
  * **7)** 이러한 과정 모든 단어에 적용하여 각 단어에 대해 **평가를 진행**한다
window 길이가 2인 문장에서 “Paris”를 **+(맞는 자리)/-(틀린 자리)** 위치로 분류:      
![image](https://user-images.githubusercontent.com/76824611/180617707-b104c32f-0db8-41b8-b459-943715cda69a.png)


----

**[NER의 쓰임]**    
* 자연어 처리를 이용한 정보 검색과 요약    
* 질문 답변    
* 지식 base build     
* 기계 번역    
* 즉, 명사를 잘못된 태그로 분류하면 번역 성능이 나빠질 수 있는데 (HOT DOG를 음식이 아닌 동물로 분류하면 해석이 이상해진다.) 이를 방지해 준다.     

-----

**[NER의 계산]**    
* NER은 center word의 위치에 대한 이진 분류(+/-)이므로 아래와 같이 표현할 수 있다.    
* **목적:** 적절한 위치에 있을때 높은 점수를 내는 supervised training을 할 것이다.    
* **계산 과정:**    
   * **1)** $$x(input)$$:    
      * $$x$$는 중심 단어인 $$x_{Paris}$$를 중심으로 window size가 2인 벡터이다.        
      * 초기 벡터는 임의의 숫자를 사용하여 채점단어가 해당 위치일 확률 변환         
   * **2)** $$h = f(W$$**$$x$$**$$+b)$$:    
      * 위의 $$x(input)$$을 신경망 레이어에 넣는다. 그 결과 밀집 벡터(정보가 추가된 벡터)가 된다.        
      * $$W$$: 신경망에서 학습할 단어의 가중치를 곱함     
      * $$b$$: 단어를 보정해줄 bias를 더함    
      * $$f$$: logistic, tanh, ReLU와 같은 non-linear function 활성화 함수       
      * $$h$$: 그 결과 신경망 안의 은닉벡터가 된다.     
   * **3)** $$s = u^{T}$$**$$h$$**:     
      * 추가 벡터와 내적한다     
      * $$s$$: 하나의 숫자로 임의의 숫자일 수 있다, 즉 점수(score)이다.    
   * **4)** $$ J_t(𝜃)$$ = **𝜎(s) = $$ \frac{1}{1+e^{-s}} $$**:     
      * 음수 샘플링할 때 봤던 것과 같이 같은 종류의 logisitc 변환                      
      *  $$ J_t(𝜃)$$: predicted model probability of class      
      *  **logisitc 변환**: 아래의 그래프와 같이 s의 값에 따라  1이나 0에 근사한 극단적인 값으로 이진 분류됨     
      ![image](https://user-images.githubusercontent.com/76824611/180619417-dd27d84b-ca95-4ff6-ae0b-fc410e963a2a.png)
   * **5)** 출력은 특정 클래스에 단어가 속할 예측 확률    
   * **6)** 이 출력을 진짜 정답과 비교하여 위치 단어일 확률에 대해 window의 각 단어를 분류할 수 있는 위치 분류기가 될 수 있음      ![image](https://user-images.githubusercontent.com/76824611/180620836-165a07a3-f1c6-48b1-bf46-e7dcd969950d.png)



<details>
<summary>📜 Remember: Stochastic Gradient Descent</summary>
<div markdown="1">
  
**[SGD]**    
현재 손실을 기반으로 업데이트    
* 앞의 식에서 진짜면 1    
* 앞의 식에서 진짜가 아니면 0      
 
데이터를 기반으로 신경망을 훈련시키면 매개변수의 기울기를 계산할 수 있어야 함     
➡ 이러면 모델의 가중치를 반복적으로 업데이트하고, 모델을 효율적으로 훈련할 수 있음     
➡ 좋은 가중치는 정확도가 높음      
 
아래 식은 업데이트 방정식,     
* 확률함수를 기반으로 손실을 최소화 하도록 계속 𝜃 값을 업데이트 해나간다.        
![image](https://user-images.githubusercontent.com/76824611/180619888-c591e511-4655-4d73-95a6-35e021e7f90b.png)

$$∇_{𝜃}J(𝜃)$$을 계산하기 위해선,     
* 손으로 계산 할 수 있다     
* 그리고 backpropagation algorithm을 이용하여 계산가능하다 ➡ 이걸 배울 것이다
 
 
</div>
</details>  


아래는 강의에는 언급되었지 않지만 슬라이드에 있어서 관연 정리 내용을 링크 걸어뒀다.     
➕ 추가: [Neural classification](https://yerimoh.github.io/DLCS31/)      
➕ 추가: [cross entropy loss](https://yerimoh.github.io/DL3/#%EA%B5%90%EC%B0%A8-%EC%97%94%ED%8A%B8%EB%A1%9C%ED%94%BC-%EC%98%A4%EC%B0%A8cross-entropy-error-cee)     
➕ 추가: [Non-linearities (i.e., “f ” on previous slide): Why they’re needed](https://yerimoh.github.io/DL201/#%EA%B0%9C%EC%84%A0-regularization)






<details>
<summary>👀 강의 슬라이드 보기</summary>
<div markdown="1">
  
![image](https://user-images.githubusercontent.com/76824611/180602482-bcf4d530-417b-4d2e-a638-ac2ae8650153.png)
![image](https://user-images.githubusercontent.com/76824611/180602496-3ca42484-bd11-49ca-a72e-48346991066a.png)
![image](https://user-images.githubusercontent.com/76824611/180618475-fcc28a71-ec17-4bd7-a009-269a7d2d2622.png)
![image](https://user-images.githubusercontent.com/76824611/180618484-a0e740d1-3730-4a5a-b17c-102dec9dcd87.png)
![image](https://user-images.githubusercontent.com/76824611/180619590-9713b4f1-c154-4d38-af66-9eae254b7fb8.png)
![image](https://user-images.githubusercontent.com/76824611/180619682-79d5c72c-1b45-4728-b185-5ed08d1e0916.png)
![image](https://user-images.githubusercontent.com/76824611/180619970-d3f97978-53f7-4eef-819e-d87cce3cc72b.png)

  
</div>
</details>



----
-----


# **Introduction** 
들어가기 전에 미분의 의미에 대해 직관적으로 파악해보자.     



---


## Gradients

**[Basic 의미]**    
* $$f(x) = x^{3}$$와 같은 방정식이 있다고 해보자     
   * $$f(x)$$: 1 output      
   * $$x^{3}$$: 1 input    
* **미분(Gradients)**     
   * $$ \frac{df}{dx} $$ $$= 3x^{2}$$: 위의 식을 미분한 것이다  
   * 이 미분값의 의미는 x가 1일 때 입력을 조금만 움직이면 출력이 3배만큼 변한다는 것이다         
   * ex1) At x = 1 it changes about 3 times as much: 1.013 = 1.03    
   * ex2) At x = 4 it changes about 48 times as much: 4.013 = 64.48     


**[복잡한 식에 적용]**    
* 그럼 이번엔 위의 미분의 의미를 되새겨 조금 더 복잡한 식의 의미를 찾아보자      
* $$f(x) = f(x_1, x_2, ... , x_n)$$과 같은 방정식이 있다고 해보자.     
   * $$f(x)$$: 1 output      
   * $$f(x_1, x2, ... , x_n)$$: n input   
* **미분(Gradients)**     
   * $$ \frac{∂f}{∂x} = $$ $$ ( \frac{∂f}{∂x_1},  \frac{∂f}{∂x_2},  \frac{∂f}{∂x_n} )$$: 위의 식을 미분한 것이다      
   * 기울기는 각 입력에 대한 편도함수의 벡터이다      
   * 우리는 $$x_1$$에서 $$x_n$$까지 n개의 입력을 얻었고,       
   $$x_1$$에 대한 $$f$$의 편도함수, $$x_2$$에 대한 $$f$$의 편도함수 등을 계산한 다음 편도함수의 벡터를 얻는다.        
   * 이 벡터의 각 요소는 하나의 변수에 대한 단순 도한수와 같다.      



<details>
<summary>👀 강의 슬라이드 보기</summary>
<div markdown="1">
  
![image](https://user-images.githubusercontent.com/76824611/180620017-c558efda-fb41-4346-9efb-c4793db461d4.png)
![image](https://user-images.githubusercontent.com/76824611/180620023-7eeac476-101e-4d59-b269-d242ebc10a65.png)
![image](https://user-images.githubusercontent.com/76824611/180621660-233b4873-3c57-4517-a984-2c682e22ee3a.png)
![image](https://user-images.githubusercontent.com/76824611/180621669-24d18c64-4178-4ab1-9035-34bf54299e07.png)

  
</div>
</details>



-----

## Jacobian Matrix
Gradient를 일반화 하는 것이다.     

**[더 복잡한 식에 적용]**    
* 그럼 위의 미분 의 이해식에서 더  복잡한 식에 적용해보자.    
* $$f(x) = f_1(x_1, x_2, ... , x_n), ..., f_m(x_1, x_2, ... , x_n)$$과 같은 방정식이 있다고 해보자.     
   * $$f_1, ..., f_m$$: m output      
   * $$f(x_1, x2, ... , x_n)$$: n input   
* **위 식의 적용**    
   * 일반적으로 신경망에 레이어와 같은 것이 있을 때,   
   n개의 입력 내에 단어 벡터와 같은 함수가 있고, 행렬을 곱하는 것과 같은 작업을 수행한 m개의 출력을 갖게 됨    
   * 이제 n개의 입력을 받고 m개의 출력하는 함수가 생성된다    
   따라서 **이 시점에서 기울기에 대해 계산**하는 것을 Jacobian Matrix라고 한다.    
* **미분(Gradients)**     
   * 위의 식을 미분한 것이다      
   * 이 미분의 결과는 편도함수의 **모든 조합**에 대한 **m x n 행렬**이다.    
      * 따라서 함수 $$f$$는 각각 m개의 출력을 생성하는 $$f_1$$에서  $$f_m$$까지의 서로 다른 **하위 함수로 분할**됨      
      * 그래서 우리는 $$x_1$$에 대한 $$f_1$$의 편도함수를, $$x_n$$에 대한 $$f_1$$의 편도함수를 통해 취한다.    
   * 입력변수 중 하나에 대해 **출력변수의** **가능한 모든 편미분**을 갖는다.    
   ![image](https://user-images.githubusercontent.com/76824611/180621565-5b97d03d-e016-4400-bc76-edecfa1425b2.png)

   * 기울기는 각 입력에 대한 편도함수의 벡터이다      
   * 우리는 $$x_1$$에서 $$x_n$$까지 n개의 입력을 얻었고,       
   $$x_1$$에 대한 $$f$$의 편도함수, $$x_2$$에 대한 $$f$$의 편도함수 등을 계산한 다음 편도함수의 벡터를 얻는다.        
   * 이 벡터의 각 요소는 하나의 변수에 대한 단순 도수와 같다.    



➕ 이해가 힘들다면 이 링크를 눌러 [Jacobian Matrix](https://angeloyeo.github.io/2020/07/24/Jacobian.html)을 이해해보고 오는걸 추천한다     






<details>
<summary>📜 Chain Rule</summary>
<div markdown="1">
  
![image](https://user-images.githubusercontent.com/76824611/180621756-3aa44c8b-d5a9-4f28-a8dc-936bb01f972a.png)
 
 
</div>
</details> 


<details>
<summary>👀 강의 슬라이드 보기</summary>
<div markdown="1">
  
![image](https://user-images.githubusercontent.com/76824611/180621682-965e5681-acb6-4c09-87ca-b8053aa8d0b3.png)
![image](https://user-images.githubusercontent.com/76824611/180621761-6c40e412-51cc-4e1d-814d-49352dc12b78.png)

  
</div>
</details>

------

## Example Jacobian
요소별 activation 함수에 적용해 볼 것이다.     

**[목표]**      
* $$h = f(z)$$ 일 때, $$ \frac{∂h}{∂z} $$ 구하기    
  * $$h,z ∈ ℝ^n$$: $$h,z$$는 모두 실수이다     
  * $$f(z)$$: 비선형벡터    
  * 이 함수는 n outputs and n inputs ➡ **n by n Jacobian**      


**[계산]**    
* 먼저 Jacobian의 정의를 보자.    
  * 각 입력에 개해 각 출력의 편도함수를 추정하는 것이다.      
  ![image](https://user-images.githubusercontent.com/76824611/180627481-74e3d85c-e80d-4f0a-8696-db5afe19bd3a.png)
* regular 1-variable derivative    
  * i=j가 아닌 입력은 출력에 아무런 영향을 미치지 않는다.     
  ![image](https://user-images.githubusercontent.com/76824611/180627484-9a9d9613-7f5e-4c8b-b5fc-6116fbabf54d.png)
* 그렇다면 $$ \frac{∂h}{∂z} $$는,     
  * 대각선이 i=j이므로 이 외의 나머지는 다 0이다.    
  * 이는 단위행렬의 형태이다.       
  ![image](https://user-images.githubusercontent.com/76824611/180627515-88851021-a446-40cd-8a0b-f64043cfc930.png)







<details>
<summary>📜 단위행렬? </summary>
<div markdown="1">
  
선형대수학에서, 단위 행렬(영어: unit matrix) 또는 항등 행렬(영어: identity matrix)은 주대각선의 원소가 모두 1이며 나머지 원소는 모두 0인 정사각 행렬이다.
 
 
 ![image](https://user-images.githubusercontent.com/76824611/180627971-1275e2b8-1406-47df-a8ce-76a52742ea36.png)
![image](https://user-images.githubusercontent.com/76824611/180627972-de3c099f-8aa2-4f05-a1fd-560f95f5509e.png)

 
 출처: [위키백과](https://ko.wikipedia.org/wiki/%EB%8B%A8%EC%9C%84%ED%96%89%EB%A0%AC#:~:text=%EC%84%A0%ED%98%95%EB%8C%80%EC%88%98%ED%95%99%EC%97%90%EC%84%9C%2C%20%EB%8B%A8%EC%9C%84%20%ED%96%89%EB%A0%AC,0%EC%9D%B8%20%EC%A0%95%EC%82%AC%EA%B0%81%20%ED%96%89%EB%A0%AC%EC%9D%B4%EB%8B%A4.)
 
</div>
</details> 

**[Other Jacobians]**     
* 그럼 위의 식을 적용해서 아래의 식을 보자   
  * 아래의 식에다가 Jacobian 행렬식을 적용할 것이다.       
  ![image](https://user-images.githubusercontent.com/76824611/180627910-df53ed7a-f0ff-47b0-8290-c0cfe6e453ab.png)
  * 아래와 같이 적용한 결과, 단위행렬이 나온다는 것을 알 수 있다(위에서 설명했다)   
  ![image](https://user-images.githubusercontent.com/76824611/180627930-7acb3575-7964-4627-9c57-f9830dbd7a13.png)
  * h의 입력차원이 있고 하나의 출역이있으므로 행벡터를 위해 u를 transpose한다(추후 자세히 설명)
  ![image](https://user-images.githubusercontent.com/76824611/180627951-740a5b8e-f3e6-49b1-ad0a-156907adf6ab.png)




<details>
<summary>👀 강의 슬라이드 보기</summary>
<div markdown="1">
  
![image](https://user-images.githubusercontent.com/76824611/180628007-341d986d-e62b-4c5d-bcf4-d21e0712c9fe.png)
![image](https://user-images.githubusercontent.com/76824611/180628014-c8a7b3af-1c47-4aaa-977c-f0d5b826c27a.png)
![image](https://user-images.githubusercontent.com/76824611/180628020-e2067ce1-38b7-42fd-a264-9d1bfcc5f5a4.png)
![image](https://user-images.githubusercontent.com/76824611/180628025-f14284ea-7f16-4301-a34b-7eb5245ed98f.png)
![image](https://user-images.githubusercontent.com/76824611/180628028-09fce1c3-6900-4833-9cf7-19cc7dbdd79f.png)

  
</div>
</details>



-------
-------


# **Matrix calculus** 
그럼 위에 배운 내용들을 아까 배운 Neural Net에 적용해보자    


---


## Back to our Neural Net

**[목표]**    
* 변수가 W, b, u가 되는 이 모델의 다른 매개변수에 대한 실수인 점수 s의 편도함수를 계산할 것이다.     
* 먼저 **편향 b의 gradient**를 중심으로 계산할 것이다.     
* 이 점수를 도출하는 식들을 계산하기 전에 **계산을 간단하게 하는 방법**을 살펴볼 것이다.      
* 계산을 간단하게 하기 위해선 식들을 분해해야 한다.    
![image](https://user-images.githubusercontent.com/76824611/180639046-2e8e949e-abe8-430f-ba50-62eb7cc9495c.png)



1️⃣ **Break up equations into simple pieces**
* 아래와 같이 식을 조그만 조각으로 조각내고 각 차원이 뭔지 계산하는 것이 중요하다.      
![image](https://user-images.githubusercontent.com/76824611/180639365-bea316ea-ea12-470f-8762-3979cde07ebf.png)


2️⃣ **Apply the chain rule**      
* 그 다음 위에서 배운 chain rule을 적용하여 식을 더 작게 분할한다.       
* 각 분할된 식은 같은 색의 네모로 표시했다.     
![image](https://user-images.githubusercontent.com/76824611/180640088-fd5a0ae2-c1d7-4c4a-914c-ee7a29993ade.png)


3️⃣ **Write out the Jacobians**     
* **Jacobians 사용**     
  * 그럼 위에서 배운식을 그대로 적용하여 분할한 식을 치환한다.    
  * 여기서 [diag()](https://kr.mathworks.com/help/matlab/ref/diag.html)는 **대각행렬**으로 단위행렬과 같이 대각선에만 0이 아닌 값들이 있지만 단위행렬과 달리 대각 성분들이 1이 아닌 다른 수이다.       
  ![image](https://user-images.githubusercontent.com/76824611/180640297-13b38c92-7070-4be3-83f9-6e05713c1f75.png)
* **변형**   
  * **①** $$diag(f'(z))I = f'(z)$$가 성립한다. 이유는 밑에 자세히 설명해두었다.      
  * **②** 또한 ⊙는 아다마르곱(Hadamard product)으로 이 또한 밑에 자세히 설명을 해두었다.     
  * 위의 두 공식이 적용되어 치환한 식을 아래와 같이 변형시킬 수 있다.     
  ![image](https://user-images.githubusercontent.com/76824611/181449725-04761e44-fa0f-446e-b0f8-8ac3b70271dd.png)
 



<details>
<summary>📜 ① $$diag(f'(z))I = f'(z)$$</summary>
<div markdown="1">
  
![image](https://user-images.githubusercontent.com/76824611/180640619-5677ba23-f36d-4769-ab82-1bd225d39b16.png)
[출처](https://mathbang.net/565)
 
 
</div>
</details> 



<details>
<summary>📜 ②  아다마르곱(Hadamard product) </summary>
<div markdown="1">
  
![image](https://user-images.githubusercontent.com/76824611/180640670-195ece9b-a1f8-4ef9-8d76-13abb0558122.png)
[출처]([https://mathbang.net/565](https://namu.wiki/w/%ED%96%89%EB%A0%AC%EA%B3%B1))
 
 
</div>
</details> 





<details>
<summary>👀 강의 슬라이드 보기</summary>
<div markdown="1">
  
![image](https://user-images.githubusercontent.com/76824611/180640848-0b0ef210-9c0a-40f2-987f-25ebed3293cc.png)
![image](https://user-images.githubusercontent.com/76824611/180640852-12c8ac12-84f0-4eff-a115-0ff862e9dbdb.png)
![image](https://user-images.githubusercontent.com/76824611/180640854-15546387-d8d2-4561-840a-e5dc47bcafc4.png)
![image](https://user-images.githubusercontent.com/76824611/180640857-c240406e-415d-451a-bf91-573b9976f775.png)
![image](https://user-images.githubusercontent.com/76824611/180640865-6029671a-4b31-4bd3-9de0-c5c2b0eddc89.png)
![image](https://user-images.githubusercontent.com/76824611/180640869-2eec3eae-5e72-4b76-adc6-adf851cba854.png)
![image](https://user-images.githubusercontent.com/76824611/180640880-25f9b7ff-c9f0-442b-9c74-18f22d8a73f7.png)
![image](https://user-images.githubusercontent.com/76824611/180640891-4eee696e-c00d-46a0-b923-63abb93f5d74.png)
![image](https://user-images.githubusercontent.com/76824611/180640895-ea25ff56-8a2a-4883-92b5-c4def2e27b6f.png)

  
</div>
</details>



------


## Re-using Computation

**[가중치 W 계산]**      
* **식 정의**    
  * 위에서는 편향 b를 계산하는 방법이었다.    
  * 그럼 가중치 W도 마찬가지로 계산하면 된다.   
  * 즉 목표는 $$ \frac{∂s}{∂W} $$ 로 점수에 가중치가 미치는 영향의 정도 계산이라고 보면 된다.     
  ➡ 목표 식의 분모가 $$∂W$$이므로 마지막 항만 다르다.    
  * **파란색 부분은 공통**이라는 것을 알 수 있다.       
  ➡ 즉 이 파란색 부분을 재사용하기 위해 위에서 배운 정보를 통해 일반화 할 것이다.      
  ![image](https://user-images.githubusercontent.com/76824611/181446441-78e88d33-68c5-416f-819b-a29acb583661.png)
* **$$𝛿$$ 치환**
  * $$𝛿$$: 이 기호의 의미는  local error signal이다.       
  * 위의 공통된 파란색 부분을 $$𝛿$$로 치환하면 이 치환한 부분은 [Back to our Neural Net](#back-to-our-neural-net)에 의해 아래와 같이 치환된다.       
  * 즉 W와 b 모두 같다는 것을 알 수 있다.      
  ➡ 마지막 다른 항은 단위 행렬이라 계산에 영향을 주지 못하므로    
  ![image](https://user-images.githubusercontent.com/76824611/181446277-4804d002-4015-4d4e-a482-25e22ff38295.png)


<details>
<summary>👀 강의 슬라이드 보기</summary>
<div markdown="1">
  
![image](https://user-images.githubusercontent.com/76824611/180641894-103d87b1-74f9-4905-b686-b47d08be209d.png)
![image](https://user-images.githubusercontent.com/76824611/180641903-7c426201-3fb2-40cf-9b7d-6a43f85bba25.png)
![image](https://user-images.githubusercontent.com/76824611/180641909-13117b48-c495-4597-ab3f-1fbc571f5d77.png)

  
</div>
</details>



------


## Output shape
위의 계산 과정에서 output shape를 조정하는 것이 중요하다.    


현재 아래 식에 따르면 1 output, nm inputs이므로,    
$$∇_{𝜃}J(𝜃)$$는 ```1 x n x m ```의  shape을 갖은 매우 긴 밀도가 낮은 벡터가 출력될 것이다.     
이렇게 **Jacobian 형식은 chain rule을 쉽게 만들지만 이는 기울기 계산을 위한 SGD의 shape convention**에 위배된다.      
![image](https://user-images.githubusercontent.com/76824611/181445789-6cb11e31-2139-4e66-a163-dbda74655d21.png)


그런데, $$ \frac{ds}{dW} $$는 점수라는 정수 하나가 출력되므로 위의 $$∇_{𝜃}J(𝜃)$$와 연산하지 못한다.     
왜냐하면 같은 shape이 아니기 때문이다.     
➡ 그러므로 계산을 위해 **shape convention**에 따라 **shape을 맞춰주는 것**이 중요하다.     

즉 위의 문제를 해결하기위해 **shape convention**을 사용하여 아래와 같이  ```1 x n x m ``` shape으로 바꿔야 한다.   
➡ 그 결과, ```shape of the gradient=  shape of the parameters```가 된다.      
![image](https://user-images.githubusercontent.com/76824611/181445958-7abd5f16-860d-4f81-972c-8f50db98b272.png)



그럼 위의 식을 앞에서 배웠던 $$𝛿$$로 나타내어 적용해보면 아래와 같다.     
![image](https://user-images.githubusercontent.com/76824611/181446043-14607c7d-5bf2-42dc-ad0f-ae01015be1f8.png)



**[Deriving local input gradient in backprop]**      
* 이 식에서 중요한 것은 $$W_{ij}$$는 오직 $$z_i$$에만 영항을 준다는 것이다.     
   * ex)  $$W_{23}$$ is only used to compute $$z_2$$ not $$z_1$$     
   ![image](https://user-images.githubusercontent.com/76824611/181445546-cff6915f-9ccf-4e19-ad3e-3da0befb24c4.png)
* 그러므로 아래 식에서 영향을 주는 식들만 남기고,   
$$ \frac{∂}{∂W_{ij}} $$는 $$∂W_{ik}$$ 와 상쇄되어 0이되므로 아래와 같은 식이 성립된다.    
![image](https://user-images.githubusercontent.com/76824611/180642579-8984606e-2906-482d-b8e9-819176272a0b.png)



**[derivatives를 위해 어떤 shape이어야 할까?]**    
* $$ \frac{∂s}{∂b} $$ = $$h^T$$ $$⊙  f'(z)$$는 행(row)벡터이다.        
➡ 그러나 **shape convention**에 의하면, b가 열(column) 벡터이기 때문에 gradient도 열(column) 벡터여야 한다     
* Jacobian 형식(체인 규칙을 쉽게 만드는 것)과 **shape convention**(SGD를 쉽게 구현하는 것) 간의 불일치       
➡ 즉 Jacobian form은 답을 계산하는 데 유용하지만 **SGD를 위해** **shape convention**에 맞춰줘야 한다.     
* **두 가지 옵션:**     
   * **1.** Jacobian 형식을 최대한 사용하고, 마지막에 shape convention을 따르도록 모양을 바꿈      
      * 우리가 방금 한 것임. $$𝛿$$ 까진 Jacobian 형식을 사용하다가 마지막에 도함수를 열 벡터로 만들기 위해 전치한다.    
      * 즉 결과는 $$𝛿^T$$이다.     
   * $$𝛿$$ 항상 shape convention을 따름      
      * 차원을 검토하여 항을 전환 및/또는 재정렬할 시기를 파악한다.      
      * hidden layer에 도착하는 오류 메시지 $$𝛿$$은 해당 hidden layer과 shape이 같음.         




<details>
<summary>📜 Why the Transposes?</summary>
<div markdown="1">
  
![image](https://user-images.githubusercontent.com/76824611/180642624-10ab5d97-6066-4c9a-9eb0-3da9223fd793.png)
![image](https://user-images.githubusercontent.com/76824611/180643254-6cdfa798-0cca-43c2-a002-c1b4c4651a8f.png)
![image](https://user-images.githubusercontent.com/76824611/180643259-d96d7697-ffaf-47c1-badc-b9b786e6597c.png)
![image](https://user-images.githubusercontent.com/76824611/180643263-12c4db47-c9b9-4be6-bdb9-e976744b7ae1.png)
![image](https://user-images.githubusercontent.com/76824611/180643266-32df660f-b9d6-4d4f-a2e4-3e7dd6790ff3.png)
![image](https://user-images.githubusercontent.com/76824611/180643271-d81e11b8-daa2-4249-8a6e-15c8e029f4bc.png)
 
 
</div>
</details>  


<details>
<summary>👀 강의 슬라이드 보기</summary>
<div markdown="1">
  
![image](https://user-images.githubusercontent.com/76824611/180643248-34fb7db9-4922-4804-bc1f-044b0b1f5b66.png)

  
</div>
</details>

-----
-----

# **Backpropagation** 
Backpropagation 즉 역전파는 아래 CS231n에서 해준 강의가 더 정확하고 이해하기 쉬워서 정리해둔 [링크](https://yerimoh.github.io/DL203/)를 첨부했다.       
링크로 걸어둔 포스트를 읽고 첨부한 슬라이드를 확인하면 이해가 정말 잘될 것이다      







