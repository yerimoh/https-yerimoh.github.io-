---
title: "[00] [INDEX] Deep learning 1 (밑바닥 부터 시작하는 딥러닝 1) "
date:   2021-03-12
excerpt: "Deep learning starting from the bottom 1"
category: [Deep Learning]
layout: post
tag:
- Deep Learning
order: 0

comments: true
---

## intro

[kor]
전 수학을 잘 알지 못하는 상경계열이므로 최대한 저의 눈높이에 맞춰 설명할 수 있는 모든 것을 설명하려고 노력했습니다 :)

[eng]
Since I'm not a very good mathematician, I've tried to explain everything I can to my level :)

---

# 목차


# [01 퍼셉트론] [learn [01]](https://yerimoh.github.io/DL1/){: .btn}

- [퍼셉트론](#퍼셉트론)
- [퍼셉트론이란?](#퍼셉트론이란?)
- [단순한 논리회로(1층퍼셉트론)](#단순한-논리회로(1층퍼셉트론))
  * [AND 게이트](#AND-게이트)
  * [NAND 게이트와 OR 게이트](#NAND-게이트와-OR-게이트)
- [구현](#구현)
  * [AND](#AND)
    + [가중치와 편향 구현하기](#가중치와-편향-구현하기)
- [퍼셉트론의 한계](#퍼셉트론의-한계)
  * [XOR 게이트](#XOR-게이트)
  * [선형과 비선형](#선형과-비선형)
- [다층 퍼셉트론의 충돌 시](#다층-퍼셉트론의-충돌-시)
  * [기존 게이트 조합(2(3)층 퍼셉트론)](#기존-게이트-조합(2(3)층-퍼셉트론))
  * [구현](#구현)
- [정리](#정리)

---

# [02 신경망]   [learn [02]](https://yerimoh.github.io/DL2/){: .btn}

- [신경망](#신경망)
- [활성화 함수(activation function)](#활성화-함수(activation-function))
- [퍼셉트론에서 이용하는 활성화 함수](#퍼셉트론에서-이용하는-활성화-함수)
  * [계단 함수 step function](#계단-함수-step-function)
- [신경망에서 이용하는 활성화 함수](#신경망에서-이용하는-활성화 함수)
  * [sigmoid function(시그모이드 함수)](#sigmoid-function(시그모이드-함수))
  * [구현](#구현)
  * [시각화](#시각화)
- [비선형 함수](#비선형-함수)
  * [활성화 함수(activation function)](#활성화 함수(activation-function))
    + [ReLU 함수](#relu-함수)
      - [구현](#구현)
- [다차원 배열의 계산](#다차원-배열의-계산)
  * [배열의 차원 수 확인](#배열의-차원-수-확인)
  * [행렬의 곱](#행렬의-곱)
  * [신경망에서의 행렬의 곱](#신경망에서의-행렬의-곱)
  * [3층 신경망 구현하기](#33층-신경망-구현하기)
- [출력층 설계](#출력층-설계)
  * [항등함수와 소프트맥스 함수 구현하기](#항등함수와-소프트맥스-함수-구현하기)
  * [소프트맥스 함수 구현 시 주의점](#소프트맥스-함수-구현-시-주의점)
  * [구현개선](#구현개선)
  * [특징](#특징)
  * [출력층의 뉴런 수 정하기](#출력층의-뉴런-수-정하기)
- [한줄정리](#한줄정리)

---

# [03 신경망 학습 ]   [learn [03]](https://yerimoh.github.io/DL3/){: .btn}

- [INTRO](#intro)
- [데이터 주도 학습](#데이터-주도-학습)
  * [훈련 데이터와 시험 데이터](#훈련-데이터와-시험-데이터)
- [손실 함수(loss function)](#손실-함수(loss-function))
  * [오차 제곱 합(sum of squares for error, SSE)](#오차-제곱-합(sum-of-squares-for-error--sse))
  * [교차 엔트로피 오차(cross entropy error, CEE)](#교차-엔트로피-오차(cross-entropy-error--cee))
  * [미니배치 학습](#미니배치-학습)
    + [(배치용) 교차 엔트로피 오차 구현하기](#(배치용)-교차-엔트로피-오차-구현하기)
  * [손실함수 설정 이유](#손실함수-설정-이유)
- [수치 미분](#수치-미분)
  * [미분](#미분)
  * [구현시 주의](#구현시-주의)
  * [중심 차분 or 중앙 차분](#중심-차분-or-중앙-차분)
  * [편미분](#편미분)
  * [경사법(경사 하강법)](#경사법(경사-하강법))
    + [수식](#수식)
  * [신경망에서의 기울기](#신경망에서의-기울기)
- [학습 알고리즘 구현하기](#학습-알고리즘-구현하기)

---

# [04 오차 역전법]  [learn [04]](https://yerimoh.github.io/DL4/){: .btn}     
- [오차역전법(backpropagation)(=‘역전파법’,‘역전파’)](#오차역전법-backpropagation----역전파법---역전파--)
- [계산그래프 computational graph](#계산그래프-computational-graph)
  * [연쇄법칙 chain rule](#연쇄법칙-chain-rule)
  * [역전파](#역전파)
    + [덧셈 노드의 역전파](#덧셈-노드의-역전파)
    + [곱셈 노드의 역전파](#곱셈-노드의-역전파)
- [활성화 함수 계층 구현](#활성화-함수-계층-구현)
  * [ReLU 계층](#relu-계층)
  * [Sigmoid 계층](#sigmoid-계층)
- [Affine/Softmax 계층 구현하기](#affine-softmax-계층-구현하기)
  * [Affine 계층](#affine-계층)
  * [Softmax-with-Loss 계층](#softmax-with-loss-계층)
- [오차역전파법 구현](#오차역전파법-구현)
  * [인스턴스 변수](#인스턴스-변수)
  * [메서드  설명](#메서드--설명)
  * [point](#point)
  * [오차역전파법으로 구한 기울기 검증](#오차역전파법으로-구한-기울기-검증)
- [정리](#정리)


---


# [05 옵티마이저]   [learn [05]](https://yerimoh.github.io/DL5/){: .btn}
- [옵티마이저 (Optimizers)](#옵티마이저--optimizers-)
  * [확률적 경사 하강법 SGD](#확률적 경사 하강법-sgd)
    + [SGD 단점](#sgd-단점)
  * [모멘텀 Momentum](#모멘텀-momentum)
  * [AdaGrad](#adagrad)
    + [RMSProp](#rmsprop)
  * [Adam](#adam)
- [어느 갱신 방법을 이용할 것인가?](#어느 갱신 방법을 이용할 것인가?)
- [가중치의 초깃값](#가중치의 초깃값)
  * [은닉층의 활성화값 분포](#은닉층의 활성화값 분포)
- [배치 정규화 Batch Normalization](#배치 정규화-batch-normalization)
- [바른 학습을 위해](#바른 학습을 위해)
  * [오버피팅](#오버피팅)
  * [가중치 감소(weight decay)](#가중치 감소-weight-decay-)
  * [드롭아웃](#드롭아웃)
- [적절한 하이퍼파라미터 값 찾기](#적절한 하이퍼파라미터 값 찾기)
  * [하이퍼파라미터 최적화](#하이퍼파라미터 최적화)
- [정리](#정리)

---

# [06] MNIST 데이터세트로 이미지 분류  [learn [06]](https://yerimoh.github.io/DL6/){: .btn}  


---


# [07] 경사하강법 자세히 알기   [learn [07]](https://yerimoh.github.io/DL7/){: .btn}  
- [경사 하강법 gradient descent method](#경사-하강법-gradient-descent-method)
  * [시작하기 전 읽어보면 좋을 개념(필수 개념)](#시작하기-전-읽어보면-좋을-개념-필수-개념-)
- [목표](#목표)
- [bulid-up 1](#bulid-up-1)
  * [MSE(Mean Squared Error)](#mse-mean-squared-error-)
    + [MSE의 수식](#mse의-수식)
    + [MSE 구현](#mse-구현)
- [bulid-up 2](#bulid-up-2)
  * [손실 곡선(The Loss Curve)](#손실-곡선-the-loss-curve-)
- [경사 하강법 gradient descent method](#경사 하강법-gradient-descent-method-1)
  * [예시](#예시)
    + [1) MSE에 대입](#1--mse에-대입)
    + [2) MSE를 확장](#2--mse를-확장)
    + [3) 이동방향 설정](#3--이동방향-설정)
    + [4) 학습률(λ)도입](#4--학습률---도입)
    + [5) 학습](#5--학습)

