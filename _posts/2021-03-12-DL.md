---
title: "[00] [INDEX] Deep learning 1"
date:   2021-03-12
excerpt: "Deep learning starting from the bottom 1"
category: [Deep Learning]
layout: post
tag:
- Deep Learning
order: 0

comments: true
---

## intro

[kor]
전 수학을 잘 알지 못하는 상경계열이므로 최대한 저의 눈높이에 맞춰 설명할 수 있는 모든 것을 설명하려고 노력했습니다 :)

[eng]
Since I'm not a very good mathematician, I've tried to explain everything I can to my level :)

---

# 목차


# [01 퍼셉트론] [learn [01]](https://yerimoh.github.io/DL1/){: .btn}
- [퍼셉트론](#퍼셉트론)
- [퍼셉트론이란?](#퍼셉트론이란?)
- [단순한 논리회로(1층퍼셉트론)](#단순한-논리회로(1층퍼셉트론))
  * [AND 게이트](#AND-게이트)
  * [NAND 게이트와 OR 게이트](#NAND-게이트와-OR-게이트)
- [구현](#구현)
  * [AND](#AND)
    + [가중치와 편향 구현하기](#가중치와-편향-구현하기)
- [퍼셉트론의 한계](#퍼셉트론의-한계)
  * [XOR 게이트](#XOR-게이트)
  * [선형과 비선형](#선형과-비선형)
- [다층 퍼셉트론의 충돌 시](#다층-퍼셉트론의-충돌-시)
  * [기존 게이트 조합(2(3)층 퍼셉트론)](#기존-게이트-조합(2(3)층-퍼셉트론))
  * [구현](#구현)
- [정리](#정리)

---

# [02 신경망 구성: 활성화 함수]   [learn [02]](https://yerimoh.github.io/DL2/){: .btn}
# 활성화 함수 
- [활성화 함수(activation function)](#활성화-함수(activation-function))
- [퍼셉트론에서 이용하는 활성화 함수](#퍼셉트론에서-이용하는-활성화-함수)
  * [계단 함수 step function](#계단-함수-step-function)
- [신경망에서 이용하는 활성화 함수](#신경망에서-이용하는-활성화-함수)
  * [비선형 함수](#비선형-함수)
  * [sigmoid function(시그모이드 함수)](#sigmoid-function(시그모이드-함수))
  * [ReLU 함수](#relu-함수)
- [다차원 배열의 계산](#다차원-배열의-계산)
  * [배열의 차원 수 확인](#배열의-차원-수-확인)
  * [행렬의 곱](#행렬의-곱)
  * [신경망에서의 행렬의 곱](#신경망에서의-행렬의-곱)
  * [3층 신경망 구현하기](#33층-신경망-구현하기)
- [출력층 설계](#출력층-설계)
  * [항등 함수 identity function](#항등-함수-identity-function)
  * [소프트맥스 함수 softmax function](#소프트맥스-함수-softmax-function)
    + [소프트맥스 함수 구현 시 주의점](#소프트맥스-함수-구현-시-주의점)
    + [출력층의 뉴런 수 정하기](#출력층의-뉴런-수-정하기)

---

# 신경망 학습
## [03 신경망 학습: 손실함수]   [learn [03]](https://yerimoh.github.io/DL3/){: .btn}
- [데이터 주도 학습](#데이터-주도-학습)
  * [훈련 데이터와 시험 데이터](#훈련-데이터와-시험-데이터)
- [손실 함수(loss function)](#손실-함수(loss-function))
  * [오차 제곱 합(sum of squares for error, SSE)](#오차-제곱-합(sum-of-squares-for-error--sse))
  * [교차 엔트로피 오차(cross entropy error, CEE)](#교차-엔트로피-오차(cross-entropy-error--cee))
  * [미니배치 학습](#미니배치-학습)
    + [(배치용) 교차 엔트로피 오차 구현하기](#(배치용)-교차-엔트로피-오차-구현하기)
  * [손실함수 설정 이유](#손실함수-설정-이유)


## [04 신경망 학습: 매개변수 갱신 ]   [learn [04]](https://yerimoh.github.io/DL3.5/){: .btn}
- [수치 미분](#수치-미분)
  * [미분](#미분)
    * [구현시 주의](#구현시-주의)
    * [중심 차분 or 중앙 차분](#중심-차분-or-중앙-차분)
  * [편미분](#편미분)
  * [경사법(경사 하강법)](#경사법(경사-하강법))
    + [plateau, 플래토](#plateau--플래토)
    + [경사법 gradient method](#경사법-gradient-method) 
    + [수식](#수식)
  * [신경망에서의 기울기](#신경망에서의-기울기)
- [학습 알고리즘 구현하기](#학습-알고리즘-구현하기)

## [+] Gradient Decent method 자세히 알기   [learn [+]](https://yerimoh.github.io/DL7/){: .btn}  
- [경사 하강법 gradient descent method](#경사-하강법-gradient-descent-method)
  * [시작하기 전 읽어보면 좋을 개념(필수 개념)](#시작하기-전-읽어보면-좋을-개념-필수-개념-)
- [목표](#목표)
- [bulid-up 1](#bulid-up-1)
  * [MSE(Mean Squared Error)](#mse-mean-squared-error-)
    + [MSE의 수식](#mse의-수식)
    + [MSE 구현](#mse-구현)
- [bulid-up 2](#bulid-up-2)
  * [손실 곡선(The Loss Curve)](#손실-곡선-the-loss-curve-)
- [경사 하강법 gradient descent method](#경사 하강법-gradient-descent-method-1)
  * [예시](#예시)
    + [1) MSE에 대입](#1--mse에-대입)
    + [2) MSE를 확장](#2--mse를-확장)
    + [3) 이동방향 설정](#3--이동방향-설정)
    + [4) 학습률(λ)도입](#4--학습률---도입)
    + [5) 학습](#5--학습)

---

# [05 오차 역전법]  [learn [04]](https://yerimoh.github.io/DL4/){: .btn}     
- [오차역전법(backpropagation)(=‘역전파법’,‘역전파’)](#오차역전법-backpropagation----역전파법---역전파--)
- [계산그래프 computational graph](#계산그래프-computational-graph)
  * [연쇄법칙 chain rule](#연쇄법칙-chain-rule)
  * [역전파](#역전파)
    + [덧셈 노드의 역전파](#덧셈-노드의-역전파)
    + [곱셈 노드의 역전파](#곱셈-노드의-역전파)
- [활성화 함수 계층 구현](#활성화-함수-계층-구현)
  * [ReLU 계층](#relu-계층)
  * [Sigmoid 계층](#sigmoid-계층)
- [Affine/Softmax 계층 구현하기](#affine-softmax-계층-구현하기)
  * [Affine 계층](#affine-계층)
  * [Softmax-with-Loss 계층](#softmax-with-loss-계층)
  * [오차역전파법 구현](#오차역전파법-구현)
- [오차역전파법으로 구한 기울기 검증](#오차역전파법으로-구한-기울기-검증)
- [정리](#정리)



---


# [06 옵티마이저]   [learn [05]](https://yerimoh.github.io/DL5/){: .btn}
- [옵티마이저 (Optimizers)](#옵티마이저--optimizers-)
  * [확률적 경사 하강법 SGD](#확률적 경사 하강법-sgd)
    + [SGD 단점](#sgd-단점)
  * [모멘텀 Momentum](#모멘텀-momentum)
  * [AdaGrad](#adagrad)
    + [RMSProp](#rmsprop)
  * [Adam](#adam)
- [어느 갱신 방법을 이용할 것인가?](#어느 갱신 방법을 이용할 것인가?)
- [가중치의 초깃값](#가중치의 초깃값)
  * [은닉층의 활성화값 분포](#은닉층의 활성화값 분포)
- [배치 정규화 Batch Normalization](#배치 정규화-batch-normalization)
- [바른 학습을 위해](#바른 학습을 위해)
  * [오버피팅](#오버피팅)
  * [가중치 감소(weight decay)](#가중치 감소-weight-decay-)
  * [드롭아웃](#드롭아웃)
- [적절한 하이퍼파라미터 값 찾기](#적절한 하이퍼파라미터 값 찾기)
  * [하이퍼파라미터 최적화](#하이퍼파라미터 최적화)
- [정리](#정리)

---

# [07] MNIST 데이터세트로 이미지 분류  [learn [06]](https://yerimoh.github.io/DL6/){: .btn}  
- [트레이닝을 위한 데이터 준비](#트레이닝을-위한-데이터-준비)
  * [1) 이미지 데이터 평탄화](#1--이미지-데이터-평탄화)
  * [2) 이미지 데이터 정규화](#2--이미지-데이터-정규화)
  * [3.1) 범주 인코딩](#31--범주-인코딩)
  * [3.2) 레이블 범주 인코딩](#32--레이블-범주-인코딩)
- [모델 생성](#모델-생성)
  * [0) 모델 인스턴스 화](#0--모델-인스턴스-화)
  * [1) 입력 레이어 생성](#1--입력-레이어-생성)
  * [2) 히든레이어 생성](#2--히든레이어-생성)
  * [3) 출력 레이어 생성](#3--출력-레이어-생성)
- [모델 요약](#모델-요약)
- [모델 컴파일](#모델-컴파일)
- [모델 트레이닝](#모델-트레이닝)
- [정확도 관찰](#정확도-관찰)
- [추론](#추론)
- [메모리 지우기](#메모리-지우기)

## [+]  미국 수화 데이터세트 이미지 분류 TEST  [learn [06.5]](https://yerimoh.github.io/DL6.5/){: .btn}  

---




----

# [08] CNN  [learn [08]](https://yerimoh.github.io/DL8/){: .btn}  
- [합성곱 신경망(convolutional neural network, CNN)](#합성곱-신경망-convolutional-neural-network--cnn-)
- [전체 구조](#전체-구조)
    + [CNN의 구조](#cnn의-구조)
- [합성곱 계층](#합성곱-계층)
  * [특징 맵 (feature map)](#특징-맵--feature-map-)
  * [합성 곱 연산](#합성-곱-연산)
  * [3차원 데이터의 합성곱 연산](#3차원-데이터의-합성곱-연산)
- [풀링(pooling) 계층](#풀링-pooling--계층)
  * [풀링의 의미](#풀링의-의미)
  * [풀링의 종류](#풀링의-종류)
  * [풀링 계층의 특징](#풀링-계층의-특징)
- [합성곱/풀링 계층 구현하기](#합성곱-풀링-계층-구현하기)
  * [im2col로 데이터 전개](#im2col로-데이터-전개)
  * [합성곱 계층의 구현 흐름](#합성곱-계층의-구현-흐름)
  * [풀링 계층 구현](#풀링-계층-구현)
- [CNN 구현하기](#cnn-구현하기)
- [CNN 시각화하기](#cnn-시각화하기)
- [정리](#정리)

## [08.5] CNN 구현   [learn [08]](https://yerimoh.github.io/DL8.5/){: .btn}  


----

# [09] 데이터 증강 DATA AUGMENTATION 구현  [learn [09]](https://yerimoh.github.io/DL9/){: .btn}  
- [데이터 증강 DATA AUGMENTATION](#데이터-증강-data-augmentation)
  * [데이터 준비](#데이터 준비)
  * [CNN을 위한 이미지 변환 reshaping](#cnn을-위한-이미지-변환--reshaping)
  * [합성곱 모델 생성](#합성곱-모델-생성)
  * [데이터 증강](#데이터-증강)
    + [옵션](#옵션)
    + [배치크기](#배치크기)
  * [데이터 생성기에 맞추기](#데이터-생성기에-맞추기)
  * [모델 컴파일](#모델-컴파일)
- [증강으로 트레이닝](#증강으로-트레이닝)
- [결과 해석](#결과-해석)
- [모델 저장](#모델-저장)
- [메모리](#메모리)
- [이 모델 이용해보기](#이-모델-이용해보기)

# [010] 데이터 증강 모델 배포    [learn [010]](https://yerimoh.github.io/DL10/){: .btn}  
- [구현하기 전 읽어 볼 지식](#구현하기-전-읽어-볼-지식)
- [모델 로드](#모델-로드)
- [모델을 위한 이미지 준비](#모델을-위한-이미지-준비)
  * [이미지 표시](#이미지-표시)
  * [이미지 확장](#이미지-확장)
  * [예측을 위한 이미지 준비](#예측을-위한-이미지-준비)
    + [정규화](#정규화)
- [예측 수행](#예측-수행)
- [예측 이해](#예측-이해)
- [TEST](#test)
- [메모리](#메모리)

------

# [011] Pre-Trained Models   [learn [011]](https://yerimoh.github.io/DL11/){: .btn}  
  * [우리가 시험할 모델](#우리가-시험할-모델)
  * [목표](#목표)
- [개구멍 자동문](#개구멍-자동문)
- [모델로드](모델로드)
- [이미지 사전처리](#이미지-사전처리)
    + [이미지 표시](#이미지-표시)
    + [사전처리](#사전처리)
- [예측 수행](#예측-수행)
  * [예측 해보기](#예측-해보기)
- [목표 완성](#목표-완성)
- [메모리](#메모리)
- [다음 학습](#다음-학습)



-----


# [012] Transfer Learning  [learn [012]](https://yerimoh.github.io/DL12/){: .btn}  
  * [구현하기 전 읽어 볼 지식](#구현하기-전-읽어-볼-지식)
- [맞춤형 분류기](#맞춤형-분류기)
  * [사전 트레이닝된 모델 다운로드](#사전-트레이닝된-모델-다운로드)
- [기본 모델 동결](#기본-모델-동결)
- [새 레이어 추가](#새-레이어-추가)
- [모델 컴파일](#모델-컴파일)
- [데이터 증강](#데이터-증강)
- [모델 트레이닝](#모델-트레이닝)
- [결과 개선](#결과-개선)
  * [파인 튜닝](#파인-튜닝)
  * [모델 파인 튜닝](#모델-파인-튜닝)
- [예측 검사](#예측-검사)
  * [Bo 분류기](#bo-분류기)
- [메모리](#메모리)
