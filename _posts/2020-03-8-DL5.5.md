---
title: "[+] Deep learning 1: 신경망 가중치 초기값 설정법"
date:   2020-03-8
excerpt: "가중치의 초깃값, 설정하면 안되는 초기값, 초기값을 0으로 설정/초기값 문제: 은닉층 활성화층 분포 불균형, 기울기 소실, 표현력 제한/각 활성화 함수 별 추천 초기값"
category: [Deep Learning]
layout: post
tag:
- Deep Learning
order: 0

comments: true
---


# 목차
- [가중치의 초깃값](#가중치의-초깃값)
- [설정하면 안되는 초기값](#설정하면-안되는-초기값)
  * [초기값을 0으로 설정](#초기값을-0으로-설정)
- [**초기값 문제: 은닉층 활성화층 분포 불균형**](#초기값 문제--은닉층-활성화층-분포-불균형)
  * [1) 기울기 소실 gradient vanishing](#1-기울기-소실-gradient-vanishing)
  * [2) 표현력 제한](#2--표현력 제한)
- [**각 활성화 함수 별 추천 초기값**](#각-활성화-함수-별-추천-초기값)
  * [sigmoid, tanh](#sigmoid--tanh)
  * [ReLU](#relu)
- [정리](#정리)




---



👀, 🤷‍♀️ , 📜    
이 아이콘들을 누르시면 코드, 개념 부가 설명을 보실 수 있습니다:)


---
----


# 가중치의 초기값
신경망 학습에서 특히 중요한 것이 가중치의 초기값이다.     

이제 신경망의 가중치 초기값으로 설정하면 안되는 값과 각 [활성화 함수](https://yerimoh.github.io/DL2/) 별 효과적인 가중치를 소개할 것이다


-----
-----

# 설정하면 안되는 초기값

그럼 가중치 감소 weight decay 기법부터 소개해 보겠다.      

**[가중치감소 기법]**   
* **오버피팅을 억제**해 범용성능을 높이는 테크닉         
* 가중치 매개변수의 값이 작아지도록 학습하는 방법     
* 가중치 값을 작게 하여 ~~오버피팅~~이 일어나지 않게 하는 것    
사실 지금까지 가중치의 초깃값은 0.01 * np.random.randn (10, 100 )처럼 정규분포에서 생성되는 값을0.01배 한 작은 값(표준편차가 0.01인 정규분포)을 사용.   

---

## 초기값을 0으로 설정  
가중치 초기값을 0으로 하면 학습이 올바로 이뤄지지 않음   
즉, 0으로 절대 설정하지 말아야 한다.    
➡️ 정확히는 ~~가중치를 균일한 값~~으로 설정 해서는 안됌      

**[이유]**     
오차역전파법(오차를 점점 거슬러 올라가면서 비율에 맞게 다시 전파하는 것을 의미)(기울기를 효율적으로 구함)에서 모든 가중치의 값이 똑같이 갱신되기 때문      

그럼 예를들어 자세하게 설명해 보겠다.      
ex) 2층 신경망에서 첫 번째와 두 번째 층의 가중치가 0이라고 해보자.        
* 순전파: 입력층의 가중치가 0이기 때문에 두 번째 층의 뉴런에 모두 같은 값이 전달됨     
* 역전파: 위의 순전파로 인해 두 번째 층의 가중치가 모두 똑같이 갱신된다(곱셈노드의 역전파)   
역전파 방법은 결과 값을 통해서 다시 역으로 input 방향으로 오차를 다시 보내며 가중치를 재업데이트 하는 것이다.        
물론 결과(오차율)에 영향을 많이 미친 노드(뉴런)에 더 많은 오차를 돌려줄 것이다.    
	 
![image](https://user-images.githubusercontent.com/76824611/128636670-571ec890-28d4-4801-9022-eac7667f9b00.png)

이 ‘가중치가 고르게 되어버리는 상황’을 막으려면 (정확히는 가중치의 대칭적인 구조를 무너뜨리려면) **초깃값을 무작위로 설정**해야 한다.


------
-----

# **초기값 문제: 은닉층 활성화층 분포 불균형**
은닉층의 활성화값(다른 문헌에서는 계층 사이를 흐르는 데이터)(활성화 함수의 출력 데이터)의 분포를 관찰하면 중요한 정보를 얻을 수있다.    
그런데 초기값을 잘못 설정하면 2가지 문제점이 있을수 있다.   
➡️ 이번엔 **가중치의 초깃값**에 따라 **은닉층 활성화**값들이 어떻게 변화하는지 확인하여 대표적인 2가지 문제를 살펴보겠다.          
   
   
   
---

## 1) 기울기 소실 gradient vanishing
가중치를 표준편차가 1인 정규분포로 초기화할 때(너무 다름),    
활성화값들이 0과 1에 치우쳐 분포      
* 역전파의 기울기 값이 점점 작아지다가 사라짐
     
![image](https://user-images.githubusercontent.com/76824611/128637055-30ad0c08-202a-4751-92bd-99be22823de9.png)



----


## 2) 표현력 제한
가중치를 표준편차가 0.01인 정규분포로 초기화할 때(너무 같음),      
0.5 부근에 집중 -> 표현력 관점에서는 큰 문제       
* 다수의 뉴런이 거의 같은 값을 출력하고 있으니 뉴런을 여러 개 둔 의미가 없어짐
     
![image](https://user-images.githubusercontent.com/76824611/128637060-06ed0799-7015-4223-87cb-48c4b185e0b0.png)

➡ 각 층의 활성화값은 적당히 고루 분포되어야 함.     
층과 층 사이에 적당하게 **다양한 데이터가 흐르게 해야** 신경망 학습이 효율적으로 이뤄지기 때문 

-----

# **각 활성화 함수 별 추천 초기값**       

## sigmoid, tanh
[sigmoid](https://yerimoh.github.io/DL2/#sigmoid-function%EC%8B%9C%EA%B7%B8%EB%AA%A8%EC%9D%B4%EB%93%9C-%ED%95%A8%EC%88%98)나 [tanh](https://commons.wikimedia.org/wiki/File:Hyperbolic_Tangent.svg) 등의 S자 모양 곡선의 초기값 설정  방법을 알아보자.    
**Xavier 초깃값($$1/√n$$)**: 권장하는 가중치 초깃값       
* **n**: 앞 계층의 노드가 n개라면 표준편차가 이것인 표준분포 사용(앞 층의 노드 수)       
* 일반적인 딥러닝 프레임워크들이 표준적으로 이용    
* Xavier 초깃값은 활성화 함수가 **선형**인 것을 전제로 이끈 결과. 
  * sigmoid 함수와 tanh 함수는 좌우 대칭이라 중앙 부근이 선형인 함수

![image](https://user-images.githubusercontent.com/76824611/128637261-380b33eb-65cf-42ee-8f59-261a7524c8ce.png)

Xavier 초깃값을 사용하면 앞 층에 노드가 많을수록 대상 노드의 초깃값으로 설정하는 가중치가 좁게 퍼짐      
➡ 많을 수록 좁게 분포
![image](https://user-images.githubusercontent.com/76824611/128637313-185bb9b7-b90c-492b-9d6b-1fbd5ef6c6d6.png)
➡ 이 결과를 보면 층이 깊어지면서 형태가 다소 일그러지지만, 앞에서 본 방식보다는 넓게 분포됨



-----


## ReLU  
[ReLU](https://yerimoh.github.io/DL2/#relu-%ED%95%A8%EC%88%98)함수를 보면 위와같은 S자 모양은 아니라는 것을 알 수 있다.      
**He 초깃값($$√(2/n)$$)** 사용 : ReLU에 특화된 초깃값을 이용 권장 이 특화된 초깃값       
* ReLU는 음의 영역이 0이라서 더 넓게 분포시키기 위해 2배의 계수가 필요하다          
(std = 0.01)        
더 정확하게 설명하면 ReLU는 활성화 못할 시 모두 0이다 즉 0에 지나치게 치우쳐져있다는 것이다.     
이렇게 되면 한쪽으로 쏠림 현상이 생기는 데 이를 방지하기 위해 0을 제외한 다른 값을 높이는 x2를 해주는 것이다.(0에는 2를 곱해도 0)        
* 신경망에 아주 작은 데이터가 흘러 기울기 작다는 특징이 있다.

그렇다면 다른 초기값을 설정했을때의 문제점을 보자      
**[Xavier 초기값]**       
* 기울기 소실(층이 깊어지면 활성화값들의 치우침)     
![image](https://user-images.githubusercontent.com/76824611/128637628-fd93a304-c9fa-4f24-a7ce-aaae7d8dfe28.png)

**[랜덤 초기값]** 
* 학습이 거의 X     
![image](https://user-images.githubusercontent.com/76824611/128637622-3ff04de2-e134-4c91-97df-2f59d3ac9885.png)


그런데 He초기값을 설정하면 아래와 같이 잘 분포된다는 것을 알 수 있다.      
**[He 초기값]**     
* 표준 분포를 더 크게 잡아 위의 문제를 완화시킴        
![image](https://user-images.githubusercontent.com/76824611/128637632-2b52aece-1b8d-4cfa-8bc0-c1d7d0e27086.png)




----

# 정리  
● 가중치 초깃값을 정하는 방법은 올바른 학습을 하는 데 매우 중요.     
● 가중치의 초깃값으로는 ‘Xavier 초깃값’과 ‘He 초깃값’이 효과적       
      
