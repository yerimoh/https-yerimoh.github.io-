---
title: "FastText: Enriching Word Vectors with Subword Information 정리"
date:   2022-11-10
excerpt: "Enriching Word Vectors with Subword Information"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---


   

# 원 논문
[Distributed Representations of Words and Phrases and their Compositionality](https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf)    

**[소스코드]**     
* [git adress]()


**[사전 학습]**
* 읽기 전 아래의 포스트들을 읽어야 무슨소린지 알아듣기 편하다..!   
* 

---

# Abstract
Continuous word representations, trained on
large unlabeled corpora are useful for many
natural language processing tasks. Popular
models that learn such representations ignore
the morphology of words, by assigning a distinct vector to each word. This is a limitation,
especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram
model, where each word is represented as a
bag of character n-grams. A vector representation is associated to each character n-gram;
words being represented as the sum of these
representations. Our method is fast, allowing to train models on large corpora quickly
and allows us to compute word representations
for words that did not appear in the training
data. We evaluate our word representations on
nine different languages, both on word similarity and analogy tasks. By comparing to
recently proposed morphological word representations, we show that our vectors achieve
state-of-the-art performance on these tasks.
