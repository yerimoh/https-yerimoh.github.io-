---
title: "FastText: Enriching Word Vectors with Subword Information 정리"
date:   2022-11-10
excerpt: "Enriching Word Vectors with Subword Information"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---


   

# 원 논문
[Distributed Representations of Words and Phrases and their Compositionality](https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf)    

**[소스코드]**     
* [git adress]()


**[사전 학습]**
* 읽기 전 아래의 포스트들을 읽어야 무슨소린지 알아듣기 편하다..!   
* 

---

# **Abstract**
**[기존 모델의 문제]**      
* 레이블이 지정되지 않은 큰 말뭉치에 대해 훈련된 Continuous word representations은 많은 자연어 처리 작업에 유용하다.    
이러한 표현을 학습하는 인기 있는 모델은 각 단어에 고유한 벡터를 할당하여 **단어의 형태를 무시**한다.    
이렇게 단어의 형상을 무시하는 문제는 특히 **어휘가 많고 희귀한 단어가 많은 언어**의 경우 더 문제가 된다.    

**[본 논문에서의 해결법]**      
* 본 논문에서는 **skipgram model**을 기반으로 한 새로운 접근법을 제안한다.     
여기서 각 단어는 문자 n-grams의 bag of character로 표현된다.     
벡터 표현은 각 문자 n-grams과 연관되어 있으며, 단어는 이러한 표현의 합으로 표현된다.     

**[skipgram model의 장점]**      
* 빠름      
* 큰 말뭉치에서 모델을 빠르게 훈련 가능      
* 훈련 데이터에 나타나지 않은 단어에 대한 단어 표현을 계산 가능      
* 단어 유사성과 유추 작업 모두에서 9개의 다른 언어로 우리의 단어 표현 평가한(ord similarity and analogy tasks)결과,          
최근 제안된 형태학적 단어 표현과 비교하여 벡터가 이러한 작업에서 최첨단 성능을 달성한다는 것을 보여준다.     

---
-----

# **Introduction**

## 분산 표현(Distributed representations)   
벡터 공간에서 단어의 **분산 표현(Distributed representations)** 은 학습 algorithms이 **유사한 단어를 그룹화**하여 자연어 처리 작업에서 더 나은 성능을 달성하는 데 도움이 된다.     
단어 표현의 가장 초기 사용 중 하나는 [Learning representations by back-propagating errors](https://www.nature.com/articles/323533a0) 이다.     
➡ 이 아이디어는 이후 통계 언어 모델링에 상당한 성공을 거두며 적용되었다. [A Neural Probabilistic Language Model](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)           
➡ 후속 작업은 아래의 논문들이 포함된다.    
* 자동 음성 인식 및 기계 번역에 대한 응용 프로그램     
    * [Statistical Language Models Based on Neural Networks](https://www.semanticscholar.org/paper/Statistical-Language-Models-Based-on-Neural-U%C4%8Den%C3%AD-Brn%C4%9B/96364af2d208ea75ca3aeb71892d2f7ce7326b55)    
    * [Continuous space language models](https://www.sciencedirect.com/science/article/pii/S0885230806000325)          
* 광범위한 NLP 작업     
    * [A unified architecture for natural language processing: deep neural networks with multitask learning](https://dl.acm.org/doi/10.1145/1390156.1390177)     
    *  [Domain adaptation for large-scale sentiment classification: a deep learning approach](https://dl.acm.org/doi/10.5555/3104482.3104547)      
    *  [Linguistic Regularities in Continuous Space Word Representations](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/rvecs.pdf)     
    *  [Recursive Deep Models for Discourse Parsing](https://aclanthology.org/D14-1220.pdf)      
    *  [From Frequency to Meaning: Vector Space Models of Semantics](https://arxiv.org/abs/1003.1141)      
    *  [Distributional Semantics Beyond Words: Supervised Learning of Analogy and Paraphrase](https://aclanthology.org/Q13-1029/)      
    *  [WSABIE: Scaling Up To Large Vocabulary Image Annotation](https://static.googleusercontent.com/media/research.google.com/ko//pubs/archive/37180.pdf)   

---

## Skip-gram


최근에, 대량의 비정형 텍스트 데이터에서 단어의 고품질 벡터 표현을 학습하는 효율적인 방법인 [Skip-gram 모델](https://arxiv.org/abs/1301.3781)을 소개했다. ([더 자세히 알아보기](https://yerimoh.github.io/DL14/))      
이전에 단어 벡터를 학습하기 위해 사용된 대부분의 신경망 아키텍처와 달리,   
Skip-gram 모델[Figure 1]의 훈련은 조밀한 행렬 곱셈(dense matrix multiplications)을 수반하지 않는다.      
이는 훈련을 매우 **효율적**으로 만든다.     
최적화된 단일 기계 구현은 하루에 1,000억 개 이상의 단어를 훈련할 수 있다.      


![image](https://user-images.githubusercontent.com/76824611/202935563-4528c5c7-fcd1-4493-bc60-a5c1a0948d06.png)
[Figure 1] The Skip-gram model architecture. 훈련 목표는 근처 단어를 예측하는 데 좋은 단어 벡터 표현을 배우는 것이다.       


신경망을 사용하여 계산된 단어 표현은 학습된 벡터가 많은 **언어적 규칙성과 패턴을 명시적으로 인코딩**하기 때문에 매우 흥미롭다. 다소 놀랍게도, 이러한 패턴들 중 많은 것들이 선형 변환으로 표현될 수 있다.   
ex) 벡터 계산 ```vec("마드리드") - vec("스페인") + vec("프랑스")```의 결과는 다른 단어 벡터보다 ```vec("파리")```에 더 가 가까움.


---

## 본 논문의 Skip-gram 개선

본 논문에서 우리는 원래의 Skip-gram model의 몇 가지 확장을 제시한다.        
* 본 논문의 장점은 훈련 중 **빈도가 높은 단어의 subsamplin이 상당한 속도 향상**(약 2배 - 10배)을 가져오고,     
**빈도가 낮은 단어 표현의 정확도를 향상**시킨다는 것을 보여준다.             
* 본 논문은 이전 연구에서 사용된 more complex hierarchical softmax에 비해 빈번한 단어에 대해 더 빠른 훈련과 더 나은 벡터 표현을 초래하는 스킵그램 모델을 훈련하기 위한 **단순화된 Noise Contrastive Estimation** (NCE)을 제시한다.

<details>
<summary>📜 Noise Contrastive Estimation (NCE) 란? </summary>
<div markdown="1">
   
CBOW와 Skip-Gram 모델에서 사용하는 비용 계산 알고리즘을 칭한다.      
전체 데이터셋에 대해 softMax 함수를 적용하는 것이 아니라 **샘플링으로 추출한 일부에 대해서만 적용**하는 방법을 말한다.     
k개의 대비되는(contrastive) 단어들을 noise distribution에서 구해서 (몬테카를로) 평균을 구하는 것이 기본 알고리즘이다.     
➡ [Hierarchical SoftMax](https://yerimoh.github.io/DL15/#1%EF%B8%8F%E2%83%A3-embedding-%EA%B3%84%EC%B8%B5-%EB%8F%84%EC%9E%85)와 [Negative Sampling](https://yerimoh.github.io/DL15/#2%EF%B8%8F%E2%83%A3-%EB%84%A4%EA%B1%B0%ED%8B%B0%EB%B8%8C-%EC%83%98%ED%94%8C%EB%A7%81%EC%9D%B4%EB%9E%80-%EC%86%90%EC%8B%A4-%ED%95%A8%EC%88%98-%EB%8F%84%EC%9E%85) 등의 여러 가지 방법이 있다.

일반적으로 단어 갯수가 많을 때 사용하고, NCE를 사용하면 문제를 (실제 분포에서 얻은 샘플)과 (인공적으로 만든 잡음 분포에서 얻은 샘플)을 구별하는 이진 분류 문제로 바꿀 수 있게 된다.

 Negative Sampling에서 사용하는 목적 함수는 결과값이 최대화될 수 있는 형태로 구성한다. 현재(목표, target, positive) 단어에는 높은 확률을 부여하고, 나머지 단어(negative, noise)에는 낮은 확률을 부여해서 가장 큰 값을 만들 수 있는 공식을 사용한다. 

   
</div>
</details>  


**[관용구 표현의 개선]**         
* 단어 표현은 개별 단어의 구성이 아닌 **관용구를 표현할 수 없기 때문에 제한**된다.      
ex) “Boston Globe”는 신문이기 때문에 “Boston”과 “Globe”의 의미가 자연스럽게 결합된 것이 아니다.     
* 따라서 벡터를 사용하여 전체 구문을 표현하면 Skip-gram model이 훨씬 더 표현력이 뛰어나다.       
* [recursive autoencoders](https://aclanthology.org/D14-1220.pdf)와 같이 단어 벡터를 구성하여 문장의 의미를 표현하는 것을 목표로 하는 다른 기술도 **단어 벡터 대신 구문 벡터를 사용**함으로써 더 나은 모델을 만들 수 있다.      


**[단어 기반 모델에서 구문 기반 모델로의 확장 방법]**      
* 단어 기반 모델에서 구문 기반 모델로의 확장 방법        
  **1)** 먼저 데이터 기반 접근 방식을 사용하여 많은 수의 구문을 식별    
  **2)** 훈련 중에 구문을 개별 토큰으로 처리    
* 구문 벡터의 품질을 평가하기 위해 단어와 구문을 모두 포함하는 일련의 아날로그 analogy 작업을 개발했다.    
  테스트 세트의 일반적인 analogy 쌍은 “Montreal”:“Montreal Canadiens”::“Toronto”:“Toronto Maple Leafs”  
  ```vec(“Montreal Canadiens”) - vec(“Montreal”) + vec(“Toronto”)```에 가장 가까운 표현이 ```vec(“Toronto Maple Leafs”)```이면 정답으로 간주된다.        




**[스킵그램 모델의 또 다른 흥미로운 특성]**       
* 마지막으로, 본 논문은 스킵그램 모델의 또 다른 흥미로운 특성을 설명한다.    
우리는 **간단한 벡터 추가가 종종 의미 있는 결과를 산출**할 수 있다는 것을 발견했다.    
* ex) vec(“Russia”) + vec(“river”)은 vec(“Volga River”)에 가깝다.           
     vec(“Germany”) + vec(“capital”)은 vec(“Berlin”)에 가깝다.       
* 이러한 구성성은 단어 벡터 표현에 대한 기본적인 **수학적 연산을 사용**하여 명확하지 않은 수준의 **언어 이해**를 얻을 수 있음을 시사한다.


----
----

# **2 The Skip-gram Model**
The training objective of the Skip-gram model is to find word representations that are useful for
predicting the surrounding words in a sentence or a document. More formally, given a sequence of
training words w1, w2, w3, . . . , wT , the objective of the Skip-gram model is to maximize the average
log probability
