---
title: "FastText: Enriching Word Vectors with Subword Information 정리"
date:   2022-11-10
excerpt: "Enriching Word Vectors with Subword Information"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---


   

# 원 논문
[Distributed Representations of Words and Phrases and their Compositionality](https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf)    

**[소스코드]**     
* [git adress]()


**[사전 학습]**
* 읽기 전 아래의 포스트들을 읽어야 무슨소린지 알아듣기 편하다..!   
* 

---

# Abstract
**[기존 모델의 문제]**      
* 레이블이 지정되지 않은 큰 말뭉치에 대해 훈련된 Continuous word representations은 많은 자연어 처리 작업에 유용하다.    
이러한 표현을 학습하는 인기 있는 모델은 각 단어에 고유한 벡터를 할당하여 **단어의 형태를 무시**한다.    
이렇게 단어의 형상을 무시하는 문제는 특히 **어휘가 많고 희귀한 단어가 많은 언어**의 경우 더 문제가 된다.    

**[본 논문에서의 해결법]**      
* 본 논문에서는 **skipgram model**을 기반으로 한 새로운 접근법을 제안한다.     
여기서 각 단어는 문자 n-grams의 bag of character로 표현된다.     
벡터 표현은 각 문자 n-grams과 연관되어 있으며, 단어는 이러한 표현의 합으로 표현된다.     

**[skipgram model의 장점]**      
* 빠름      
* 큰 말뭉치에서 모델을 빠르게 훈련 가능      
* 훈련 데이터에 나타나지 않은 단어에 대한 단어 표현을 계산 가능      
* 단어 유사성과 유추 작업 모두에서 9개의 다른 언어로 우리의 단어 표현 평가한(ord similarity and analogy tasks)결과,          
최근 제안된 형태학적 단어 표현과 비교하여 벡터가 이러한 작업에서 최첨단 성능을 달성한다는 것을 보여준다.     

---
-----

# Introduction

## 분산 표현(Distributed representations)   
벡터 공간에서 단어의 **분산 표현(Distributed representations)** 은 학습 algorithms이 **유사한 단어를 그룹화**하여 자연어 처리 작업에서 더 나은 성능을 달성하는 데 도움이 된다.     
단어 표현의 가장 초기 사용 중 하나는 [Learning representations by back-propagating errors](https://www.nature.com/articles/323533a0) 이다.     
➡ 이 아이디어는 이후 통계 언어 모델링에 상당한 성공을 거두며 적용되었다. [A Neural Probabilistic Language Model](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)           
➡ 후속 작업은 아래의 논문들이 포함된다.    
* 자동 음성 인식 및 기계 번역에 대한 응용 프로그램     
    * [Statistical Language Models Based on Neural Networks](https://www.semanticscholar.org/paper/Statistical-Language-Models-Based-on-Neural-U%C4%8Den%C3%AD-Brn%C4%9B/96364af2d208ea75ca3aeb71892d2f7ce7326b55)    
    * [Continuous space language models](https://www.sciencedirect.com/science/article/pii/S0885230806000325)          
* 광범위한 NLP 작업     
    * [A unified architecture for natural language processing: deep neural networks with multitask learning](https://dl.acm.org/doi/10.1145/1390156.1390177)     
    *  [Domain adaptation for large-scale sentiment classification: a deep learning approach](https://dl.acm.org/doi/10.5555/3104482.3104547)      
    *  [Linguistic Regularities in Continuous Space Word Representations](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/rvecs.pdf)     
    *  [Recursive Deep Models for Discourse Parsing](https://aclanthology.org/D14-1220.pdf)      
    *  [From Frequency to Meaning: Vector Space Models of Semantics](https://arxiv.org/abs/1003.1141)      
    *  [Distributional Semantics Beyond Words: Supervised Learning of Analogy and Paraphrase](https://aclanthology.org/Q13-1029/)      
    *  [WSABIE: Scaling Up To Large Vocabulary Image Annotation](https://static.googleusercontent.com/media/research.google.com/ko//pubs/archive/37180.pdf)   

---

## Skip-gram


최근에, 대량의 비정형 텍스트 데이터에서 단어의 고품질 벡터 표현을 학습하는 효율적인 방법인 [Skip-gram 모델](https://arxiv.org/abs/1301.3781)을 소개했다. ([더 자세히 알아보기](https://yerimoh.github.io/DL14/))      
이전에 단어 벡터를 학습하기 위해 사용된 대부분의 신경망 아키텍처와 달리,   
Skip-gram 모델[Figure 1]의 훈련은 조밀한 행렬 곱셈(dense matrix multiplications)을 수반하지 않는다.      
이는 훈련을 매우 **효율적**으로 만든다.     
최적화된 단일 기계 구현은 하루에 1,000억 개 이상의 단어를 훈련할 수 있다.      


![image](https://user-images.githubusercontent.com/76824611/202935563-4528c5c7-fcd1-4493-bc60-a5c1a0948d06.png)
[Figure 1] The Skip-gram model architecture. 훈련 목표는 근처 단어를 예측하는 데 좋은 단어 벡터 표현을 배우는 것이다.       


신경망을 사용하여 계산된 단어 표현은 학습된 벡터가 많은 **언어적 규칙성과 패턴을 명시적으로 인코딩**하기 때문에 매우 흥미롭다. 다소 놀랍게도, 이러한 패턴들 중 많은 것들이 선형 변환으로 표현될 수 있다.   
ex) 벡터 계산 vec("마드리드") - vec("스페인") + vec("프랑스")의 결과는 다른 단어 벡터보다 vec("파리")에 더 가 가까움.


---

## 본 논문의 Skip-gram 개선

In this paper we present several extensions of the original Skip-gram model. We show that subsampling of frequent words during training results in a significant speedup (around 2x - 10x), and
improves accuracy of the representations of less frequent words. In addition, we present a simplified variant of Noise Contrastive Estimation (NCE) [4] for training the Skip-gram model that results
in faster training and better vector representations for frequent words, compared to more complex
hierarchical softmax that was used in the prior work [8].

본 논문에서 우리는 원래의 Skip-gram model의 몇 가지 확장을 제시한다.        
* 본 논문의 장점은 훈련 중 **빈도가 높은 단어의 subsamplin이 상당한 속도 향상**(약 2배 - 10배)을 가져오고,     
**빈도가 낮은 단어 표현의 정확도를 향상**시킨다는 것을 보여준다.             
* 본 논문은 이전 연구에서 사용된 more complex hierarchical softmax에 비해 빈번한 단어에 대해 더 빠른 훈련과 더 나은 벡터 표현을 초래하는 스킵그램 모델을 훈련하기 위한 **단순화된 Noise Contrastive Estimation** (NCE)을 제시한다.

<details>
<summary>📜 Noise Contrastive Estimation (NCE) 란? </summary>
<div markdown="1">
   
CBOW와 Skip-Gram 모델에서 사용하는 비용 계산 알고리즘을 칭한다.      
전체 데이터셋에 대해 softMax 함수를 적용하는 것이 아니라 **샘플링으로 추출한 일부에 대해서만 적용**하는 방법을 말한다.     
k개의 대비되는(contrastive) 단어들을 noise distribution에서 구해서 (몬테카를로) 평균을 구하는 것이 기본 알고리즘이다.     
➡ Hierarchical SoftMax와 [Negative Sampling](https://yerimoh.github.io/DL15/#2%EF%B8%8F%E2%83%A3-%EB%84%A4%EA%B1%B0%ED%8B%B0%EB%B8%8C-%EC%83%98%ED%94%8C%EB%A7%81%EC%9D%B4%EB%9E%80-%EC%86%90%EC%8B%A4-%ED%95%A8%EC%88%98-%EB%8F%84%EC%9E%85) 등의 여러 가지 방법이 있다.

일반적으로 단어 갯수가 많을 때 사용하고, NCE를 사용하면 문제를 (실제 분포에서 얻은 샘플)과 (인공적으로 만든 잡음 분포에서 얻은 샘플)을 구별하는 이진 분류 문제로 바꿀 수 있게 된다.

 Negative Sampling에서 사용하는 목적 함수는 결과값이 최대화될 수 있는 형태로 구성한다. 현재(목표, target, positive) 단어에는 높은 확률을 부여하고, 나머지 단어(negative, noise)에는 낮은 확률을 부여해서 가장 큰 값을 만들 수 있는 공식을 사용한다. 
  
   
<details>
<summary>📜 Hierarchical SoftMax 란? </summary>
<div markdown="1">
   
CBOW와 Skip-Gram 모델은 내부적으로 SoftMax 알고리즘을 사용해서 계산을 진행하는데, 모든 단어에 대해 계산을 하고 normalization을 진행해야 하는데, 이것은 시간이 너무 오래 걸릴 수 밖에 없다.    
계산량을 줄일 수 있는 방법으로 Hierarchical SoftMax와 Negative Sampling 알고리즘이 있다.    
Hierarchical SoftMax 알고리즘은 계산량이 많은 SoftMax 함수를 빠르게 계산가능한 multinomial distribution 함수로 대체한다. 트리 자료구조에는 데이터를 저장하는 노드가 있고, 처음 노드를 루트(root), 마지막 노드를 리프(leaf) 또는 단말(terminal)이라고 부른다. multinomial distribution 함수는 루트에서 리프까지 가는 경로를 확률과 연동시켜서 계산 시간을 단축시킨다.     
Word2Vec 논문에서는 사용 빈도가 높은 단어에 대해 짧은 경로를 부여하는 Binary Huffman Tree를 사용한다.  Huffman Tree는 경로의 길이가 일정한 full tree의 성질을 갖고 있기 때문에 성능 향상에는 더욱 이상적이게 된다.
  
</div>
</details>     
   
   
</div>
</details>  

Word representations are limited by their inability to represent idiomatic phrases that are not compositions of the individual words. For example, “Boston Globe” is a newspaper, and so it is not a
natural combination of the meanings of “Boston” and “Globe”. Therefore, using vectors to represent the whole phrases makes the Skip-gram model considerably more expressive. Other techniques
that aim to represent meaning of sentences by composing the word vectors, such as the recursive
autoencoders [15], would also benefit from using phrase vectors instead of the word vectors.

단어 표현은 개별 단어의 구성이 아닌 관용구를 표현할 수 없기 때문에 제한된다. 예를 들어, "보스턴 글로브"는 신문이기 때문에 "보스턴"과 "글로브"의 의미가 자연스럽게 결합된 것은 아니다. 따라서 벡터를 사용하여 전체 구문을 표현하면 스킵그램 모델이 훨씬 더 표현력이 뛰어나다. 재귀적 자동 인코더[15]와 같이 단어 벡터를 구성하여 문장의 의미를 표현하는 것을 목표로 하는 다른 기술도 단어 벡터 대신 구문 벡터를 사용함으로써 이익을 얻을 수 있다.


https://pythonkim.tistory.com/92

The extension from word based to phrase based models is relatively simple. First we identify a large
number of phrases using a data-driven approach, and then we treat the phrases as individual tokens
during the training. To evaluate the quality of the phrase vectors, we developed a test set of analogical reasoning tasks that contains both words and phrases. A typical analogy pair from our test set is
“Montreal”:“Montreal Canadiens”::“Toronto”:“Toronto Maple Leafs”. It is considered to have been
answered correctly if the nearest representation to vec(“Montreal Canadiens”) - vec(“Montreal”) +
vec(“Toronto”) is vec(“Toronto Maple Leafs”).

단어 기반 모델에서 구문 기반 모델로의 확장은 비교적 간단하다. 먼저 데이터 기반 접근 방식을 사용하여 많은 수의 구문을 식별한 다음, 훈련 중에 구문을 개별 토큰으로 처리한다. 구문 벡터의 품질을 평가하기 위해 단어와 구문을 모두 포함하는 일련의 아날로그 추론 작업을 개발했다. 테스트 세트의 일반적인 유추 쌍은 "몬트리올"입니다."몬트리올 캐나다인":"토론토":"토론토 단풍잎" vec ("몬트리올 캐나다인") - vec ("몬트리올") + vec ("토론토")에 가장 가까운 표현이 vec ("토론토 메이플 리프스")이면 정답으로 간주된다.

Finally, we describe another interesting property of the Skip-gram model. We found that simple
vector addition can often produce meaningful results. For example, vec(“Russia”) + vec(“river”) is
close to vec(“Volga River”), and vec(“Germany”) + vec(“capital”) is close to vec(“Berlin”). This
compositionality suggests that a non-obvious degree of language understanding can be obtained by
using basic mathematical operations on the word vector representations.

마지막으로, 우리는 스킵그램 모델의 또 다른 흥미로운 특성을 설명한다. 우리는 간단한 벡터 추가가 종종 의미 있는 결과를 산출할 수 있다는 것을 발견했다. 예를 들어, vec("러시아") + vec("강")은 vec("볼가 강")에 가깝고, vec("독일") + vec("수도")은 vec("베를린")에 가깝다. 이러한 구성성은 단어 벡터 표현에 대한 기본적인 수학적 연산을 사용하여 명확하지 않은 수준의 언어 이해를 얻을 수 있음을 시사한다.
