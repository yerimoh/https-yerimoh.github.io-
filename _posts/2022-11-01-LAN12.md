---
title: "FastText: Enriching Word Vectors with Subword Information 정리"
date:   2022-11-10
excerpt: "Enriching Word Vectors with Subword Information"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---


   

# 원 논문
[Distributed Representations of Words and Phrases and their Compositionality](https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf)    

**[소스코드]**     
* [git adress]()


**[사전 학습]**
* 읽기 전 아래의 포스트들을 읽어야 무슨소린지 알아듣기 편하다..!   
* 

---

# Abstract
**[기존 모델의 문제]**      
레이블이 지정되지 않은 큰 말뭉치에 대해 훈련된 Continuous word representations은 많은 자연어 처리 작업에 유용하다.    
이러한 표현을 학습하는 인기 있는 모델은 각 단어에 고유한 벡터를 할당하여 **단어의 형태를 무시**한다.    
이렇게 단어의 형상을 무시하는 문제는 특히 **어휘가 많고 희귀한 단어가 많은 언어**의 경우 더 문제가 된다.    

**[본 논문에서의 해결법]**      
본 논문에서는 **skipgram model**을 기반으로 한 새로운 접근법을 제안한다.     
여기서 각 단어는 문자 n-grams의 bag of character로 표현된다.     
벡터 표현은 각 문자 n-grams과 연관되어 있으며, 단어는 이러한 표현의 합으로 표현된다.     

**[skipgram model의 장점]**      
* 빠름      
* 큰 말뭉치에서 모델을 빠르게 훈련 가능      
* 훈련 데이터에 나타나지 않은 단어에 대한 단어 표현을 계산 가능      
* 단어 유사성과 유추 작업 모두에서 9개의 다른 언어로 우리의 단어 표현 평가한(ord similarity and analogy tasks)결과,          
최근 제안된 형태학적 단어 표현과 비교하여 벡터가 이러한 작업에서 최첨단 성능을 달성한다는 것을 보여준다.     

---

# Introduction
Distributed representations of words in a vector space help learning algorithms to achieve better
performance in natural language processing tasks by grouping similar words. One of the earliest use
of word representations dates back to 1986 due to Rumelhart, Hinton, and Williams [13]. This idea
has since been applied to statistical language modeling with considerable success [1]. The follow
up work includes applications to automatic speech recognition and machine translation [14, 7], and
a wide range of NLP tasks [2, 20, 15, 3, 18, 19, 9].


Recently, Mikolov et al. [8] introduced the Skip-gram model, an efficient method for learning highquality vector representations of words from large amounts of unstructured text data. Unlike most
of the previously used neural network architectures for learning word vectors, training of the Skipgram model (see Figure 1) does not involve dense matrix multiplications. This makes the training
extremely efficient: an optimized single-machine implementation can train on more than 100 billion
words in one day.


The word representations computed using neural networks are very interesting because the learned
vectors explicitly encode many linguistic regularities and patterns. Somewhat surprisingly, many of
these patterns can be represented as linear translations. For example, the result of a vector calculation vec(“Madrid”) - vec(“Spain”) + vec(“France”) is closer to vec(“Paris”) than to any other word
vector [9, 8].

In this paper we present several extensions of the original Skip-gram model. We show that subsampling of frequent words during training results in a significant speedup (around 2x - 10x), and
improves accuracy of the representations of less frequent words. In addition, we present a simplified variant of Noise Contrastive Estimation (NCE) [4] for training the Skip-gram model that results
in faster training and better vector representations for frequent words, compared to more complex
hierarchical softmax that was used in the prior work [8].


Word representations are limited by their inability to represent idiomatic phrases that are not compositions of the individual words. For example, “Boston Globe” is a newspaper, and so it is not a
natural combination of the meanings of “Boston” and “Globe”. Therefore, using vectors to represent the whole phrases makes the Skip-gram model considerably more expressive. Other techniques
that aim to represent meaning of sentences by composing the word vectors, such as the recursive
autoencoders [15], would also benefit from using phrase vectors instead of the word vectors.


The extension from word based to phrase based models is relatively simple. First we identify a large
number of phrases using a data-driven approach, and then we treat the phrases as individual tokens
during the training. To evaluate the quality of the phrase vectors, we developed a test set of analogical reasoning tasks that contains both words and phrases. A typical analogy pair from our test set is
“Montreal”:“Montreal Canadiens”::“Toronto”:“Toronto Maple Leafs”. It is considered to have been
answered correctly if the nearest representation to vec(“Montreal Canadiens”) - vec(“Montreal”) +
vec(“Toronto”) is vec(“Toronto Maple Leafs”).


Finally, we describe another interesting property of the Skip-gram model. We found that simple
vector addition can often produce meaningful results. For example, vec(“Russia”) + vec(“river”) is
close to vec(“Volga River”), and vec(“Germany”) + vec(“capital”) is close to vec(“Berlin”). This
compositionality suggests that a non-obvious degree of language understanding can be obtained by
using basic mathematical operations on the word vector representations.

