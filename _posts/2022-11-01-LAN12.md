---
title: "FastText: Enriching Word Vectors with Subword Information 정리"
date:   2022-11-10
excerpt: "Enriching Word Vectors with Subword Information"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---


   

# 원 논문
[Distributed Representations of Words and Phrases and their Compositionality](https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf)    

**[소스코드]**     
* [git adress]()


**[사전 학습]**
* 읽기 전 아래의 포스트들을 읽어야 무슨소린지 알아듣기 편하다..!   
* 

---

# Abstract
**[기존 모델의 문제]**      
레이블이 지정되지 않은 큰 말뭉치에 대해 훈련된 Continuous word representations은 많은 자연어 처리 작업에 유용하다.    
이러한 표현을 학습하는 인기 있는 모델은 각 단어에 고유한 벡터를 할당하여 **단어의 형태를 무시**한다.    
이렇게 단어의 형상을 무시하는 문제는 특히 **어휘가 많고 희귀한 단어가 많은 언어**의 경우 더 문제가 된다.    

**[본 논문에서의 해결법]**      
본 논문에서는 **skipgram model**을 기반으로 한 새로운 접근법을 제안한다.     
여기서 각 단어는 문자 n-grams의 bag of character로 표현된다.     
벡터 표현은 각 문자 n-grams과 연관되어 있으며, 단어는 이러한 표현의 합으로 표현된다.     

**[skipgram model의 장점]**      
* 빠름      
* 큰 말뭉치에서 모델을 빠르게 훈련 가능      
* 훈련 데이터에 나타나지 않은 단어에 대한 단어 표현을 계산 가능      
* 단어 유사성과 유추 작업 모두에서 9개의 다른 언어로 우리의 단어 표현 평가한(ord similarity and analogy tasks)결과,          
최근 제안된 형태학적 단어 표현과 비교하여 벡터가 이러한 작업에서 최첨단 성능을 달성한다는 것을 보여준다.     

---

# Introduction

벡터 공간에서 단어의 **분산 표현(Distributed representations)** 은 학습 algorithms이 **유사한 단어를 그룹화**하여 자연어 처리 작업에서 더 나은 성능을 달성하는 데 도움이 된다.     
단어 표현의 가장 초기 사용 중 하나는 [Learning representations by back-propagating errors](https://www.nature.com/articles/323533a0) 이다.     
* 이 아이디어는 이후 통계 언어 모델링에 상당한 성공을 거두며 적용되었다. [A Neural Probabilistic Language Model](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)           
* 후속 작업은 아래의 논문들이 포함된다.    
    * 자동 음성 인식 및 기계 번역에 대한 응용 프로그램     
       * [Statistical Language Models Based on Neural Networks](https://www.semanticscholar.org/paper/Statistical-Language-Models-Based-on-Neural-U%C4%8Den%C3%AD-Brn%C4%9B/96364af2d208ea75ca3aeb71892d2f7ce7326b55)    
       * [Continuous space language models](https://www.sciencedirect.com/science/article/pii/S0885230806000325)          
    * 광범위한 NLP 작업     
       * [A unified architecture for natural language processing: deep neural networks with multitask learning](https://dl.acm.org/doi/10.1145/1390156.1390177)     
       *  [Domain adaptation for large-scale sentiment classification: a deep learning approach](https://dl.acm.org/doi/10.5555/3104482.3104547)      
       *  [Linguistic Regularities in Continuous Space Word Representations](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/rvecs.pdf)     
       *  [Recursive Deep Models for Discourse Parsing](https://aclanthology.org/D14-1220.pdf)      
       *  [From Frequency to Meaning: Vector Space Models of Semantics](https://arxiv.org/abs/1003.1141)      
       *  [Distributional Semantics Beyond Words: Supervised Learning of Analogy and Paraphrase](https://aclanthology.org/Q13-1029/)      
       *  [WSABIE: Scaling Up To Large Vocabulary Image Annotation](https://static.googleusercontent.com/media/research.google.com/ko//pubs/archive/37180.pdf)   



Recently, Mikolov et al. [8] introduced the Skip-gram model, an efficient method for learning highquality vector representations of words from large amounts of unstructured text data. Unlike most
of the previously used neural network architectures for learning word vectors, training of the Skipgram model (see Figure 1) does not involve dense matrix multiplications. This makes the training
extremely efficient: an optimized single-machine implementation can train on more than 100 billion
words in one day.


The word representations computed using neural networks are very interesting because the learned
vectors explicitly encode many linguistic regularities and patterns. Somewhat surprisingly, many of
these patterns can be represented as linear translations. For example, the result of a vector calculation vec(“Madrid”) - vec(“Spain”) + vec(“France”) is closer to vec(“Paris”) than to any other word
vector [9, 8].



최근에, Mikolov 외. [8] 대량의 비정형 텍스트 데이터에서 단어의 고품질 벡터 표현을 학습하는 효율적인 방법인 Skip-gram 모델을 소개했다. 이전에 단어 벡터를 학습하기 위해 사용된 대부분의 신경망 아키텍처와 달리, 스킵그램 모델(그림 1 참조)의 훈련은 조밀한 행렬 곱셈을 수반하지 않는다. 이는 훈련을 매우 효율적으로 만든다. 최적화된 단일 기계 구현은 하루에 1,000억 개 이상의 단어를 훈련할 수 있다.

신경망을 사용하여 계산된 단어 표현은 학습된 벡터가 많은 언어적 규칙성과 패턴을 명시적으로 인코딩하기 때문에 매우 흥미롭다. 다소 놀랍게도, 이러한 패턴들 중 많은 것들이 선형 변환으로 표현될 수 있다. 예를 들어, 벡터 계산 vec("마드리드") - vec("스페인") + vec("프랑스")의 결과는 다른 단어 벡터보다 vec("파리")에 더 가깝다[9, 8].ㅠ




In this paper we present several extensions of the original Skip-gram model. We show that subsampling of frequent words during training results in a significant speedup (around 2x - 10x), and
improves accuracy of the representations of less frequent words. In addition, we present a simplified variant of Noise Contrastive Estimation (NCE) [4] for training the Skip-gram model that results
in faster training and better vector representations for frequent words, compared to more complex
hierarchical softmax that was used in the prior work [8].


Word representations are limited by their inability to represent idiomatic phrases that are not compositions of the individual words. For example, “Boston Globe” is a newspaper, and so it is not a
natural combination of the meanings of “Boston” and “Globe”. Therefore, using vectors to represent the whole phrases makes the Skip-gram model considerably more expressive. Other techniques
that aim to represent meaning of sentences by composing the word vectors, such as the recursive
autoencoders [15], would also benefit from using phrase vectors instead of the word vectors.


The extension from word based to phrase based models is relatively simple. First we identify a large
number of phrases using a data-driven approach, and then we treat the phrases as individual tokens
during the training. To evaluate the quality of the phrase vectors, we developed a test set of analogical reasoning tasks that contains both words and phrases. A typical analogy pair from our test set is
“Montreal”:“Montreal Canadiens”::“Toronto”:“Toronto Maple Leafs”. It is considered to have been
answered correctly if the nearest representation to vec(“Montreal Canadiens”) - vec(“Montreal”) +
vec(“Toronto”) is vec(“Toronto Maple Leafs”).


Finally, we describe another interesting property of the Skip-gram model. We found that simple
vector addition can often produce meaningful results. For example, vec(“Russia”) + vec(“river”) is
close to vec(“Volga River”), and vec(“Germany”) + vec(“capital”) is close to vec(“Berlin”). This
compositionality suggests that a non-obvious degree of language understanding can be obtained by
using basic mathematical operations on the word vector representations.

