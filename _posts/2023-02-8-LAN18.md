---
title: "Self-Attention with Relative Position Representations 정리"
date:   2023-02-8
excerpt: "Self-Attention with Relative Position Representations paper review"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---



# 목차 



**[INTRO]**      
NIPS에서 Google이 소개했던 Transformer는 NLP 학계에서 정말 큰 주목을 받음  
CNN 과 RNN 이 주를 이뤘던 연구들에서 벗어나 아예 새로운 모델을 제안했기 때문    

**[핵심]**     

**[읽기 위해 필요한 지식]**    
* [attention](https://yerimoh.github.io/DL19/)       
* [transformer](https://yerimoh.github.io/Lan/)    


**[원 논문]**    
[Self-Attention with Relative Position Representations](https://arxiv.org/pdf/1803.02155.pdf)  


----




<span style="background-color:#F5F5F5">**[position embeddings의 필요성]**</span>         
self-attention는 각 단어를 병렬으로 처리하기 때문에(순서 고려못함), 순서를 부여하는 것이 일반적이다.     
즉 I love you란 문장이 있으면, 아래와 같이 **한 encoder 안에서 따로 처리**되는 것을 확인할 수 있다.    
이렇게 단어 각각이 각각 임베딩되고 **병렬적으로 학습**되므로 **이 단어들의 순서가 무시**된다는 것이다.       
즉 아래 예시에서도 어짜피 순서없이 동시에(병렬로)학습되니 왼쪽과 오른쪽의 차이가 없다.     
⚠️ 즉, **병렬처리로 인해 문장 내의 순서정보가 무시**되어 **이 순서 정보를 넣어주기위해 position embeddings을 추가**하는 것이다.   
![image](https://user-images.githubusercontent.com/76824611/223513725-b980210f-1360-4dc9-b623-68228836ef86.png)


<span style="background-color:#F5F5F5">**[position embeddings의 필요성]**</span>         

원래 Transformer는 sinusoidal [position signal](https://yerimoh.github.io/Lan/#positional-encoding)를 사용하거나 position embeddings을 학습했지만, 최근에는 relative position embeddings을 사용하는 것이 더 일반화되었다.     
* 기존 position embeddings: 각 위치에 고정된 임베딩을 사용     
* 최근 relative position embeddings: self-attention에서 비교되는 “key” 와 “query” 사이의 오프셋에 따라 다른 학습 임베딩을 생성한다.      

Relative position embedding은 self-attention의 key와 query 사이의 거리에 따라 학습된 embedding을 생성한다.

T5에서는 position embedding으로 단순하게 scalar을 사용한다.

그리고 이 스칼라는 attention weight를 계산할 때 logit에 더해주는 값으로 사용한다. 

 

뿐만 아니라 효율성을 위해 position embedding 값은 모든 레이어에서 공유하되,

각 레이어의 attention head에서는 각기 다른 학습된 position embedding을 사용한다.

즉, 가능한 key-query 거리의 범위에 따라 고정된 개수의 embedding이 학습되게 된다.




<span style="background-color:#F5F5F5">**[position embeddings의 필요성]**</span>         
그러나 절대 위치가 아닌 상대 위치에 관한 다른 종류의 데이터를 다루는 경우에는 어떻게 될까.   
아래 그림의 예시를 보자.      
아래 그림과 같은 경우에는 특정노드가(예를 들어 you가) 첫번째라고 말하기엔 임의성이 다분하다.    
그래서 아래와 같은 경우 각 노드(네모)에 순서를 부여하는 과정에선 많은 문제가 생길 수 있다.        
 ![image](https://user-images.githubusercontent.com/76824611/223515775-c59db6cd-1305-4002-95ad-1869c8e11d9b.png)    
 
그래서 **절대 순서를 없애고** 그래프 또는 **시퀀스의 요소 간 거리 관한 상대적 위치**로 인코딩을 하는 것이다.   


상대 임베딩의 경우는 각 토큰에 고유한 위치 임베딩이 있는 기존 position embedding과는 다르다.   
상대적인 표현을 사용하면 각 단어 또는 토큰에 하나의 위치 임베딩만 있는 것이 아니라 **토큰 간의 관계를 설명하기 위해 시퀀스에 있는 토큰 수 만큼 많은 위치 임베딩을 만든다**.    
만드는 방법은 아래 과정과 같다.   
<iframe src="https://www.slideshare.net/slideshow/embed_code/key/oRhvMuYOW21nWC?hostedIn=slideshare&page=upload" width="476" height="400" frameborder="0" marginwidth="0" marginheight="0" scrolling="no"></iframe>

즉 위와 같이 표현함에 따라 각 토큰이 다른 토큰에 대한 위치 관계를 인코딩하게 된다.    
즉 각 토큰($$w_n$$)은 자기 토큰과 다른 토큰과의 위치정보를(거리 정보를)담아 두 토큰관의 관계를 설명한다.    
즉 $$x_4$$의 예시를 들면, 아래와 같다.    
각 position token($$w_n$$)의 n의 기준은 자신과 자신의 위치정보 $$w_0$$을 기준으로 오른쪽으로 이동하면 $$w_1,w_2$$, 왼쪽으로 이동하면 $$w_{-1}, w_{-2}$$가 된다.    

이러한 벡터는 한 토큰과 다른 토큰 사이의 관계에서 비롯되므로 이를 위해 아래와 같은 표기법을 사용할 수 있다.      

논문에선 **k에서 cliping을 실험**한다.     
즉 특정 거리 후에 위치 임베딩이 동일한 값을 얻는다.    
즉 k=2인 경우  $$w_3,w_4$$는 모두  $$w_2$의 값을 갖는다.   
그러니까 아래와 같이 대각선을 기준으로 대칭인 값끼리 같은 값을 갖는다는 것이다.    
![image](https://user-images.githubusercontent.com/76824611/223534198-bccc5acc-a730-4469-8cf2-e4a6777b43e1.png)

자 그런데 $$x_1$$을 표현하기위해 이 토큰($$w_1,w_2,w_3,w_4,w_5)들을 다 더해버리면 위치정보가 이상해진다.     

그러므로 이 위치정보를 유지하면서도 토큰을 표현하기위해 self attention을 사용한다.     

이렇게 self attention을 이용하는 식은 아래와 같다.   
![image](https://user-images.githubusercontent.com/76824611/223534400-1685ee37-d95d-48e5-a5d9-cdc6db658dda.png)


* 1) $$x_jW^v$$L 각 토큰 벡터는 먼저 선형변환으로 변환된다.     
* 2) 그런 다음 각 토큰에 대한 새로운 표현 $$z_i$$는 모든 토큰에 대한 가중치의 합계이며, 가중치는 일종의 중요도 점수이다.     
➡ 따라서 더 중요한 토큰엔 거 많은 가중치가 부여된다.     
* 그래서 여기서 새 표현이 계산되는 위치에 위치 정보를 $$a_{ij}^v$$를 더해줌으로써 추가할 수 있다.(이 과정에서 차원이 더 높아진다.)     


즉 이러한 표현을 통해 "각 토큰은 의미론적 정보를 가지고있지만 어떤 차원에서의 내 위치는 다른 모든 두번째 이웃과 비슷하기 때문에 오른쪽에 있는 두 번째 이웃이기도 한다"를 나타낼 수 있다.      

이제 새로운 표현 z는 상대위치에 대한 정보를 받지만, self attention의 가중치 계수는 그렇지 않다.   
따라서 가중치 계수는 중요도 점수가 위치 정보에 기반한 결정을 내리도록 self position에 대한 푸쉬($$a_{ik}^K$$)도 받는다.   
![image](https://user-images.githubusercontent.com/76824611/223534443-95c8213e-8bbd-45d4-b10e-3f8973cb7e30.png)



그 결과 이제 모든 토큰에는 두가지 변형의 시퀀스에 있는 토큰만큼 많은 Relative Position 임베딩을 갖는다,   
* 첫 번쨰는 위에서 설명한 것과 같이 값에 대한 상대적인 임베딩이다.  
![image](https://user-images.githubusercontent.com/76824611/223534400-1685ee37-d95d-48e5-a5d9-cdc6db658dda.png)
* 두번째도 위에서 설명한 attention 가중치를 알리는 key에 대한 임베딩이다.   
![image](https://user-images.githubusercontent.com/76824611/223534443-95c8213e-8bbd-45d4-b10e-3f8973cb7e30.png)

위으 임베딩을 바탕으로 벡터의 정확한 값이 학습되며, 이는 모델이 스스로 최상의 balance가 무엇인지 파악할 수 있게한다.     

이 논문은 텍스트로만 실험했으며 이는 분명히 시퀀스이며, 기계번역에서 약간의 성능을 얻는따.

그러나 단어 체인은 단순한 그래프일 뿐이며 이를 넘어  일반적으로 요소간의 쌍 관계가 있는 모든 그래프 표현에 이 방법을 적용할 수 있다.   

이러한 상대적 표현은  우리가 시퀀스에 있는지 아니면 더 목잡한 그래프에 있는지에 관계없이 토큰이 서로 얼마나 떨어져있는 지에만 의존하기 때문이다.     

또다른 장점은 k에 대해 클리핑할 때 이 학습된 위치 정보가 모든 시퀀스의 길이로 일반화된다는 것이다.    

k에대해서 클리핑하면 장기종속성 정보가 제거되기때문에 조금 이상하다고 느낄 수 있다.    
그래서 우리는 무언가가 가까이 있거나 멀리 있다는 것 만 저장할 수 있지만 그것이 얼마나 떨어져있는지는 알 수 없다.     

많은 반복을 통해 더 멀리있는 정보가 관심 지점으로 확산될 수있지만 다중 혹은 멀리있는 noise를 포함 할 수 있으므로 항상 좋다고볼 수 ㄴ없다.    


따라서 상대적 위치표현에서 k에서의 클리핑은 대부분 로컬 종속성에 의존하는 작업을 해치치 않는 상대적 위치 이벤트 호라이즌과 약간 비슷하게 작용한다.     





