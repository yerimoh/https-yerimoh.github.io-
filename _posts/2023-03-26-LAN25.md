---
title: "(t0) MULTITASK PROMPTED TRAINING ENABLES ZERO-SHOT TASK GENERALIZATION"
date:   2023-03-26
excerpt: "Named Entity Recognition and Normalization Applied to LargeScale Information Extraction from the Materials Science Literature paper review"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---

# ABSTRACT
Large language models have recently been shown to attain reasonable zero-shot
generalization on a diverse set of tasks (Brown et al., 2020). It has been hypothesized that this is a consequence of implicit multitask learning in language models’
pretraining (Radford et al., 2019). Can zero-shot generalization instead be directly
induced by explicit multitask learning? To test this question at scale, we develop
a system for easily mapping any natural language tasks into a human-readable
prompted form. We convert a large set of supervised datasets, each with multiple
prompts with diverse wording. These prompted datasets allow for benchmarking
the ability of a model to perform completely held-out tasks. We fine-tune a pretrained encoder-decoder model (Raffel et al., 2020; Lester et al., 2021) on this multitask mixture covering a wide variety of tasks. The model attains strong zero-shot
performance on several standard datasets, often outperforming models up to 16×
its size. Further, our approach attains strong performance on a subset of tasks from
the BIG-bench benchmark, outperforming models up to 6× its size. All trained
models are available at https://github.com/bigscience-workshop/t-zero, and all
prompts are available at https://github.com/bigscience-workshop/promptsource.




# 1 INTRODUCTION
Recent work has shown that large language models exhibit the ability to perform reasonable zeroshot generalization to new tasks (Brown et al., 2020; Kim et al., 2021). Despite being trained on only
language modeling objectives, these models can perform relatively well at new tasks that they have
not been explicitly trained to perform, for instance answering a question on a passage or performing
summarization. An influential hypothesis is that large language models generalize to new tasks as a
result of an implicit process of multitask learning (Radford et al., 2019). As a byproduct of learning
to predict the next word, a language model is forced to learn from a mixture of implicit tasks included
in their pretraining corpus. For example, by training on generic text from a web forum, a model might
implicitly learn the format and structure of question answering. This gives large language models
the ability to generalize to held-out tasks presented with natural language prompts, going beyond
prior multitask studies on generalization to held-out datasets (Khashabi et al., 2020a; Ye et al.,
2021). However, this ability requires a sufficiently large model and is sensitive to the wording of its
prompts (Perez et al., 2021; Zhao et al., 2021; Reynolds and McDonell, 2021).

